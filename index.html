<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2024-12-18T00:00:00Z">2024-12-18</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">0</span>
                    </Summary>
                    <div class="details-content">
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Energy-Efficient <span class="highlight-title">SLAM</span> via Joint Design of Sensing, Communication, and
  Exploration Speed 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13912v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13912v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zidong Han, Ruibo Jin, Xiaoyang Li, Bingpeng Zhou, Qinyu Zhang, Yi Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To support future spatial machine intelligence applications, lifelong
simultaneous localization and mapping (SLAM) has drawn significant attentions.
SLAM is usually realized based on various types of mobile robots performing
simultaneous and continuous sensing and communication. This paper focuses on
analyzing the energy efficiency of robot operation for lifelong SLAM by jointly
considering sensing, communication and mechanical factors. The system model is
built based on a robot equipped with a 2D light detection and ranging (LiDAR)
and an odometry. The cloud point raw data as well as the odometry data are
wirelessly transmitted to data center where real-time map reconstruction is
realized based on an unsupervised deep learning based method. The sensing
duration, transmit power, transmit duration and exploration speed are jointly
optimized to minimize the energy consumption. Simulations and experiments
demonstrate the performance of our proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-12-17T00:00:00Z">2024-12-17</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NFL-BA: Improving Endoscopic <span class="highlight-title">SLAM</span> with Near-Field Light Bundle
  Adjustment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13176v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13176v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Dunn Beltran, Daniel Rho, Marc Niethammer, Roni Sengupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simultaneous Localization And Mapping (SLAM) from a monocular endoscopy video
can enable autonomous navigation, guidance to unsurveyed regions, and 3D
visualizations, which can significantly improve endoscopy experience for
surgeons and patient outcomes. Existing dense SLAM algorithms often assume
distant and static lighting and textured surfaces, and alternate between
optimizing scene geometry and camera parameters by minimizing a photometric
rendering loss, often called Photometric Bundle Adjustment. However, endoscopic
environments exhibit dynamic near-field lighting due to the co-located light
and camera moving extremely close to the surface, textureless surfaces, and
strong specular reflections due to mucus layers. When not considered, these
near-field lighting effects can cause significant performance reductions for
existing SLAM algorithms from indoor/outdoor scenes when applied to endoscopy
videos. To mitigate this problem, we introduce a new Near-Field Lighting Bundle
Adjustment Loss $(L_{NFL-BA})$ that can also be alternatingly optimized, along
with the Photometric Bundle Adjustment loss, such that the captured images'
intensity variations match the relative distance and orientation between the
surface and the co-located light and camera. We derive a general NFL-BA loss
function for 3D Gaussian surface representations and demonstrate that adding
$L_{NFL-BA}$ can significantly improve the tracking and mapping performance of
two state-of-the-art 3DGS-SLAM systems, MonoGS (35% improvement in tracking,
48% improvement in mapping with predicted depth maps) and EndoGSLAM (22%
improvement in tracking, marginal improvement in mapping with predicted
depths), on the C3VD endoscopy dataset for colons. The project page is
available at https://asdunnbe.github.io/NFL-BA/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InCrowd-VI: A Realistic Visual-Inertial Dataset for Evaluating <span class="highlight-title">SLAM</span> in
  Indoor Pedestrian-Rich Spaces for Human Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14358v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14358v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marziyeh Bamdad, Hans-Peter Hutter, Alireza Darvishy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simultaneous localization and mapping (SLAM) techniques can be used to
navigate the visually impaired, but the development of robust SLAM solutions
for crowded spaces is limited by the lack of realistic datasets. To address
this, we introduce InCrowd-VI, a novel visual-inertial dataset specifically
designed for human navigation in indoor pedestrian-rich environments. Recorded
using Meta Aria Project glasses, it captures realistic scenarios without
environmental control. InCrowd-VI features 58 sequences totaling a 5 km
trajectory length and 1.5 hours of recording time, including RGB, stereo
images, and IMU measurements. The dataset captures important challenges such as
pedestrian occlusions, varying crowd densities, complex layouts, and lighting
changes. Ground-truth trajectories, accurate to approximately 2 cm, are
provided in the dataset, originating from the Meta Aria project machine
perception SLAM service. In addition, a semi-dense 3D point cloud of scenes is
provided for each sequence. The evaluation of state-of-the-art visual odometry
(VO) and SLAM algorithms on InCrowd-VI revealed severe performance limitations
in these realistic scenarios. Under challenging conditions, systems exceeded
the required localization accuracy of 0.5 meters and the 1\% drift threshold,
with classical methods showing drift up to 5-10\%. While deep learning-based
approaches maintained high pose estimation coverage (>90\%), they failed to
achieve real-time processing speeds necessary for walking pace navigation.
These results demonstrate the need and value of a new dataset to advance SLAM
research for visually impaired navigation in complex indoor environments. The
dataset and associated tools are publicly available at
https://incrowd-vi.cloudlab.zhaw.ch/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 8 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InCrowd-VI: A Realistic Visual-Inertial Dataset for Evaluating <span class="highlight-title">SLAM</span> in
  Indoor Pedestrian-Rich Spaces for Human Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14358v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14358v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marziyeh Bamdad, Hans-Peter Hutter, Alireza Darvishy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simultaneous localization and mapping (SLAM) techniques can be used to
navigate the visually impaired, but the development of robust SLAM solutions
for crowded spaces is limited by the lack of realistic datasets. To address
this, we introduce InCrowd-VI, a novel visual-inertial dataset specifically
designed for human navigation in indoor pedestrian-rich environments. Recorded
using Meta Aria Project glasses, it captures realistic scenarios without
environmental control. InCrowd-VI features 58 sequences totaling a 5 km
trajectory length and 1.5 hours of recording time, including RGB, stereo
images, and IMU measurements. The dataset captures important challenges such as
pedestrian occlusions, varying crowd densities, complex layouts, and lighting
changes. Ground-truth trajectories, accurate to approximately 2 cm, are
provided in the dataset, originating from the Meta Aria project machine
perception SLAM service. In addition, a semi-dense 3D point cloud of scenes is
provided for each sequence. The evaluation of state-of-the-art visual odometry
(VO) and SLAM algorithms on InCrowd-VI revealed severe performance limitations
in these realistic scenarios. Under challenging conditions, systems exceeded
the required localization accuracy of 0.5 meters and the 1\% drift threshold,
with classical methods showing drift up to 5-10\%. While deep learning-based
approaches maintained high pose estimation coverage (>90\%), they failed to
achieve real-time processing speeds necessary for walking pace navigation.
These results demonstrate the need and value of a new dataset to advance SLAM
research for visually impaired navigation in complex indoor environments. The
dataset and associated tools are publicly available at
https://incrowd-vi.cloudlab.zhaw.ch/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 8 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-12-16T00:00:00Z">2024-12-16</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MASt3R-<span class="highlight-title">SLAM</span>: Real-Time Dense <span class="highlight-title">SLAM</span> with 3D Reconstruction Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12392v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12392v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riku Murai, Eric Dexheimer, Andrew J. Davison
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a real-time monocular dense SLAM system designed bottom-up from
MASt3R, a two-view 3D reconstruction and matching prior. Equipped with this
strong prior, our system is robust on in-the-wild video sequences despite
making no assumption on a fixed or parametric camera model beyond a unique
camera centre. We introduce efficient methods for pointmap matching, camera
tracking and local fusion, graph construction and loop closure, and
second-order global optimisation. With known calibration, a simple modification
to the system achieves state-of-the-art performance across various benchmarks.
Altogether, we propose a plug-and-play monocular SLAM system capable of
producing globally-consistent poses and dense geometry while operating at 15
FPS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally to this work. Project Page:
  https://edexheim.github.io/mast3r-slam/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Global <span class="highlight-title">SLAM</span> in Visual-Inertial Systems with 5G Time-of-Arrival
  Integration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meisam Kabiri, Holger Voos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel approach to improve global localization and
mapping in indoor drone navigation by integrating 5G Time of Arrival (ToA)
measurements into ORB-SLAM3, a Simultaneous Localization and Mapping (SLAM)
system. By incorporating ToA data from 5G base stations, we align the SLAM's
local reference frame with a global coordinate system, enabling accurate and
consistent global localization. We extend ORB-SLAM3's optimization pipeline to
integrate ToA measurements alongside bias estimation, transforming the
inherently local estimation into a globally consistent one. This integration
effectively resolves scale ambiguity in monocular SLAM systems and enhances
robustness, particularly in challenging scenarios where standard SLAM may fail.
Our method is evaluated using five real-world indoor datasets collected with
RGB-D cameras and inertial measurement units (IMUs), augmented with simulated
5G ToA measurements at 28 GHz and 78 GHz frequencies using MATLAB and QuaDRiGa.
We tested four SLAM configurations: RGB-D, RGB-D-Inertial, Monocular, and
Monocular-Inertial. The results demonstrate that while local estimation
accuracy remains comparable due to the high precision of RGB-D-based ORB-SLAM3
compared to ToA measurements, the inclusion of ToA measurements facilitates
robust global positioning. In scenarios where standard mono-inertial ORB-SLAM3
loses tracking, our approach maintains accurate localization throughout the
trajectory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MASt3R-<span class="highlight-title">SLAM</span>: Real-Time Dense <span class="highlight-title">SLAM</span> with 3D Reconstruction Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12392v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12392v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riku Murai, Eric Dexheimer, Andrew J. Davison
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a real-time monocular dense SLAM system designed bottom-up from
MASt3R, a two-view 3D reconstruction and matching prior. Equipped with this
strong prior, our system is robust on in-the-wild video sequences despite
making no assumption on a fixed or parametric camera model beyond a unique
camera centre. We introduce efficient methods for pointmap matching, camera
tracking and local fusion, graph construction and loop closure, and
second-order global optimisation. With known calibration, a simple modification
to the system achieves state-of-the-art performance across various benchmarks.
Altogether, we propose a plug-and-play monocular SLAM system capable of
producing globally-consistent poses and dense geometry while operating at 15
FPS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally to this work. Project Page:
  https://edexheim.github.io/mast3r-slam/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-12-15T00:00:00Z">2024-12-15</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">0</span>
                    </Summary>
                    <div class="details-content">
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">0</span>
                    </Summary>
                    <div class="details-content">
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-12-14T00:00:00Z">2024-12-14</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">0</span>
                    </Summary>
                    <div class="details-content">
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">0</span>
                    </Summary>
                    <div class="details-content">
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-12-13T00:00:00Z">2024-12-13</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RP-<span class="highlight-title">SLAM</span>: Real-time Photorealistic <span class="highlight-title">SLAM</span> with Efficient 3D Gaussian
  Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09868v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09868v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lizhi Bai, Chunqi Tian, Jun Yang, Siyu Zhang, Masanori Suganuma, Takayuki Okatani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting has emerged as a promising technique for high-quality
3D rendering, leading to increasing interest in integrating 3DGS into realism
SLAM systems. However, existing methods face challenges such as Gaussian
primitives redundancy, forgetting problem during continuous optimization, and
difficulty in initializing primitives in monocular case due to lack of depth
information. In order to achieve efficient and photorealistic mapping, we
propose RP-SLAM, a 3D Gaussian splatting-based vision SLAM method for monocular
and RGB-D cameras. RP-SLAM decouples camera poses estimation from Gaussian
primitives optimization and consists of three key components. Firstly, we
propose an efficient incremental mapping approach to achieve a compact and
accurate representation of the scene through adaptive sampling and Gaussian
primitives filtering. Secondly, a dynamic window optimization method is
proposed to mitigate the forgetting problem and improve map consistency.
Finally, for the monocular case, a monocular keyframe initialization method
based on sparse point cloud is proposed to improve the initialization accuracy
of Gaussian primitives, which provides a geometric basis for subsequent
optimization. The results of numerous experiments demonstrate that RP-SLAM
achieves state-of-the-art map rendering accuracy while ensuring real-time
performance and model compactness.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RP-<span class="highlight-title">SLAM</span>: Real-time Photorealistic <span class="highlight-title">SLAM</span> with Efficient 3D Gaussian
  Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09868v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09868v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lizhi Bai, Chunqi Tian, Jun Yang, Siyu Zhang, Masanori Suganuma, Takayuki Okatani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting has emerged as a promising technique for high-quality
3D rendering, leading to increasing interest in integrating 3DGS into realism
SLAM systems. However, existing methods face challenges such as Gaussian
primitives redundancy, forgetting problem during continuous optimization, and
difficulty in initializing primitives in monocular case due to lack of depth
information. In order to achieve efficient and photorealistic mapping, we
propose RP-SLAM, a 3D Gaussian splatting-based vision SLAM method for monocular
and RGB-D cameras. RP-SLAM decouples camera poses estimation from Gaussian
primitives optimization and consists of three key components. Firstly, we
propose an efficient incremental mapping approach to achieve a compact and
accurate representation of the scene through adaptive sampling and Gaussian
primitives filtering. Secondly, a dynamic window optimization method is
proposed to mitigate the forgetting problem and improve map consistency.
Finally, for the monocular case, a monocular keyframe initialization method
based on sparse point cloud is proposed to improve the initialization accuracy
of Gaussian primitives, which provides a geometric basis for subsequent
optimization. The results of numerous experiments demonstrate that RP-SLAM
achieves state-of-the-art map rendering accuracy while ensuring real-time
performance and model compactness.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-12-12T00:00:00Z">2024-12-12</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">SLAM</span>3R: Real-Time Dense Scene Reconstruction from Monocular RGB Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09401v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09401v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzheng Liu, Siyan Dong, Shuzhe Wang, Yingda Yin, Yanchao Yang, Qingnan Fan, Baoquan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce \textbf{SLAM3R}, a novel and effective monocular
RGB SLAM system for real-time and high-quality dense 3D reconstruction. SLAM3R
provides an end-to-end solution by seamlessly integrating local 3D
reconstruction and global coordinate registration through feed-forward neural
networks. Given an input video, the system first converts it into overlapping
clips using a sliding window mechanism. Unlike traditional pose
optimization-based methods, SLAM3R directly regresses 3D pointmaps from RGB
images in each window and progressively aligns and deforms these local
pointmaps to create a globally consistent scene reconstruction - all without
explicitly solving any camera parameters. Experiments across datasets
consistently show that SLAM3R achieves state-of-the-art reconstruction accuracy
and completeness while maintaining real-time performance at 20+ FPS. Code and
weights at: \url{https://github.com/PKU-VCL-3DV/SLAM3R}.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Drift-free Visual <span class="highlight-title">SLAM</span> using Digital Twins 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08496v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08496v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roxane Merat, Giovanni Cioffi, Leonard Bauersfeld, Davide Scaramuzza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Globally-consistent localization in urban environments is crucial for
autonomous systems such as self-driving vehicles and drones, as well as
assistive technologies for visually impaired people. Traditional
Visual-Inertial Odometry (VIO) and Visual Simultaneous Localization and Mapping
(VSLAM) methods, though adequate for local pose estimation, suffer from drift
in the long term due to reliance on local sensor data. While GPS counteracts
this drift, it is unavailable indoors and often unreliable in urban areas. An
alternative is to localize the camera to an existing 3D map using
visual-feature matching. This can provide centimeter-level accurate
localization but is limited by the visual similarities between the current view
and the map. This paper introduces a novel approach that achieves accurate and
globally-consistent localization by aligning the sparse 3D point cloud
generated by the VIO/VSLAM system to a digital twin using point-to-plane
matching; no visual data association is needed. The proposed method provides a
6-DoF global measurement tightly integrated into the VIO/VSLAM system.
Experiments run on a high-fidelity GPS simulator and real-world data collected
from a drone demonstrate that our approach outperforms state-of-the-art VIO-GPS
systems and offers superior robustness against viewpoint changes compared to
the state-of-the-art Visual SLAM systems.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-12-11T00:00:00Z">2024-12-11</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">0</span>
                    </Summary>
                    <div class="details-content">
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Drift-free Visual <span class="highlight-title">SLAM</span> using Digital Twins 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roxane Merat, Giovanni Cioffi, Leonard Bauersfeld, Davide Scaramuzza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Globally-consistent localization in urban environments is crucial for
autonomous systems such as self-driving vehicles and drones, as well as
assistive technologies for visually impaired people. Traditional
Visual-Inertial Odometry (VIO) and Visual Simultaneous Localization and Mapping
(VSLAM) methods, though adequate for local pose estimation, suffer from drift
in the long term due to reliance on local sensor data. While GPS counteracts
this drift, it is unavailable indoors and often unreliable in urban areas. An
alternative is to localize the camera to an existing 3D map using
visual-feature matching. This can provide centimeter-level accurate
localization but is limited by the visual similarities between the current view
and the map. This paper introduces a novel approach that achieves accurate and
globally-consistent localization by aligning the sparse 3D point cloud
generated by the VIO/VSLAM system to a digital twin using point-to-plane
matching; no visual data association is needed. The proposed method provides a
6-DoF global measurement tightly integrated into the VIO/VSLAM system.
Experiments run on a high-fidelity GPS simulator and real-world data collected
from a drone demonstrate that our approach outperforms state-of-the-art VIO-GPS
systems and offers superior robustness against viewpoint changes compared to
the state-of-the-art Visual SLAM systems.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2024-12-19T05:30:58.977820184Z">
            2024-12-19 05:30:58 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
