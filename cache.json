{"2024-12-11T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2412.08647v1","updated":"2024-12-11T18:59:57Z","published":"2024-12-11T18:59:57Z","title":"SegFace: Face Segmentation of Long-Tail Classes","summary":"  Face parsing refers to the semantic segmentation of human faces into key\nfacial regions such as eyes, nose, hair, etc. It serves as a prerequisite for\nvarious advanced applications, including face editing, face swapping, and\nfacial makeup, which often require segmentation masks for classes like\neyeglasses, hats, earrings, and necklaces. These infrequently occurring classes\nare called long-tail classes, which are overshadowed by more frequently\noccurring classes known as head classes. Existing methods, primarily CNN-based,\ntend to be dominated by head classes during training, resulting in suboptimal\nrepresentation for long-tail classes. Previous works have largely overlooked\nthe problem of poor segmentation performance of long-tail classes. To address\nthis issue, we propose SegFace, a simple and efficient approach that uses a\nlightweight transformer-based model which utilizes learnable class-specific\ntokens. The transformer decoder leverages class-specific tokens, allowing each\ntoken to focus on its corresponding class, thereby enabling independent\nmodeling of each class. The proposed approach improves the performance of\nlong-tail classes, thereby boosting overall performance. To the best of our\nknowledge, SegFace is the first work to employ transformer models for face\nparsing. Moreover, our approach can be adapted for low-compute edge devices,\nachieving 95.96 FPS. We conduct extensive experiments demonstrating that\nSegFace significantly outperforms previous state-of-the-art models, achieving a\nmean F1 score of 88.96 (+2.82) on the CelebAMask-HQ dataset and 93.03 (+0.65)\non the LaPa dataset. Code: https://github.com/Kartik-3004/SegFace\n","authors":["Kartik Narayan","Vibashan VS","Vishal M. Patel"],"pdf_url":"https://arxiv.org/pdf/2412.08647v1.pdf","comment":"Accepted to AAAI 2025. Project Page:\n  https://kartik-3004.github.io/SegFace/"},{"id":"http://arxiv.org/abs/2412.08646v1","updated":"2024-12-11T18:59:54Z","published":"2024-12-11T18:59:54Z","title":"StreamChat: Chatting with Streaming Video","summary":"  This paper presents StreamChat, a novel approach that enhances the\ninteraction capabilities of Large Multimodal Models (LMMs) with streaming video\ncontent. In streaming interaction scenarios, existing methods rely solely on\nvisual information available at the moment a question is posed, resulting in\nsignificant delays as the model remains unaware of subsequent changes in the\nstreaming video. StreamChat addresses this limitation by innovatively updating\nthe visual context at each decoding step, ensuring that the model utilizes\nup-to-date video content throughout the decoding process. Additionally, we\nintroduce a flexible and efficient crossattention-based architecture to process\ndynamic streaming inputs while maintaining inference efficiency for streaming\ninteractions. Furthermore, we construct a new dense instruction dataset to\nfacilitate the training of streaming interaction models, complemented by a\nparallel 3D-RoPE mechanism that encodes the relative temporal information of\nvisual and text tokens. Experimental results demonstrate that StreamChat\nachieves competitive performance on established image and video benchmarks and\nexhibits superior capabilities in streaming interaction scenarios compared to\nstate-of-the-art video LMM.\n","authors":["Jihao Liu","Zhiding Yu","Shiyi Lan","Shihao Wang","Rongyao Fang","Jan Kautz","Hongsheng Li","Jose M. Alvare"],"pdf_url":"https://arxiv.org/pdf/2412.08646v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08645v1","updated":"2024-12-11T18:59:53Z","published":"2024-12-11T18:59:53Z","title":"ObjectMate: A Recurrence Prior for Object Insertion and Subject-Driven\n  Generation","summary":"  This paper introduces a tuning-free method for both object insertion and\nsubject-driven generation. The task involves composing an object, given\nmultiple views, into a scene specified by either an image or text. Existing\nmethods struggle to fully meet the task's challenging objectives: (i)\nseamlessly composing the object into the scene with photorealistic pose and\nlighting, and (ii) preserving the object's identity. We hypothesize that\nachieving these goals requires large scale supervision, but manually collecting\nsufficient data is simply too expensive. The key observation in this paper is\nthat many mass-produced objects recur across multiple images of large unlabeled\ndatasets, in different scenes, poses, and lighting conditions. We use this\nobservation to create massive supervision by retrieving sets of diverse views\nof the same object. This powerful paired dataset enables us to train a\nstraightforward text-to-image diffusion architecture to map the object and\nscene descriptions to the composited image. We compare our method, ObjectMate,\nwith state-of-the-art methods for object insertion and subject-driven\ngeneration, using a single or multiple references. Empirically, ObjectMate\nachieves superior identity preservation and more photorealistic composition.\nDifferently from many other multi-reference methods, ObjectMate does not\nrequire slow test-time tuning.\n","authors":["Daniel Winter","Asaf Shul","Matan Cohen","Dana Berman","Yael Pritch","Alex Rav-Acha","Yedid Hoshen"],"pdf_url":"https://arxiv.org/pdf/2412.08645v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08643v1","updated":"2024-12-11T18:59:51Z","published":"2024-12-11T18:59:51Z","title":"GPD-1: Generative Pre-training for Driving","summary":"  Modeling the evolutions of driving scenarios is important for the evaluation\nand decision-making of autonomous driving systems. Most existing methods focus\non one aspect of scene evolution such as map generation, motion prediction, and\ntrajectory planning. In this paper, we propose a unified Generative\nPre-training for Driving (GPD-1) model to accomplish all these tasks altogether\nwithout additional fine-tuning. We represent each scene with ego, agent, and\nmap tokens and formulate autonomous driving as a unified token generation\nproblem. We adopt the autoregressive transformer architecture and use a\nscene-level attention mask to enable intra-scene bi-directional interactions.\nFor the ego and agent tokens, we propose a hierarchical positional tokenizer to\neffectively encode both 2D positions and headings. For the map tokens, we train\na map vector-quantized autoencoder to efficiently compress ego-centric semantic\nmaps into discrete tokens. We pre-train our GPD-1 on the large-scale nuPlan\ndataset and conduct extensive experiments to evaluate its effectiveness. With\ndifferent prompts, our GPD-1 successfully generalizes to various tasks without\nfinetuning, including scene generation, traffic simulation, closed-loop\nsimulation, map prediction, and motion planning. Code:\nhttps://github.com/wzzheng/GPD.\n","authors":["Zixun Xie","Sicheng Zuo","Wenzhao Zheng","Yunpeng Zhang","Dalong Du","Jie Zhou","Jiwen Lu","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.08643v1.pdf","comment":"Code is available at: https://github.com/wzzheng/GPD"},{"id":"http://arxiv.org/abs/2412.08641v1","updated":"2024-12-11T18:59:17Z","published":"2024-12-11T18:59:17Z","title":"3D Mesh Editing using Masked LRMs","summary":"  We present a novel approach to mesh shape editing, building on recent\nprogress in 3D reconstruction from multi-view images. We formulate shape\nediting as a conditional reconstruction problem, where the model must\nreconstruct the input shape with the exception of a specified 3D region, in\nwhich the geometry should be generated from the conditional signal. To this\nend, we train a conditional Large Reconstruction Model (LRM) for masked\nreconstruction, using multi-view consistent masks rendered from a randomly\ngenerated 3D occlusion, and using one clean viewpoint as the conditional\nsignal. During inference, we manually define a 3D region to edit and provide an\nedited image from a canonical viewpoint to fill in that region. We demonstrate\nthat, in just a single forward pass, our method not only preserves the input\ngeometry in the unmasked region through reconstruction capabilities on par with\nSoTA, but is also expressive enough to perform a variety of mesh edits from a\nsingle image guidance that past works struggle with, while being 10x faster\nthan the top-performing competing prior work.\n","authors":["Will Gao","Dilin Wang","Yuchen Fan","Aljaz Bozic","Tuur Stuyck","Zhengqin Li","Zhao Dong","Rakesh Ranjan","Nikolaos Sarafianos"],"pdf_url":"https://arxiv.org/pdf/2412.08641v1.pdf","comment":"Project Page: https://chocolatebiscuit.github.io/MaskedLRM/"},{"id":"http://arxiv.org/abs/2412.08640v1","updated":"2024-12-11T18:59:08Z","published":"2024-12-11T18:59:08Z","title":"BLADE: Single-view Body Mesh Learning through Accurate Depth Estimation","summary":"  Single-image human mesh recovery is a challenging task due to the ill-posed\nnature of simultaneous body shape, pose, and camera estimation. Existing\nestimators work well on images taken from afar, but they break down as the\nperson moves close to the camera. Moreover, current methods fail to achieve\nboth accurate 3D pose and 2D alignment at the same time. Error is mainly\nintroduced by inaccurate perspective projection heuristically derived from\northographic parameters. To resolve this long-standing challenge, we present\nour method BLADE which accurately recovers perspective parameters from a single\nimage without heuristic assumptions. We start from the inverse relationship\nbetween perspective distortion and the person's Z-translation Tz, and we show\nthat Tz can be reliably estimated from the image. We then discuss the important\nrole of Tz for accurate human mesh recovery estimated from close-range images.\nFinally, we show that, once Tz and the 3D human mesh are estimated, one can\naccurately recover the focal length and full 3D translation. Extensive\nexperiments on standard benchmarks and real-world close-range images show that\nour method is the first to accurately recover projection parameters from a\nsingle image, and consequently attain state-of-the-art accuracy on 3D pose\nestimation and 2D alignment for a wide range of images.\nhttps://research.nvidia.com/labs/amri/projects/blade/\n","authors":["Shengze Wang","Jiefeng Li","Tianye Li","Ye Yuan","Henry Fuchs","Koki Nagano","Shalini De Mello","Michael Stengel"],"pdf_url":"https://arxiv.org/pdf/2412.08640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08639v1","updated":"2024-12-11T18:58:41Z","published":"2024-12-11T18:58:41Z","title":"Fast Prompt Alignment for Text-to-Image Generation","summary":"  Text-to-image generation has advanced rapidly, yet aligning complex textual\nprompts with generated visuals remains challenging, especially with intricate\nobject relationships and fine-grained details. This paper introduces Fast\nPrompt Alignment (FPA), a prompt optimization framework that leverages a\none-pass approach, enhancing text-to-image alignment efficiency without the\niterative overhead typical of current methods like OPT2I. FPA uses large\nlanguage models (LLMs) for single-iteration prompt paraphrasing, followed by\nfine-tuning or in-context learning with optimized prompts to enable real-time\ninference, reducing computational demands while preserving alignment fidelity.\nExtensive evaluations on the COCO Captions and PartiPrompts datasets\ndemonstrate that FPA achieves competitive text-image alignment scores at a\nfraction of the processing time, as validated through both automated metrics\n(TIFA, VQA) and human evaluation. A human study with expert annotators further\nreveals a strong correlation between human alignment judgments and automated\nscores, underscoring the robustness of FPA's improvements. The proposed method\nshowcases a scalable, efficient alternative to iterative prompt optimization,\nenabling broader applicability in real-time, high-demand settings. The codebase\nis provided to facilitate further research:\nhttps://github.com/tiktok/fast_prompt_alignment\n","authors":["Khalil Mrini","Hanlin Lu","Linjie Yang","Weilin Huang","Heng Wang"],"pdf_url":"https://arxiv.org/pdf/2412.08639v1.pdf","comment":"TikTok Technical Report"},{"id":"http://arxiv.org/abs/2412.08637v1","updated":"2024-12-11T18:58:40Z","published":"2024-12-11T18:58:40Z","title":"DMin: Scalable Training Data Influence Estimation for Diffusion Models","summary":"  Identifying the training data samples that most influence a generated image\nis a critical task in understanding diffusion models, yet existing influence\nestimation methods are constrained to small-scale or LoRA-tuned models due to\ncomputational limitations. As diffusion models scale up, these methods become\nimpractical. To address this challenge, we propose DMin (Diffusion Model\ninfluence), a scalable framework for estimating the influence of each training\ndata sample on a given generated image. By leveraging efficient gradient\ncompression and retrieval techniques, DMin reduces storage requirements from\n339.39 TB to only 726 MB and retrieves the top-k most influential training\nsamples in under 1 second, all while maintaining performance. Our empirical\nresults demonstrate DMin is both effective in identifying influential training\nsamples and efficient in terms of computational and storage requirements.\n","authors":["Huawei Lin","Yingjie Lao","Weijie Zhao"],"pdf_url":"https://arxiv.org/pdf/2412.08637v1.pdf","comment":"14 pages, 6 figures, 8 tables. Under Review"},{"id":"http://arxiv.org/abs/2412.08635v1","updated":"2024-12-11T18:57:32Z","published":"2024-12-11T18:57:32Z","title":"Multimodal Latent Language Modeling with Next-Token Diffusion","summary":"  Multimodal generative models require a unified approach to handle both\ndiscrete data (e.g., text and code) and continuous data (e.g., image, audio,\nvideo). In this work, we propose Latent Language Modeling (LatentLM), which\nseamlessly integrates continuous and discrete data using causal Transformers.\nSpecifically, we employ a variational autoencoder (VAE) to represent continuous\ndata as latent vectors and introduce next-token diffusion for autoregressive\ngeneration of these vectors. Additionally, we develop $\\sigma$-VAE to address\nthe challenges of variance collapse, which is crucial for autoregressive\nmodeling. Extensive experiments demonstrate the effectiveness of LatentLM\nacross various modalities. In image generation, LatentLM surpasses Diffusion\nTransformers in both performance and scalability. When integrated into\nmultimodal large language models, LatentLM provides a general-purpose interface\nthat unifies multimodal generation and understanding. Experimental results show\nthat LatentLM achieves favorable performance compared to Transfusion and vector\nquantized models in the setting of scaling up training tokens. In\ntext-to-speech synthesis, LatentLM outperforms the state-of-the-art VALL-E 2\nmodel in speaker similarity and robustness, while requiring 10x fewer decoding\nsteps. The results establish LatentLM as a highly effective and scalable\napproach to advance large multimodal models.\n","authors":["Yutao Sun","Hangbo Bao","Wenhui Wang","Zhiliang Peng","Li Dong","Shaohan Huang","Jianyong Wang","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2412.08635v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08629v1","updated":"2024-12-11T18:50:29Z","published":"2024-12-11T18:50:29Z","title":"FlowEdit: Inversion-Free Text-Based Editing Using Pre-Trained Flow\n  Models","summary":"  Editing real images using a pre-trained text-to-image (T2I) diffusion/flow\nmodel often involves inverting the image into its corresponding noise map.\nHowever, inversion by itself is typically insufficient for obtaining\nsatisfactory results, and therefore many methods additionally intervene in the\nsampling process. Such methods achieve improved results but are not seamlessly\ntransferable between model architectures. Here, we introduce FlowEdit, a\ntext-based editing method for pre-trained T2I flow models, which is\ninversion-free, optimization-free and model agnostic. Our method constructs an\nODE that directly maps between the source and target distributions\n(corresponding to the source and target text prompts) and achieves a lower\ntransport cost than the inversion approach. This leads to state-of-the-art\nresults, as we illustrate with Stable Diffusion 3 and FLUX. Code and examples\nare available on the project's webpage.\n","authors":["Vladimir Kulikov","Matan Kleiner","Inbar Huberman-Spiegelglas","Tomer Michaeli"],"pdf_url":"https://arxiv.org/pdf/2412.08629v1.pdf","comment":"Project's webpage at https://matankleiner.github.io/flowedit/"},{"id":"http://arxiv.org/abs/2412.08628v1","updated":"2024-12-11T18:48:20Z","published":"2024-12-11T18:48:20Z","title":"EOV-Seg: Efficient Open-Vocabulary Panoptic Segmentation","summary":"  Open-vocabulary panoptic segmentation aims to segment and classify everything\nin diverse scenes across an unbounded vocabulary. Existing methods typically\nemploy two-stage or single-stage framework. The two-stage framework involves\ncropping the image multiple times using masks generated by a mask generator,\nfollowed by feature extraction, while the single-stage framework relies on a\nheavyweight mask decoder to make up for the lack of spatial position\ninformation through self-attention and cross-attention in multiple stacked\nTransformer blocks. Both methods incur substantial computational overhead,\nthereby hindering the efficiency of model inference. To fill the gap in\nefficiency, we propose EOV-Seg, a novel single-stage, shared, efficient, and\nspatial-aware framework designed for open-vocabulary panoptic segmentation.\nSpecifically, EOV-Seg innovates in two aspects. First, a Vocabulary-Aware\nSelection (VAS) module is proposed to improve the semantic comprehension of\nvisual aggregated features and alleviate the feature interaction burden on the\nmask decoder. Second, we introduce a Two-way Dynamic Embedding Experts (TDEE),\nwhich efficiently utilizes the spatial awareness capabilities of ViT-based CLIP\nbackbone. To the best of our knowledge, EOV-Seg is the first open-vocabulary\npanoptic segmentation framework towards efficiency, which runs faster and\nachieves competitive performance compared with state-of-the-art methods.\nSpecifically, with COCO training only, EOV-Seg achieves 24.2 PQ, 31.6 mIoU, and\n12.7 FPS on the ADE20K dataset for panoptic and semantic segmentation tasks and\nthe inference time of EOV-Seg is 4-21 times faster than state-of-the-art\nmethods. Especially, equipped with ResNet-50 backbone, EOV-Seg runs 25 FPS with\nonly 71M parameters on a single RTX 3090 GPU. Code is available at\n\\url{https://github.com/nhw649/EOV-Seg}.\n","authors":["Hongwei Niu","Jie Hu","Jianghang Lin","Shengchuan Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.08628v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.08619v1","updated":"2024-12-11T18:40:16Z","published":"2024-12-11T18:40:16Z","title":"Synthetic Vision: Training Vision-Language Models to Understand Physics","summary":"  Physical reasoning, which involves the interpretation, understanding, and\nprediction of object behavior in dynamic environments, remains a significant\nchallenge for current Vision-Language Models (VLMs). In this work, we propose\ntwo methods to enhance VLMs' physical reasoning capabilities using simulated\ndata. First, we fine-tune a pre-trained VLM using question-answer (QA) pairs\ngenerated from simulations relevant to physical reasoning tasks. Second, we\nintroduce Physics Context Builders (PCBs), specialized VLMs fine-tuned to\ncreate scene descriptions enriched with physical properties and processes.\nDuring physical reasoning tasks, these PCBs can be leveraged as context to\nassist a Large Language Model (LLM) to improve its performance. We evaluate\nboth of our approaches using multiple benchmarks, including a new stability\ndetection QA dataset called Falling Tower, which includes both simulated and\nreal-world scenes, and CLEVRER. We demonstrate that a small QA fine-tuned VLM\ncan significantly outperform larger state-of-the-art foundational models. We\nalso show that integrating PCBs boosts the performance of foundational LLMs on\nphysical reasoning tasks. Using the real-world scenes from the Falling Tower\ndataset, we also validate the robustness of both approaches in Sim2Real\ntransfer. Our results highlight the utility that simulated data can have in the\ncreation of learning systems capable of advanced physical reasoning.\n","authors":["Vahid Balazadeh","Mohammadmehdi Ataei","Hyunmin Cheong","Amir Hosein Khasahmadi","Rahul G. Krishnan"],"pdf_url":"https://arxiv.org/pdf/2412.08619v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08618v1","updated":"2024-12-11T18:39:32Z","published":"2024-12-11T18:39:32Z","title":"Image Retrieval Methods in the Dissimilarity Space","summary":"  Image retrieval methods rely on metric learning to train backbone feature\nextraction models that can extract discriminant queries and reference (gallery)\nfeature representations for similarity matching. Although state-of-the-art\naccuracy has improved considerably with the advent of deep learning (DL) models\ntrained on large datasets, image retrieval remains challenging in many\nreal-world video analytics and surveillance applications, e.g., person\nre-identification. Using the Euclidean space for matching limits the\nperformance in real-world applications due to the curse of dimensionality,\noverfitting, and sensitivity to noisy data.\n  We argue that the feature dissimilarity space is more suitable for similarity\nmatching, and propose a dichotomy transformation to project query and reference\nembeddings into a single embedding in the dissimilarity space.\n  We also advocate for end-to-end training of a backbone and binary\nclassification models for pair-wise matching. As opposed to comparing the\ndistance between queries and reference embeddings, we show the benefits of\nclassifying the single dissimilarity space embedding (as similar or\ndissimilar), especially when trained end-to-end. We propose a method to train\nthe max-margin classifier together with the backbone feature extractor by\napplying constraints to the L2 norm of the classifier weights along with the\nhinge loss.\n  Our extensive experiments on challenging image retrieval datasets and using\ndiverse feature extraction backbones highlight the benefits of similarity\nmatching in the dissimilarity space. In particular, when jointly training the\nfeature extraction backbone and regularised classifier for matching, the\ndissimilarity space provides a higher level of accuracy.\n","authors":["Madhu Kiran","Kartikey Vishnu","Rafael M. O. Cruz","Eric Granger"],"pdf_url":"https://arxiv.org/pdf/2412.08618v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2412.08614v1","updated":"2024-12-11T18:37:42Z","published":"2024-12-11T18:37:42Z","title":"Benchmarking Large Vision-Language Models via Directed Scene Graph for\n  Comprehensive Image Captioning","summary":"  Generating detailed captions comprehending text-rich visual content in images\nhas received growing attention for Large Vision-Language Models (LVLMs).\nHowever, few studies have developed benchmarks specifically tailored for\ndetailed captions to measure their accuracy and comprehensiveness. In this\npaper, we introduce a detailed caption benchmark, termed as CompreCap, to\nevaluate the visual context from a directed scene graph view. Concretely, we\nfirst manually segment the image into semantically meaningful regions (i.e.,\nsemantic segmentation mask) according to common-object vocabulary, while also\ndistinguishing attributes of objects within all those regions. Then directional\nrelation labels of these objects are annotated to compose a directed scene\ngraph that can well encode rich compositional information of the image. Based\non our directed scene graph, we develop a pipeline to assess the generated\ndetailed captions from LVLMs on multiple levels, including the object-level\ncoverage, the accuracy of attribute descriptions, the score of key\nrelationships, etc. Experimental results on the CompreCap dataset confirm that\nour evaluation method aligns closely with human evaluation scores across LVLMs.\n","authors":["Fan Lu","Wei Wu","Kecheng Zheng","Shuailei Ma","Biao Gong","Jiawei Liu","Wei Zhai","Yang Cao","Yujun Shen","Zheng-Jun Zha"],"pdf_url":"https://arxiv.org/pdf/2412.08614v1.pdf","comment":"21 pages, 17 figures. Code and Dataset:\n  https://github.com/LuFan31/CompreCap"},{"id":"http://arxiv.org/abs/2412.08613v1","updated":"2024-12-11T18:36:35Z","published":"2024-12-11T18:36:35Z","title":"Fair Primal Dual Splitting Method for Image Inverse Problems","summary":"  Image inverse problems have numerous applications, including image\nprocessing, super-resolution, and computer vision, which are important areas in\nimage science. These application models can be seen as a three-function\ncomposite optimization problem solvable by a variety of primal dual-type\nmethods. We propose a fair primal dual algorithmic framework that incorporates\nthe smooth term not only into the primal subproblem but also into the dual\nsubproblem. We unify the global convergence and establish the convergence rates\nof our proposed fair primal dual method. Experiments on image denoising and\nsuper-resolution reconstruction demonstrate the superiority of the proposed\nmethod over the current state-of-the-art.\n","authors":["Yunfei Qu","Deren Han"],"pdf_url":"https://arxiv.org/pdf/2412.08613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07012v2","updated":"2024-12-11T18:28:00Z","published":"2024-12-09T21:44:02Z","title":"ProVision: Programmatically Scaling Vision-centric Instruction Data for\n  Multimodal Language Models","summary":"  With the rise of multimodal applications, instruction data has become\ncritical for training multimodal language models capable of understanding\ncomplex image-based queries. Existing practices rely on powerful but costly\nlarge language models (LLMs) or multimodal language models (MLMs) to produce\ninstruction data. These are often prone to hallucinations, licensing issues and\nthe generation process is often hard to scale and interpret. In this work, we\npresent a programmatic approach that employs scene graphs as symbolic\nrepresentations of images and human-written programs to systematically\nsynthesize vision-centric instruction data. Our approach ensures the\ninterpretability and controllability of the data generation process and scales\nefficiently while maintaining factual accuracy. By implementing a suite of 24\nsingle-image, 14 multi-image instruction generators, and a scene graph\ngeneration pipeline, we build a scalable, cost-effective system: ProVision\nwhich produces diverse question-answer pairs concerning objects, attributes,\nrelations, depth, etc., for any given image. Applied to Visual Genome and\nDataComp datasets, we generate over 10 million instruction data points,\nProVision-10M, and leverage them in both pretraining and instruction tuning\nstages of MLMs. When adopted in the instruction tuning stage, our single-image\ninstruction data yields up to a 7% improvement on the 2D split and 8% on the 3D\nsplit of CVBench, along with a 3% increase in performance on QBench2,\nRealWorldQA, and MMMU. Our multi-image instruction data leads to an 8%\nimprovement on Mantis-Eval. Incorporation of our data in both pre-training and\nfine-tuning stages of xGen-MM-4B leads to an averaged improvement of 1.6%\nacross 11 benchmarks.\n","authors":["Jieyu Zhang","Le Xue","Linxin Song","Jun Wang","Weikai Huang","Manli Shu","An Yan","Zixian Ma","Juan Carlos Niebles","silvio savarese","Caiming Xiong","Zeyuan Chen","Ranjay Krishna","Ran Xu"],"pdf_url":"https://arxiv.org/pdf/2412.07012v2.pdf","comment":"code: https://github.com/JieyuZ2/ProVision dataset:\n  https://huggingface.co/datasets/Salesforce/ProVision-10M"},{"id":"http://arxiv.org/abs/2412.08603v1","updated":"2024-12-11T18:26:45Z","published":"2024-12-11T18:26:45Z","title":"Design2GarmentCode: Turning Design Concepts to Tangible Garments Through\n  Program Synthesis","summary":"  Sewing patterns, the essential blueprints for fabric cutting and tailoring,\nact as a crucial bridge between design concepts and producible garments.\nHowever, existing uni-modal sewing pattern generation models struggle to\neffectively encode complex design concepts with a multi-modal nature and\ncorrelate them with vectorized sewing patterns that possess precise geometric\nstructures and intricate sewing relations. In this work, we propose a novel\nsewing pattern generation approach Design2GarmentCode based on Large Multimodal\nModels (LMMs), to generate parametric pattern-making programs from multi-modal\ndesign concepts. LMM offers an intuitive interface for interpreting diverse\ndesign inputs, while pattern-making programs could serve as well-structured and\nsemantically meaningful representations of sewing patterns, and act as a robust\nbridge connecting the cross-domain pattern-making knowledge embedded in LMMs\nwith vectorized sewing patterns. Experimental results demonstrate that our\nmethod can flexibly handle various complex design expressions such as images,\ntextual descriptions, designer sketches, or their combinations, and convert\nthem into size-precise sewing patterns with correct stitches. Compared to\nprevious methods, our approach significantly enhances training efficiency,\ngeneration quality, and authoring flexibility. Our code and data will be\npublicly available.\n","authors":["Feng Zhou","Ruiyang Liu","Chen Liu","Gaofeng He","Yong-Lu Li","Xiaogang Jin","Huamin Wang"],"pdf_url":"https://arxiv.org/pdf/2412.08603v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12151v3","updated":"2024-12-11T18:12:43Z","published":"2024-03-18T18:08:44Z","title":"Fusing Domain-Specific Content from Large Language Models into Knowledge\n  Graphs for Enhanced Zero Shot Object State Classification","summary":"  Domain-specific knowledge can significantly contribute to addressing a wide\nvariety of vision tasks. However, the generation of such knowledge entails\nconsiderable human labor and time costs. This study investigates the potential\nof Large Language Models (LLMs) in generating and providing domain-specific\ninformation through semantic embeddings. To achieve this, an LLM is integrated\ninto a pipeline that utilizes Knowledge Graphs and pre-trained semantic vectors\nin the context of the Vision-based Zero-shot Object State Classification task.\nWe thoroughly examine the behavior of the LLM through an extensive ablation\nstudy. Our findings reveal that the integration of LLM-based embeddings, in\ncombination with general-purpose pre-trained embeddings, leads to substantial\nperformance improvements. Drawing insights from this ablation study, we conduct\na comparative analysis against competing models, thereby highlighting the\nstate-of-the-art performance achieved by the proposed approach.\n","authors":["Filippos Gouidis","Katerina Papantoniou","Konstantinos Papoutsakis","Theodore Patkos","Antonis Argyros","Dimitris Plexousakis"],"pdf_url":"https://arxiv.org/pdf/2403.12151v3.pdf","comment":"Accepted at the AAAI-MAKE 2024"},{"id":"http://arxiv.org/abs/2412.08594v1","updated":"2024-12-11T18:12:06Z","published":"2024-12-11T18:12:06Z","title":"ASDnB: Merging Face with Body Cues For Robust Active Speaker Detection","summary":"  State-of-the-art Active Speaker Detection (ASD) approaches mainly use audio\nand facial features as input. However, the main hypothesis in this paper is\nthat body dynamics is also highly correlated to \"speaking\" (and \"listening\")\nactions and should be particularly useful in wild conditions (e.g.,\nsurveillance settings), where face cannot be reliably accessed. We propose\nASDnB, a model that singularly integrates face with body information by merging\nthe inputs at different steps of feature extraction. Our approach splits 3D\nconvolution into 2D and 1D to reduce computation cost without loss of\nperformance, and is trained with adaptive weight feature importance for\nimproved complement of face with body data. Our experiments show that ASDnB\nachieves state-of-the-art results in the benchmark dataset (AVA-ActiveSpeaker),\nin the challenging data of WASD, and in cross-domain settings using Columbia.\nThis way, ASDnB can perform in multiple settings, which is positively regarded\nas a strong baseline for robust ASD models (code available at\nhttps://github.com/Tiago-Roxo/ASDnB).\n","authors":["Tiago Roxo","Joana C. Costa","Pedro Inácio","Hugo Proença"],"pdf_url":"https://arxiv.org/pdf/2412.08594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08591v1","updated":"2024-12-11T18:10:21Z","published":"2024-12-11T18:10:21Z","title":"RoomTour3D: Geometry-Aware Video-Instruction Tuning for Embodied\n  Navigation","summary":"  Vision-and-Language Navigation (VLN) suffers from the limited diversity and\nscale of training data, primarily constrained by the manual curation of\nexisting simulators. To address this, we introduce RoomTour3D, a\nvideo-instruction dataset derived from web-based room tour videos that capture\nreal-world indoor spaces and human walking demonstrations. Unlike existing VLN\ndatasets, RoomTour3D leverages the scale and diversity of online videos to\ngenerate open-ended human walking trajectories and open-world navigable\ninstructions. To compensate for the lack of navigation data in online videos,\nwe perform 3D reconstruction and obtain 3D trajectories of walking paths\naugmented with additional information on the room types, object locations and\n3D shape of surrounding scenes. Our dataset includes $\\sim$100K open-ended\ndescription-enriched trajectories with $\\sim$200K instructions, and 17K\naction-enriched trajectories from 1847 room tour environments. We demonstrate\nexperimentally that RoomTour3D enables significant improvements across multiple\nVLN tasks including CVDN, SOON, R2R, and REVERIE. Moreover, RoomTour3D\nfacilitates the development of trainable zero-shot VLN agents, showcasing the\npotential and challenges of advancing towards open-world navigation.\n","authors":["Mingfei Han","Liang Ma","Kamila Zhumakhanova","Ekaterina Radionova","Jingyi Zhang","Xiaojun Chang","Xiaodan Liang","Ivan Laptev"],"pdf_url":"https://arxiv.org/pdf/2412.08591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08589v1","updated":"2024-12-11T18:08:06Z","published":"2024-12-11T18:08:06Z","title":"SPACE-SUIT: An Artificial Intelligence based chromospheric feature\n  extractor and classifier for SUIT","summary":"  The Solar Ultraviolet Imaging Telescope(SUIT) onboard Aditya-L1 is an imager\nthat observes the solar photosphere and chromosphere through observations in\nthe wavelength range of 200-400 nm. A comprehensive understanding of the plasma\nand thermodynamic properties of chromospheric and photospheric morphological\nstructures requires a large sample statistical study, necessitating the\ndevelopment of automatic feature detection methods. To this end, we develop the\nfeature detection algorithm SPACE-SUIT: Solar Phenomena Analysis and\nClassification using Enhanced vision techniques for SUIT, to detect and\nclassify the solar chromospheric features to be observed from SUIT's Mg II k\nfilter. Specifically, we target plage regions, sunspots, filaments, and\noff-limb structures. SPACE uses You Only Look Once(YOLO), a neural\nnetwork-based model to identify regions of interest. We train and validate\nSPACE using mock-SUIT images developed from Interface Region Imaging\nSpectrometer(IRIS) full-disk mosaic images in Mg II k line, while we also\nperform detection on Level-1 SUIT data. SPACE achieves an approximate precision\nof 0.788, recall 0.863 and MAP of 0.874 on the validation mock SUIT FITS\ndataset. Given the manual labeling of our dataset, we perform \"self-validation\"\nby applying statistical measures and Tamura features on the ground truth and\npredicted bounding boxes. We find the distributions of entropy, contrast,\ndissimilarity, and energy to show differences in the features. These\ndifferences are qualitatively captured by the detected regions predicted by\nSPACE and validated with the observed SUIT images, even in the absence of\nlabeled ground truth. This work not only develops a chromospheric feature\nextractor but also demonstrates the effectiveness of statistical metrics and\nTamura features for distinguishing chromospheric features, offering independent\nvalidation for future detection schemes.\n","authors":["Pranava Seth","Vishal Upendran","Megha Anand","Janmejoy Sarkar","Soumya Roy","Priyadarshan Chaki","Pratyay Chowdhury","Borishan Ghosh","Durgesh Tripathi"],"pdf_url":"https://arxiv.org/pdf/2412.08589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08582v1","updated":"2024-12-11T17:57:25Z","published":"2024-12-11T17:57:25Z","title":"Utilizing Multi-step Loss for Single Image Reflection Removal","summary":"  Image reflection removal is crucial for restoring image quality. Distorted\nimages can negatively impact tasks like object detection and image\nsegmentation. In this paper, we present a novel approach for image reflection\nremoval using a single image. Instead of focusing on model architecture, we\nintroduce a new training technique that can be generalized to image-to-image\nproblems, with input and output being similar in nature. This technique is\nembodied in our multi-step loss mechanism, which has proven effective in the\nreflection removal task. Additionally, we address the scarcity of reflection\nremoval training data by synthesizing a high-quality, non-linear synthetic\ndataset called RefGAN using Pix2Pix GAN. This dataset significantly enhances\nthe model's ability to learn better patterns for reflection removal. We also\nutilize a ranged depth map, extracted from the depth estimation of the ambient\nimage, as an auxiliary feature, leveraging its property of lacking depth\nestimations for reflections. Our approach demonstrates superior performance on\nthe SIR^2 benchmark and other real-world datasets, proving its effectiveness by\noutperforming other state-of-the-art models.\n","authors":["Abdelrahman Elnenaey","Marwan Torki"],"pdf_url":"https://arxiv.org/pdf/2412.08582v1.pdf","comment":"6 pages, 6 figures, IEEE ICASSP 2024"},{"id":"http://arxiv.org/abs/2412.08580v1","updated":"2024-12-11T17:57:10Z","published":"2024-12-11T17:57:10Z","title":"LAION-SG: An Enhanced Large-Scale Dataset for Training Complex\n  Image-Text Models with Structural Annotations","summary":"  Recent advances in text-to-image (T2I) generation have shown remarkable\nsuccess in producing high-quality images from text. However, existing T2I\nmodels show decayed performance in compositional image generation involving\nmultiple objects and intricate relationships. We attribute this problem to\nlimitations in existing datasets of image-text pairs, which lack precise\ninter-object relationship annotations with prompts only. To address this\nproblem, we construct LAION-SG, a large-scale dataset with high-quality\nstructural annotations of scene graphs (SG), which precisely describe\nattributes and relationships of multiple objects, effectively representing the\nsemantic structure in complex scenes. Based on LAION-SG, we train a new\nfoundation model SDXL-SG to incorporate structural annotation information into\nthe generation process. Extensive experiments show advanced models trained on\nour LAION-SG boast significant performance improvements in complex scene\ngeneration over models on existing datasets. We also introduce CompSG-Bench, a\nbenchmark that evaluates models on compositional image generation, establishing\na new standard for this domain.\n","authors":["Zejian Li","Chenye Meng","Yize Li","Ling Yang","Shengyuan Zhang","Jiarui Ma","Jiayi Li","Guang Yang","Changyuan Yang","Zhiyuan Yang","Jinxiong Chang","Lingyun Sun"],"pdf_url":"https://arxiv.org/pdf/2412.08580v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08575v1","updated":"2024-12-11T17:47:00Z","published":"2024-12-11T17:47:00Z","title":"Annotation-Efficient Task Guidance for Medical Segment Anything","summary":"  Medical image segmentation is a key task in the imaging workflow, influencing\nmany image-based decisions. Traditional, fully-supervised segmentation models\nrely on large amounts of labeled training data, typically obtained through\nmanual annotation, which can be an expensive, time-consuming, and error-prone\nprocess. This signals a need for accurate, automatic, and annotation-efficient\nmethods of training these models. We propose SAM-Mix, a novel multitask\nlearning framework for medical image segmentation that uses class activation\nmaps produced by an auxiliary classifier to guide the predictions of the\nsemi-supervised segmentation branch, which is based on the SAM framework.\nExperimental evaluations on the public LiTS dataset confirm the effectiveness\nof SAM-Mix for simultaneous classification and segmentation of the liver from\nabdominal computed tomography (CT) scans. When trained for 90% fewer epochs on\nonly 50 labeled 2D slices, representing just 0.04% of the available labeled\ntraining data, SAM-Mix achieves a Dice improvement of 5.1% over the best\nbaseline model. The generalization results for SAM-Mix are even more\nimpressive, with the same model configuration yielding a 25.4% Dice improvement\non a cross-domain segmentation task. Our code is available at\nhttps://github.com/tbwa233/SAM-Mix.\n","authors":["Tyler Ward","Abdullah-Al-Zubaer Imran"],"pdf_url":"https://arxiv.org/pdf/2412.08575v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14612v2","updated":"2024-12-11T17:44:49Z","published":"2024-10-18T17:05:03Z","title":"MultiOrg: A Multi-rater Organoid-detection Dataset","summary":"  High-throughput image analysis in the biomedical domain has gained\nsignificant attention in recent years, driving advancements in drug discovery,\ndisease prediction, and personalized medicine. Organoids, specifically, are an\nactive area of research, providing excellent models for human organs and their\nfunctions. Automating the quantification of organoids in microscopy images\nwould provide an effective solution to overcome substantial manual\nquantification bottlenecks, particularly in high-throughput image analysis.\nHowever, there is a notable lack of open biomedical datasets, in contrast to\nother domains, such as autonomous driving, and, notably, only few of them have\nattempted to quantify annotation uncertainty. In this work, we present MultiOrg\na comprehensive organoid dataset tailored for object detection tasks with\nuncertainty quantification. This dataset comprises over 400 high-resolution 2d\nmicroscopy images and curated annotations of more than 60,000 organoids. Most\nimportantly, it includes three label sets for the test data, independently\nannotated by two experts at distinct time points. We additionally provide a\nbenchmark for organoid detection, and make the best model available through an\neasily installable, interactive plugin for the popular image visualization tool\nNapari, to perform organoid quantification.\n","authors":["Christina Bukas","Harshavardhan Subramanian","Fenja See","Carina Steinchen","Ivan Ezhov","Gowtham Boosarpu","Sara Asgharpour","Gerald Burgstaller","Mareike Lehmann","Florian Kofler","Marie Piraud"],"pdf_url":"https://arxiv.org/pdf/2410.14612v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08573v1","updated":"2024-12-11T17:41:53Z","published":"2024-12-11T17:41:53Z","title":"TryOffAnyone: Tiled Cloth Generation from a Dressed Person","summary":"  The fashion industry is increasingly leveraging computer vision and deep\nlearning technologies to enhance online shopping experiences and operational\nefficiencies. In this paper, we address the challenge of generating\nhigh-fidelity tiled garment images essential for personalized recommendations,\noutfit composition, and virtual try-on systems from photos of garments worn by\nmodels. Inspired by the success of Latent Diffusion Models (LDMs) in\nimage-to-image translation, we propose a novel approach utilizing a fine-tuned\nStableDiffusion model. Our method features a streamlined single-stage network\ndesign, which integrates garmentspecific masks to isolate and process target\nclothing items effectively. By simplifying the network architecture through\nselective training of transformer blocks and removing unnecessary\ncrossattention layers, we significantly reduce computational complexity while\nachieving state-of-the-art performance on benchmark datasets like VITON-HD.\nExperimental results demonstrate the effectiveness of our approach in producing\nhigh-quality tiled garment images for both full-body and half-body inputs. Code\nand model are available at: https://github.com/ixarchakos/try-off-anyone\n","authors":["Ioannis Xarchakos","Theodoros Koukopoulos"],"pdf_url":"https://arxiv.org/pdf/2412.08573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04844v2","updated":"2024-12-11T17:40:32Z","published":"2024-11-07T16:32:29Z","title":"Discretized Gaussian Representation for Tomographic Reconstruction","summary":"  Computed Tomography (CT) is a widely used imaging technique that provides\ndetailed cross-sectional views of objects. Over the past decade, Deep\nLearning-based Reconstruction (DLR) methods have led efforts to enhance image\nquality and reduce noise, yet they often require large amounts of data and are\ncomputationally intensive. Inspired by recent advancements in scene\nreconstruction, some approaches have adapted NeRF and 3D Gaussian Splatting\n(3DGS) techniques for CT reconstruction. However, these methods are not ideal\nfor direct 3D volume reconstruction. In this paper, we reconsider the\nrepresentation of CT reconstruction and propose a novel Discretized Gaussian\nRepresentation (DGR) specifically designed for CT. Unlike the popular 3D\nGaussian Splatting, our representation directly reconstructs the 3D volume\nusing a set of discretized Gaussian functions in an end-to-end manner.\nAdditionally, we introduce a Fast Volume Reconstruction technique that\nefficiently aggregates the contributions of these Gaussians into a discretized\nvolume. Extensive experiments on both real-world and synthetic datasets\ndemonstrate the effectiveness of our method in improving reconstruction quality\nand computational efficiency. Our code has been provided for review purposes\nand will be made publicly available upon acceptance.\n","authors":["Shaokai Wu","Yuxiang Lu","Wei Ji","Suizhi Huang","Fengyu Yang","Shalayiding Sirejiding","Qichen He","Jing Tong","Yanbiao Ji","Yue Ding","Hongtao Lu"],"pdf_url":"https://arxiv.org/pdf/2411.04844v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08564v1","updated":"2024-12-11T17:32:21Z","published":"2024-12-11T17:32:21Z","title":"Can We Generate Visual Programs Without Prompting LLMs?","summary":"  Visual programming prompts LLMs (large language mod-els) to generate\nexecutable code for visual tasks like visual question answering (VQA).\nPrompt-based methods are difficult to improve while also being unreliable and\ncostly in both time and money. Our goal is to develop an efficient visual\nprogramming system without 1) using prompt-based LLMs at inference time and 2)\na large set of program and answer annotations. We develop a synthetic data\naugmentation approach and alternative program generation method based on\ndecoupling programs into higher-level skills called templates and the\ncorresponding arguments. Our results show that with data augmentation,\nprompt-free smaller LLMs ($\\approx$ 1B parameters) are competitive with\nstate-of-the art models with the added benefit of much faster inference\n","authors":["Michal Shlapentokh-Rothman","Yu-Xiong Wang","Derek Hoiem"],"pdf_url":"https://arxiv.org/pdf/2412.08564v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08563v1","updated":"2024-12-11T17:31:17Z","published":"2024-12-11T17:31:17Z","title":"Physics Based Differentiable Rendering for Inverse Problems and Beyond","summary":"  Physics-based differentiable rendering (PBDR) has become an efficient method\nin computer vision, graphics, and machine learning for addressing an array of\ninverse problems. PBDR allows patterns to be generated from perceptions which\ncan be applied to enhance object attributes like geometry, substances, and\nlighting by adding physical models of light propagation and materials\ninteraction. Due to these capabilities, distinguished rendering has been\nemployed in a wider range of sectors such as autonomous navigation, scene\nreconstruction, and material design. We provide an extensive overview of PBDR\ntechniques in this study, emphasizing their creation, effectiveness, and\nlimitations while managing inverse situations. We demonstrate modern techniques\nand examine their value in everyday situations.\n","authors":["Preetish Kakkar","Srijani Mukherjee","Hariharan Ragothaman","Vishal Mehta"],"pdf_url":"https://arxiv.org/pdf/2412.08563v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10476v2","updated":"2024-12-11T17:09:02Z","published":"2024-04-16T11:38:44Z","title":"Enhanced Facial Feature Extraction and Recignation Using Optimal Fully\n  Dispersed Haar-like Filters","summary":"  Haar-like filters are renowned for their simplicity, speed, and accuracy in\nvarious computer vision tasks. This paper proposes a novel algorithm to\nidentify optimal fully dispersed Haar-like filters for enhanced facial feature\nextraction and recognation. Unlike traditional Haar-like filters, these novel\nfilters allow pixels to move freely within images, enabling more effictive\ncapture of intricate local features...\n","authors":["Zeinab Sedaghatjoo","Hossein Hosseinzadeh","Ahmad shirzadi"],"pdf_url":"https://arxiv.org/pdf/2404.10476v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08545v1","updated":"2024-12-11T17:00:51Z","published":"2024-12-11T17:00:51Z","title":"Improving Satellite Imagery Masking using Multi-task and Transfer\n  Learning","summary":"  Many remote sensing applications employ masking of pixels in satellite\nimagery for subsequent measurements. For example, estimating water quality\nvariables, such as Suspended Sediment Concentration (SSC) requires isolating\npixels depicting water bodies unaffected by clouds, their shadows, terrain\nshadows, and snow and ice formation. A significant bottleneck is the reliance\non a variety of data products (e.g., satellite imagery, elevation maps), and a\nlack of precision in individual steps affecting estimation accuracy. We propose\nto improve both the accuracy and computational efficiency of masking by\ndeveloping a system that predicts all required masks from Harmonized Landsat\nand Sentinel (HLS) imagery. Our model employs multi-tasking to share\ncomputation and enable higher accuracy across tasks. We experiment with recent\nadvances in deep network architectures and show that masking models can benefit\nfrom these, especially when combined with pre-training on large satellite\nimagery datasets. We present a collection of models offering different\nspeed/accuracy trade-offs for masking. MobileNet variants are the fastest, and\nperform competitively with larger architectures. Transformer-based\narchitectures are the slowest, but benefit the most from pre-training on large\nsatellite imagery datasets. Our models provide a 9% F1 score improvement\ncompared to previous work on water pixel identification. When integrated with\nan SSC estimation system, our models result in a 30x speedup while reducing\nestimation error by 2.64 mg/L, allowing for global-scale analysis. We also\nevaluate our model on a recently proposed cloud and cloud shadow estimation\nbenchmark, where we outperform the current state-of-the-art model by at least\n6% in F1 score.\n","authors":["Rangel Daroya","Luisa Vieira Lucchese","Travis Simmons","Punwath Prum","Tamlin Pavelsky","John Gardner","Colin J. Gleason","Subhransu Maji"],"pdf_url":"https://arxiv.org/pdf/2412.08545v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06140v2","updated":"2024-12-11T16:59:51Z","published":"2024-10-08T15:40:22Z","title":"Estimating the Number of HTTP/3 Responses in QUIC Using Deep Learning","summary":"  QUIC, a new and increasingly used transport protocol, enhances TCP by\noffering improved security, performance, and stream multiplexing. These\nfeatures, however, also impose challenges for network middle-boxes that need to\nmonitor and analyze web traffic. This paper proposes a novel method to estimate\nthe number of HTTP/3 responses in a given QUIC connection by an observer. This\nestimation reveals server behavior, client-server interactions, and data\ntransmission efficiency, which is crucial for various applications such as\ndesigning a load balancing solution and detecting HTTP/3 flood attacks. The\nproposed scheme transforms QUIC connection traces into image sequences and uses\nmachine learning (ML) models, guided by a tailored loss function, to predict\nresponse counts. Evaluations on more than seven million images-derived from\n100,000 traces collected across 44,000 websites over four months-achieve up to\n97% accuracy in both known and unknown server settings and 92% accuracy on\npreviously unseen complete QUIC traces.\n","authors":["Barak Gahtan","Robert J. Shahla","Reuven Cohen","Alex M. Bronstein"],"pdf_url":"https://arxiv.org/pdf/2410.06140v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2410.03728"},{"id":"http://arxiv.org/abs/2412.08536v1","updated":"2024-12-11T16:52:14Z","published":"2024-12-11T16:52:14Z","title":"SenCLIP: Enhancing zero-shot land-use mapping for Sentinel-2 with\n  ground-level prompting","summary":"  Pre-trained vision-language models (VLMs), such as CLIP, demonstrate\nimpressive zero-shot classification capabilities with free-form prompts and\neven show some generalization in specialized domains. However, their\nperformance on satellite imagery is limited due to the underrepresentation of\nsuch data in their training sets, which predominantly consist of ground-level\nimages. Existing prompting techniques for satellite imagery are often\nrestricted to generic phrases like a satellite image of ..., limiting their\neffectiveness for zero-shot land-use and land-cover (LULC) mapping. To address\nthese challenges, we introduce SenCLIP, which transfers CLIPs representation to\nSentinel-2 imagery by leveraging a large dataset of Sentinel-2 images paired\nwith geotagged ground-level photos from across Europe. We evaluate SenCLIP\nalongside other SOTA remote sensing VLMs on zero-shot LULC mapping tasks using\nthe EuroSAT and BigEarthNet datasets with both aerial and ground-level\nprompting styles. Our approach, which aligns ground-level representations with\nsatellite imagery, demonstrates significant improvements in classification\naccuracy across both prompt styles, opening new possibilities for applying\nfree-form textual descriptions in zero-shot LULC mapping.\n","authors":["Pallavi Jain","Dino Ienco","Roberto Interdonato","Tristan Berchoux","Diego Marcos"],"pdf_url":"https://arxiv.org/pdf/2412.08536v1.pdf","comment":"Accepted at WACV'25"},{"id":"http://arxiv.org/abs/2412.08524v1","updated":"2024-12-11T16:36:45Z","published":"2024-12-11T16:36:45Z","title":"Learning to Decouple the Lights for 3D Face Texture Modeling","summary":"  Existing research has made impressive strides in reconstructing human facial\nshapes and textures from images with well-illuminated faces and minimal\nexternal occlusions. Nevertheless, it remains challenging to recover accurate\nfacial textures from scenarios with complicated illumination affected by\nexternal occlusions, e.g. a face that is partially obscured by items such as a\nhat. Existing works based on the assumption of single and uniform illumination\ncannot correctly process these data. In this work, we introduce a novel\napproach to model 3D facial textures under such unnatural illumination. Instead\nof assuming single illumination, our framework learns to imitate the unnatural\nillumination as a composition of multiple separate light conditions combined\nwith learned neural representations, named Light Decoupling. According to\nexperiments on both single images and video sequences, we demonstrate the\neffectiveness of our approach in modeling facial textures under challenging\nillumination affected by occlusions. Please check\nhttps://tianxinhuang.github.io/projects/Deface for our videos and codes.\n","authors":["Tianxin Huang","Zhenyu Zhang","Ying Tai","Gim Hee Lee"],"pdf_url":"https://arxiv.org/pdf/2412.08524v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2407.09392v4","updated":"2024-12-11T16:27:18Z","published":"2024-07-12T16:16:24Z","title":"Open-Canopy: Towards Very High Resolution Forest Monitoring","summary":"  Estimating canopy height and its changes at meter resolution from satellite\nimagery is a significant challenge in computer vision with critical\nenvironmental applications. However, the lack of open-access datasets at this\nresolution hinders the reproducibility and evaluation of models. We introduce\nOpen-Canopy, the first open-access, country-scale benchmark for very\nhigh-resolution (1.5 m) canopy height estimation, covering over 87,000 km$^2$\nacross France with 1.5 m resolution satellite imagery and aerial LiDAR data.\nAdditionally, we present Open-Canopy-$\\Delta$, a benchmark for canopy height\nchange detection between images from different years at tree level-a\nchallenging task for current computer vision models. We evaluate\nstate-of-the-art architectures on these benchmarks, highlighting significant\nchallenges and opportunities for improvement. Our datasets and code are\npublicly available at https://github.com/fajwel/Open-Canopy.\n","authors":["Fajwel Fogel","Yohann Perron","Nikola Besic","Laurent Saint-André","Agnès Pellissier-Tanon","Martin Schwartz","Thomas Boudras","Ibrahim Fayad","Alexandre d'Aspremont","Loic Landrieu","Philippe Ciais"],"pdf_url":"https://arxiv.org/pdf/2407.09392v4.pdf","comment":"25 pages, 6+6 figures, Submitted to CVPR25"},{"id":"http://arxiv.org/abs/2412.03848v2","updated":"2024-12-11T16:26:09Z","published":"2024-12-05T03:31:48Z","title":"INRetouch: Context Aware Implicit Neural Representation for Photography\n  Retouching","summary":"  Professional photo editing remains challenging, requiring extensive knowledge\nof imaging pipelines and significant expertise. With the ubiquity of smartphone\nphotography, there is an increasing demand for accessible yet sophisticated\nimage editing solutions. While recent deep learning approaches, particularly\nstyle transfer methods, have attempted to automate this process, they often\nstruggle with output fidelity, editing control, and complex retouching\ncapabilities. We propose a novel retouch transfer approach that learns from\nprofessional edits through before-after image pairs, enabling precise\nreplication of complex editing operations. To facilitate this research\ndirection, we introduce a comprehensive Photo Retouching Dataset comprising\n100,000 high-quality images edited using over 170 professional Adobe Lightroom\npresets. We develop a context-aware Implicit Neural Representation that learns\nto apply edits adaptively based on image content and context, requiring no\npretraining and capable of learning from a single example. Our method extracts\nimplicit transformations from reference edits and adaptively applies them to\nnew images. Through extensive evaluation, we demonstrate that our approach not\nonly surpasses existing methods in photo retouching but also enhances\nperformance in related image reconstruction tasks like Gamut Mapping and Raw\nReconstruction. By bridging the gap between professional editing capabilities\nand automated solutions, our work presents a significant step toward making\nsophisticated photo editing more accessible while maintaining high-fidelity\nresults. Check the Project Page at https://omaralezaby.github.io/inretouch for\nmore Results and information about Code and Dataset availability.\n","authors":["Omar Elezabi","Marcos V. Conde","Zongwei Wu","Radu Timofte"],"pdf_url":"https://arxiv.org/pdf/2412.03848v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08511v1","updated":"2024-12-11T16:24:08Z","published":"2024-12-11T16:24:08Z","title":"Combining Neural Fields and Deformation Models for Non-Rigid 3D Motion\n  Reconstruction from Partial Data","summary":"  We introduce a novel, data-driven approach for reconstructing temporally\ncoherent 3D motion from unstructured and potentially partial observations of\nnon-rigidly deforming shapes. Our goal is to achieve high-fidelity motion\nreconstructions for shapes that undergo near-isometric deformations, such as\nhumans wearing loose clothing. The key novelty of our work lies in its ability\nto combine implicit shape representations with explicit mesh-based deformation\nmodels, enabling detailed and temporally coherent motion reconstructions\nwithout relying on parametric shape models or decoupling shape and motion. Each\nframe is represented as a neural field decoded from a feature space where\nobservations over time are fused, hence preserving geometric details present in\nthe input data. Temporal coherence is enforced with a near-isometric\ndeformation constraint between adjacent frames that applies to the underlying\nsurface in the neural field. Our method outperforms state-of-the-art\napproaches, as demonstrated by its application to human and animal motion\nsequences reconstructed from monocular depth videos.\n","authors":["Aymen Merrouche","Stefanie Wuhrer","Edmond Boyer"],"pdf_url":"https://arxiv.org/pdf/2412.08511v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01179v2","updated":"2024-12-11T16:19:47Z","published":"2024-09-02T11:19:54Z","title":"Recoverable Compression: A Multimodal Vision Token Recovery Mechanism\n  Guided by Text Information","summary":"  With the advancement of large-scale language modeling techniques, large\nmultimodal models combining visual encoders with large language models have\ndemonstrated exceptional performance in various visual tasks. Most of the\ncurrent large-scale multimodal models achieve this by mapping visual features\nobtained from the visual encoder into a large language model and using them as\ninputs alongside text for downstream tasks. Therefore, the number of visual\ntokens directly affects the training and inference speed of the model. There\nhas been significant work on token pruning for visual transformers, but for\nlarge multimodal models, only relying on visual information for token pruning\nor compression may lead to significant loss of important information. On the\nother hand, the textual input in the form of a question may contain valuable\ninformation that can aid in answering the question, providing additional\nknowledge to the model. To address the potential oversimplification and\nexcessive pruning that can occur with most purely visual token pruning methods,\nwe propose a text information-guided dynamic visual token recovery mechanism\nthat does not require training. This mechanism leverages the similarity between\nthe question text and visual tokens to recover visually meaningful tokens with\nimportant text information while merging other less important tokens.\nExperimental results demonstrate that our proposed method achieves comparable\nperformance to the original approach while compressing the visual tokens to an\naverage of 10% of the original quantity. Our source code will be made publicly\navailable following acceptance.\n","authors":["Yi Chen","Jian Xu","Xu-Yao Zhang","Wen-Zhuo Liu","Yang-Yang Liu","Cheng-Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2409.01179v2.pdf","comment":"AAAI2025 Accepted"},{"id":"http://arxiv.org/abs/2412.08506v1","updated":"2024-12-11T16:18:17Z","published":"2024-12-11T16:18:17Z","title":"Orchestrating the Symphony of Prompt Distribution Learning for\n  Human-Object Interaction Detection","summary":"  Human-object interaction (HOI) detectors with popular query-transformer\narchitecture have achieved promising performance. However, accurately\nidentifying uncommon visual patterns and distinguishing between ambiguous HOIs\ncontinue to be difficult for them. We observe that these difficulties may arise\nfrom the limited capacity of traditional detector queries in representing\ndiverse intra-category patterns and inter-category dependencies. To address\nthis, we introduce the Interaction Prompt Distribution Learning (InterProDa)\napproach. InterProDa learns multiple sets of soft prompts and estimates\ncategory distributions from various prompts. It then incorporates HOI queries\nwith category distributions, making them capable of representing near-infinite\nintra-category dynamics and universal cross-category relationships. Our\nInterProDa detector demonstrates competitive performance on HICO-DET and vcoco\nbenchmarks. Additionally, our method can be integrated into most\ntransformer-based HOI detectors, significantly enhancing their performance with\nminimal additional parameters.\n","authors":["Mingda Jia","Liming Zhao","Ge Li","Yun Zheng"],"pdf_url":"https://arxiv.org/pdf/2412.08506v1.pdf","comment":"in Proceedings of the 39th AAAI Conference on Artificial Intelligence\n  (AAAI-25)"},{"id":"http://arxiv.org/abs/2312.08977v4","updated":"2024-12-11T16:18:16Z","published":"2023-12-14T14:26:57Z","title":"Weighted Ensemble Models Are Strong Continual Learners","summary":"  In this work, we study the problem of continual learning (CL) where the goal\nis to learn a model on a sequence of tasks, such that the data from the\nprevious tasks becomes unavailable while learning on the current task data. CL\nis essentially a balancing act between being able to learn on the new task\n(i.e., plasticity) and maintaining the performance on the previously learned\nconcepts (i.e., stability). Intending to address the stability-plasticity\ntrade-off, we propose to perform weight-ensembling of the model parameters of\nthe previous and current tasks. This weighted-ensembled model, which we call\nContinual Model Averaging (or CoMA), attains high accuracy on the current task\nby leveraging plasticity, while not deviating too far from the previous weight\nconfiguration, ensuring stability. We also propose an improved variant of CoMA,\nnamed Continual Fisher-weighted Model Averaging (or CoFiMA), that selectively\nweighs each parameter in the weights ensemble by leveraging the Fisher\ninformation of the weights of the model. Both variants are conceptually simple,\neasy to implement, and effective in attaining state-of-the-art performance on\nseveral standard CL benchmarks. Code is available at:\nhttps://github.com/IemProg/CoFiMA.\n","authors":["Imad Eddine Marouf","Subhankar Roy","Enzo Tartaglione","Stéphane Lathuilière"],"pdf_url":"https://arxiv.org/pdf/2312.08977v4.pdf","comment":"Accepted for ECCV2024, Code: https://github.com/IemProg/CoFiMA"},{"id":"http://arxiv.org/abs/2412.08503v1","updated":"2024-12-11T16:13:23Z","published":"2024-12-11T16:13:23Z","title":"StyleStudio: Text-Driven Style Transfer with Selective Control of Style\n  Elements","summary":"  Text-driven style transfer aims to merge the style of a reference image with\ncontent described by a text prompt. Recent advancements in text-to-image models\nhave improved the nuance of style transformations, yet significant challenges\nremain, particularly with overfitting to reference styles, limiting stylistic\ncontrol, and misaligning with textual content. In this paper, we propose three\ncomplementary strategies to address these issues. First, we introduce a\ncross-modal Adaptive Instance Normalization (AdaIN) mechanism for better\nintegration of style and text features, enhancing alignment. Second, we develop\na Style-based Classifier-Free Guidance (SCFG) approach that enables selective\ncontrol over stylistic elements, reducing irrelevant influences. Finally, we\nincorporate a teacher model during early generation stages to stabilize spatial\nlayouts and mitigate artifacts. Our extensive evaluations demonstrate\nsignificant improvements in style transfer quality and alignment with textual\nprompts. Furthermore, our approach can be integrated into existing style\ntransfer frameworks without fine-tuning.\n","authors":["Mingkun Lei","Xue Song","Beier Zhu","Hao Wang","Chi Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.08503v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2108.10201v3","updated":"2024-12-11T16:03:31Z","published":"2021-08-23T14:37:58Z","title":"Diverse Similarity Encoder for Deep GAN Inversion","summary":"  Current deep generative adversarial networks (GANs) can synthesize\nhigh-quality (HQ) images, so learning representation with GANs is favorable.\nGAN inversion is one of emerging approaches that study how to invert images\ninto latent space. Existing GAN encoders can invert images on StyleGAN, but\ncannot adapt to other deep GANs. We propose a novel approach to address this\nissue. By evaluating diverse similarity in latent vectors and images, we design\nan adaptive encoder, named diverse similarity encoder (DSE), that can be\nexpanded to a variety of state-of-the-art GANs. DSE makes GANs reconstruct\nhigher fidelity images from HQ images, no matter whether they are synthesized\nor real images. DSE has unified convolutional blocks and adapts well to\nmainstream deep GANs, e.g., PGGAN, StyleGAN, and BigGAN.\n","authors":["Cheng Yu","Wenmin Wang","Roberto Bugiolacchi"],"pdf_url":"https://arxiv.org/pdf/2108.10201v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08489v1","updated":"2024-12-11T15:53:13Z","published":"2024-12-11T15:53:13Z","title":"A Dual-Module Denoising Approach with Curriculum Learning for Enhancing\n  Multimodal Aspect-Based Sentiment Analysis","summary":"  Multimodal Aspect-Based Sentiment Analysis (MABSA) combines text and images\nto perform sentiment analysis but often struggles with irrelevant or misleading\nvisual information. Existing methodologies typically address either\nsentence-image denoising or aspect-image denoising but fail to comprehensively\ntackle both types of noise. To address these limitations, we propose DualDe, a\nnovel approach comprising two distinct components: the Hybrid Curriculum\nDenoising Module (HCD) and the Aspect-Enhance Denoising Module (AED). The HCD\nmodule enhances sentence-image denoising by incorporating a flexible curriculum\nlearning strategy that prioritizes training on clean data. Concurrently, the\nAED module mitigates aspect-image noise through an aspect-guided attention\nmechanism that filters out noisy visual regions which unrelated to the specific\naspects of interest. Our approach demonstrates effectiveness in addressing both\nsentence-image and aspect-image noise, as evidenced by experimental evaluations\non benchmark datasets.\n","authors":["Nguyen Van Doan","Dat Tran Nguyen","Cam-Van Thi Nguyen"],"pdf_url":"https://arxiv.org/pdf/2412.08489v1.pdf","comment":"Accepted at PACLIC 2024"},{"id":"http://arxiv.org/abs/2407.03771v3","updated":"2024-12-11T15:52:12Z","published":"2024-07-04T09:32:12Z","title":"SpikeGS: Reconstruct 3D scene via fast-moving bio-inspired sensors","summary":"  3D Gaussian Splatting (3DGS) demonstrates unparalleled superior performance\nin 3D scene reconstruction. However, 3DGS heavily relies on the sharp images.\nFulfilling this requirement can be challenging in real-world scenarios\nespecially when the camera moves fast, which severely limits the application of\n3DGS. To address these challenges, we proposed Spike Gausian Splatting\n(SpikeGS), the first framework that integrates the spike streams into 3DGS\npipeline to reconstruct 3D scenes via a fast-moving bio-inspired camera. With\naccumulation rasterization, interval supervision, and a specially designed\npipeline, SpikeGS extracts detailed geometry and texture from high temporal\nresolution but texture lacking spike stream, reconstructs 3D scenes captured in\n1 second. Extensive experiments on multiple synthetic and real-world datasets\ndemonstrate the superiority of SpikeGS compared with existing spike-based and\ndeblur 3D scene reconstruction methods. Codes and data will be released soon.\n","authors":["Yijia Guo","Liwen Hu","Yuanxi Bai","Lei Ma","Tiejun Huang"],"pdf_url":"https://arxiv.org/pdf/2407.03771v3.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.08486v1","updated":"2024-12-11T15:51:14Z","published":"2024-12-11T15:51:14Z","title":"Learning Flow Fields in Attention for Controllable Person Image\n  Generation","summary":"  Controllable person image generation aims to generate a person image\nconditioned on reference images, allowing precise control over the person's\nappearance or pose. However, prior methods often distort fine-grained textural\ndetails from the reference image, despite achieving high overall image quality.\nWe attribute these distortions to inadequate attention to corresponding regions\nin the reference image. To address this, we thereby propose learning flow\nfields in attention (Leffa), which explicitly guides the target query to attend\nto the correct reference key in the attention layer during training.\nSpecifically, it is realized via a regularization loss on top of the attention\nmap within a diffusion-based baseline. Our extensive experiments show that\nLeffa achieves state-of-the-art performance in controlling appearance (virtual\ntry-on) and pose (pose transfer), significantly reducing fine-grained detail\ndistortion while maintaining high image quality. Additionally, we show that our\nloss is model-agnostic and can be used to improve the performance of other\ndiffusion models.\n","authors":["Zijian Zhou","Shikun Liu","Xiao Han","Haozhe Liu","Kam Woh Ng","Tian Xie","Yuren Cong","Hang Li","Mengmeng Xu","Juan-Manuel Pérez-Rúa","Aditya Patel","Tao Xiang","Miaojing Shi","Sen He"],"pdf_url":"https://arxiv.org/pdf/2412.08486v1.pdf","comment":"github: https://github.com/franciszzj/Leffa, demo:\n  https://huggingface.co/spaces/franciszzj/Leffa, model:\n  https://huggingface.co/franciszzj/Leffa"},{"id":"http://arxiv.org/abs/2412.08484v1","updated":"2024-12-11T15:48:25Z","published":"2024-12-11T15:48:25Z","title":"ConvMesh: Reimagining Mesh Quality Through Convex Optimization","summary":"  Mesh generation has become a critical topic in recent years, forming the\nfoundation of all 3D objects used across various applications, such as virtual\nreality, gaming, and 3D printing. With advancements in computational resources\nand machine learning, neural networks have emerged as powerful tools for\ngenerating high-quality 3D object representations, enabling accurate scene and\nobject reconstructions. Despite these advancements, many methods produce meshes\nthat lack realism or exhibit geometric and textural flaws, necessitating\nadditional processing to improve their quality. This research introduces a\nconvex optimization programming called disciplined convex programming to\nenhance existing meshes by refining their texture and geometry with a conic\nsolver. By focusing on a sparse set of point clouds from both the original and\ntarget meshes, this method demonstrates significant improvements in mesh\nquality with minimal data requirements. To evaluate the approach, the classical\ndolphin mesh dataset from Facebook AI was used as a case study, with\noptimization performed using the CVXPY library. The results reveal promising\npotential for streamlined and effective mesh refinement.\n","authors":["Alexander Valverde"],"pdf_url":"https://arxiv.org/pdf/2412.08484v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2412.08482v1","updated":"2024-12-11T15:47:54Z","published":"2024-12-11T15:47:54Z","title":"SAM-Mamba: Mamba Guided SAM Architecture for Generalized Zero-Shot Polyp\n  Segmentation","summary":"  Polyp segmentation in colonoscopy is crucial for detecting colorectal cancer.\nHowever, it is challenging due to variations in the structure, color, and size\nof polyps, as well as the lack of clear boundaries with surrounding tissues.\nTraditional segmentation models based on Convolutional Neural Networks (CNNs)\nstruggle to capture detailed patterns and global context, limiting their\nperformance. Vision Transformer (ViT)-based models address some of these issues\nbut have difficulties in capturing local context and lack strong zero-shot\ngeneralization. To this end, we propose the Mamba-guided Segment Anything Model\n(SAM-Mamba) for efficient polyp segmentation. Our approach introduces a\nMamba-Prior module in the encoder to bridge the gap between the general\npre-trained representation of SAM and polyp-relevant trivial clues. It injects\nsalient cues of polyp images into the SAM image encoder as a domain prior while\ncapturing global dependencies at various scales, leading to more accurate\nsegmentation results. Extensive experiments on five benchmark datasets show\nthat SAM-Mamba outperforms traditional CNN, ViT, and Adapter-based models in\nboth quantitative and qualitative measures. Additionally, SAM-Mamba\ndemonstrates excellent adaptability to unseen datasets, making it highly\nsuitable for real-time clinical use.\n","authors":["Tapas Kumar Dutta","Snehashis Majhi","Deepak Ranjan Nayak","Debesh Jha"],"pdf_url":"https://arxiv.org/pdf/2412.08482v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08480v1","updated":"2024-12-11T15:47:11Z","published":"2024-12-11T15:47:11Z","title":"InvDiff: Invariant Guidance for Bias Mitigation in Diffusion Models","summary":"  As one of the most successful generative models, diffusion models have\ndemonstrated remarkable efficacy in synthesizing high-quality images. These\nmodels learn the underlying high-dimensional data distribution in an\nunsupervised manner. Despite their success, diffusion models are highly\ndata-driven and prone to inheriting the imbalances and biases present in\nreal-world data. Some studies have attempted to address these issues by\ndesigning text prompts for known biases or using bias labels to construct\nunbiased data. While these methods have shown improved results, real-world\nscenarios often contain various unknown biases, and obtaining bias labels is\nparticularly challenging. In this paper, we emphasize the necessity of\nmitigating bias in pre-trained diffusion models without relying on auxiliary\nbias annotations. To tackle this problem, we propose a framework, InvDiff,\nwhich aims to learn invariant semantic information for diffusion guidance.\nSpecifically, we propose identifying underlying biases in the training data and\ndesigning a novel debiasing training objective. Then, we employ a lightweight\ntrainable module that automatically preserves invariant semantic information\nand uses it to guide the diffusion model's sampling process toward unbiased\noutcomes simultaneously. Notably, we only need to learn a small number of\nparameters in the lightweight learnable module without altering the pre-trained\ndiffusion model. Furthermore, we provide a theoretical guarantee that the\nimplementation of InvDiff is equivalent to reducing the error upper bound of\ngeneralization. Extensive experimental results on three publicly available\nbenchmarks demonstrate that InvDiff effectively reduces biases while\nmaintaining the quality of image generation. Our code is available at\nhttps://github.com/Hundredl/InvDiff.\n","authors":["Min Hou","Yueying Wu","Chang Xu","Yu-Hao Huang","Chenxi Bai","Le Wu","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2412.08480v1.pdf","comment":"KDD 2025"},{"id":"http://arxiv.org/abs/2412.08479v1","updated":"2024-12-11T15:47:01Z","published":"2024-12-11T15:47:01Z","title":"CAT: Class Aware Adaptive Thresholding for Semi-Supervised Domain\n  Generalization","summary":"  Domain Generalization (DG) seeks to transfer knowledge from multiple source\ndomains to unseen target domains, even in the presence of domain shifts.\nAchieving effective generalization typically requires a large and diverse set\nof labeled source data to learn robust representations that can generalize to\nnew, unseen domains. However, obtaining such high-quality labeled data is often\ncostly and labor-intensive, limiting the practical applicability of DG. To\naddress this, we investigate a more practical and challenging problem:\nsemi-supervised domain generalization (SSDG) under a label-efficient paradigm.\nIn this paper, we propose a novel method, CAT, which leverages semi-supervised\nlearning with limited labeled data to achieve competitive generalization\nperformance under domain shifts. Our method addresses key limitations of\nprevious approaches, such as reliance on fixed thresholds and sensitivity to\nnoisy pseudo-labels. CAT combines adaptive thresholding with noisy label\nrefinement techniques, creating a straightforward yet highly effective solution\nfor SSDG tasks. Specifically, our approach uses flexible thresholding to\ngenerate high-quality pseudo-labels with higher class diversity while refining\nnoisy pseudo-labels to improve their reliability. Extensive experiments across\nmultiple benchmark datasets demonstrate the superior performance of our method,\nhighlighting its effectiveness in achieving robust generalization under domain\nshift.\n","authors":["Sumaiya Zoha","Jeong-Gun Lee","Young-Woong Ko"],"pdf_url":"https://arxiv.org/pdf/2412.08479v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2412.08477v1","updated":"2024-12-11T15:44:08Z","published":"2024-12-11T15:44:08Z","title":"Accurate Water Level Monitoring in AWD Rice Cultivation Using\n  Convolutional Neural Networks","summary":"  The Alternate Wetting and Drying (AWD) method is a rice-growing water\nmanagement technique promoted as a sustainable alternative to Continuous\nFlooding (CF). Climate change has placed the agricultural sector in a\nchallenging position, particularly as global water resources become\nincreasingly scarce, affecting rice production on irrigated lowlands. Rice, a\nstaple food for over half of the world's population, demands significantly more\nwater than other major crops. In Bangladesh, \\textit{Boro} rice, in particular,\nrequires considerable water inputs during its cultivation. Traditionally,\nfarmers manually measure water levels, a process that is both time-consuming\nand prone to errors. While ultrasonic sensors offer improvements in water\nheight measurement, they still face limitations, such as susceptibility to\nweather conditions and environmental factors. To address these issues, we\npropose a novel approach that automates water height measurement using computer\nvision, specifically through a convolutional neural network (CNN). Our\nattention-based architecture achieved an $R^2$ score of 0.9885 and a Mean\nSquared Error (MSE) of 0.2766, providing a more accurate and efficient solution\nfor managing AWD systems.\n","authors":["Ahmed Rafi Hasan","Niloy Kumar Kundu","Saad Hasan","Mohammad Rashedul Hoque","Swakkhar Shatabda"],"pdf_url":"https://arxiv.org/pdf/2412.08477v1.pdf","comment":"15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.08468v1","updated":"2024-12-11T15:33:35Z","published":"2024-12-11T15:33:35Z","title":"Multi-GraspLLM: A Multimodal LLM for Multi-Hand Semantic Guided Grasp\n  Generation","summary":"  Multi-hand semantic grasp generation aims to generate feasible and\nsemantically appropriate grasp poses for different robotic hands based on\nnatural language instructions. Although the task is highly valuable, due to the\nlack of multi-hand grasp datasets with fine-grained contact description between\nrobotic hands and objects, it is still a long-standing difficult task. In this\npaper, we present Multi-GraspSet, the first large-scale multi-hand grasp\ndataset with automatically contact annotations. Based on Multi-GraspSet, we\npropose Multi-GraspLLM, a unified language-guided grasp generation framework.\nIt leverages large language models (LLM) to handle variable-length sequences,\ngenerating grasp poses for diverse robotic hands in a single unified\narchitecture. Multi-GraspLLM first aligns the encoded point cloud features and\ntext features into a unified semantic space. It then generates grasp bin tokens\nwhich are subsequently converted into grasp pose for each robotic hand via\nhand-aware linear mapping. The experimental results demonstrate that our\napproach significantly outperforms existing methods on Multi-GraspSet. More\ninformation can be found on our project page https://multi-graspllm.github.io.\n","authors":["Haosheng Li","Weixin Mao","Weipeng Deng","Chenyu Meng","Haoqiang Fan","Tiancai Wang","Ping Tan","Hongan Wang","Xiaoming Deng"],"pdf_url":"https://arxiv.org/pdf/2412.08468v1.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.08467v1","updated":"2024-12-11T15:32:24Z","published":"2024-12-11T15:32:24Z","title":"Bootstrapping Language-Guided Navigation Learning with Self-Refining\n  Data Flywheel","summary":"  Creating high-quality data for training robust language-instructed agents is\na long-lasting challenge in embodied AI. In this paper, we introduce a\nSelf-Refining Data Flywheel (SRDF) that generates high-quality and large-scale\nnavigational instruction-trajectory pairs by iteratively refining the data pool\nthrough the collaboration between two models, the instruction generator and the\nnavigator, without any human-in-the-loop annotation. Specifically, SRDF starts\nwith using a base generator to create an initial data pool for training a base\nnavigator, followed by applying the trained navigator to filter the data pool.\nThis leads to higher-fidelity data to train a better generator, which can, in\nturn, produce higher-quality data for training the next-round navigator. Such a\nflywheel establishes a data self-refining process, yielding a continuously\nimproved and highly effective dataset for large-scale language-guided\nnavigation learning. Our experiments demonstrate that after several flywheel\nrounds, the navigator elevates the performance boundary from 70% to 78% SPL on\nthe classic R2R test set, surpassing human performance (76%) for the first\ntime. Meanwhile, this process results in a superior generator, evidenced by a\nSPICE increase from 23.5 to 26.2, better than all previous VLN instruction\ngeneration methods. Finally, we demonstrate the scalability of our method\nthrough increasing environment and instruction diversity, and the\ngeneralization ability of our pre-trained navigator across various downstream\nnavigation tasks, surpassing state-of-the-art methods by a large margin in all\ncases.\n","authors":["Zun Wang","Jialu Li","Yicong Hong","Songze Li","Kunchang Li","Shoubin Yu","Yi Wang","Yu Qiao","Yali Wang","Mohit Bansal","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2412.08467v1.pdf","comment":"28 pages, Code and data are available at\n  https://github.com/wz0919/VLN-SRDF"},{"id":"http://arxiv.org/abs/2412.08464v1","updated":"2024-12-11T15:30:06Z","published":"2024-12-11T15:30:06Z","title":"CC-Diff: Enhancing Contextual Coherence in Remote Sensing Image\n  Synthesis","summary":"  Accurately depicting real-world landscapes in remote sensing (RS) images\nrequires precise alignment between objects and their environment. However, most\nexisting synthesis methods for natural images prioritize foreground control,\noften reducing the background to plain textures. This neglects the interaction\nbetween foreground and background, which can lead to incoherence in RS\nscenarios. In this paper, we introduce CC-Diff, a Diffusion Model-based\napproach for RS image generation with enhanced Context Coherence. To capture\nspatial interdependence, we propose a sequential pipeline where background\ngeneration is conditioned on synthesized foreground instances. Distinct\nlearnable queries are also employed to model both the complex background\ntexture and its semantic relation to the foreground. Extensive experiments\ndemonstrate that CC-Diff outperforms state-of-the-art methods in visual\nfidelity, semantic accuracy, and positional precision, excelling in both RS and\nnatural image domains. CC-Diff also shows strong trainability, improving\ndetection accuracy by 2.04 mAP on DOTA and 2.25 mAP on the COCO benchmark.\n","authors":["Mu Zhang","Yunfan Liu","Yue Liu","Hongtian Yu","Qixiang Ye"],"pdf_url":"https://arxiv.org/pdf/2412.08464v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.00477v2","updated":"2024-12-11T15:26:17Z","published":"2024-11-30T13:29:36Z","title":"LineGS : 3D Line Segment Representation on 3D Gaussian Splatting","summary":"  Abstract representations of 3D scenes play a crucial role in computer vision,\nenabling a wide range of applications such as mapping, localization, surface\nreconstruction, and even advanced tasks like SLAM and rendering. Among these\nrepresentations, line segments are widely used because of their ability to\nsuccinctly capture the structural features of a scene. However, existing 3D\nreconstruction methods often face significant challenges. Methods relying on 2D\nprojections suffer from instability caused by errors in multi-view matching and\nocclusions, while direct 3D approaches are hampered by noise and sparsity in 3D\npoint cloud data. This paper introduces LineGS, a novel method that combines\ngeometry-guided 3D line reconstruction with a 3D Gaussian splatting model to\naddress these challenges and improve representation ability. The method\nleverages the high-density Gaussian point distributions along the edge of the\nscene to refine and optimize initial line segments generated from traditional\ngeometric approaches. By aligning these segments with the underlying geometric\nfeatures of the scene, LineGS achieves a more precise and reliable\nrepresentation of 3D structures. The results show significant improvements in\nboth geometric accuracy and model compactness compared to baseline methods.\n","authors":["Chenggang Yang","Yuang Shi","Wei Tsang Ooi"],"pdf_url":"https://arxiv.org/pdf/2412.00477v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11118v2","updated":"2024-12-11T15:14:55Z","published":"2024-04-17T07:06:22Z","title":"FastFace: Fast-converging Scheduler for Large-scale Face Recognition\n  Training with One GPU","summary":"  Computing power has evolved into a foundational and indispensable resource in\nthe area of deep learning, particularly in tasks such as Face Recognition (FR)\nmodel training on large-scale datasets, where multiple GPUs are often a\nnecessity. Recognizing this challenge, some FR methods have started exploring\nways to compress the fully-connected layer in FR models. Unlike other\napproaches, our observations reveal that without prompt scheduling of the\nlearning rate (LR) during FR model training, the loss curve tends to exhibit\nnumerous stationary subsequences. To address this issue, we introduce a novel\nLR scheduler leveraging Exponential Moving Average (EMA) and Haar Convolutional\nKernel (HCK) to eliminate stationary subsequences, resulting in a significant\nreduction in converging time. However, the proposed scheduler incurs a\nconsiderable computational overhead due to its time complexity. To overcome\nthis limitation, we propose FastFace, a fast-converging scheduler with\nnegligible time complexity, i.e. O(1) per iteration, during training. In\npractice, FastFace is able to accelerate FR model training to a quarter of its\noriginal time without sacrificing more than 1% accuracy, making large-scale FR\ntraining feasible even with just one single GPU in terms of both time and space\ncomplexity. Extensive experiments validate the efficiency and effectiveness of\nFastFace. The code is publicly available at:\nhttps://github.com/amoonfana/FastFace\n","authors":["Xueyuan Gong","Zhiquan Liu","Yain-Whar Si","Xiaochen Yuan","Ke Wang","Xiaoxiang Liu","Cong Lin","Xinyuan Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.11118v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.00777v2","updated":"2024-12-11T15:11:09Z","published":"2024-12-01T11:48:58Z","title":"Local vs. Global: Local Land-Use and Land-Cover Models Deliver Higher\n  Quality Maps","summary":"  In 2023, 58.0% of the African population experienced moderate to severe food\ninsecurity, with 21.6% facing severe food insecurity. Land-use and land-cover\nmaps provide crucial insights for addressing food insecurity by improving\nagricultural efforts, including mapping and monitoring crop types and\nestimating yield. The development of global land-cover maps has been\nfacilitated by the increasing availability of earth observation data and\nadvancements in geospatial machine learning. However, these global maps exhibit\nlower accuracy and inconsistencies in Africa, partly due to the lack of\nrepresentative training data. To address this issue, we propose a data-centric\nframework with a teacher-student model setup, which uses diverse data sources\nof satellite images and label examples to produce local land-cover maps. Our\nmethod trains a high-resolution teacher model on images with a resolution of\n0.331 m/pixel and a low-resolution student model on publicly available images\nwith a resolution of 10 m/pixel. The student model also utilizes the teacher\nmodel's output as its weak label examples through knowledge transfer. We\nevaluated our framework using Murang'a county in Kenya, renowned for its\nagricultural productivity, as a use case. Our local models achieved higher\nquality maps, with improvements of 0.14 in the F1 score and 0.21 in\nIntersection-over-Union, compared to the best global model. Our evaluation also\nrevealed inconsistencies in existing global maps, with a maximum agreement rate\nof 0.30 among themselves. Our work provides valuable guidance to\ndecision-makers for driving informed decisions to enhance food security.\n","authors":["Girmaw Abebe Tadesse","Caleb Robinson","Charles Mwangi","Esther Maina","Joshua Nyakundi","Luana Marotti","Gilles Quentin Hacheme","Hamed Alemohammad","Rahul Dodhia","Juan M. Lavista Ferres"],"pdf_url":"https://arxiv.org/pdf/2412.00777v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08443v1","updated":"2024-12-11T15:08:25Z","published":"2024-12-11T15:08:25Z","title":"POINTS1.5: Building a Vision-Language Model towards Real World\n  Applications","summary":"  Vision-language models have made significant strides recently, demonstrating\nsuperior performance across a range of tasks, e.g. optical character\nrecognition and complex diagram analysis. Building on this trend, we introduce\na new vision-language model, POINTS1.5, designed to excel in various real-world\napplications. POINTS1.5 is an enhancement of POINTS1.0 and incorporates several\nkey innovations: i) We replace the original CLIP vision encoder, which had a\nfixed image resolution, with a NaViT-style vision encoder that supports native\ndynamic high resolution. This allows POINTS1.5 to process images of any\nresolution without needing to split them into tiles. ii) We add bilingual\nsupport to POINTS1.5, significantly enhancing its capability in Chinese. Due to\nthe scarcity of open-source Chinese datasets for vision-language models, we\ncollect numerous images from the Internet and annotate them using a combination\nof manual and automatic methods. iii) We propose a set of rigorous filtering\nmethods for visual instruction tuning datasets. We comprehensively evaluate all\nthese filtering methods, and choose the most effective ones to obtain the final\nvisual instruction tuning set. Thanks to these innovations, POINTS1.5\nsignificantly outperforms POINTS1.0 and demonstrates strong performance across\na range of real-world applications. Notably, POINTS1.5-7B is trained on fewer\nthan 4 billion tokens and ranks first on the OpenCompass leaderboard among\nmodels with fewer than 10 billion parameters\n","authors":["Yuan Liu","Le Tian","Xiao Zhou","Xinyu Gao","Kavio Yu","Yang Yu","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.08443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08441v1","updated":"2024-12-11T15:03:27Z","published":"2024-12-11T15:03:27Z","title":"Dynamic Disentangled Fusion Network for RGBT Tracking","summary":"  RGBT tracking usually suffers from various challenging factors of low\nresolution, similar appearance, extreme illumination, thermal crossover and\nocclusion, to name a few. Existing works often study complex fusion models to\nhandle challenging scenarios, but can not well adapt to various challenges,\nwhich might limit tracking performance. To handle this problem, we propose a\nnovel Dynamic Disentangled Fusion Network called DDFNet, which disentangles the\nfusion process into several dynamic fusion models via the challenge attributes\nto adapt to various challenging scenarios, for robust RGBT tracking. In\nparticular, we design six attribute-based fusion models to integrate RGB and\nthermal features under the six challenging scenarios respectively.Since each\nfusion model is to deal with the corresponding challenges, such disentangled\nfusion scheme could increase the fusion capacity without the dependence on\nlarge-scale training data. Considering that every challenging scenario also has\ndifferent levels of difficulty, we propose to optimize the combination of\nmultiple fusion units to form each attribute-based fusion model in a dynamic\nmanner, which could well adapt to the difficulty of the corresponding\nchallenging scenario. To address the issue that which fusion models should be\nactivated in the tracking process, we design an adaptive aggregation fusion\nmodule to integrate all features from attribute-based fusion models in an\nadaptive manner with a three-stage training algorithm. In addition, we design\nan enhancement fusion module to further strengthen the aggregated feature and\nmodality-specific features. Experimental results on benchmark datasets\ndemonstrate the effectiveness of our DDFNet against other state-of-the-art\nmethods.\n","authors":["Chenglong Li","Tao Wang","Zhaodong Ding","Yun Xiao","Jin Tang"],"pdf_url":"https://arxiv.org/pdf/2412.08441v1.pdf","comment":"14 pages,14 figures"},{"id":"http://arxiv.org/abs/2408.13928v2","updated":"2024-12-11T15:00:00Z","published":"2024-08-25T20:09:46Z","title":"GeoPlant: Spatial Plant Species Prediction Dataset","summary":"  The difficulty of monitoring biodiversity at fine scales and over large areas\nlimits ecological knowledge and conservation efforts. To fill this gap, Species\nDistribution Models (SDMs) predict species across space from spatially explicit\nfeatures. Yet, they face the challenge of integrating the rich but\nheterogeneous data made available over the past decade, notably millions of\nopportunistic species observations and standardized surveys, as well as\nmultimodal remote sensing data. In light of that, we have designed and\ndeveloped a new European-scale dataset for SDMs at high spatial resolution\n(10--50m), including more than 10k species (i.e., most of the European flora).\nThe dataset comprises 5M heterogeneous Presence-Only records and 90k exhaustive\nPresence-Absence survey records, all accompanied by diverse environmental\nrasters (e.g., elevation, human footprint, and soil) traditionally used in\nSDMs. In addition, it provides Sentinel-2 RGB and NIR satellite images with 10\nm resolution, a 20-year time series of climatic variables, and satellite time\nseries from the Landsat program. In addition to the data, we provide an openly\naccessible SDM benchmark (hosted on Kaggle), which has already attracted an\nactive community and a set of strong baselines for single predictor/modality\nand multimodal approaches. All resources, e.g., the dataset, pre-trained\nmodels, and baseline methods (in the form of notebooks), are available on\nKaggle, allowing one to start with our dataset literally with two mouse clicks.\n","authors":["Lukas Picek","Christophe Botella","Maximilien Servajean","César Leblanc","Rémi Palard","Théo Larcher","Benjamin Deneu","Diego Marcos","Pierre Bonnet","Alexis Joly"],"pdf_url":"https://arxiv.org/pdf/2408.13928v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.11759v2","updated":"2024-12-11T14:47:01Z","published":"2022-07-24T15:13:45Z","title":"Spatial-Temporal Federated Learning for Lifelong Person\n  Re-identification on Distributed Edges","summary":"  Data drift is a thorny challenge when deploying person re-identification\n(ReID) models into real-world devices, where the data distribution is\nsignificantly different from that of the training environment and keeps\nchanging. To tackle this issue, we propose a federated spatial-temporal\nincremental learning approach, named FedSTIL, which leverages both lifelong\nlearning and federated learning to continuously optimize models deployed on\nmany distributed edge clients. Unlike previous efforts, FedSTIL aims to mine\nspatial-temporal correlations among the knowledge learnt from different edge\nclients. Specifically, the edge clients first periodically extract general\nrepresentations of drifted data to optimize their local models. Then, the\nlearnt knowledge from edge clients will be aggregated by centralized parameter\nserver, where the knowledge will be selectively and attentively distilled from\nspatial- and temporal-dimension with carefully designed mechanisms. Finally,\nthe distilled informative spatial-temporal knowledge will be sent back to\ncorrelated edge clients to further improve the recognition accuracy of each\nedge client with a lifelong learning method. Extensive experiments on a mixture\nof five real-world datasets demonstrate that our method outperforms others by\nnearly 4% in Rank-1 accuracy, while reducing communication cost by 62%. All\nimplementation codes are publicly available on\nhttps://github.com/MSNLAB/Federated-Lifelong-Person-ReID\n","authors":["Lei Zhang","Guanyu Gao","Huaizheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2207.11759v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.07782v3","updated":"2024-12-11T14:46:02Z","published":"2024-01-15T15:43:56Z","title":"Exploring Masked Autoencoders for Sensor-Agnostic Image Retrieval in\n  Remote Sensing","summary":"  Self-supervised learning through masked autoencoders (MAEs) has recently\nattracted great attention for remote sensing (RS) image representation\nlearning, and thus embodies a significant potential for content-based image\nretrieval (CBIR) from ever-growing RS image archives. However, the existing MAE\nbased CBIR studies in RS assume that the considered RS images are acquired by a\nsingle image sensor, and thus are only suitable for uni-modal CBIR problems.\nThe effectiveness of MAEs for cross-sensor CBIR, which aims to search\nsemantically similar images across different image modalities, has not been\nexplored yet. In this paper, we take the first step to explore the\neffectiveness of MAEs for sensor-agnostic CBIR in RS. To this end, we present a\nsystematic overview on the possible adaptations of the vanilla MAE to exploit\nmasked image modeling on multi-sensor RS image archives (denoted as\ncross-sensor masked autoencoders [CSMAEs]) in the context of CBIR. Based on\ndifferent adjustments applied to the vanilla MAE, we introduce different CSMAE\nmodels. We also provide an extensive experimental analysis of these CSMAE\nmodels. We finally derive a guideline to exploit masked image modeling for\nuni-modal and cross-modal CBIR problems in RS. The code of this work is\npublicly available at https://github.com/jakhac/CSMAE.\n","authors":["Jakob Hackstein","Gencer Sumbul","Kai Norman Clasen","Begüm Demir"],"pdf_url":"https://arxiv.org/pdf/2401.07782v3.pdf","comment":"Accepted at the IEEE Transactions on Geoscience and Remote Sensing.\n  Our code is available at https://github.com/jakhac/CSMAE"},{"id":"http://arxiv.org/abs/2412.05185v2","updated":"2024-12-11T14:43:02Z","published":"2024-12-06T17:04:42Z","title":"LinVT: Empower Your Image-level Large Language Model to Understand\n  Videos","summary":"  Large Language Models (LLMs) have been widely used in various tasks,\nmotivating us to develop an LLM-based assistant for videos. Instead of training\nfrom scratch, we propose a module to transform arbitrary well-trained\nimage-based LLMs into video-LLMs (after being trained on video data). To better\nadapt image-LLMs for processing videos, we introduce two design principles:\nlinear transformation to preserve the original visual-language alignment and\nrepresentative information condensation from redundant video content. Guided by\nthese principles, we propose a plug-and-play Linear Video Tokenizer(LinVT),\nwhich enables existing image-LLMs to understand videos. We benchmark LinVT with\nsix recent visual LLMs: Aquila, Blip-3, InternVL2, Mipha, Molmo and Qwen2-VL,\nshowcasing the high compatibility of LinVT. LinVT-based LLMs achieve\nstate-of-the-art performance across various video benchmarks, illustrating the\neffectiveness of LinVT in multi-modal video understanding.\n","authors":["Lishuai Gao","Yujie Zhong","Yingsen Zeng","Haoxian Tan","Dengjie Li","Zheng Zhao"],"pdf_url":"https://arxiv.org/pdf/2412.05185v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08421v1","updated":"2024-12-11T14:37:21Z","published":"2024-12-11T14:37:21Z","title":"PointCFormer: a Relation-based Progressive Feature Extraction Network\n  for Point Cloud Completion","summary":"  Point cloud completion aims to reconstruct the complete 3D shape from\nincomplete point clouds, and it is crucial for tasks such as 3D object\ndetection and segmentation. Despite the continuous advances in point cloud\nanalysis techniques, feature extraction methods are still confronted with\napparent limitations. The sparse sampling of point clouds, used as inputs in\nmost methods, often results in a certain loss of global structure information.\nMeanwhile, traditional local feature extraction methods usually struggle to\ncapture the intricate geometric details. To overcome these drawbacks, we\nintroduce PointCFormer, a transformer framework optimized for robust global\nretention and precise local detail capture in point cloud completion. This\nframework embraces several key advantages. First, we propose a relation-based\nlocal feature extraction method to perceive local delicate geometry\ncharacteristics. This approach establishes a fine-grained relationship metric\nbetween the target point and its k-nearest neighbors, quantifying each\nneighboring point's contribution to the target point's local features.\nSecondly, we introduce a progressive feature extractor that integrates our\nlocal feature perception method with self-attention. Starting with a denser\nsampling of points as input, it iteratively queries long-distance global\ndependencies and local neighborhood relationships. This extractor maintains\nenhanced global structure and refined local details, without generating\nsubstantial computational overhead. Additionally, we develop a correction\nmodule after generating point proxies in the latent space to reintroduce denser\ninformation from the input points, enhancing the representation capability of\nthe point proxies. PointCFormer demonstrates state-of-the-art performance on\nseveral widely used benchmarks.\n","authors":["Yi Zhong","Weize Quan","Dong-ming Yan","Jie Jiang","Yingmei Wei"],"pdf_url":"https://arxiv.org/pdf/2412.08421v1.pdf","comment":"9 pages, 8 figures, AAAI 2025, references added"},{"id":"http://arxiv.org/abs/2412.08412v1","updated":"2024-12-11T14:30:24Z","published":"2024-12-11T14:30:24Z","title":"Pragmatist: Multiview Conditional Diffusion Models for High-Fidelity 3D\n  Reconstruction from Unposed Sparse Views","summary":"  Inferring 3D structures from sparse, unposed observations is challenging due\nto its unconstrained nature. Recent methods propose to predict implicit\nrepresentations directly from unposed inputs in a data-driven manner, achieving\npromising results. However, these methods do not utilize geometric priors and\ncannot hallucinate the appearance of unseen regions, thus making it challenging\nto reconstruct fine geometric and textural details. To tackle this challenge,\nour key idea is to reformulate this ill-posed problem as conditional novel view\nsynthesis, aiming to generate complete observations from limited input views to\nfacilitate reconstruction. With complete observations, the poses of the input\nviews can be easily recovered and further used to optimize the reconstructed\nobject. To this end, we propose a novel pipeline Pragmatist. First, we generate\na complete observation of the object via a multiview conditional diffusion\nmodel. Then, we use a feed-forward large reconstruction model to obtain the\nreconstructed mesh. To further improve the reconstruction quality, we recover\nthe poses of input views by inverting the obtained 3D representations and\nfurther optimize the texture using detailed input views. Unlike previous\napproaches, our pipeline improves reconstruction by efficiently leveraging\nunposed inputs and generative priors, circumventing the direct resolution of\nhighly ill-posed problems. Extensive experiments show that our approach\nachieves promising performance in several benchmarks.\n","authors":["Songchun Zhang","Chunhui Zhao"],"pdf_url":"https://arxiv.org/pdf/2412.08412v1.pdf","comment":"Accepted by AAAI 2025. 13 pages, 8 figures"},{"id":"http://arxiv.org/abs/2306.04877v3","updated":"2024-12-11T14:30:20Z","published":"2023-06-08T02:17:29Z","title":"TRIGS: Trojan Identification from Gradient-based Signatures","summary":"  Training machine learning models can be very expensive or even unaffordable.\nThis may be, for example, due to data limitations, such as unavailability or\nbeing too large, or computational power limitations. Therefore, it is a common\npractice to rely on open-source pre-trained models whenever possible.However,\nthis practice is alarming from a security perspective. Pre-trained models can\nbe infected with Trojan attacks, in which the attacker embeds a trigger in the\nmodel such that the model's behavior can be controlled by the attacker when the\ntrigger is present in the input. In this paper, we present a novel method for\ndetecting Trojan models. Our method creates a signature for a model based on\nactivation optimization. A classifier is then trained to detect a Trojan model\ngiven its signature. We call our method TRIGS for TRojan Identification from\nGradient-based Signatures. TRIGS achieves state-of-the-art performance on two\npublic datasets of convolutional models. Additionally, we introduce a new\nchallenging dataset of ImageNet models based on the vision transformer\narchitecture. TRIGS delivers the best performance on the new dataset,\nsurpassing the baseline methods by a large margin. Our experiments also show\nthat TRIGS requires only a small amount of clean samples to achieve good\nperformance, and works reasonably well even if the defender does not have prior\nknowledge about the attacker's model architecture. Our code and data are\npublicly available.\n","authors":["Mohamed E. Hussein","Sudharshan Subramaniam Janakiraman","Wael AbdAlmageed"],"pdf_url":"https://arxiv.org/pdf/2306.04877v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08410v1","updated":"2024-12-11T14:29:35Z","published":"2024-12-11T14:29:35Z","title":"Pysical Informed Driving World Model","summary":"  Autonomous driving requires robust perception models trained on high-quality,\nlarge-scale multi-view driving videos for tasks like 3D object detection,\nsegmentation and trajectory prediction. While world models provide a\ncost-effective solution for generating realistic driving videos, challenges\nremain in ensuring these videos adhere to fundamental physical principles, such\nas relative and absolute motion, spatial relationship like occlusion and\nspatial consistency, and temporal consistency. To address these, we propose\nDrivePhysica, an innovative model designed to generate realistic multi-view\ndriving videos that accurately adhere to essential physical principles through\nthree key advancements: (1) a Coordinate System Aligner module that integrates\nrelative and absolute motion features to enhance motion interpretation, (2) an\nInstance Flow Guidance module that ensures precise temporal consistency via\nefficient 3D flow extraction, and (3) a Box Coordinate Guidance module that\nimproves spatial relationship understanding and accurately resolves occlusion\nhierarchies. Grounded in physical principles, we achieve state-of-the-art\nperformance in driving video generation quality (3.96 FID and 38.06 FVD on the\nNuscenes dataset) and downstream perception tasks. Our project homepage:\nhttps://metadrivescape.github.io/papers_project/DrivePhysica/page.html\n","authors":["Zhuoran Yang","Xi Guo","Chenjing Ding","Chiyu Wang","Wei Wu"],"pdf_url":"https://arxiv.org/pdf/2412.08410v1.pdf","comment":"project homepage:\n  https://metadrivescape.github.io/papers_project/DrivePhysica/page.html"},{"id":"http://arxiv.org/abs/2412.08406v1","updated":"2024-12-11T14:27:30Z","published":"2024-12-11T14:27:30Z","title":"Embedding and Enriching Explicit Semantics for Visible-Infrared Person\n  Re-Identification","summary":"  Visible-infrared person re-identification (VIReID) retrieves pedestrian\nimages with the same identity across different modalities. Existing methods\nlearn visual content solely from images, lacking the capability to sense\nhigh-level semantics. In this paper, we propose an Embedding and Enriching\nExplicit Semantics (EEES) framework to learn semantically rich cross-modality\npedestrian representations. Our method offers several contributions. First,\nwith the collaboration of multiple large language-vision models, we develop\nExplicit Semantics Embedding (ESE), which automatically supplements language\ndescriptions for pedestrians and aligns image-text pairs into a common space,\nthereby learning visual content associated with explicit semantics. Second,\nrecognizing the complementarity of multi-view information, we present\nCross-View Semantics Compensation (CVSC), which constructs multi-view\nimage-text pair representations, establishes their many-to-many matching, and\npropagates knowledge to single-view representations, thus compensating visual\ncontent with its missing cross-view semantics. Third, to eliminate noisy\nsemantics such as conflicting color attributes in different modalities, we\ndesign Cross-Modality Semantics Purification (CMSP), which constrains the\ndistance between inter-modality image-text pair representations to be close to\nthat between intra-modality image-text pair representations, further enhancing\nthe modality-invariance of visual content. Finally, experimental results\ndemonstrate the effectiveness and superiority of the proposed EEES.\n","authors":["Neng Dong","Shuanglin Yan","Liyan Zhang","Jinhui Tang"],"pdf_url":"https://arxiv.org/pdf/2412.08406v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02571v3","updated":"2024-12-11T14:18:12Z","published":"2024-10-03T15:18:28Z","title":"SuperGS: Super-Resolution 3D Gaussian Splatting Enhanced by Variational\n  Residual Features and Uncertainty-Augmented Learning","summary":"  Recently, 3D Gaussian Splatting (3DGS) has exceled in novel view synthesis\n(NVS) with its real-time rendering capabilities and superior quality. However,\nit faces challenges for high-resolution novel view synthesis (HRNVS) due to the\ncoarse nature of primitives derived from low-resolution input views. To address\nthis issue, we propose Super-Resolution 3DGS (SuperGS), which is an expansion\nof 3DGS designed with a two-stage coarse-to-fine training framework. In this\nframework, we use a latent feature field to represent the low-resolution scene,\nserving as both the initialization and foundational information for\nsuper-resolution optimization. Additionally, we introduce variational residual\nfeatures to enhance high-resolution details, using their variance as\nuncertainty estimates to guide the densification process and loss computation.\nFurthermore, the introduction of a multi-view joint learning approach helps\nmitigate ambiguities caused by multi-view inconsistencies in the pseudo labels.\nExtensive experiments demonstrate that SuperGS surpasses state-of-the-art HRNVS\nmethods on both real-world and synthetic datasets using only low-resolution\ninputs. Code is available at https://github.com/SYXieee/SuperGS.\n","authors":["Shiyun Xie","Zhiru Wang","Xu Wang","Yinghao Zhu","Chengwei Pan","Xiwang Dong"],"pdf_url":"https://arxiv.org/pdf/2410.02571v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.17567v2","updated":"2024-12-11T14:16:51Z","published":"2023-06-30T11:40:35Z","title":"Counting Guidance for High Fidelity Text-to-Image Synthesis","summary":"  Recently, there have been significant improvements in the quality and\nperformance of text-to-image generation, largely due to the impressive results\nattained by diffusion models. However, text-to-image diffusion models sometimes\nstruggle to create high-fidelity content for the given input prompt. One\nspecific issue is their difficulty in generating the precise number of objects\nspecified in the text prompt. For example, when provided with the prompt \"five\napples and ten lemons on a table,\" images generated by diffusion models often\ncontain an incorrect number of objects. In this paper, we present a method to\nimprove diffusion models so that they accurately produce the correct object\ncount based on the input prompt. We adopt a counting network that performs\nreference-less class-agnostic counting for any given image. We calculate the\ngradients of the counting network and refine the predicted noise for each step.\nTo address the presence of multiple types of objects in the prompt, we utilize\nnovel attention map guidance to obtain high-quality masks for each object.\nFinally, we guide the denoising process using the calculated gradients for each\nobject. Through extensive experiments and evaluation, we demonstrate that the\nproposed method significantly enhances the fidelity of diffusion models with\nrespect to object count.\n","authors":["Wonjun Kang","Kevin Galim","Hyung Il Koo","Nam Ik Cho"],"pdf_url":"https://arxiv.org/pdf/2306.17567v2.pdf","comment":"Accepted at WACV 2025"},{"id":"http://arxiv.org/abs/2409.01329v2","updated":"2024-12-11T14:15:21Z","published":"2024-09-02T15:30:27Z","title":"Assessing the Impact of Image Dataset Features on Privacy-Preserving\n  Machine Learning","summary":"  Machine Learning (ML) is crucial in many sectors, including computer vision.\nHowever, ML models trained on sensitive data face security challenges, as they\ncan be attacked and leak information. Privacy-Preserving Machine Learning\n(PPML) addresses this by using Differential Privacy (DP) to balance utility and\nprivacy. This study identifies image dataset characteristics that affect the\nutility and vulnerability of private and non-private Convolutional Neural\nNetwork (CNN) models. Through analyzing multiple datasets and privacy budgets,\nwe find that imbalanced datasets increase vulnerability in minority classes,\nbut DP mitigates this issue. Datasets with fewer classes improve both model\nutility and privacy, while high entropy or low Fisher Discriminant Ratio (FDR)\ndatasets deteriorate the utility-privacy trade-off. These insights offer\nvaluable guidance for practitioners and researchers in estimating and\noptimizing the utility-privacy trade-off in image datasets, helping to inform\ndata and privacy modifications for better outcomes based on dataset\ncharacteristics.\n","authors":["Lucas Lange","Maurice-Maximilian Heykeroth","Erhard Rahm"],"pdf_url":"https://arxiv.org/pdf/2409.01329v2.pdf","comment":"Accepted at 21st Conference on Database Systems for Business,\n  Technology and Web (BTW 2025)"},{"id":"http://arxiv.org/abs/2405.17825v3","updated":"2024-12-11T13:58:19Z","published":"2024-05-28T04:47:54Z","title":"Diffusion Model Patching via Mixture-of-Prompts","summary":"  We present Diffusion Model Patching (DMP), a simple method to boost the\nperformance of pre-trained diffusion models that have already reached\nconvergence, with a negligible increase in parameters. DMP inserts a small,\nlearnable set of prompts into the model's input space while keeping the\noriginal model frozen. The effectiveness of DMP is not merely due to the\naddition of parameters but stems from its dynamic gating mechanism, which\nselects and combines a subset of learnable prompts at every timestep (i.e.,\nreverse denoising steps). This strategy, which we term \"mixture-of-prompts\",\nenables the model to draw on the distinct expertise of each prompt, essentially\n\"patching\" the model's functionality at every timestep with minimal yet\nspecialized parameters. Uniquely, DMP enhances the model by further training on\nthe original dataset already used for pre-training, even in a scenario where\nsignificant improvements are typically not expected due to model convergence.\nNotably, DMP significantly enhances the FID of converged DiT-L/2 by 10.38% on\nFFHQ, achieved with only a 1.43% parameter increase and 50K additional training\niterations.\n","authors":["Seokil Ham","Sangmin Woo","Jin-Young Kim","Hyojun Go","Byeongjun Park","Changick Kim"],"pdf_url":"https://arxiv.org/pdf/2405.17825v3.pdf","comment":"AAAI 2025; Project: https://sangminwoo.github.io/DMP/"},{"id":"http://arxiv.org/abs/2412.08388v1","updated":"2024-12-11T13:55:42Z","published":"2024-12-11T13:55:42Z","title":"LOMA: Language-assisted Semantic Occupancy Network via Triplane Mamba","summary":"  Vision-based 3D occupancy prediction has become a popular research task due\nto its versatility and affordability. Nowadays, conventional methods usually\nproject the image-based vision features to 3D space and learn the geometric\ninformation through the attention mechanism, enabling the 3D semantic occupancy\nprediction. However, these works usually face two main challenges: 1) Limited\ngeometric information. Due to the lack of geometric information in the image\nitself, it is challenging to directly predict 3D space information, especially\nin large-scale outdoor scenes. 2) Local restricted interaction. Due to the\nquadratic complexity of the attention mechanism, they often use modified local\nattention to fuse features, resulting in a restricted fusion. To address these\nproblems, in this paper, we propose a language-assisted 3D semantic occupancy\nprediction network, named LOMA. In the proposed vision-language framework, we\nfirst introduce a VL-aware Scene Generator (VSG) module to generate the 3D\nlanguage feature of the scene. By leveraging the vision-language model, this\nmodule provides implicit geometric knowledge and explicit semantic information\nfrom the language. Furthermore, we present a Tri-plane Fusion Mamba (TFM) block\nto efficiently fuse the 3D language feature and 3D vision feature. The proposed\nmodule not only fuses the two features with global modeling but also avoids too\nmuch computation costs. Experiments on the SemanticKITTI and SSCBench-KITTI360\ndatasets show that our algorithm achieves new state-of-the-art performances in\nboth geometric and semantic completion tasks. Our code will be open soon.\n","authors":["Yubo Cui","Zhiheng Li","Jiaqiang Wang","Zheng Fang"],"pdf_url":"https://arxiv.org/pdf/2412.08388v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2404.01188v5","updated":"2024-12-11T13:52:27Z","published":"2024-04-01T15:45:58Z","title":"MonoBox: Tightness-free Box-supervised Polyp Segmentation using\n  Monotonicity Constraint","summary":"  We propose MonoBox, an innovative box-supervised segmentation method\nconstrained by monotonicity to liberate its training from the user-unfriendly\nbox-tightness assumption. In contrast to conventional box-supervised\nsegmentation, where the box edges must precisely touch the target boundaries,\nMonoBox leverages imprecisely-annotated boxes to achieve robust pixel-wise\nsegmentation. The 'linchpin' is that, within the noisy zones around box edges,\nMonoBox discards the traditional misguiding multiple-instance learning loss,\nand instead optimizes a carefully-designed objective, termed monotonicity\nconstraint. Along directions transitioning from the foreground to background,\nthis new constraint steers responses to adhere to a trend of monotonically\ndecreasing values. Consequently, the originally unreliable learning within the\nnoisy zones is transformed into a correct and effective monotonicity\noptimization. Moreover, an adaptive label correction is introduced, enabling\nMonoBox to enhance the tightness of box annotations using predicted masks from\nthe previous epoch and dynamically shrink the noisy zones as training\nprogresses. We verify MonoBox in the box-supervised segmentation task of\npolyps, where satisfying box-tightness is challenging due to the vague\nboundaries between the polyp and normal tissues. Experiments on both public\nsynthetic and in-house real noisy datasets demonstrate that MonoBox exceeds\nother anti-noise state-of-the-arts by improving Dice by at least 5.5% and 3.3%,\nrespectively. Codes are at https://github.com/Huster-Hq/MonoBox.\n","authors":["Qiang Hu","Zhenyu Yi","Ying Zhou","Fan Huang","Mei Liu","Qiang Li","Zhiwei Wang"],"pdf_url":"https://arxiv.org/pdf/2404.01188v5.pdf","comment":"Accepted to AAAI 2025. Code and models:\n  https://github.com/Huster-Hq/MonoBox"},{"id":"http://arxiv.org/abs/2412.08378v1","updated":"2024-12-11T13:41:21Z","published":"2024-12-11T13:41:21Z","title":"HyViLM: Enhancing Fine-Grained Recognition with a Hybrid Encoder for\n  Vision-Language Models","summary":"  Recently, there has been growing interest in the capability of multimodal\nlarge language models (MLLMs) to process high-resolution images. A common\napproach currently involves dynamically cropping the original high-resolution\nimage into smaller sub-images, which are then fed into a vision encoder that\nwas pre-trained on lower-resolution images. However, this cropping approach\noften truncates objects and connected areas in the original image, causing\nsemantic breaks. To address this limitation, we introduce HyViLM, designed to\nprocess images of any resolution while retaining the overall context during\nencoding. Specifically, we: (i) Design a new visual encoder called Hybrid\nEncoder that not only encodes individual sub-images but also interacts with\ndetailed global visual features, significantly improving the model's ability to\nencode high-resolution images. (ii) Propose an optimal feature fusion strategy\nfor the dynamic cropping approach, effectively leveraging information from\ndifferent layers of the vision encoder. Compared with the state-of-the-art\nMLLMs under the same setting, our HyViLM outperforms existing MLLMs in nine out\nof ten tasks. Specifically, HyViLM achieves a 9.6% improvement in performance\non the TextVQA task and a 6.9% enhancement on the DocVQA task.\n","authors":["Shiding Zhu","Wenhui Dong","Jun Song","Yanan Guo","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2412.08378v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2412.08376v1","updated":"2024-12-11T13:36:18Z","published":"2024-12-11T13:36:18Z","title":"Reloc3r: Large-Scale Training of Relative Camera Pose Regression for\n  Generalizable, Fast, and Accurate Visual Localization","summary":"  Visual localization aims to determine the camera pose of a query image\nrelative to a database of posed images. In recent years, deep neural networks\nthat directly regress camera poses have gained popularity due to their fast\ninference capabilities. However, existing methods struggle to either generalize\nwell to new scenes or provide accurate camera pose estimates. To address these\nissues, we present \\textbf{Reloc3r}, a simple yet effective visual localization\nframework. It consists of an elegantly designed relative pose regression\nnetwork, and a minimalist motion averaging module for absolute pose estimation.\nTrained on approximately 8 million posed image pairs, Reloc3r achieves\nsurprisingly good performance and generalization ability. We conduct extensive\nexperiments on 6 public datasets, consistently demonstrating the effectiveness\nand efficiency of the proposed method. It provides high-quality camera pose\nestimates in real time and generalizes to novel scenes. Code, weights, and data\nat: \\url{https://github.com/ffrivera0/reloc3r}.\n","authors":["Siyan Dong","Shuzhe Wang","Shaohui Liu","Lulu Cai","Qingnan Fan","Juho Kannala","Yanchao Yang"],"pdf_url":"https://arxiv.org/pdf/2412.08376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15092v3","updated":"2024-12-11T13:13:20Z","published":"2024-09-23T15:06:37Z","title":"M2OST: Many-to-one Regression for Predicting Spatial Transcriptomics\n  from Digital Pathology Images","summary":"  The advancement of Spatial Transcriptomics (ST) has facilitated the\nspatially-aware profiling of gene expressions based on histopathology images.\nAlthough ST data offers valuable insights into the micro-environment of tumors,\nits acquisition cost remains expensive. Therefore, directly predicting the ST\nexpressions from digital pathology images is desired. Current methods usually\nadopt existing regression backbones along with patch-sampling for this task,\nwhich ignores the inherent multi-scale information embedded in the pyramidal\ndata structure of digital pathology images, and wastes the inter-spot visual\ninformation crucial for accurate gene expression prediction. To address these\nlimitations, we propose M2OST, a many-to-one regression Transformer that can\naccommodate the hierarchical structure of the pathology images via a decoupled\nmulti-scale feature extractor. Unlike traditional models that are trained with\none-to-one image-label pairs, M2OST uses multiple images from different levels\nof the digital pathology image to jointly predict the gene expressions in their\ncommon corresponding spot. Built upon our many-to-one scheme, M2OST can be\neasily scaled to fit different numbers of inputs, and its network structure\ninherently incorporates nearby inter-spot features, enhancing regression\nperformance. We have tested M2OST on three public ST datasets and the\nexperimental results show that M2OST can achieve state-of-the-art performance\nwith fewer parameters and floating-point operations (FLOPs). The code is\navailable at: https://github.com/Dootmaan/M2OST.\n","authors":["Hongyi Wang","Xiuju Du","Jing Liu","Shuyi Ouyang","Yen-Wei Chen","Lanfen Lin"],"pdf_url":"https://arxiv.org/pdf/2409.15092v3.pdf","comment":"Accepted by AAAI 2025. arXiv admin note: substantial text overlap\n  with arXiv:2401.10608"},{"id":"http://arxiv.org/abs/2408.08578v2","updated":"2024-12-11T13:07:45Z","published":"2024-08-16T07:24:19Z","title":"TAMER: Tree-Aware Transformer for Handwritten Mathematical Expression\n  Recognition","summary":"  Handwritten Mathematical Expression Recognition (HMER) has extensive\napplications in automated grading and office automation. However, existing\nsequence-based decoding methods, which directly predict $\\LaTeX$ sequences,\nstruggle to understand and model the inherent tree structure of $\\LaTeX$ and\noften fail to ensure syntactic correctness in the decoded results. To address\nthese challenges, we propose a novel model named TAMER (Tree-Aware Transformer)\nfor handwritten mathematical expression recognition. TAMER introduces an\ninnovative Tree-aware Module while maintaining the flexibility and efficient\ntraining of Transformer. TAMER combines the advantages of both sequence\ndecoding and tree decoding models by jointly optimizing sequence prediction and\ntree structure prediction tasks, which enhances the model's understanding and\ngeneralization of complex mathematical expression structures. During inference,\nTAMER employs a Tree Structure Prediction Scoring Mechanism to improve the\nstructural validity of the generated $\\LaTeX$ sequences. Experimental results\non CROHME datasets demonstrate that TAMER outperforms traditional sequence\ndecoding and tree decoding models, especially in handling complex mathematical\nstructures, achieving state-of-the-art (SOTA) performance.\n","authors":["Jianhua Zhu","Wenqi Zhao","Yu Li","Xingjian Hu","Liangcai Gao"],"pdf_url":"https://arxiv.org/pdf/2408.08578v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17418v2","updated":"2024-12-11T13:02:25Z","published":"2024-11-26T13:25:53Z","title":"Multimodal Outer Arithmetic Block Dual Fusion of Whole Slide Images and\n  Omics Data for Precision Oncology","summary":"  The integration of DNA methylation data with a Whole Slide Image (WSI) offers\nsignificant potential for enhancing the diagnostic precision of central nervous\nsystem (CNS) tumor classification in neuropathology. While existing approaches\ntypically integrate encoded omic data with histology at either an early or late\nfusion stage, the potential of reintroducing omic data through dual fusion\nremains unexplored. In this paper, we propose the use of omic embeddings during\nearly and late fusion to capture complementary information from local\n(patch-level) to global (slide-level) interactions, boosting performance\nthrough multimodal integration. In the early fusion stage, omic embeddings are\nprojected onto WSI patches in latent-space, which generates embeddings that\nencapsulate per-patch molecular and morphological insights. This effectively\nincorporates omic information into the spatial representation of the WSI. These\nembeddings are then refined with a Multiple Instance Learning gated attention\nmechanism which attends to diagnostic patches. In the late fusion stage, we\nreintroduce the omic data by fusing it with slide-level omic-WSI embeddings\nusing a Multimodal Outer Arithmetic Block (MOAB), which richly intermingles\nfeatures from both modalities, capturing their correlations and\ncomplementarity. We demonstrate accurate CNS tumor subtyping across 20\nfine-grained subtypes and validate our approach on benchmark datasets,\nachieving improved survival prediction on TCGA-BLCA and competitive performance\non TCGA-BRCA compared to state-of-the-art methods. This dual fusion strategy\nenhances interpretability and classification performance, highlighting its\npotential for clinical diagnostics.\n","authors":["Omnia Alwazzan","Amaya Gallagher-Syed","Thomas O. Millner","Sebastian Brandner","Ioannis Patras","Silvia Marino","Gregory Slabaugh"],"pdf_url":"https://arxiv.org/pdf/2411.17418v2.pdf","comment":"Revised to 10 pages, with corrected typos, updated references (some\n  added, others removed), improved figure quality, modified text for better\n  method validation, added one more co-author, and identified the IEEE member"},{"id":"http://arxiv.org/abs/2412.08357v1","updated":"2024-12-11T13:02:09Z","published":"2024-12-11T13:02:09Z","title":"Video Summarization using Denoising Diffusion Probabilistic Model","summary":"  Video summarization aims to eliminate visual redundancy while retaining key\nparts of video to construct concise and comprehensive synopses. Most existing\nmethods use discriminative models to predict the importance scores of video\nframes. However, these methods are susceptible to annotation inconsistency\ncaused by the inherent subjectivity of different annotators when annotating the\nsame video. In this paper, we introduce a generative framework for video\nsummarization that learns how to generate summaries from a probability\ndistribution perspective, effectively reducing the interference of subjective\nannotation noise. Specifically, we propose a novel diffusion summarization\nmethod based on the Denoising Diffusion Probabilistic Model (DDPM), which\nlearns the probability distribution of training data through noise prediction,\nand generates summaries by iterative denoising. Our method is more resistant to\nsubjective annotation noise, and is less prone to overfitting the training data\nthan discriminative methods, with strong generalization ability. Moreover, to\nfacilitate training DDPM with limited data, we employ an unsupervised video\nsummarization model to implement the earlier denoising process. Extensive\nexperiments on various datasets (TVSum, SumMe, and FPVSum) demonstrate the\neffectiveness of our method.\n","authors":["Zirui Shang","Yubo Zhu","Hongxi Li","Shuo yang","Xinxiao Wu"],"pdf_url":"https://arxiv.org/pdf/2412.08357v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.07169v2","updated":"2024-12-11T12:50:45Z","published":"2024-12-10T04:03:46Z","title":"Rate-In: Information-Driven Adaptive Dropout Rates for Improved\n  Inference-Time Uncertainty Estimation","summary":"  Accurate uncertainty estimation is crucial for deploying neural networks in\nrisk-sensitive applications such as medical diagnosis. Monte Carlo Dropout is a\nwidely used technique for approximating predictive uncertainty by performing\nstochastic forward passes with dropout during inference. However, using static\ndropout rates across all layers and inputs can lead to suboptimal uncertainty\nestimates, as it fails to adapt to the varying characteristics of individual\ninputs and network layers. Existing approaches optimize dropout rates during\ntraining using labeled data, resulting in fixed inference-time parameters that\ncannot adjust to new data distributions, compromising uncertainty estimates in\nMonte Carlo simulations.\n  In this paper, we propose Rate-In, an algorithm that dynamically adjusts\ndropout rates during inference by quantifying the information loss induced by\ndropout in each layer's feature maps. By treating dropout as controlled noise\ninjection and leveraging information-theoretic principles, Rate-In adapts\ndropout rates per layer and per input instance without requiring ground truth\nlabels. By quantifying the functional information loss in feature maps, we\nadaptively tune dropout rates to maintain perceptual quality across diverse\nmedical imaging tasks and architectural configurations. Our extensive empirical\nstudy on synthetic data and real-world medical imaging tasks demonstrates that\nRate-In improves calibration and sharpens uncertainty estimates compared to\nfixed or heuristic dropout rates without compromising predictive performance.\nRate-In offers a practical, unsupervised, inference-time approach to optimizing\ndropout for more reliable predictive uncertainty estimation in critical\napplications.\n","authors":["Tal Zeevi","Ravid Shwartz-Ziv","Yann LeCun","Lawrence H. Staib","John A. Onofrey"],"pdf_url":"https://arxiv.org/pdf/2412.07169v2.pdf","comment":"Updated author affiliation"},{"id":"http://arxiv.org/abs/2412.08350v1","updated":"2024-12-11T12:45:17Z","published":"2024-12-11T12:45:17Z","title":"Benchmarking learned algorithms for computed tomography image\n  reconstruction tasks","summary":"  Computed tomography (CT) is a widely used non-invasive diagnostic method in\nvarious fields, and recent advances in deep learning have led to significant\nprogress in CT image reconstruction. However, the lack of large-scale,\nopen-access datasets has hindered the comparison of different types of learned\nmethods. To address this gap, we use the 2DeteCT dataset, a real-world\nexperimental computed tomography dataset, for benchmarking machine learning\nbased CT image reconstruction algorithms. We categorize these methods into\npost-processing networks, learned/unrolled iterative methods, learned\nregularizer methods, and plug-and-play methods, and provide a pipeline for easy\nimplementation and evaluation. Using key performance metrics, including SSIM\nand PSNR, our benchmarking results showcase the effectiveness of various\nalgorithms on tasks such as full data reconstruction, limited-angle\nreconstruction, sparse-angle reconstruction, low-dose reconstruction, and\nbeam-hardening corrected reconstruction. With this benchmarking study, we\nprovide an evaluation of a range of algorithms representative for different\ncategories of learned reconstruction methods on a recently published dataset of\nreal-world experimental CT measurements. The reproducible setup of methods and\nCT image reconstruction tasks in an open-source toolbox enables straightforward\naddition and comparison of new methods later on. The toolbox also provides the\noption to load the 2DeteCT dataset differently for extensions to other problems\nand different CT reconstruction tasks.\n","authors":["Maximilian B. Kiss","Ander Biguri","Zakhar Shumaylov","Ferdia Sherry","K. Joost Batenburg","Carola-Bibiane Schönlieb","Felix Lucka"],"pdf_url":"https://arxiv.org/pdf/2412.08350v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08345v1","updated":"2024-12-11T12:34:49Z","published":"2024-12-11T12:34:49Z","title":"ConDSeg: A General Medical Image Segmentation Framework via\n  Contrast-Driven Feature Enhancement","summary":"  Medical image segmentation plays an important role in clinical decision\nmaking, treatment planning, and disease tracking. However, it still faces two\nmajor challenges. On the one hand, there is often a ``soft boundary'' between\nforeground and background in medical images, with poor illumination and low\ncontrast further reducing the distinguishability of foreground and background\nwithin the image. On the other hand, co-occurrence phenomena are widespread in\nmedical images, and learning these features is misleading to the model's\njudgment. To address these challenges, we propose a general framework called\nContrast-Driven Medical Image Segmentation (ConDSeg). First, we develop a\ncontrastive training strategy called Consistency Reinforcement. It is designed\nto improve the encoder's robustness in various illumination and contrast\nscenarios, enabling the model to extract high-quality features even in adverse\nenvironments. Second, we introduce a Semantic Information Decoupling module,\nwhich is able to decouple features from the encoder into foreground,\nbackground, and uncertainty regions, gradually acquiring the ability to reduce\nuncertainty during training. The Contrast-Driven Feature Aggregation module\nthen contrasts the foreground and background features to guide multi-level\nfeature fusion and key feature enhancement, further distinguishing the entities\nto be segmented. We also propose a Size-Aware Decoder to solve the scale\nsingularity of the decoder. It accurately locate entities of different sizes in\nthe image, thus avoiding erroneous learning of co-occurrence features.\nExtensive experiments on five medical image datasets across three scenarios\ndemonstrate the state-of-the-art performance of our method, proving its\nadvanced nature and general applicability to various medical image segmentation\nscenarios. Our released code is available at\n\\url{https://github.com/Mengqi-Lei/ConDSeg}.\n","authors":["Mengqi Lei","Haochen Wu","Xinhua Lv","Xin Wang"],"pdf_url":"https://arxiv.org/pdf/2412.08345v1.pdf","comment":"This paper has been accepted by AAAI-2025"},{"id":"http://arxiv.org/abs/2412.08344v1","updated":"2024-12-11T12:34:37Z","published":"2024-12-11T12:34:37Z","title":"CoDTS: Enhancing Sparsely Supervised Collaborative Perception with a\n  Dual Teacher-Student Framework","summary":"  Current collaborative perception methods often rely on fully annotated\ndatasets, which can be expensive to obtain in practical situations. To reduce\nannotation costs, some works adopt sparsely supervised learning techniques and\ngenerate pseudo labels for the missing instances. However, these methods fail\nto achieve an optimal confidence threshold that harmonizes the quality and\nquantity of pseudo labels. To address this issue, we propose an end-to-end\nCollaborative perception Dual Teacher-Student framework (CoDTS), which employs\nadaptive complementary learning to produce both high-quality and high-quantity\npseudo labels. Specifically, the Main Foreground Mining (MFM) module generates\nhigh-quality pseudo labels based on the prediction of the static teacher.\nSubsequently, the Supplement Foreground Mining (SFM) module ensures a balance\nbetween the quality and quantity of pseudo labels by adaptively identifying\nmissing instances based on the prediction of the dynamic teacher. Additionally,\nthe Neighbor Anchor Sampling (NAS) module is incorporated to enhance the\nrepresentation of pseudo labels. To promote the adaptive complementary\nlearning, we implement a staged training strategy that trains the student and\ndynamic teacher in a mutually beneficial manner. Extensive experiments\ndemonstrate that the CoDTS effectively ensures an optimal balance of pseudo\nlabels in both quality and quantity, establishing a new state-of-the-art in\nsparsely supervised collaborative perception.\n","authors":["Yushan Han","Hui Zhang","Honglei Zhang","Jing Wang","Yidong Li"],"pdf_url":"https://arxiv.org/pdf/2412.08344v1.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2412.08341v1","updated":"2024-12-11T12:31:30Z","published":"2024-12-11T12:31:30Z","title":"ALoRE: Efficient Visual Adaptation via Aggregating Low Rank Experts","summary":"  Parameter-efficient transfer learning (PETL) has become a promising paradigm\nfor adapting large-scale vision foundation models to downstream tasks. Typical\nmethods primarily leverage the intrinsic low rank property to make\ndecomposition, learning task-specific weights while compressing parameter size.\nHowever, such approaches predominantly manipulate within the original feature\nspace utilizing a single-branch structure, which might be suboptimal for\ndecoupling the learned representations and patterns. In this paper, we propose\nALoRE, a novel PETL method that reuses the hypercomplex parameterized space\nconstructed by Kronecker product to Aggregate Low Rank Experts using a\nmulti-branch paradigm, disentangling the learned cognitive patterns during\ntraining. Thanks to the artful design, ALoRE maintains negligible extra\nparameters and can be effortlessly merged into the frozen backbone via\nre-parameterization in a sequential manner, avoiding additional inference\nlatency. We conduct extensive experiments on 24 image classification tasks\nusing various backbone variants. Experimental results demonstrate that ALoRE\noutperforms the full fine-tuning strategy and other state-of-the-art PETL\nmethods in terms of performance and parameter efficiency. For instance, ALoRE\nobtains 3.06% and 9.97% Top-1 accuracy improvement on average compared to full\nfine-tuning on the FGVC datasets and VTAB-1k benchmark by only updating 0.15M\nparameters.\n","authors":["Sinan Du","Guosheng Zhang","Keyao Wang","Yuanrui Wang","Haixiao Yue","Gang Zhang","Errui Ding","Jingdong Wang","Zhengzhuo Xu","Chun Yuan"],"pdf_url":"https://arxiv.org/pdf/2412.08341v1.pdf","comment":"23 pages, 7 figures"},{"id":"http://arxiv.org/abs/2412.08331v1","updated":"2024-12-11T12:18:30Z","published":"2024-12-11T12:18:30Z","title":"SLGaussian: Fast Language Gaussian Splatting in Sparse Views","summary":"  3D semantic field learning is crucial for applications like autonomous\nnavigation, AR/VR, and robotics, where accurate comprehension of 3D scenes from\nlimited viewpoints is essential. Existing methods struggle under sparse view\nconditions, relying on inefficient per-scene multi-view optimizations, which\nare impractical for many real-world tasks. To address this, we propose\nSLGaussian, a feed-forward method for constructing 3D semantic fields from\nsparse viewpoints, allowing direct inference of 3DGS-based scenes. By ensuring\nconsistent SAM segmentations through video tracking and using low-dimensional\nindexing for high-dimensional CLIP features, SLGaussian efficiently embeds\nlanguage information in 3D space, offering a robust solution for accurate 3D\nscene understanding under sparse view conditions. In experiments on two-view\nsparse 3D object querying and segmentation in the LERF and 3D-OVS datasets,\nSLGaussian outperforms existing methods in chosen IoU, Localization Accuracy,\nand mIoU. Moreover, our model achieves scene inference in under 30 seconds and\nopen-vocabulary querying in just 0.011 seconds per query.\n","authors":["Kangjie Chen","BingQuan Dai","Minghan Qin","Dongbin Zhang","Peihao Li","Yingshuang Zou","Haoqian Wang"],"pdf_url":"https://arxiv.org/pdf/2412.08331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08326v1","updated":"2024-12-11T12:09:37Z","published":"2024-12-11T12:09:37Z","title":"Digging into Intrinsic Contextual Information for High-fidelity 3D Point\n  Cloud Completion","summary":"  The common occurrence of occlusion-induced incompleteness in point clouds has\nmade point cloud completion (PCC) a highly-concerned task in the field of\ngeometric processing. Existing PCC methods typically produce complete point\nclouds from partial point clouds in a coarse-to-fine paradigm, with the coarse\nstage generating entire shapes and the fine stage improving texture details.\nThough diffusion models have demonstrated effectiveness in the coarse stage,\nthe fine stage still faces challenges in producing high-fidelity results due to\nthe ill-posed nature of PCC. The intrinsic contextual information for texture\ndetails in partial point clouds is the key to solving the challenge. In this\npaper, we propose a high-fidelity PCC method that digs into both short and\nlong-range contextual information from the partial point cloud in the fine\nstage. Specifically, after generating the coarse point cloud via a\ndiffusion-based coarse generator, a mixed sampling module introduces\nshort-range contextual information from partial point clouds into the fine\nstage. A surface freezing modules safeguards points from noise-free partial\npoint clouds against disruption. As for the long-range contextual information,\nwe design a similarity modeling module to derive similarity with rigid\ntransformation invariance between points, conducting effective matching of\ngeometric manifold features globally. In this way, the high-quality components\npresent in the partial point cloud serve as valuable references for refining\nthe coarse point cloud with high fidelity. Extensive experiments have\ndemonstrated the superiority of the proposed method over SOTA competitors. Our\ncode is available at https://github.com/JS-CHU/ContextualCompletion.\n","authors":["Jisheng Chu","Wenrui Li","Xingtao Wang","Kanglin Ning","Yidan Lu","Xiaopeng Fan"],"pdf_url":"https://arxiv.org/pdf/2412.08326v1.pdf","comment":"Accepted to AAAI2025"},{"id":"http://arxiv.org/abs/2406.01078v2","updated":"2024-12-11T12:00:36Z","published":"2024-06-03T07:58:09Z","title":"Unseen Visual Anomaly Generation","summary":"  Visual anomaly detection (AD) presents significant challenges due to the\nscarcity of anomalous data samples. While numerous works have been proposed to\nsynthesize anomalous samples, these synthetic anomalies often lack authenticity\nor require extensive training data, limiting their applicability in real-world\nscenarios. In this work, we propose Anomaly Anything (AnomalyAny), a novel\nframework that leverages Stable Diffusion (SD)'s image generation capabilities\nto generate diverse and realistic unseen anomalies. By conditioning on a single\nnormal sample during test time, AnomalyAny is able to generate unseen anomalies\nfor arbitrary object types with text descriptions. Within AnomalyAny, we\npropose attention-guided anomaly optimization to direct SD attention on\ngenerating hard anomaly concepts. Additionally, we introduce prompt-guided\nanomaly refinement, incorporating detailed descriptions to further improve the\ngeneration quality. Extensive experiments on MVTec AD and VisA datasets\ndemonstrate AnomalyAny's ability in generating high-quality unseen anomalies\nand its effectiveness in enhancing downstream AD performance.\n","authors":["Han Sun","Yunkang Cao","Hao Dong","Olga Fink"],"pdf_url":"https://arxiv.org/pdf/2406.01078v2.pdf","comment":"8 pages excluding appendix"},{"id":"http://arxiv.org/abs/2412.08321v1","updated":"2024-12-11T11:57:05Z","published":"2024-12-11T11:57:05Z","title":"TGOSPA Metric Parameters Selection and Evaluation for Visual\n  Multi-object Tracking","summary":"  Multi-object tracking algorithms are deployed in various applications, each\nwith unique performance requirements. For example, track switches pose\nsignificant challenges for offline scene understanding, as they hinder the\naccuracy of data interpretation. Conversely, in online surveillance\napplications, their impact is often minimal. This disparity underscores the\nneed for application-specific performance evaluations that are both simple and\nmathematically sound. The trajectory generalized optimal sub-pattern assignment\n(TGOSPA) metric offers a principled approach to evaluate multi-object tracking\nperformance. It accounts for localization errors, the number of missed and\nfalse objects, and the number of track switches, providing a comprehensive\nassessment framework. This paper illustrates the effective use of the TGOSPA\nmetric in computer vision tasks, addressing challenges posed by the need for\napplication-specific scoring methodologies. By exploring the TGOSPA parameter\nselection, we enable users to compare, comprehend, and optimize the performance\nof algorithms tailored for specific tasks, such as target tracking and training\nof detector or re-ID modules.\n","authors":["Jan Krejčí","Oliver Kost","Ondřej Straka","Yuxuan Xia","Lennart Svensson","Ángel F. García-Fernández"],"pdf_url":"https://arxiv.org/pdf/2412.08321v1.pdf","comment":"Submitted to IEEE Transactions on Aerospace and Electronic Systems"},{"id":"http://arxiv.org/abs/2412.08315v1","updated":"2024-12-11T11:52:16Z","published":"2024-12-11T11:52:16Z","title":"Lightweight Method for Interactive 3D Medical Image Segmentation with\n  Multi-Round Result Fusion","summary":"  In medical imaging, precise annotation of lesions or organs is often\nrequired. However, 3D volumetric images typically consist of hundreds or\nthousands of slices, making the annotation process extremely time-consuming and\nlaborious. Recently, the Segment Anything Model (SAM) has drawn widespread\nattention due to its remarkable zero-shot generalization capabilities in\ninteractive segmentation. While researchers have explored adapting SAM for\nmedical applications, such as using SAM adapters or constructing 3D SAM models,\na key question remains: Can traditional CNN networks achieve the same strong\nzero-shot generalization in this task? In this paper, we propose the\nLightweight Interactive Network for 3D Medical Image Segmentation (LIM-Net), a\nnovel approach demonstrating the potential of compact CNN-based models. Built\nupon a 2D CNN backbone, LIM-Net initiates segmentation by generating a 2D\nprompt mask from user hints. This mask is then propagated through the 3D\nsequence via the Memory Module. To refine and stabilize results during\ninteraction, the Multi-Round Result Fusion (MRF) Module selects and merges\noptimal masks from multiple rounds. Our extensive experiments across multiple\ndatasets and modalities demonstrate LIM-Net's competitive performance. It\nexhibits stronger generalization to unseen data compared to SAM-based models,\nwith competitive accuracy while requiring fewer interactions. Notably,\nLIM-Net's lightweight design offers significant advantages in deployment and\ninference efficiency, with low GPU memory consumption suitable for\nresource-constrained environments. These promising results demonstrate LIM-Net\ncan serve as a strong baseline, complementing and contrasting with popular SAM\nmodels to further boost effective interactive medical image segmentation. The\ncode will be released at \\url{https://github.com/goodtime-123/LIM-Net}.\n","authors":["Bingzhi Shen","Lufan Chang","Siqi Chen","Shuxiang Guo","Hao Liu"],"pdf_url":"https://arxiv.org/pdf/2412.08315v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08313v1","updated":"2024-12-11T11:50:06Z","published":"2024-12-11T11:50:06Z","title":"Post-Hoc MOTS: Exploring the Capabilities of Time-Symmetric Multi-Object\n  Tracking","summary":"  Temporal forward-tracking has been the dominant approach for multi-object\nsegmentation and tracking (MOTS). However, a novel time-symmetric tracking\nmethodology has recently been introduced for the detection, segmentation, and\ntracking of budding yeast cells in pre-recorded samples. Although this\narchitecture has demonstrated a unique perspective on stable and consistent\ntracking, as well as missed instance re-interpolation, its evaluation has so\nfar been largely confined to settings related to videomicroscopic environments.\nIn this work, we aim to reveal the broader capabilities, advantages, and\npotential challenges of this architecture across various specifically designed\nscenarios, including a pedestrian tracking dataset. We also conduct an ablation\nstudy comparing the model against its restricted variants and the widely used\nKalman filter. Furthermore, we present an attention analysis of the tracking\narchitecture for both pretrained and non-pretrained models\n","authors":["Gergely Szabó","Zsófia Molnár","András Horváth"],"pdf_url":"https://arxiv.org/pdf/2412.08313v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13276v2","updated":"2024-12-11T11:49:34Z","published":"2024-11-20T12:43:40Z","title":"Analysis and Synthesis Denoisers for Forward-Backward Plug-and-Play\n  Algorithms","summary":"  In this work we study the behavior of the forward-backward (FB) algorithm\nwhen the proximity operator is replaced by a sub-iterative procedure to\napproximate a Gaussian denoiser, in a Plug-and-Play (PnP) fashion. In\nparticular, we consider both analysis and synthesis Gaussian denoisers within a\ndictionary framework, obtained by unrolling dual-FB iterations or FB\niterations, respectively. We analyze the associated minimization problems as\nwell as the asymptotic behavior of the resulting FB-PnP iterations. In\nparticular, we show that the synthesis Gaussian denoising problem can be viewed\nas a proximity operator. For each case, analysis and synthesis, we show that\nthe FB-PnP algorithms solve the same problem whether we use only one or an\ninfinite number of sub-iteration to solve the denoising problem at each\niteration. To this aim, we show that each \"one sub-iteration\" strategy within\nthe FB-PnP can be interpreted as a primal-dual algorithm when a warm-restart\nstrategy is used. We further present similar results when using a Moreau-Yosida\nsmoothing of the global problem, for an arbitrary number of sub-iterations.\nFinally, we provide numerical simulations to illustrate our theoretical\nresults. In particular we first consider a toy compressive sensing example, as\nwell as an image restoration problem in a deep dictionary framework.\n","authors":["Matthieu Kowalski","Benoît Malézieux","Thomas Moreau","Audrey Repetti"],"pdf_url":"https://arxiv.org/pdf/2411.13276v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.18562v3","updated":"2024-12-11T11:48:44Z","published":"2024-11-27T18:03:26Z","title":"DexHandDiff: Interaction-aware Diffusion Planning for Adaptive Dexterous\n  Manipulation","summary":"  Dexterous manipulation with contact-rich interactions is crucial for advanced\nrobotics. While recent diffusion-based planning approaches show promise for\nsimpler manipulation tasks, they often produce unrealistic ghost states (e.g.,\nthe object automatically moves without hand contact) or lack adaptability when\nhandling complex sequential interactions. In this work, we introduce\nDexHandDiff, an interaction-aware diffusion planning framework for adaptive\ndexterous manipulation. DexHandDiff models joint state-action dynamics through\na dual-phase diffusion process which consists of pre-interaction contact\nalignment and post-contact goal-directed control, enabling goal-adaptive\ngeneralizable dexterous manipulation. Additionally, we incorporate dynamics\nmodel-based dual guidance and leverage large language models for automated\nguidance function generation, enhancing generalizability for physical\ninteractions and facilitating diverse goal adaptation through language cues.\nExperiments on physical interaction tasks such as door opening, pen and block\nre-orientation, and hammer striking demonstrate DexHandDiff's effectiveness on\ngoals outside training distributions, achieving over twice the average success\nrate (59.2% vs. 29.5%) compared to existing methods. Our framework achieves\n70.0% success on 30-degree door opening, 40.0% and 36.7% on pen and block\nhalf-side re-orientation respectively, and 46.7% on hammer nail half drive,\nhighlighting its robustness and flexibility in contact-rich manipulation.\n","authors":["Zhixuan Liang","Yao Mu","Yixiao Wang","Tianxing Chen","Wenqi Shao","Wei Zhan","Masayoshi Tomizuka","Ping Luo","Mingyu Ding"],"pdf_url":"https://arxiv.org/pdf/2411.18562v3.pdf","comment":"27 pages (new name). Project page: https://dexdiffuser.github.io/"},{"id":"http://arxiv.org/abs/2412.07590v2","updated":"2024-12-11T11:40:15Z","published":"2024-12-10T15:25:18Z","title":"Motion Artifact Removal in Pixel-Frequency Domain via Alternate Masks\n  and Diffusion Model","summary":"  Motion artifacts present in magnetic resonance imaging (MRI) can seriously\ninterfere with clinical diagnosis. Removing motion artifacts is a\nstraightforward solution and has been extensively studied. However, paired data\nare still heavily relied on in recent works and the perturbations in k-space\n(frequency domain) are not well considered, which limits their applications in\nthe clinical field. To address these issues, we propose a novel unsupervised\npurification method which leverages pixel-frequency information of noisy MRI\nimages to guide a pre-trained diffusion model to recover clean MRI images.\nSpecifically, considering that motion artifacts are mainly concentrated in\nhigh-frequency components in k-space, we utilize the low-frequency components\nas the guide to ensure correct tissue textures. Additionally, given that\nhigh-frequency and pixel information are helpful for recovering shape and\ndetail textures, we design alternate complementary masks to simultaneously\ndestroy the artifact structure and exploit useful information. Quantitative\nexperiments are performed on datasets from different tissues and show that our\nmethod achieves superior performance on several metrics. Qualitative\nevaluations with radiologists also show that our method provides better\nclinical feedback. Our code is available at https://github.com/medcx/PFAD.\n","authors":["Jiahua Xu","Dawei Zhou","Lei Hu","Jianfeng Guo","Feng Yang","Zaiyi Liu","Nannan Wang","Xinbo Gao"],"pdf_url":"https://arxiv.org/pdf/2412.07590v2.pdf","comment":"12 pages, 8 figures, AAAI 2025"},{"id":"http://arxiv.org/abs/2412.08307v1","updated":"2024-12-11T11:39:42Z","published":"2024-12-11T11:39:42Z","title":"Template Matters: Understanding the Role of Instruction Templates in\n  Multimodal Language Model Evaluation and Training","summary":"  Current multimodal language models (MLMs) evaluation and training approaches\noverlook the influence of instruction format, presenting an\nelephant-in-the-room problem. Previous research deals with this problem by\nmanually crafting instructions, failing to yield significant insights due to\nlimitations in diversity and scalability. In this work, we propose a\nprogrammatic instruction template generator capable of producing over 39B\nunique template combinations by filling randomly sampled positional synonyms\ninto weighted sampled meta templates, enabling us to comprehensively examine\nthe MLM's performance across diverse instruction templates. Our experiments\nacross eight common MLMs on five benchmark datasets reveal that MLMs have high\ntemplate sensitivities with at most 29% performance gaps between different\ntemplates. We further augment the instruction tuning dataset of LLaVA-1.5 with\nour template generator and perform instruction tuning on LLaVA-1.5-7B and\nLLaVA-1.5-13B. Models tuned on our augmented dataset achieve the best overall\nperformance when compared with the same scale MLMs tuned on at most 75 times\nthe scale of our augmented dataset, highlighting the importance of instruction\ntemplates in MLM training. The code is available at\nhttps://github.com/shijian2001/TemplateMatters .\n","authors":["Shijian Wang","Linxin Song","Jieyu Zhang","Ryotaro Shimizu","Ao Luo","Li Yao","Cunjian Chen","Julian McAuley","Hanqian Wu"],"pdf_url":"https://arxiv.org/pdf/2412.08307v1.pdf","comment":"Code: https://github.com/shijian2001/TemplateMatters"},{"id":"http://arxiv.org/abs/2409.02664v2","updated":"2024-12-11T11:12:14Z","published":"2024-09-04T12:46:30Z","title":"Standing on the Shoulders of Giants: Reprogramming Visual-Language Model\n  for General Deepfake Detection","summary":"  The proliferation of deepfake faces poses huge potential negative impacts on\nour daily lives. Despite substantial advancements in deepfake detection over\nthese years, the generalizability of existing methods against forgeries from\nunseen datasets or created by emerging generative models remains constrained.\nIn this paper, inspired by the zero-shot advantages of Vision-Language Models\n(VLMs), we propose a novel approach that repurposes a well-trained VLM for\ngeneral deepfake detection. Motivated by the model reprogramming paradigm that\nmanipulates the model prediction via data perturbations, our method can\nreprogram a pretrained VLM model (e.g., CLIP) solely based on manipulating its\ninput without tuning the inner parameters. Furthermore, we insert a pseudo-word\nguided by facial identity into the text prompt. Extensive experiments on\nseveral popular benchmarks demonstrate that (1) the cross-dataset and\ncross-manipulation performances of deepfake detection can be significantly and\nconsistently improved (e.g., over 88% AUC in cross-dataset setting from FF++ to\nWildDeepfake) using a pre-trained CLIP model with our proposed reprogramming\nmethod; (2) our superior performances are at less cost of trainable parameters,\nmaking it a promising approach for real-world applications.\n","authors":["Kaiqing Lin","Yuzhen Lin","Weixiang Li","Taiping Yao","Bin Li"],"pdf_url":"https://arxiv.org/pdf/2409.02664v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.06182v2","updated":"2024-12-11T11:07:35Z","published":"2024-12-09T03:41:28Z","title":"Towards Long Video Understanding via Fine-detailed Video Story\n  Generation","summary":"  Long video understanding has become a critical task in computer vision,\ndriving advancements across numerous applications from surveillance to content\nretrieval. Existing video understanding methods suffer from two challenges when\ndealing with long video understanding: intricate long-context relationship\nmodeling and interference from redundancy. To tackle these challenges, we\nintroduce Fine-Detailed Video Story generation (FDVS), which interprets long\nvideos into detailed textual representations. Specifically, to achieve\nfine-grained modeling of long-temporal content, we propose a Bottom-up Video\nInterpretation Mechanism that progressively interprets video content from clips\nto video. To avoid interference from redundant information in videos, we\nintroduce a Semantic Redundancy Reduction mechanism that removes redundancy at\nboth the visual and textual levels. Our method transforms long videos into\nhierarchical textual representations that contain multi-granularity information\nof the video. With these representations, FDVS is applicable to various tasks\nwithout any fine-tuning. We evaluate the proposed method across eight datasets\nspanning three tasks. The performance demonstrates the effectiveness and\nversatility of our method.\n","authors":["Zeng You","Zhiquan Wen","Yaofo Chen","Xin Li","Runhao Zeng","Yaowei Wang","Mingkui Tan"],"pdf_url":"https://arxiv.org/pdf/2412.06182v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17905v2","updated":"2024-12-11T11:07:02Z","published":"2024-07-25T09:51:09Z","title":"StreamMOS: Streaming Moving Object Segmentation with Multi-View\n  Perception and Dual-Span Memory","summary":"  Moving object segmentation based on LiDAR is a crucial and challenging task\nfor autonomous driving and mobile robotics. Most approaches explore\nspatio-temporal information from LiDAR sequences to predict moving objects in\nthe current frame. However, they often focus on transferring temporal cues in a\nsingle inference and regard every prediction as independent of others. This may\ncause inconsistent segmentation results for the same object in different\nframes. To overcome this issue, we propose a streaming network with a memory\nmechanism, called StreamMOS, to build the association of features and\npredictions among multiple inferences. Specifically, we utilize a short-term\nmemory to convey historical features, which can be regarded as spatial prior of\nmoving objects and adopted to enhance current inference by temporal fusion.\nMeanwhile, we build a long-term memory to store previous predictions and\nexploit them to refine the present forecast at voxel and instance levels\nthrough voting. Besides, we present multi-view encoder with cascade projection\nand asymmetric convolution to extract motion feature of objects in different\nrepresentations. Extensive experiments validate that our algorithm gets\ncompetitive performance on SemanticKITTI and Sipailou Campus datasets. Code\nwill be released at https://github.com/NEU-REAL/StreamMOS.git.\n","authors":["Zhiheng Li","Yubo Cui","Jiexi Zhong","Zheng Fang"],"pdf_url":"https://arxiv.org/pdf/2407.17905v2.pdf","comment":"Accepted for publication at IEEE Robotics and Automation Letters\n  (RAL)"},{"id":"http://arxiv.org/abs/2312.00343v8","updated":"2024-12-11T11:04:22Z","published":"2023-12-01T04:35:47Z","title":"OpenStereo: A Comprehensive Benchmark for Stereo Matching and Strong\n  Baseline","summary":"  Stereo matching aims to estimate the disparity between matching pixels in a\nstereo image pair, which is important to robotics, autonomous driving, and\nother computer vision tasks. Despite the development of numerous impressive\nmethods in recent years, determining the most suitable architecture for\npractical application remains challenging. Addressing this gap, our paper\nintroduces a comprehensive benchmark focusing on practical applicability rather\nthan solely on individual models for optimized performance. Specifically, we\ndevelop a flexible and efficient stereo matching codebase, called OpenStereo.\nOpenStereo includes training and inference codes of more than 10 network\nmodels, making it, to our knowledge, the most complete stereo matching toolbox\navailable. Based on OpenStereo, we conducted experiments and have achieved or\nsurpassed the performance metrics reported in the original paper. Additionally,\nwe conduct an exhaustive analysis and deconstruction of recent developments in\nstereo matching through comprehensive ablative experiments. These\ninvestigations inspired the creation of StereoBase, a strong baseline model.\nOur StereoBase ranks 1st on SceneFlow, KITTI 2015, 2012 (Reflective) among\npublished methods and achieves the best performance across all metrics. In\naddition, StereoBase has strong cross-dataset generalization. Code is available\nat \\url{https://github.com/XiandaGuo/OpenStereo}.\n","authors":["Xianda Guo","Chenming Zhang","Juntao Lu","Yiqun Duan","Yiqi Wang","Tian Yang","Zheng Zhu","Long Chen"],"pdf_url":"https://arxiv.org/pdf/2312.00343v8.pdf","comment":"Code is available at: https://github.com/XiandaGuo/OpenStereo"},{"id":"http://arxiv.org/abs/2308.16561v2","updated":"2024-12-11T11:03:53Z","published":"2023-08-31T08:54:59Z","title":"MoMA: Momentum Contrastive Learning with Multi-head Attention-based\n  Knowledge Distillation for Histopathology Image Analysis","summary":"  There is no doubt that advanced artificial intelligence models and high\nquality data are the keys to success in developing computational pathology\ntools. Although the overall volume of pathology data keeps increasing, a lack\nof quality data is a common issue when it comes to a specific task due to\nseveral reasons including privacy and ethical issues with patient data. In this\nwork, we propose to exploit knowledge distillation, i.e., utilize the existing\nmodel to learn a new, target model, to overcome such issues in computational\npathology. Specifically, we employ a student-teacher framework to learn a\ntarget model from a pre-trained, teacher model without direct access to source\ndata and distill relevant knowledge via momentum contrastive learning with\nmulti-head attention mechanism, which provides consistent and context-aware\nfeature representations. This enables the target model to assimilate\ninformative representations of the teacher model while seamlessly adapting to\nthe unique nuances of the target data. The proposed method is rigorously\nevaluated across different scenarios where the teacher model was trained on the\nsame, relevant, and irrelevant classification tasks with the target model.\nExperimental results demonstrate the accuracy and robustness of our approach in\ntransferring knowledge to different domains and tasks, outperforming other\nrelated methods. Moreover, the results provide a guideline on the learning\nstrategy for different types of tasks and scenarios in computational pathology.\nCode is available at: \\url{https://github.com/trinhvg/MoMA}.\n","authors":["Trinh Thi Le Vuong","Jin Tae Kwak"],"pdf_url":"https://arxiv.org/pdf/2308.16561v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21302v4","updated":"2024-12-11T10:58:10Z","published":"2024-10-21T22:52:25Z","title":"Domain-Adaptive Pre-training of Self-Supervised Foundation Models for\n  Medical Image Classification in Gastrointestinal Endoscopy","summary":"  Video capsule endoscopy has transformed gastrointestinal endoscopy (GIE)\ndiagnostics by offering a non-invasive method for capturing detailed images of\nthe gastrointestinal tract, enabling early disease detection. However, its\npotential is limited by the sheer volume of images generated during the imaging\nprocedure, which can take anywhere from 6-8 hours and often produce up to 1\nmillion images, necessitating automated analysis. Additionally, the variability\nof these images, combined with the need for expert annotations and the scarcity\nof large, high-quality labeled datasets, constrains the effectiveness of\ncurrent medical image analysis models. To address this, we introduce a novel\nlarge GIE dataset, called EndoExtend24, created by merging ten existing public\nand private datasets, ensuring patient integrity across splits. EndoExtend24\nincludes over 226,000 labeled images, as well as dynamic class mappings, which\nallow unified training across datasets with differing labeling granularity,\nsupporting up to 123 distinct pathological findings. Further, we propose to\nleverage domain adaptive pre-training of foundation models trained with\nself-supervision on generic image data, to adapt them to the task of GIE\nmedical image diagnosis. Specifically, the EVA-02 model, which is based on the\nViT architecture and trained on ImageNet-22k with masked image modeling (using\nEVA-CLIP as a MIM teacher), is pre-trained on the EndoExtend24 dataset to\nachieve domain adaptation, and finally trained on the Capsule Endoscopy 2024\nChallenge dataset. Our model demonstrates robust performance, securing third\nplace in the Capsule Endoscopy 2024 Challenge. We achieved a macro AUC of 0.762\nand a balanced accuracy of 37.1% on the test set. These results emphasize the\neffectiveness of our domain-adaptive pre-training approach and the enriched\nEndoExtend24 dataset in advancing gastrointestinal endoscopy diagnostics.\n","authors":["Marcel Roth","Micha V. Nowak","Adrian Krenzer","Frank Puppe"],"pdf_url":"https://arxiv.org/pdf/2410.21302v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00779v3","updated":"2024-12-11T10:55:10Z","published":"2024-10-01T15:19:16Z","title":"Local-to-Global Self-Supervised Representation Learning for Diabetic\n  Retinopathy Grading","summary":"  Artificial intelligence algorithms have demonstrated their image\nclassification and segmentation ability in the past decade. However, artificial\nintelligence algorithms perform less for actual clinical data than those used\nfor simulations. This research aims to present a novel hybrid learning model\nusing self-supervised learning and knowledge distillation, which can achieve\nsufficient generalization and robustness. The self-attention mechanism and\ntokens employed in ViT, besides the local-to-global learning approach used in\nthe hybrid model, enable the proposed algorithm to extract a high-dimensional\nand high-quality feature space from images. To demonstrate the proposed neural\nnetwork's capability in classifying and extracting feature spaces from medical\nimages, we use it on a dataset of Diabetic Retinopathy images, specifically the\nEyePACS dataset. This dataset is more complex structurally and challenging\nregarding damaged areas than other medical images. For the first time in this\nstudy, self-supervised learning and knowledge distillation are used to classify\nthis dataset. In our algorithm, for the first time among all self-supervised\nlearning and knowledge distillation models, the test dataset is 50% larger than\nthe training dataset. Unlike many studies, we have not removed any images from\nthe dataset. Finally, our algorithm achieved an accuracy of 79.1% in the linear\nclassifier and 74.36% in the k-NN algorithm for multiclass classification.\nCompared to a similar state-of-the-art model, our results achieved higher\naccuracy and more effective representation spaces.\n","authors":["Mostafa Hajighasemlou","Samad Sheikhaei","Hamid Soltanian-Zadeh"],"pdf_url":"https://arxiv.org/pdf/2410.00779v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.03998v2","updated":"2024-12-11T10:52:47Z","published":"2024-04-05T10:23:10Z","title":"Physics-Inspired Synthesized Underwater Image Dataset","summary":"  This paper introduces the physics-inspired synthesized underwater image\ndataset (PHISWID), a dataset tailored for enhancing underwater image processing\nthrough physics-inspired image synthesis. For underwater image enhancement,\ndata-driven approaches (e.g., deep neural networks) typically demand extensive\ndatasets, yet acquiring paired clean and degraded underwater images poses\nsignificant challenges. Existing datasets have limited contributions to image\nenhancement due to lack of physics models, publicity, and ground-truth images.\nPHISWID addresses these issues by offering a set of paired ground-truth\n(atmospheric) and underwater images synthetically degraded by color degradation\nand marine snow artifacts. Generating underwater images from atmospheric RGB-D\nimages based on physical models provides pairs of real-world ground-truth and\ndegraded images. Our synthetic approach generates a large quantity of the\npairs, enabling effective training of deep neural networks and objective image\nquality assessment. Through benchmark experiment with some datasets and image\nenhance methods, we validate that our dataset can improve the image enhancement\nperformance. Our dataset, which is publicly available, contributes to the\ndevelopment in underwater image processing.\n","authors":["Reina Kaneko","Takumi Ueda","Hiroshi Higashi","Yuichi Tanaka"],"pdf_url":"https://arxiv.org/pdf/2404.03998v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08276v1","updated":"2024-12-11T10:49:15Z","published":"2024-12-11T10:49:15Z","title":"Local Features Meet Stochastic Anonymization: Revolutionizing\n  Privacy-Preserving Face Recognition for Black-Box Models","summary":"  The task of privacy-preserving face recognition (PPFR) currently faces two\nmajor unsolved challenges: (1) existing methods are typically effective only on\nspecific face recognition models and struggle to generalize to black-box face\nrecognition models; (2) current methods employ data-driven reversible\nrepresentation encoding for privacy protection, making them susceptible to\nadversarial learning and reconstruction of the original image. We observe that\nface recognition models primarily rely on local features ({e.g., face contour,\nskin texture, and so on) for identification. Thus, by disrupting global\nfeatures while enhancing local features, we achieve effective recognition even\nin black-box environments. Additionally, to prevent adversarial models from\nlearning and reversing the anonymization process, we adopt an adversarial\nlearning-based approach with irreversible stochastic injection to ensure the\nstochastic nature of the anonymization. Experimental results demonstrate that\nour method achieves an average recognition accuracy of 94.21\\% on black-box\nmodels, outperforming existing methods in both privacy protection and\nanti-reconstruction capabilities.\n","authors":["Yuanwei Liu","Chengyu Jia","Ruqi Xiao","Xuemai Jia","Hui Wei","Kui Jiang","Zheng Wang"],"pdf_url":"https://arxiv.org/pdf/2412.08276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08271v1","updated":"2024-12-11T10:43:11Z","published":"2024-12-11T10:43:11Z","title":"Position-aware Guided Point Cloud Completion with CLIP Model","summary":"  Point cloud completion aims to recover partial geometric and topological\nshapes caused by equipment defects or limited viewpoints. Current methods\neither solely rely on the 3D coordinates of the point cloud to complete it or\nincorporate additional images with well-calibrated intrinsic parameters to\nguide the geometric estimation of the missing parts. Although these methods\nhave achieved excellent performance by directly predicting the location of\ncomplete points, the extracted features lack fine-grained information regarding\nthe location of the missing area. To address this issue, we propose a rapid and\nefficient method to expand an unimodal framework into a multimodal framework.\nThis approach incorporates a position-aware module designed to enhance the\nspatial information of the missing parts through a weighted map learning\nmechanism. In addition, we establish a Point-Text-Image triplet corpus PCI-TI\nand MVP-TI based on the existing unimodal point cloud completion dataset and\nuse the pre-trained vision-language model CLIP to provide richer detail\ninformation for 3D shapes, thereby enhancing performance. Extensive\nquantitative and qualitative experiments demonstrate that our method\noutperforms state-of-the-art point cloud completion methods.\n","authors":["Feng Zhou","Qi Zhang","Ju Dai","Lei Li","Qing Fan","Junliang Xing"],"pdf_url":"https://arxiv.org/pdf/2412.08271v1.pdf","comment":"Accepted by AAAI25"},{"id":"http://arxiv.org/abs/2412.08266v1","updated":"2024-12-11T10:31:06Z","published":"2024-12-11T10:31:06Z","title":"Neural Observation Field Guided Hybrid Optimization of Camera Placement","summary":"  Camera placement is crutial in multi-camera systems such as virtual reality,\nautonomous driving, and high-quality reconstruction. The camera placement\nchallenge lies in the nonlinear nature of high-dimensional parameters and the\nunavailability of gradients for target functions like coverage and visibility.\nConsequently, most existing methods tackle this challenge by leveraging\nnon-gradient-based optimization methods.In this work, we present a hybrid\ncamera placement optimization approach that incorporates both gradient-based\nand non-gradient-based optimization methods. This design allows our method to\nenjoy the advantages of smooth optimization convergence and robustness from\ngradient-based and non-gradient-based optimization, respectively. To bridge the\ntwo disparate optimization methods, we propose a neural observation field,\nwhich implicitly encodes the coverage and observation quality. The neural\nobservation field provides the measurements of the camera observations and\ncorresponding gradients without the assumption of target scenes, making our\nmethod applicable to diverse scenarios, including 2D planar shapes, 3D objects,\nand room-scale 3D scenes.Extensive experiments on diverse datasets demonstrate\nthat our method achieves state-of-the-art performance, while requiring only a\nfraction (8x less) of the typical computation time. Furthermore, we conducted a\nreal-world experiment using a custom-built capture system, confirming the\nresilience of our approach to real-world environmental noise.\n","authors":["Yihan Cao","Jiazhao Zhang","Zhinan Yu","Kai Xu"],"pdf_url":"https://arxiv.org/pdf/2412.08266v1.pdf","comment":"Accepted by Robotics and Automation Letters (RAL 2024)"},{"id":"http://arxiv.org/abs/2411.16316v6","updated":"2024-12-11T10:14:04Z","published":"2024-11-25T12:09:43Z","title":"Monocular Lane Detection Based on Deep Learning: A Survey","summary":"  Lane detection plays an important role in autonomous driving perception\nsystems. As deep learning algorithms gain popularity, monocular lane detection\nmethods based on them have demonstrated superior performance and emerged as a\nkey research direction in autonomous driving perception. The core designs of\nthese algorithmic frameworks can be summarized as follows: (1) Task paradigm,\nfocusing on lane instance-level discrimination; (2) Lane modeling, representing\nlanes as a set of learnable parameters in the neural network; (3) Global\ncontext supplementation, enhancing inference on the obscure lanes; (4)\nPerspective effect elimination, providing accurate 3D lanes for downstream\napplications. From these perspectives, this paper presents a comprehensive\noverview of existing methods, encompassing both the increasingly mature 2D lane\ndetection approaches and the developing 3D lane detection works. Besides, this\npaper compares the performance of mainstream methods on different benchmarks\nand investigates their inference speed under a unified setting for fair\ncomparison. Moreover, we present some extended works on lane detection,\nincluding multi-task perception, video lane detection, online high-definition\nmap construction, and lane topology reasoning, to offer readers a comprehensive\nroadmap for the evolution of lane detection. Finally, we point out some\npotential future research directions in this field. We exhaustively collect the\npapers and codes of existing works at\nhttps://github.com/Core9724/Awesome-Lane-Detection and will keep tracing the\nresearch.\n","authors":["Xin He","Haiyun Guo","Kuan Zhu","Bingke Zhu","Xu Zhao","Jianwu Fang","Jinqiao Wang"],"pdf_url":"https://arxiv.org/pdf/2411.16316v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07485v2","updated":"2024-12-11T10:12:48Z","published":"2024-07-10T09:16:14Z","title":"Zero-Shot Class Unlearning in CLIP with Synthetic Samples","summary":"  Machine unlearning is a crucial area of research. It is driven by the need to\nremove sensitive information from models to safeguard individuals' right to be\nforgotten under rigorous regulations such as GDPR. In this work, we focus on\nunlearning within CLIP, a dual vision-language encoder model trained on a\nmassive dataset of image-text pairs using contrastive loss. To achieve\nforgetting we expand the application of Lipschitz regularization to the\nmultimodal context of CLIP. Specifically, we ensure the smoothing of both\nvisual and textual embeddings associated with the class intended to be\nforgotten relative to the perturbation introduced to the samples from that\nclass. Additionally, importantly, we remove the necessity for real forgetting\ndata by generating synthetic samples through gradient ascent maximizing the\ntarget class. Our forgetting procedure is iterative, where we track accuracy on\na synthetic forget set and stop when accuracy falls below a chosen threshold.\nWe employ a selective layers update strategy based on their average absolute\ngradient value to mitigate over-forgetting. We validate our approach on several\nstandard datasets and provide thorough ablation analysis and comparisons with\nprevious work.\n","authors":["A. Kravets","V. Namboodiri"],"pdf_url":"https://arxiv.org/pdf/2407.07485v2.pdf","comment":"Published in WACV 2025 conference"},{"id":"http://arxiv.org/abs/2411.07941v2","updated":"2024-12-11T10:01:03Z","published":"2024-11-12T17:11:18Z","title":"DuoLift-GAN:Reconstructing CT from Single-view and Biplanar X-Rays with\n  Generative Adversarial Networks","summary":"  Computed tomography (CT) provides highly detailed three-dimensional (3D)\nmedical images but is costly, time-consuming, and often inaccessible in\nintraoperative settings (Organization et al. 2011). Recent advancements have\nexplored reconstructing 3D chest volumes from sparse 2D X-rays, such as\nsingle-view or orthogonal double-view images. However, current models tend to\nprocess 2D images in a planar manner, prioritizing visual realism over\nstructural accuracy. In this work, we introduce DuoLift Generative Adversarial\nNetworks (DuoLift-GAN), a novel architecture with dual branches that\nindependently elevate 2D images and their features into 3D representations.\nThese 3D outputs are merged into a unified 3D feature map and decoded into a\ncomplete 3D chest volume, enabling richer 3D information capture. We also\npresent a masked loss function that directs reconstruction towards critical\nanatomical regions, improving structural accuracy and visual quality. This\npaper demonstrates that DuoLift-GAN significantly enhances reconstruction\naccuracy while achieving superior visual realism compared to existing methods.\n","authors":["Zhaoxi Zhang","Yueliang Ying"],"pdf_url":"https://arxiv.org/pdf/2411.07941v2.pdf","comment":"9 pages, LaTeX; removed the superscript numbers associated with the\n  authors' names for clarity, typos corrected"},{"id":"http://arxiv.org/abs/2412.08247v1","updated":"2024-12-11T09:55:09Z","published":"2024-12-11T09:55:09Z","title":"MoMuSE: Momentum Multi-modal Target Speaker Extraction for Real-time\n  Scenarios with Impaired Visual Cues","summary":"  Audio-visual Target Speaker Extraction (AV-TSE) aims to isolate the speech of\na specific target speaker from an audio mixture using time-synchronized visual\ncues. In real-world scenarios, visual cues are not always available due to\nvarious impairments, which undermines the stability of AV-TSE. Despite this\nchallenge, humans can maintain attentional momentum over time, even when the\ntarget speaker is not visible. In this paper, we introduce the Momentum\nMulti-modal target Speaker Extraction (MoMuSE), which retains a speaker\nidentity momentum in memory, enabling the model to continuously track the\ntarget speaker. Designed for real-time inference, MoMuSE extracts the current\nspeech window with guidance from both visual cues and dynamically updated\nspeaker momentum. Experimental results demonstrate that MoMuSE exhibits\nsignificant improvement, particularly in scenarios with severe impairment of\nvisual cues.\n","authors":["Junjie Li","Ke Zhang","Shuai Wang","Kong Aik Lee","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2412.08247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08243v1","updated":"2024-12-11T09:53:10Z","published":"2024-12-11T09:53:10Z","title":"Hierarchical Context Alignment with Disentangled Geometric and Temporal\n  Modeling for Semantic Occupancy Prediction","summary":"  Camera-based 3D Semantic Occupancy Prediction (SOP) is crucial for\nunderstanding complex 3D scenes from limited 2D image observations. Existing\nSOP methods typically aggregate contextual features to assist the occupancy\nrepresentation learning, alleviating issues like occlusion or ambiguity.\nHowever, these solutions often face misalignment issues wherein the\ncorresponding features at the same position across different frames may have\ndifferent semantic meanings during the aggregation process, which leads to\nunreliable contextual fusion results and an unstable representation learning\nprocess. To address this problem, we introduce a new Hierarchical context\nalignment paradigm for a more accurate SOP (Hi-SOP). Hi-SOP first disentangles\nthe geometric and temporal context for separate alignment, which two branches\nare then composed to enhance the reliability of SOP. This parsing of the visual\ninput into a local-global alignment hierarchy includes: (I) disentangled\ngeometric and temporal separate alignment, within each leverages depth\nconfidence and camera pose as prior for relevant feature matching respectively;\n(II) global alignment and composition of the transformed geometric and temporal\nvolumes based on semantics consistency. Our method outperforms SOTAs for\nsemantic scene completion on the SemanticKITTI & NuScenes-Occupancy datasets\nand LiDAR semantic segmentation on the NuScenes dataset.\n","authors":["Bohan Li","Xin Jin","Jiajun Deng","Yasheng Sun","Xiaofeng Wang","Wenjun Zeng"],"pdf_url":"https://arxiv.org/pdf/2412.08243v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08241v1","updated":"2024-12-11T09:52:08Z","published":"2024-12-11T09:52:08Z","title":"Adversarial Contrastive Domain-Generative Learning for Bacteria Raman\n  Spectrum Joint Denoising and Cross-Domain Identification","summary":"  Raman spectroscopy, as a label-free detection technology, has been widely\nutilized in the clinical diagnosis of pathogenic bacteria. However, Raman\nsignals are naturally weak and sensitive to the condition of the acquisition\nprocess. The characteristic spectra of a bacteria can manifest varying\nsignal-to-noise ratios and domain discrepancies under different acquisition\nconditions. Consequently, existing methods often face challenges when making\nidentification for unobserved acquisition conditions, i.e., the testing\nacquisition conditions are unavailable during model training. In this article,\na generic framework, namely, an adversarial contrastive domain-generative\nlearning framework, is proposed for joint Raman spectroscopy denoising and\ncross-domain identification. The proposed method is composed of a domain\ngeneration module and a domain task module. Through adversarial learning\nbetween these two modules, it utilizes only a single available source domain\nspectral data to generate extended denoised domains that are semantically\nconsistent with the source domain and extracts domain-invariant\nrepresentations. Comprehensive case studies indicate that the proposed method\ncan simultaneously conduct spectral denoising without necessitating noise-free\nground-truth and can achieve improved diagnostic accuracy and robustness under\ncross-domain unseen spectral acquisition conditions. This suggests that the\nproposed method holds remarkable potential as a diagnostic tool in real\nclinical cases.\n","authors":["Haiming Yao","Wei Luo","Xue Wang"],"pdf_url":"https://arxiv.org/pdf/2412.08241v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08240v1","updated":"2024-12-11T09:52:01Z","published":"2024-12-11T09:52:01Z","title":"Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse\n  Brain Tumors in MRI from Gliomas to Pediatric Tumors","summary":"  Accurate segmentation of brain tumors from 3D multimodal MRI is vital for\ndiagnosis and treatment planning across diverse brain tumors. This paper\naddresses the challenges posed by the BraTS 2023, presenting a unified transfer\nlearning approach that applies to a broader spectrum of brain tumors. We\nintroduce HT-CNNs, an ensemble of Hybrid Transformers and Convolutional Neural\nNetworks optimized through transfer learning for varied brain tumor\nsegmentation. This method captures spatial and contextual details from MRI\ndata, fine-tuned on diverse datasets representing common tumor types. Through\ntransfer learning, HT-CNNs utilize the learned representations from one task to\nimprove generalization in another, harnessing the power of pre-trained models\non large datasets and fine-tuning them on specific tumor types. We preprocess\ndiverse datasets from multiple international distributions, ensuring\nrepresentativeness for the most common brain tumors. Our rigorous evaluation\nemploys standardized quantitative metrics across all tumor types, ensuring\nrobustness and generalizability. The proposed ensemble model achieves superior\nsegmentation results across the BraTS validation datasets over the previous\nwinning methods. Comprehensive quantitative evaluations using the DSC and HD95\ndemonstrate the effectiveness of our approach. Qualitative segmentation\npredictions further validate the high-quality outputs produced by our model.\nOur findings underscore the potential of transfer learning and ensemble\napproaches in medical image segmentation, indicating a substantial enhancement\nin clinical decision-making and patient care. Despite facing challenges related\nto post-processing and domain gaps, our study sets a new precedent for future\nresearch for brain tumor segmentation. The docker image for the code and models\nhas been made publicly available, https://hub.docker.com/r/razeineldin/ht-cnns.\n","authors":["Ramy A. Zeineldin","Franziska Mathis-Ullrich"],"pdf_url":"https://arxiv.org/pdf/2412.08240v1.pdf","comment":"Accepted in the Computer Assisted Radiology and Surgery (CARS 2024)\n  Conference"},{"id":"http://arxiv.org/abs/2408.06010v2","updated":"2024-12-11T09:48:08Z","published":"2024-08-12T08:56:49Z","title":"DEEPTalk: Dynamic Emotion Embedding for Probabilistic Speech-Driven 3D\n  Face Animation","summary":"  Speech-driven 3D facial animation has garnered lots of attention thanks to\nits broad range of applications. Despite recent advancements in achieving\nrealistic lip motion, current methods fail to capture the nuanced emotional\nundertones conveyed through speech and produce monotonous facial motion. These\nlimitations result in blunt and repetitive facial animations, reducing user\nengagement and hindering their applicability. To address these challenges, we\nintroduce DEEPTalk, a novel approach that generates diverse and emotionally\nrich 3D facial expressions directly from speech inputs. To achieve this, we\nfirst train DEE (Dynamic Emotion Embedding), which employs probabilistic\ncontrastive learning to forge a joint emotion embedding space for both speech\nand facial motion. This probabilistic framework captures the uncertainty in\ninterpreting emotions from speech and facial motion, enabling the derivation of\nemotion vectors from its multifaceted space. Moreover, to generate dynamic\nfacial motion, we design TH-VQVAE (Temporally Hierarchical VQ-VAE) as an\nexpressive and robust motion prior overcoming limitations of VAEs and VQ-VAEs.\nUtilizing these strong priors, we develop DEEPTalk, A talking head generator\nthat non-autoregressively predicts codebook indices to create dynamic facial\nmotion, incorporating a novel emotion consistency loss. Extensive experiments\non various datasets demonstrate the effectiveness of our approach in creating\ndiverse, emotionally expressive talking faces that maintain accurate lip-sync.\nSource code will be made publicly available soon.\n","authors":["Jisoo Kim","Jungbin Cho","Joonho Park","Soonmin Hwang","Da Eun Kim","Geon Kim","Youngjae Yu"],"pdf_url":"https://arxiv.org/pdf/2408.06010v2.pdf","comment":"First two authors contributed equally. This is a revised version of\n  the original submission, which has been accepted for publication at AAAI 2025"},{"id":"http://arxiv.org/abs/2412.08231v1","updated":"2024-12-11T09:31:03Z","published":"2024-12-11T09:31:03Z","title":"Dynamic Modality-Camera Invariant Clustering for Unsupervised\n  Visible-Infrared Person Re-identification","summary":"  Unsupervised learning visible-infrared person re-identification (USL-VI-ReID)\noffers a more flexible and cost-effective alternative compared to supervised\nmethods. This field has gained increasing attention due to its promising\npotential. Existing methods simply cluster modality-specific samples and employ\nstrong association techniques to achieve instance-to-cluster or\ncluster-to-cluster cross-modality associations. However, they ignore\ncross-camera differences, leading to noticeable issues with excessive splitting\nof identities. Consequently, this undermines the accuracy and reliability of\ncross-modal associations. To address these issues, we propose a novel Dynamic\nModality-Camera Invariant Clustering (DMIC) framework for USL-VI-ReID.\nSpecifically, our DMIC naturally integrates Modality-Camera Invariant Expansion\n(MIE), Dynamic Neighborhood Clustering (DNC) and Hybrid Modality Contrastive\nLearning (HMCL) into a unified framework, which eliminates both the\ncross-modality and cross-camera discrepancies in clustering. MIE fuses\ninter-modal and inter-camera distance coding to bridge the gaps between\nmodalities and cameras at the clustering level. DNC employs two dynamic search\nstrategies to refine the network's optimization objective, transitioning from\nimproving discriminability to enhancing cross-modal and cross-camera\ngeneralizability. Moreover, HMCL is designed to optimize instance-level and\ncluster-level distributions. Memories for intra-modality and inter-modality\ntraining are updated using randomly selected samples, facilitating real-time\nexploration of modality-invariant representations. Extensive experiments have\ndemonstrated that our DMIC addresses the limitations present in current\nclustering approaches and achieve competitive performance, which significantly\nreduces the performance gap with supervised methods.\n","authors":["Yiming Yang","Weipeng Hu","Haifeng Hu"],"pdf_url":"https://arxiv.org/pdf/2412.08231v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02965v2","updated":"2024-12-11T09:30:19Z","published":"2024-03-05T13:41:25Z","title":"ChatGPT and biometrics: an assessment of face recognition, gender\n  detection, and age estimation capabilities","summary":"  This paper explores the application of large language models (LLMs), like\nChatGPT, for biometric tasks. We specifically examine the capabilities of\nChatGPT in performing biometric-related tasks, with an emphasis on face\nrecognition, gender detection, and age estimation. Since biometrics are\nconsidered as sensitive information, ChatGPT avoids answering direct prompts,\nand thus we crafted a prompting strategy to bypass its safeguard and evaluate\nthe capabilities for biometrics tasks. Our study reveals that ChatGPT\nrecognizes facial identities and differentiates between two facial images with\nconsiderable accuracy. Additionally, experimental results demonstrate\nremarkable performance in gender detection and reasonable accuracy for the age\nestimation tasks. Our findings shed light on the promising potentials in the\napplication of LLMs and foundation models for biometrics.\n","authors":["Ahmad Hassanpour","Yasamin Kowsari","Hatef Otroshi Shahreza","Bian Yang","Sebastien Marcel"],"pdf_url":"https://arxiv.org/pdf/2403.02965v2.pdf","comment":"Published as a conference paper at IEEE International Conference on\n  Image Processing (ICIP) 2024"},{"id":"http://arxiv.org/abs/2412.08228v1","updated":"2024-12-11T09:28:30Z","published":"2024-12-11T09:28:30Z","title":"Hierarchical Classification for Automated Image Annotation of Coral Reef\n  Benthic Structures","summary":"  Automated benthic image annotation is crucial to efficiently monitor and\nprotect coral reefs against climate change. Current machine learning approaches\nfail to capture the hierarchical nature of benthic organisms covering reef\nsubstrata, i.e., coral taxonomic levels and health condition. To address this\nlimitation, we propose to annotate benthic images using hierarchical\nclassification. Experiments on a custom dataset from a Northeast Brazilian\ncoral reef show that our approach outperforms flat classifiers, improving both\nF1 and hierarchical F1 scores by approximately 2\\% across varying amounts of\ntraining data. In addition, this hierarchical method aligns more closely with\necological objectives.\n","authors":["Célia Blondin","Joris Guérin","Kelly Inagaki","Guilherme Longo","Laure Berti-Équille"],"pdf_url":"https://arxiv.org/pdf/2412.08228v1.pdf","comment":"Poster at Tackling Climate Change with Machine Learning: workshop at\n  NeurIPS 2024"},{"id":"http://arxiv.org/abs/2406.17462v2","updated":"2024-12-11T09:23:17Z","published":"2024-06-25T11:05:26Z","title":"EvolvED: Evolutionary Embeddings to Understand the Generation Process of\n  Diffusion Models","summary":"  Diffusion models, widely used in image generation, rely on iterative\nrefinement to generate images from noise. Understanding this data evolution is\nimportant for model development and interpretability, yet challenging due to\nits high-dimensional, iterative nature. Prior works often focus on static or\ninstance-level analyses, missing the iterative and holistic aspects of the\ngenerative path. While dimensionality reduction can visualize image evolution\nfor few instances, it does preserve the iterative structure. To address these\ngaps, we introduce EvolvED, a method that presents a holistic view of the\niterative generative process in diffusion models. EvolvED goes beyond instance\nexploration by leveraging predefined research questions to streamline\ngenerative space exploration. Tailored prompts aligned with these questions are\nused to extract intermediate images, preserving iterative context. Targeted\nfeature extractors trace the evolution of key image attribute evolution,\naddressing the complexity of high-dimensional outputs. Central to EvolvED is a\nnovel evolutionary embedding algorithm that encodes iterative steps while\nmaintaining semantic relations. It enhances the visualization of data evolution\nby clustering semantically similar elements within each iteration with t-SNE,\ngrouping elements by iteration, and aligning an instance's elements across\niterations. We present rectilinear and radial layouts to represent iterations\nand support exploration. We apply EvolvED to diffusion models like GLIDE and\nStable Diffusion, demonstrating its ability to provide valuable insights into\nthe generative process.\n","authors":["Vidya Prasad","Hans van Gorp","Christina Humer","Ruud J. G. van Sloun","Anna Vilanova","Nicola Pezzotti"],"pdf_url":"https://arxiv.org/pdf/2406.17462v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08221v1","updated":"2024-12-11T09:17:39Z","published":"2024-12-11T09:17:39Z","title":"Generate Any Scene: Evaluating and Improving Text-to-Vision Generation\n  with Scene Graph Programming","summary":"  DALL-E and Sora have gained attention by producing implausible images, such\nas \"astronauts riding a horse in space.\" Despite the proliferation of\ntext-to-vision models that have inundated the internet with synthetic visuals,\nfrom images to 3D assets, current benchmarks predominantly evaluate these\nmodels on real-world scenes paired with captions. We introduce Generate Any\nScene, a framework that systematically enumerates scene graphs representing a\nvast array of visual scenes, spanning realistic to imaginative compositions.\nGenerate Any Scene leverages 'scene graph programming', a method for\ndynamically constructing scene graphs of varying complexity from a structured\ntaxonomy of visual elements. This taxonomy includes numerous objects,\nattributes, and relations, enabling the synthesis of an almost infinite variety\nof scene graphs. Using these structured representations, Generate Any Scene\ntranslates each scene graph into a caption, enabling scalable evaluation of\ntext-to-vision models through standard metrics. We conduct extensive\nevaluations across multiple text-to-image, text-to-video, and text-to-3D\nmodels, presenting key findings on model performance. We find that DiT-backbone\ntext-to-image models align more closely with input captions than UNet-backbone\nmodels. Text-to-video models struggle with balancing dynamics and consistency,\nwhile both text-to-video and text-to-3D models show notable gaps in human\npreference alignment. We demonstrate the effectiveness of Generate Any Scene by\nconducting three practical applications leveraging captions generated by\nGenerate Any Scene: 1) a self-improving framework where models iteratively\nenhance their performance using generated data, 2) a distillation process to\ntransfer specific strengths from proprietary models to open-source\ncounterparts, and 3) improvements in content moderation by identifying and\ngenerating challenging synthetic data.\n","authors":["Ziqi Gao","Weikai Huang","Jieyu Zhang","Aniruddha Kembhavi","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2412.08221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.09006v2","updated":"2024-12-11T09:07:25Z","published":"2024-01-17T06:59:32Z","title":"Generalized Face Liveness Detection via De-fake Face Generator","summary":"  Previous Face Anti-spoofing (FAS) methods face the challenge of generalizing\nto unseen domains, mainly because most existing FAS datasets are relatively\nsmall and lack data diversity. Thanks to the development of face recognition in\nthe past decade, numerous real face images are available publicly, which are\nhowever neglected previously by the existing literature. In this paper, we\npropose an Anomalous cue Guided FAS (AG-FAS) method, which can effectively\nleverage large-scale additional real faces for improving model generalization\nvia a De-fake Face Generator (DFG). Specifically, by training on a large-scale\nreal face only dataset, the generator obtains the knowledge of what a real face\nshould be like, and thus has the capability of generating a \"real\" version of\nany input face image. Consequently, the difference between the input face and\nthe generated \"real\" face can be treated as cues of attention for the fake\nfeature learning. With the above ideas, an Off-real Attention Network (OA-Net)\nis proposed which allocates its attention to the spoof region of the input\naccording to the anomalous cue. Extensive experiments on a total of nine public\ndatasets show our method achieves state-of-the-art results under cross-domain\nevaluations with unseen scenarios and unknown presentation attacks. Besides, we\nprovide theoretical analysis demonstrating the effectiveness of the proposed\nanomalous cues.\n","authors":["Xingming Long","Jie Zhang","Shiguang Shan"],"pdf_url":"https://arxiv.org/pdf/2401.09006v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2411.17251v3","updated":"2024-12-11T09:04:22Z","published":"2024-11-26T09:29:27Z","title":"DGNN-YOLO: Interpretable Dynamic Graph Neural Networks with YOLO11 for\n  Small Object Detection and Tracking in Traffic Surveillance","summary":"  Accurate detection and tracking of small objects, such as pedestrians,\ncyclists, and motorbikes, is critical for traffic surveillance systems, which\nare crucial for improving road safety and decision-making in intelligent\ntransportation systems. However, traditional methods face challenges such as\nocclusion, low resolution, and dynamic traffic conditions, necessitating\ninnovative approaches to address these limitations. This paper introduces\nDGNN-YOLO, a novel framework integrating dynamic graph neural networks (DGNN)\nwith YOLO11 to enhance small-object detection and tracking in traffic\nsurveillance systems. The framework leverages YOLO11's advanced spatial feature\nextraction capabilities for precise object detection and incorporates a DGNN to\nmodel spatial-temporal relationships for robust real-time tracking dynamically.\nBy constructing and updating graph structures, DGNN-YOLO effectively represents\nobjects as nodes and their interactions as edges, thereby ensuring adaptive and\naccurate tracking in complex and dynamic environments. Additionally, Grad-CAM,\nGrad-CAM++, and Eigen-CAM visualization techniques were applied to DGNN-YOLO to\nprovide model-agnostic interpretability and deeper insights into the model's\ndecision-making process, enhancing its transparency and trustworthiness.\nExtensive experiments demonstrated that DGNN-YOLO consistently outperformed\nstate-of-the-art methods in detecting and tracking small objects under diverse\ntraffic conditions, achieving the highest precision (0.8382), recall (0.6875),\nand mAP@0.5:0.95 (0.6476), showing its robustness and scalability, particularly\nin challenging scenarios involving small and occluded objects. This study\nprovides a scalable, real-time traffic surveillance and analysis solution,\nsignificantly contributing to intelligent transportation systems.\n","authors":["Shahriar Soudeep","M. F. Mridha","Md Abrar Jahin","Nilanjan Dey"],"pdf_url":"https://arxiv.org/pdf/2411.17251v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08210v1","updated":"2024-12-11T08:59:04Z","published":"2024-12-11T08:59:04Z","title":"Unicorn: Unified Neural Image Compression with One Number Reconstruction","summary":"  Prevalent lossy image compression schemes can be divided into: 1) explicit\nimage compression (EIC), including traditional standards and neural end-to-end\nalgorithms; 2) implicit image compression (IIC) based on implicit neural\nrepresentations (INR). The former is encountering impasses of either leveling\noff bitrate reduction at a cost of tremendous complexity while the latter\nsuffers from excessive smoothing quality as well as lengthy decoder models. In\nthis paper, we propose an innovative paradigm, which we dub \\textbf{Unicorn}\n(\\textbf{U}nified \\textbf{N}eural \\textbf{I}mage \\textbf{C}ompression with\n\\textbf{O}ne \\textbf{N}number \\textbf{R}econstruction). By conceptualizing the\nimages as index-image pairs and learning the inherent distribution of pairs in\na subtle neural network model, Unicorn can reconstruct a visually pleasing\nimage from a randomly generated noise with only one index number. The neural\nmodel serves as the unified decoder of images while the noises and indexes\ncorresponds to explicit representations. As a proof of concept, we propose an\neffective and efficient prototype of Unicorn based on latent diffusion models\nwith tailored model designs. Quantitive and qualitative experimental results\ndemonstrate that our prototype achieves significant bitrates reduction compared\nwith EIC and IIC algorithms. More impressively, benefitting from the unified\ndecoder, our compression ratio escalates as the quantity of images increases.\nWe envision that more advanced model designs will endow Unicorn with greater\npotential in image compression. We will release our codes in\n\\url{https://github.com/uniqzheng/Unicorn-Laduree}.\n","authors":["Qi Zheng","Haozhi Wang","Zihao Liu","Jiaming Liu","Peiye Liu","Zhijian Hao","Yanheng Lu","Dimin Niu","Jinjia Zhou","Minge Jing","Yibo Fan"],"pdf_url":"https://arxiv.org/pdf/2412.08210v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05184v3","updated":"2024-12-11T08:56:37Z","published":"2024-06-07T18:04:21Z","title":"The Unmet Promise of Synthetic Training Images: Using Retrieved Real\n  Images Performs Better","summary":"  Generative text-to-image models enable us to synthesize unlimited amounts of\nimages in a controllable manner, spurring many recent efforts to train vision\nmodels with synthetic data. However, every synthetic image ultimately\noriginates from the upstream data used to train the generator. Does the\nintermediate generator provide additional information over directly training on\nrelevant parts of the upstream data? Grounding this question in the setting of\nimage classification, we compare finetuning on task-relevant, targeted\nsynthetic data generated by Stable Diffusion -- a generative model trained on\nthe LAION-2B dataset -- against finetuning on targeted real images retrieved\ndirectly from LAION-2B. We show that while synthetic data can benefit some\ndownstream tasks, it is universally matched or outperformed by real data from\nthe simple retrieval baseline. Our analysis suggests that this underperformance\nis partially due to generator artifacts and inaccurate task-relevant visual\ndetails in the synthetic images. Overall, we argue that targeted retrieval is a\ncritical baseline to consider when training with synthetic data -- a baseline\nthat current methods do not yet surpass. We release code, data, and models at\nhttps://github.com/scottgeng00/unmet-promise.\n","authors":["Scott Geng","Cheng-Yu Hsieh","Vivek Ramanujan","Matthew Wallingford","Chun-Liang Li","Pang Wei Koh","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2406.05184v3.pdf","comment":"Correspondence to sgeng at cs dot washington dot edu. RK and PWK\n  equally advised the project"},{"id":"http://arxiv.org/abs/2412.08200v1","updated":"2024-12-11T08:43:52Z","published":"2024-12-11T08:43:52Z","title":"GN-FR:Generalizable Neural Radiance Fields for Flare Removal","summary":"  Flare, an optical phenomenon resulting from unwanted scattering and\nreflections within a lens system, presents a significant challenge in imaging.\nThe diverse patterns of flares, such as halos, streaks, color bleeding, and\nhaze, complicate the flare removal process. Existing traditional and\nlearning-based methods have exhibited limited efficacy due to their reliance on\nsingle-image approaches, where flare removal is highly ill-posed. We address\nthis by framing flare removal as a multi-view image problem, taking advantage\nof the view-dependent nature of flare artifacts. This approach leverages\ninformation from neighboring views to recover details obscured by flare in\nindividual images. Our proposed framework, GN-FR (Generalizable Neural Radiance\nFields for Flare Removal), can render flare-free views from a sparse set of\ninput images affected by lens flare and generalizes across different scenes in\nan unsupervised manner. GN-FR incorporates several modules within the\nGeneralizable NeRF Transformer (GNT) framework: Flare-occupancy Mask Generation\n(FMG), View Sampler (VS), and Point Sampler (PS). To overcome the\nimpracticality of capturing both flare-corrupted and flare-free data, we\nintroduce a masking loss function that utilizes mask information in an\nunsupervised setting. Additionally, we present a 3D multi-view flare dataset,\ncomprising 17 real flare scenes with 782 images, 80 real flare patterns, and\ntheir corresponding annotated flare-occupancy masks. To our knowledge, this is\nthe first work to address flare removal within a Neural Radiance Fields (NeRF)\nframework.\n","authors":["Gopi Raju Matta","Rahul Siddartha","Rongali Simhachala Venkata Girish","Sumit Sharma","Kaushik Mitra"],"pdf_url":"https://arxiv.org/pdf/2412.08200v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04653v2","updated":"2024-12-11T08:42:20Z","published":"2024-12-05T22:50:42Z","title":"Hidden in the Noise: Two-Stage Robust Watermarking for Images","summary":"  As the quality of image generators continues to improve, deepfakes become a\ntopic of considerable societal debate. Image watermarking allows responsible\nmodel owners to detect and label their AI-generated content, which can mitigate\nthe harm. Yet, current state-of-the-art methods in image watermarking remain\nvulnerable to forgery and removal attacks. This vulnerability occurs in part\nbecause watermarks distort the distribution of generated images,\nunintentionally revealing information about the watermarking techniques.\n  In this work, we first demonstrate a distortion-free watermarking method for\nimages, based on a diffusion model's initial noise. However, detecting the\nwatermark requires comparing the initial noise reconstructed for an image to\nall previously used initial noises. To mitigate these issues, we propose a\ntwo-stage watermarking framework for efficient detection. During generation, we\naugment the initial noise with generated Fourier patterns to embed information\nabout the group of initial noises we used. For detection, we (i) retrieve the\nrelevant group of noises, and (ii) search within the given group for an initial\nnoise that might match our image. This watermarking approach achieves\nstate-of-the-art robustness to forgery and removal against a large battery of\nattacks.\n","authors":["Kasra Arabi","Benjamin Feuer","R. Teal Witter","Chinmay Hegde","Niv Cohen"],"pdf_url":"https://arxiv.org/pdf/2412.04653v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08197v1","updated":"2024-12-11T08:40:37Z","published":"2024-12-11T08:40:37Z","title":"SAFIRE: Segment Any Forged Image Region","summary":"  Most techniques approach the problem of image forgery localization as a\nbinary segmentation task, training neural networks to label original areas as 0\nand forged areas as 1. In contrast, we tackle this issue from a more\nfundamental perspective by partitioning images according to their originating\nsources. To this end, we propose Segment Any Forged Image Region (SAFIRE),\nwhich solves forgery localization using point prompting. Each point on an image\nis used to segment the source region containing itself. This allows us to\npartition images into multiple source regions, a capability achieved for the\nfirst time. Additionally, rather than memorizing certain forgery traces, SAFIRE\nnaturally focuses on uniform characteristics within each source region. This\napproach leads to more stable and effective learning, achieving superior\nperformance in both the new task and the traditional binary forgery\nlocalization.\n","authors":["Myung-Joon Kwon","Wonjun Lee","Seung-Hun Nam","Minji Son","Changick Kim"],"pdf_url":"https://arxiv.org/pdf/2412.08197v1.pdf","comment":"Accepted at AAAI 2025. Code is available at:\n  https://github.com/mjkwon2021/SAFIRE"},{"id":"http://arxiv.org/abs/2412.08196v1","updated":"2024-12-11T08:36:50Z","published":"2024-12-11T08:36:50Z","title":"DocSum: Domain-Adaptive Pre-training for Document Abstractive\n  Summarization","summary":"  Abstractive summarization has made significant strides in condensing and\nrephrasing large volumes of text into coherent summaries. However, summarizing\nadministrative documents presents unique challenges due to domain-specific\nterminology, OCR-generated errors, and the scarcity of annotated datasets for\nmodel fine-tuning. Existing models often struggle to adapt to the intricate\nstructure and specialized content of such documents. To address these\nlimitations, we introduce DocSum, a domain-adaptive abstractive summarization\nframework tailored for administrative documents. Leveraging pre-training on\nOCR-transcribed text and fine-tuning with an innovative integration of\nquestion-answer pairs, DocSum enhances summary accuracy and relevance. This\napproach tackles the complexities inherent in administrative content, ensuring\noutputs that align with real-world business needs. To evaluate its\ncapabilities, we define a novel downstream task setting-Document Abstractive\nSummarization-which reflects the practical requirements of business and\norganizational settings. Comprehensive experiments demonstrate DocSum's\neffectiveness in producing high-quality summaries, showcasing its potential to\nimprove decision-making and operational workflows across the public and private\nsectors.\n","authors":["Phan Phuong Mai Chau","Souhail Bakkali","Antoine Doucet"],"pdf_url":"https://arxiv.org/pdf/2412.08196v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08195v1","updated":"2024-12-11T08:36:36Z","published":"2024-12-11T08:36:36Z","title":"Semantic Scene Completion Based 3D Traversability Estimation for\n  Off-Road Terrains","summary":"  Off-road environments present significant challenges for autonomous ground\nvehicles due to the absence of structured roads and the presence of complex\nobstacles, such as uneven terrain, vegetation, and occlusions. Traditional\nperception algorithms, designed primarily for structured environments, often\nfail under these conditions, leading to inaccurate traversability estimations.\nIn this paper, ORDformer, a novel multimodal method that combines LiDAR point\nclouds with monocular images, is proposed to generate dense traversable\noccupancy predictions from a forward-facing perspective. By integrating\nmultimodal data, environmental feature extraction is enhanced, which is crucial\nfor accurate occupancy estimation in complex terrains. Furthermore, RELLIS-OCC,\na dataset with 3D traversable occupancy annotations, is introduced,\nincorporating geometric features such as step height, slope, and unevenness.\nThrough a comprehensive analysis of vehicle obstacle-crossing conditions and\nthe incorporation of vehicle body structure constraints, four traversability\ncost labels are generated: lethal, medium-cost, low-cost, and free.\nExperimental results demonstrate that ORDformer outperforms existing approaches\nin 3D traversable area recognition, particularly in off-road environments with\nirregular geometries and partial occlusions. Specifically, ORDformer achieves\nover a 20\\% improvement in scene completion IoU compared to other models. The\nproposed framework is scalable and adaptable to various vehicle platforms,\nallowing for adjustments to occupancy grid parameters and the integration of\nadvanced dynamic models for traversability cost estimation.\n","authors":["Zitong Chen","Chao Sun","Shida Nie","Chen Min","Changjiu Ning","Haoyu Li","Bo Wang"],"pdf_url":"https://arxiv.org/pdf/2412.08195v1.pdf","comment":"12 pages,14 figures"},{"id":"http://arxiv.org/abs/2412.08189v1","updated":"2024-12-11T08:31:47Z","published":"2024-12-11T08:31:47Z","title":"Breaking the Bias: Recalibrating the Attention of Industrial Anomaly\n  Detection","summary":"  Due to the scarcity and unpredictable nature of defect samples, industrial\nanomaly detection (IAD) predominantly employs unsupervised learning. However,\nall unsupervised IAD methods face a common challenge: the inherent bias in\nnormal samples, which causes models to focus on variable regions while\noverlooking potential defects in invariant areas. To effectively overcome this,\nit is essential to decompose and recalibrate attention, guiding the model to\nsuppress irrelevant variations and concentrate on subtle, defect-susceptible\nareas. In this paper, we propose Recalibrating Attention of Industrial Anomaly\nDetection (RAAD), a framework that systematically decomposes and recalibrates\nattention maps. RAAD employs a two-stage process: first, it reduces attention\nbias through quantization, and second, it fine-tunes defect-prone regions for\nimproved sensitivity. Central to this framework is Hierarchical Quantization\nScoring (HQS), which dynamically allocates bit-widths across layers based on\ntheir anomaly detection contributions. HQS dynamically adjusts bit-widths based\non the hierarchical nature of attention maps, compressing lower layers that\nproduce coarse and noisy attention while preserving deeper layers with sharper,\ndefect-focused attention. This approach optimizes both computational efficiency\nand the model' s sensitivity to anomalies. We validate the effectiveness of\nRAAD on 32 datasets using a single 3090ti. Experiments demonstrate that RAAD,\nbalances the complexity and expressive power of the model, enhancing its\nanomaly detection capability.\n","authors":["Xin Chen","Liujuan Cao","Shengchuan Zhang","Xiewu Zheng","Yan Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.08189v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12347v2","updated":"2024-12-11T08:30:10Z","published":"2024-04-18T17:24:28Z","title":"AniClipart: Clipart Animation with Text-to-Video Priors","summary":"  Clipart, a pre-made art form, offers a convenient and efficient way of\ncreating visual content. However, traditional workflows for animating static\nclipart are laborious and time-consuming, involving steps like rigging,\nkeyframing, and inbetweening. Recent advancements in text-to-video generation\nhold great potential in resolving this challenge. Nevertheless, direct\napplication of text-to-video models often struggles to preserve the visual\nidentity of clipart or generate cartoon-style motion, resulting in subpar\nanimation outcomes. In this paper, we introduce AniClipart, a computational\nsystem that converts static clipart into high-quality animations guided by\ntext-to-video priors. To generate natural, smooth, and coherent motion, we\nfirst parameterize the motion trajectories of the keypoints defined over the\ninitial clipart image by cubic B\\'ezier curves. We then align these motion\ntrajectories with a given text prompt by optimizing a video Score Distillation\nSampling (SDS) loss and a skeleton fidelity loss. By incorporating\ndifferentiable As-Rigid-As-Possible (ARAP) shape deformation and differentiable\nrendering, AniClipart can be end-to-end optimized while maintaining deformation\nrigidity. Extensive experimental results show that the proposed AniClipart\nconsistently outperforms the competing methods, in terms of text-video\nalignment, visual identity preservation, and temporal consistency.\nAdditionally, we showcase the versatility of AniClipart by adapting it to\ngenerate layered animations, which allow for topological changes.\n","authors":["Ronghuan Wu","Wanchao Su","Kede Ma","Jing Liao"],"pdf_url":"https://arxiv.org/pdf/2404.12347v2.pdf","comment":"IJCV 2024, Project Page: https://aniclipart.github.io/"},{"id":"http://arxiv.org/abs/2412.08188v1","updated":"2024-12-11T08:27:33Z","published":"2024-12-11T08:27:33Z","title":"Textured Mesh Saliency: Bridging Geometry and Texture for Human\n  Perception in 3D Graphics","summary":"  Textured meshes significantly enhance the realism and detail of objects by\nmapping intricate texture details onto the geometric structure of 3D models.\nThis advancement is valuable across various applications, including\nentertainment, education, and industry. While traditional mesh saliency studies\nfocus on non-textured meshes, our work explores the complexities introduced by\ndetailed texture patterns. We present a new dataset for textured mesh saliency,\ncreated through an innovative eye-tracking experiment in a six degrees of\nfreedom (6-DOF) VR environment. This dataset addresses the limitations of\nprevious studies by providing comprehensive eye-tracking data from multiple\nviewpoints, thereby advancing our understanding of human visual behavior and\nsupporting more accurate and effective 3D content creation. Our proposed model\npredicts saliency maps for textured mesh surfaces by treating each triangular\nface as an individual unit and assigning a saliency density value to reflect\nthe importance of each local surface region. The model incorporates a texture\nalignment module and a geometric extraction module, combined with an\naggregation module to integrate texture and geometry for precise saliency\nprediction. We believe this approach will enhance the visual fidelity of\ngeometric processing algorithms while ensuring efficient use of computational\nresources, which is crucial for real-time rendering and high-detail\napplications such as VR and gaming.\n","authors":["Kaiwei Zhang","Dandan Zhu","Xiongkuo Min","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2412.08188v1.pdf","comment":"to be published in AAAI 2025"},{"id":"http://arxiv.org/abs/2412.03002v2","updated":"2024-12-11T08:14:13Z","published":"2024-12-04T03:42:39Z","title":"AdvDreamer Unveils: Are Vision-Language Models Truly Ready for\n  Real-World 3D Variations?","summary":"  Vision Language Models (VLMs) have exhibited remarkable generalization\ncapabilities, yet their robustness in dynamic real-world scenarios remains\nlargely unexplored. To systematically evaluate VLMs' robustness to real-world\n3D variations, we propose AdvDreamer, the first framework that generates\nphysically reproducible adversarial 3D transformation (Adv-3DT) samples from\nsingle-view images. AdvDreamer integrates advanced generative techniques with\ntwo key innovations and aims to characterize the worst-case distributions of 3D\nvariations from natural images. To ensure adversarial effectiveness and method\ngenerality, we introduce an Inverse Semantic Probability Objective that\nexecutes adversarial optimization on fundamental vision-text alignment spaces,\nwhich can be generalizable across different VLM architectures and downstream\ntasks. To mitigate the distribution discrepancy between generated and\nreal-world samples while maintaining physical reproducibility, we design a\nNaturalness Reward Model that provides regularization feedback during\nadversarial optimization, preventing convergence towards hallucinated and\nunnatural elements. Leveraging AdvDreamer, we establish MM3DTBench, the first\nVQA dataset for benchmarking VLMs' 3D variations robustness. Extensive\nevaluations on representative VLMs with diverse architectures highlight that 3D\nvariations in the real world may pose severe threats to model performance\nacross various tasks.\n","authors":["Shouwei Ruan","Hanqing Liu","Yao Huang","Xiaoqi Wang","Caixin Kang","Hang Su","Yinpeng Dong","Xingxing Wei"],"pdf_url":"https://arxiv.org/pdf/2412.03002v2.pdf","comment":"11pages, 8 figures"},{"id":"http://arxiv.org/abs/2412.06664v2","updated":"2024-12-11T08:11:12Z","published":"2024-12-09T17:01:42Z","title":"Knowledge Transfer and Domain Adaptation for Fine-Grained Remote Sensing\n  Image Segmentation","summary":"  Fine-grained remote sensing image segmentation is essential for accurately\nidentifying detailed objects in remote sensing images. Recently, vision\ntransformer models (VTMs) pre-trained on large-scale datasets have demonstrated\nstrong zero-shot generalization. However, directly applying them to specific\ntasks may lead to domain shift. We introduce a novel end-to-end learning\nparadigm combining knowledge guidance with domain refinement to enhance\nperformance. We present two key components: the Feature Alignment Module (FAM)\nand the Feature Modulation Module (FMM). FAM aligns features from a CNN-based\nbackbone with those from the pretrained VTM's encoder using channel\ntransformation and spatial interpolation, and transfers knowledge via KL\ndivergence and L2 normalization constraint. FMM further adapts the knowledge to\nthe specific domain to address domain shift. We also introduce a fine-grained\ngrass segmentation dataset and demonstrate, through experiments on two\ndatasets, that our method achieves a significant improvement of 2.57 mIoU on\nthe grass dataset and 3.73 mIoU on the cloud dataset. The results highlight the\npotential of combining knowledge transfer and domain adaptation to overcome\ndomain-related challenges and data limitations. The project page is available\nat https://xavierjiezou.github.io/KTDA/.\n","authors":["Shun Zhang","Xuechao Zou","Kai Li","Congyan Lang","Shiying Wang","Pin Tao","Tengfei Cao"],"pdf_url":"https://arxiv.org/pdf/2412.06664v2.pdf","comment":"6 pages, 3 figures, 6 tables"},{"id":"http://arxiv.org/abs/2412.08176v1","updated":"2024-12-11T08:07:12Z","published":"2024-12-11T08:07:12Z","title":"TextRefiner: Internal Visual Feature as Efficient Refiner for\n  Vision-Language Models Prompt Tuning","summary":"  Despite the efficiency of prompt learning in transferring vision-language\nmodels (VLMs) to downstream tasks, existing methods mainly learn the prompts in\na coarse-grained manner where the learned prompt vectors are shared across all\ncategories. Consequently, the tailored prompts often fail to discern\nclass-specific visual concepts, thereby hindering the transferred performance\nfor classes that share similar or complex visual attributes. Recent advances\nmitigate this challenge by leveraging external knowledge from Large Language\nModels (LLMs) to furnish class descriptions, yet incurring notable inference\ncosts. In this paper, we introduce TextRefiner, a plug-and-play method to\nrefine the text prompts of existing methods by leveraging the internal\nknowledge of VLMs. Particularly, TextRefiner builds a novel local cache module\nto encapsulate fine-grained visual concepts derivedfrom local tokens within the\nimage branch. By aggregating and aligning the cached visual descriptions with\nthe original output of the text branch, TextRefiner can efficiently refine and\nenrich the learned prompts from existing methods without relying on any\nexternal expertise. For example, it improves the performance of CoOp from 71.66\n% to 76.94 % on 11 benchmarks, surpassing CoCoOp which introduces instance-wise\nfeatures for text prompts. Equipped with TextRefiner, PromptKD achieves\nstate-of-the-art performance and is efficient in inference. Our code is relesed\nat https://github.com/xjjxmu/TextRefiner\n","authors":["Jingjing Xie","Yuxin Zhang","Jun Peng","Zhaohong Huang","Liujuan Cao"],"pdf_url":"https://arxiv.org/pdf/2412.08176v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.08175v1","updated":"2024-12-11T08:05:35Z","published":"2024-12-11T08:05:35Z","title":"Analyzing and Improving Model Collapse in Rectified Flow Models","summary":"  Generative models aim to produce synthetic data indistinguishable from real\ndistributions, but iterative training on self-generated data can lead to\n\\emph{model collapse (MC)}, where performance degrades over time. In this work,\nwe provide the first theoretical analysis of MC in Rectified Flow by framing it\nwithin the context of Denoising Autoencoders (DAEs). We show that when DAE\nmodels are trained on recursively generated synthetic data with small noise\nvariance, they suffer from MC with progressive diminishing generation quality.\nTo address this MC issue, we propose methods that strategically incorporate\nreal data into the training process, even when direct noise-image pairs are\nunavailable. Our proposed techniques, including Reverse Collapse-Avoiding (RCA)\nReflow and Online Collapse-Avoiding Reflow (OCAR), effectively prevent MC while\nmaintaining the efficiency benefits of Rectified Flow. Extensive experiments on\nstandard image datasets demonstrate that our methods not only mitigate MC but\nalso improve sampling efficiency, leading to higher-quality image generation\nwith fewer sampling steps.\n","authors":["Huminhao Zhu","Fangyikang Wang","Tianyu Ding","Qing Qu","Zhihui Zhu"],"pdf_url":"https://arxiv.org/pdf/2412.08175v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11069v2","updated":"2024-12-11T08:05:28Z","published":"2024-11-17T13:18:05Z","title":"Skeleton-Guided Spatial-Temporal Feature Learning for Video-Based\n  Visible-Infrared Person Re-Identification","summary":"  Video-based visible-infrared person re-identification (VVI-ReID) is\nchallenging due to significant modality feature discrepancies. Spatial-temporal\ninformation in videos is crucial, but the accuracy of spatial-temporal\ninformation is often influenced by issues like low quality and occlusions in\nvideos. Existing methods mainly focus on reducing modality differences, but pay\nlimited attention to improving spatial-temporal features, particularly for\ninfrared videos. To address this, we propose a novel Skeleton-guided\nspatial-Temporal feAture leaRning (STAR) method for VVI-ReID. By using skeleton\ninformation, which is robust to issues such as poor image quality and\nocclusions, STAR improves the accuracy of spatial-temporal features in videos\nof both modalities. Specifically, STAR employs two levels of skeleton-guided\nstrategies: frame level and sequence level. At the frame level, the robust\nstructured skeleton information is used to refine the visual features of\nindividual frames. At the sequence level, we design a feature aggregation\nmechanism based on skeleton key points graph, which learns the contribution of\ndifferent body parts to spatial-temporal features, further enhancing the\naccuracy of global features. Experiments on benchmark datasets demonstrate that\nSTAR outperforms state-of-the-art methods. Code will be open source soon.\n","authors":["Wenjia Jiang","Xiaoke Zhu","Jiakang Gao","Di Liao"],"pdf_url":"https://arxiv.org/pdf/2411.11069v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08169v1","updated":"2024-12-11T07:51:18Z","published":"2024-12-11T07:51:18Z","title":"Illusory VQA: Benchmarking and Enhancing Multimodal Models on Visual\n  Illusions","summary":"  In recent years, Visual Question Answering (VQA) has made significant\nstrides, particularly with the advent of multimodal models that integrate\nvision and language understanding. However, existing VQA datasets often\noverlook the complexities introduced by image illusions, which pose unique\nchallenges for both human perception and model interpretation. In this study,\nwe introduce a novel task called Illusory VQA, along with four specialized\ndatasets: IllusionMNIST, IllusionFashionMNIST, IllusionAnimals, and\nIllusionChar. These datasets are designed to evaluate the performance of\nstate-of-the-art multimodal models in recognizing and interpreting visual\nillusions. We assess the zero-shot performance of various models, fine-tune\nselected models on our datasets, and propose a simple yet effective solution\nfor illusion detection using Gaussian and blur low-pass filters. We show that\nthis method increases the performance of models significantly and in the case\nof BLIP-2 on IllusionAnimals without any fine-tuning, it outperforms humans.\nOur findings highlight the disparity between human and model perception of\nillusions and demonstrate that fine-tuning and specific preprocessing\ntechniques can significantly enhance model robustness. This work contributes to\nthe development of more human-like visual understanding in multimodal models\nand suggests future directions for adapting filters using learnable parameters.\n","authors":["Mohammadmostafa Rostamkhani","Baktash Ansari","Hoorieh Sabzevari","Farzan Rahmani","Sauleh Eetemadi"],"pdf_url":"https://arxiv.org/pdf/2412.08169v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.03814v2","updated":"2024-12-11T07:50:40Z","published":"2024-12-05T02:11:51Z","title":"Exploring Real&Synthetic Dataset and Linear Attention in Image\n  Restoration","summary":"  Image restoration (IR) aims to recover high-quality images from degraded\ninputs, with recent deep learning advancements significantly enhancing\nperformance. However, existing methods lack a unified training benchmark for\niterations and configurations. We also identify a bias in image complexity\ndistributions between commonly used IR training and testing datasets, resulting\nin suboptimal restoration outcomes. To address this, we introduce a large-scale\nIR dataset called ReSyn, which employs a novel image filtering method based on\nimage complexity to ensure a balanced distribution and includes both real and\nAIGC synthetic images. We establish a unified training standard that specifies\niterations and configurations for image restoration models, focusing on\nmeasuring model convergence and restoration capability. Additionally, we\nenhance transformer-based image restoration models using linear attention\nmechanisms by proposing RWKV-IR, which integrates linear complexity RWKV into\nthe transformer structure, allowing for both global and local receptive fields.\nInstead of directly using Vision-RWKV, we replace the original Q-Shift in RWKV\nwith a Depth-wise Convolution shift to better model local dependencies,\ncombined with Bi-directional attention for comprehensive linear attention. We\nalso introduce a Cross-Bi-WKV module that merges two Bi-WKV modules with\ndifferent scanning orders for balanced horizontal and vertical attention.\nExtensive experiments validate the effectiveness of our RWKV-IR model.\n","authors":["Yuzhen Du","Teng Hu","Jiangning Zhang","Ran Yi Chengming Xu","Xiaobin Hu","Kai Wu","Donghao Luo","Yabiao Wang","Lizhuang Ma"],"pdf_url":"https://arxiv.org/pdf/2412.03814v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03349v2","updated":"2024-12-11T07:49:41Z","published":"2023-11-28T21:14:02Z","title":"Image Inpainting via Tractable Steering of Diffusion Models","summary":"  Diffusion models are the current state of the art for generating\nphotorealistic images. Controlling the sampling process for constrained image\ngeneration tasks such as inpainting, however, remains challenging since exact\nconditioning on such constraints is intractable. While existing methods use\nvarious techniques to approximate the constrained posterior, this paper\nproposes to exploit the ability of Tractable Probabilistic Models (TPMs) to\nexactly and efficiently compute the constrained posterior, and to leverage this\nsignal to steer the denoising process of diffusion models. Specifically, this\npaper adopts a class of expressive TPMs termed Probabilistic Circuits (PCs).\nBuilding upon prior advances, we further scale up PCs and make them capable of\nguiding the image generation process of diffusion models. Empirical results\nsuggest that our approach can consistently improve the overall quality and\nsemantic coherence of inpainted images across three natural image datasets\n(i.e., CelebA-HQ, ImageNet, and LSUN) with only $\\sim\\! 10 \\%$ additional\ncomputational overhead brought by the TPM. Further, with the help of an image\nencoder and decoder, our method can readily accept semantic constraints on\nspecific regions of the image, which opens up the potential for more controlled\nimage generation tasks. In addition to proposing a new framework for\nconstrained image generation, this paper highlights the benefit of more\ntractable models and motivates the development of expressive TPMs.\n","authors":["Anji Liu","Mathias Niepert","Guy Van den Broeck"],"pdf_url":"https://arxiv.org/pdf/2401.03349v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04508v2","updated":"2024-12-11T07:45:38Z","published":"2024-12-04T05:25:17Z","title":"Video Quality Assessment: A Comprehensive Survey","summary":"  Video quality assessment (VQA) is an important processing task, aiming at\npredicting the quality of videos in a manner highly consistent with human\njudgments of perceived quality. Traditional VQA models based on natural image\nand/or video statistics, which are inspired both by models of projected images\nof the real world and by dual models of the human visual system, deliver only\nlimited prediction performances on real-world user-generated content (UGC), as\nexemplified in recent large-scale VQA databases containing large numbers of\ndiverse video contents crawled from the web. Fortunately, recent advances in\ndeep neural networks and Large Multimodality Models (LMMs) have enabled\nsignificant progress in solving this problem, yielding better results than\nprior handcrafted models. Numerous deep learning-based VQA models have been\ndeveloped, with progress in this direction driven by the creation of\ncontent-diverse, large-scale human-labeled databases that supply ground truth\npsychometric video quality data. Here, we present a comprehensive survey of\nrecent progress in the development of VQA algorithms and the benchmarking\nstudies and databases that make them possible. We also analyze open research\ndirections on study design and VQA algorithm architectures. Github link:\nhttps://github.com/taco-group/Video-Quality-Assessment-A-Comprehensive-Survey.\n","authors":["Qi Zheng","Yibo Fan","Leilei Huang","Tianyu Zhu","Jiaming Liu","Zhijian Hao","Shuo Xing","Chia-Ju Chen","Xiongkuo Min","Alan C. Bovik","Zhengzhong Tu"],"pdf_url":"https://arxiv.org/pdf/2412.04508v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08161v1","updated":"2024-12-11T07:33:18Z","published":"2024-12-11T07:33:18Z","title":"Collaborative Hybrid Propagator for Temporal Misalignment in\n  Audio-Visual Segmentation","summary":"  Audio-visual video segmentation (AVVS) aims to generate pixel-level maps of\nsound-producing objects that accurately align with the corresponding audio.\nHowever, existing methods often face temporal misalignment, where audio cues\nand segmentation results are not temporally coordinated. Audio provides two\ncritical pieces of information: i) target object-level details and ii) the\ntiming of when objects start and stop producing sounds. Current methods focus\nmore on object-level information but neglect the boundaries of audio semantic\nchanges, leading to temporal misalignment. To address this issue, we propose a\nCollaborative Hybrid Propagator Framework~(Co-Prop). This framework includes\ntwo main steps: Preliminary Audio Boundary Anchoring and Frame-by-Frame\nAudio-Insert Propagation. To Anchor the audio boundary, we employ\nretrieval-assist prompts with Qwen large language models to identify control\npoints of audio semantic changes. These control points split the audio into\nsemantically consistent audio portions. After obtaining the control point\nlists, we propose the Audio Insertion Propagator to process each audio portion\nusing a frame-by-frame audio insertion propagation and matching approach. We\ncurated a compact dataset comprising diverse source conversion cases and\ndevised a metric to assess alignment rates. Compared to traditional\nsimultaneous processing methods, our approach reduces memory requirements and\nfacilitates frame alignment. Experimental results demonstrate the effectiveness\nof our approach across three datasets and two backbones. Furthermore, our\nmethod can be integrated with existing AVVS approaches, offering plug-and-play\nfunctionality to enhance their performance.\n","authors":["Kexin Li","Zongxin Yang","Yi Yang","Jun Xiao"],"pdf_url":"https://arxiv.org/pdf/2412.08161v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08158v1","updated":"2024-12-11T07:29:04Z","published":"2024-12-11T07:29:04Z","title":"How Vision-Language Tasks Benefit from Large Pre-trained Models: A\n  Survey","summary":"  The exploration of various vision-language tasks, such as visual captioning,\nvisual question answering, and visual commonsense reasoning, is an important\narea in artificial intelligence and continuously attracts the research\ncommunity's attention. Despite the improvements in overall performance, classic\nchallenges still exist in vision-language tasks and hinder the development of\nthis area. In recent years, the rise of pre-trained models is driving the\nresearch on vision-language tasks. Thanks to the massive scale of training data\nand model parameters, pre-trained models have exhibited excellent performance\nin numerous downstream tasks. Inspired by the powerful capabilities of\npre-trained models, new paradigms have emerged to solve the classic challenges.\nSuch methods have become mainstream in current research with increasing\nattention and rapid advances. In this paper, we present a comprehensive\noverview of how vision-language tasks benefit from pre-trained models. First,\nwe review several main challenges in vision-language tasks and discuss the\nlimitations of previous solutions before the era of pre-training. Next, we\nsummarize the recent advances in incorporating pre-trained models to address\nthe challenges in vision-language tasks. Finally, we analyze the potential\nrisks associated with the inherent limitations of pre-trained models and\ndiscuss possible solutions, attempting to provide future research directions.\n","authors":["Yayun Qi","Hongxi Li","Yiqi Song","Xinxiao Wu","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2412.08158v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2412.08156v1","updated":"2024-12-11T07:22:51Z","published":"2024-12-11T07:22:51Z","title":"Antelope: Potent and Concealed Jailbreak Attack Strategy","summary":"  Due to the remarkable generative potential of diffusion-based models,\nnumerous researches have investigated jailbreak attacks targeting these\nframeworks. A particularly concerning threat within image models is the\ngeneration of Not-Safe-for-Work (NSFW) content. Despite the implementation of\nsecurity filters, numerous efforts continue to explore ways to circumvent these\nsafeguards. Current attack methodologies primarily encompass adversarial prompt\nengineering or concept obfuscation, yet they frequently suffer from slow search\nefficiency, conspicuous attack characteristics and poor alignment with targets.\nTo overcome these challenges, we propose Antelope, a more robust and covert\njailbreak attack strategy designed to expose security vulnerabilities inherent\nin generative models. Specifically, Antelope leverages the confusion of\nsensitive concepts with similar ones, facilitates searches in the semantically\nadjacent space of these related concepts and aligns them with the target\nimagery, thereby generating sensitive images that are consistent with the\ntarget and capable of evading detection. Besides, we successfully exploit the\ntransferability of model-based attacks to penetrate online black-box services.\nExperimental evaluations demonstrate that Antelope outperforms existing\nbaselines across multiple defensive mechanisms, underscoring its efficacy and\nversatility.\n","authors":["Xin Zhao","Xiaojun Chen","Haoyu Gao"],"pdf_url":"https://arxiv.org/pdf/2412.08156v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08152v1","updated":"2024-12-11T07:13:02Z","published":"2024-12-11T07:13:02Z","title":"ProGDF: Progressive Gaussian Differential Field for Controllable and\n  Flexible 3D Editing","summary":"  3D editing plays a crucial role in editing and reusing existing 3D assets,\nthereby enhancing productivity. Recently, 3DGS-based methods have gained\nincreasing attention due to their efficient rendering and flexibility. However,\nachieving desired 3D editing results often requires multiple adjustments in an\niterative loop, resulting in tens of minutes of training time cost for each\nattempt and a cumbersome trial-and-error cycle for users. This in-the-loop\ntraining paradigm results in a poor user experience. To address this issue, we\nintroduce the concept of process-oriented modelling for 3D editing and propose\nthe Progressive Gaussian Differential Field (ProGDF), an out-of-loop training\napproach that requires only a single training session to provide users with\ncontrollable editing capability and variable editing results through a\nuser-friendly interface in real-time. ProGDF consists of two key components:\nProgressive Gaussian Splatting (PGS) and Gaussian Differential Field (GDF). PGS\nintroduces the progressive constraint to extract the diverse intermediate\nresults of the editing process and employs rendering quality regularization to\nimprove the quality of these results. Based on these intermediate results, GDF\nleverages a lightweight neural network to model the editing process. Extensive\nresults on two novel applications, namely controllable 3D editing and flexible\nfine-grained 3D manipulation, demonstrate the effectiveness, practicality and\nflexibility of the proposed ProGDF.\n","authors":["Yian Zhao","Wanshi Xu","Yang Wu","Weiheng Huang","Zhongqian Sun","Wei Yang"],"pdf_url":"https://arxiv.org/pdf/2412.08152v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08149v1","updated":"2024-12-11T07:08:07Z","published":"2024-12-11T07:08:07Z","title":"AsyncDSB: Schedule-Asynchronous Diffusion Schrödinger Bridge for Image\n  Inpainting","summary":"  Image inpainting is an important image generation task, which aims to restore\ncorrupted image from partial visible area. Recently, diffusion Schr\\\"odinger\nbridge methods effectively tackle this task by modeling the translation between\ncorrupted and target images as a diffusion Schr\\\"odinger bridge process along a\nnoising schedule path. Although these methods have shown superior performance,\nin this paper, we find that 1) existing methods suffer from a\nschedule-restoration mismatching issue, i.e., the theoretical schedule and\npractical restoration processes usually exist a large discrepancy, which\ntheoretically results in the schedule not fully leveraged for restoring images;\nand 2) the key reason causing such issue is that the restoration process of all\npixels are actually asynchronous but existing methods set a synchronous noise\nschedule to them, i.e., all pixels shares the same noise schedule. To this end,\nwe propose a schedule-Asynchronous Diffusion Schr\\\"odinger Bridge (AsyncDSB)\nfor image inpainting. Our insight is preferentially scheduling pixels with high\nfrequency (i.e., large gradients) and then low frequency (i.e., small\ngradients). Based on this insight, given a corrupted image, we first train a\nnetwork to predict its gradient map in corrupted area. Then, we regard the\npredicted image gradient as prior and design a simple yet effective\npixel-asynchronous noise schedule strategy to enhance the diffusion\nSchr\\\"odinger bridge. Thanks to the asynchronous schedule at pixels, the\ntemporal interdependence of restoration process between pixels can be fully\ncharacterized for high-quality image inpainting. Experiments on real-world\ndatasets show that our AsyncDSB achieves superior performance, especially on\nFID with around 3% - 14% improvement over state-of-the-art baseline methods.\n","authors":["Zihao Han","Baoquan Zhang","Lisai Zhang","Shanshan Feng","Kenghong Lin","Guotao Liang","Yunming Ye","Xiaochen Qi","Guangming Ye"],"pdf_url":"https://arxiv.org/pdf/2412.08149v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2408.08149v3","updated":"2024-12-11T07:07:19Z","published":"2024-08-15T13:35:59Z","title":"Unsupervised Variational Translator for Bridging Image Restoration and\n  High-Level Vision Tasks","summary":"  Recent research tries to extend image restoration capabilities from human\nperception to machine perception, thereby enhancing the performance of\nhigh-level vision tasks in degraded environments. These methods, primarily\nbased on supervised learning, typically involve the retraining of restoration\nnetworks or high-level vision networks. However, collecting paired data in\nreal-world scenarios and retraining large-scale models are challenge. To this\nend, we propose an unsupervised learning method called \\textbf{Va}riational\n\\textbf{T}ranslator (VaT), which does not require retraining existing\nrestoration and high-level vision networks. Instead, it establishes a\nlightweight network that serves as an intermediate bridge between them. By\nvariational inference, VaT approximates the joint distribution of restoration\noutput and high-level vision input, dividing the optimization objective into\npreserving content and maximizing marginal likelihood associated with\nhigh-level vision tasks. By cleverly leveraging self-training paradigms, VaT\nachieves the above optimization objective without requiring labels. As a\nresult, the translated images maintain a close resemblance to their original\ncontent while also demonstrating exceptional performance on high-level vision\ntasks. Extensive experiments in dehazing and low-light enhancement for\ndetection and classification show the superiority of our method over other\nstate-of-the-art unsupervised counterparts, even significantly surpassing\nsupervised methods in some complex real-world scenarios.Code is available at\nhttps://github.com/Fire-friend/VaT.\n","authors":["Jiawei Wu","Zhi Jin"],"pdf_url":"https://arxiv.org/pdf/2408.08149v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08148v1","updated":"2024-12-11T07:06:53Z","published":"2024-12-11T07:06:53Z","title":"A Review of Intelligent Device Fault Diagnosis Technologies Based on\n  Machine Vision","summary":"  This paper provides a comprehensive review of mechanical equipment fault\ndiagnosis methods, focusing on the advancements brought by Transformer-based\nmodels. It details the structure, working principles, and benefits of\nTransformers, particularly their self-attention mechanism and parallel\ncomputation capabilities, which have propelled their widespread application in\nnatural language processing and computer vision. The discussion highlights key\nTransformer model variants, such as Vision Transformers (ViT) and their\nextensions, which leverage self-attention to improve accuracy and efficiency in\nvisual tasks. Furthermore, the paper examines the application of\nTransformer-based approaches in intelligent fault diagnosis for mechanical\nsystems, showcasing their superior ability to extract and recognize patterns\nfrom complex sensor data for precise fault identification. Despite these\nadvancements, challenges remain, including the reliance on extensive labeled\ndatasets, significant computational demands, and difficulties in deploying\nmodels on resource-limited devices. To address these limitations, the paper\nproposes future research directions, such as developing lightweight Transformer\narchitectures, integrating multimodal data sources, and enhancing adaptability\nto diverse operational conditions. These efforts aim to further expand the\napplication of Transformer-based methods in mechanical fault diagnosis, making\nthem more robust, efficient, and suitable for real-world industrial\nenvironments.\n","authors":["Guiran Liu","Binrong Zhu"],"pdf_url":"https://arxiv.org/pdf/2412.08148v1.pdf","comment":"9 pages, This paper has been accepted for publication at RICAI 2024"},{"id":"http://arxiv.org/abs/2412.08139v1","updated":"2024-12-11T06:54:39Z","published":"2024-12-11T06:54:39Z","title":"Wasserstein Distance Rivals Kullback-Leibler Divergence for Knowledge\n  Distillation","summary":"  Since pioneering work of Hinton et al., knowledge distillation based on\nKullback-Leibler Divergence (KL-Div) has been predominant, and recently its\nvariants have achieved compelling performance. However, KL-Div only compares\nprobabilities of the corresponding category between the teacher and student\nwhile lacking a mechanism for cross-category comparison. Besides, KL-Div is\nproblematic when applied to intermediate layers, as it cannot handle\nnon-overlapping distributions and is unaware of geometry of the underlying\nmanifold. To address these downsides, we propose a methodology of Wasserstein\nDistance (WD) based knowledge distillation. Specifically, we propose a logit\ndistillation method called WKD-L based on discrete WD, which performs\ncross-category comparison of probabilities and thus can explicitly leverage\nrich interrelations among categories. Moreover, we introduce a feature\ndistillation method called WKD-F, which uses a parametric method for modeling\nfeature distributions and adopts continuous WD for transferring knowledge from\nintermediate layers. Comprehensive evaluations on image classification and\nobject detection have shown (1) for logit distillation WKD-L outperforms very\nstrong KL-Div variants; (2) for feature distillation WKD-F is superior to the\nKL-Div counterparts and state-of-the-art competitors. The source code is\navailable at https://peihuali.org/WKD\n","authors":["Jiaming Lv","Haoyuan Yang","Peihua Li"],"pdf_url":"https://arxiv.org/pdf/2412.08139v1.pdf","comment":"Accepted to NeurIPS 2024. Equal contribution from first two authors"},{"id":"http://arxiv.org/abs/2406.07540v2","updated":"2024-12-11T06:53:55Z","published":"2024-06-11T17:59:01Z","title":"Ctrl-X: Controlling Structure and Appearance for Text-To-Image\n  Generation Without Guidance","summary":"  Recent controllable generation approaches such as FreeControl and Diffusion\nSelf-Guidance bring fine-grained spatial and appearance control to\ntext-to-image (T2I) diffusion models without training auxiliary modules.\nHowever, these methods optimize the latent embedding for each type of score\nfunction with longer diffusion steps, making the generation process\ntime-consuming and limiting their flexibility and use. This work presents\nCtrl-X, a simple framework for T2I diffusion controlling structure and\nappearance without additional training or guidance. Ctrl-X designs feed-forward\nstructure control to enable the structure alignment with a structure image and\nsemantic-aware appearance transfer to facilitate the appearance transfer from a\nuser-input image. Extensive qualitative and quantitative experiments illustrate\nthe superior performance of Ctrl-X on various condition inputs and model\ncheckpoints. In particular, Ctrl-X supports novel structure and appearance\ncontrol with arbitrary condition images of any modality, exhibits superior\nimage quality and appearance transfer compared to existing works, and provides\ninstant plug-and-play functionality to any T2I and text-to-video (T2V)\ndiffusion model. See our project page for an overview of the results:\nhttps://genforce.github.io/ctrl-x\n","authors":["Kuan Heng Lin","Sicheng Mo","Ben Klingher","Fangzhou Mu","Bolei Zhou"],"pdf_url":"https://arxiv.org/pdf/2406.07540v2.pdf","comment":"22 pages, 17 figures, see project page at\n  https://genforce.github.io/ctrl-x"},{"id":"http://arxiv.org/abs/2412.08135v1","updated":"2024-12-11T06:44:22Z","published":"2024-12-11T06:44:22Z","title":"DOGE: An Extrinsic Orientation and Gyroscope Bias Estimation for\n  Visual-Inertial Odometry Initialization","summary":"  Most existing visual-inertial odometry (VIO) initialization methods rely on\naccurate pre-calibrated extrinsic parameters. However, during long-term use,\nirreversible structural deformation caused by temperature changes, mechanical\nsqueezing, etc. will cause changes in extrinsic parameters, especially in the\nrotational part. Existing initialization methods that simultaneously estimate\nextrinsic parameters suffer from poor robustness, low precision, and long\ninitialization latency due to the need for sufficient translational motion. To\naddress these problems, we propose a novel VIO initialization method, which\njointly considers extrinsic orientation and gyroscope bias within the normal\nepipolar constraints, achieving higher precision and better robustness without\ndelayed rotational calibration. First, a rotation-only constraint is designed\nfor extrinsic orientation and gyroscope bias estimation, which tightly couples\ngyroscope measurements and visual observations and can be solved in\npure-rotation cases. Second, we propose a weighting strategy together with a\nfailure detection strategy to enhance the precision and robustness of the\nestimator. Finally, we leverage Maximum A Posteriori to refine the results\nbefore enough translation parallax comes. Extensive experiments have\ndemonstrated that our method outperforms the state-of-the-art methods in both\naccuracy and robustness while maintaining competitive efficiency.\n","authors":["Zewen Xu","Yijia He","Hao Wei","Yihong Wu"],"pdf_url":"https://arxiv.org/pdf/2412.08135v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08131v1","updated":"2024-12-11T06:36:55Z","published":"2024-12-11T06:36:55Z","title":"DiffRaman: A Conditional Latent Denoising Diffusion Probabilistic Model\n  for Bacterial Raman Spectroscopy Identification Under Limited Data Conditions","summary":"  Raman spectroscopy has attracted significant attention in various biochemical\ndetection fields, especially in the rapid identification of pathogenic\nbacteria. The integration of this technology with deep learning to facilitate\nautomated bacterial Raman spectroscopy diagnosis has emerged as a key focus in\nrecent research. However, the diagnostic performance of existing deep learning\nmethods largely depends on a sufficient dataset, and in scenarios where there\nis a limited availability of Raman spectroscopy data, it is inadequate to fully\noptimize the numerous parameters of deep neural networks. To address these\nchallenges, this paper proposes a data generation method utilizing deep\ngenerative models to expand the data volume and enhance the recognition\naccuracy of bacterial Raman spectra. Specifically, we introduce DiffRaman, a\nconditional latent denoising diffusion probability model for Raman spectra\ngeneration. Experimental results demonstrate that synthetic bacterial Raman\nspectra generated by DiffRaman can effectively emulate real experimental\nspectra, thereby enhancing the performance of diagnostic models, especially\nunder conditions of limited data. Furthermore, compared to existing generative\nmodels, the proposed DiffRaman offers improvements in both generation quality\nand computational efficiency. Our DiffRaman approach offers a well-suited\nsolution for automated bacteria Raman spectroscopy diagnosis in data-scarce\nscenarios, offering new insights into alleviating the labor of spectroscopic\nmeasurements and enhancing rare bacteria identification.\n","authors":["Haiming Yao","Wei Luo","Ang Gao","Tao Zhou","Xue Wang"],"pdf_url":"https://arxiv.org/pdf/2412.08131v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.01168v2","updated":"2024-12-11T06:30:11Z","published":"2024-04-01T15:16:33Z","title":"Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting","summary":"  3D Gaussian Splatting (3DGS) has significantly advanced 3D scene\nreconstruction and novel view synthesis. However, like Neural Radiance Fields\n(NeRF), 3DGS struggles with accurately modeling physical reflections,\nparticularly in mirrors, leading to incorrect reconstructions and inconsistent\nreflective properties. To address this challenge, we introduce Mirror-3DGS, a\nnovel framework designed to accurately handle mirror geometries and\nreflections, thereby generating realistic mirror reflections. By incorporating\nmirror attributes into 3DGS and leveraging plane mirror imaging principles,\nMirror-3DGS simulates a mirrored viewpoint from behind the mirror, enhancing\nthe realism of scene renderings. Extensive evaluations on both synthetic and\nreal-world scenes demonstrate that our method can render novel views with\nimproved fidelity in real-time, surpassing the state-of-the-art Mirror-NeRF,\nespecially in mirror regions.\n","authors":["Jiarui Meng","Haijie Li","Yanmin Wu","Qiankun Gao","Shuzhou Yang","Jian Zhang","Siwei Ma"],"pdf_url":"https://arxiv.org/pdf/2404.01168v2.pdf","comment":"IEEE International Conference on Visual Communications and Image\n  Processing (VCIP 2024, Oral)"},{"id":"http://arxiv.org/abs/2406.15658v2","updated":"2024-12-11T06:23:12Z","published":"2024-06-21T21:33:16Z","title":"TorchSpatial: A Location Encoding Framework and Benchmark for Spatial\n  Representation Learning","summary":"  Spatial representation learning (SRL) aims at learning general-purpose neural\nnetwork representations from various types of spatial data (e.g., points,\npolylines, polygons, networks, images, etc.) in their native formats. Learning\ngood spatial representations is a fundamental problem for various downstream\napplications such as species distribution modeling, weather forecasting,\ntrajectory generation, geographic question answering, etc. Even though SRL has\nbecome the foundation of almost all geospatial artificial intelligence (GeoAI)\nresearch, we have not yet seen significant efforts to develop an extensive deep\nlearning framework and benchmark to support SRL model development and\nevaluation. To fill this gap, we propose TorchSpatial, a learning framework and\nbenchmark for location (point) encoding, which is one of the most fundamental\ndata types of spatial representation learning. TorchSpatial contains three key\ncomponents: 1) a unified location encoding framework that consolidates 15\ncommonly recognized location encoders, ensuring scalability and reproducibility\nof the implementations; 2) the LocBench benchmark tasks encompassing 7\ngeo-aware image classification and 10 geo-aware image regression datasets; 3) a\ncomprehensive suite of evaluation metrics to quantify geo-aware models' overall\nperformance as well as their geographic bias, with a novel Geo-Bias Score\nmetric. Finally, we provide a detailed analysis and insights into the model\nperformance and geographic bias of different location encoders. We believe\nTorchSpatial will foster future advancement of spatial representation learning\nand spatial fairness in GeoAI research. The TorchSpatial model framework,\nLocBench, and Geo-Bias Score evaluation framework are available at\nhttps://github.com/seai-lab/TorchSpatial.\n","authors":["Nemin Wu","Qian Cao","Zhangyu Wang","Zeping Liu","Yanlin Qi","Jielu Zhang","Joshua Ni","Xiaobai Yao","Hongxu Ma","Lan Mu","Stefano Ermon","Tanuja Ganu","Akshay Nambi","Ni Lao","Gengchen Mai"],"pdf_url":"https://arxiv.org/pdf/2406.15658v2.pdf","comment":"10 pages, 2 figures. Accepted by NeurIPS 2024 Datasets and Benchmarks\n  Track"},{"id":"http://arxiv.org/abs/2412.08125v1","updated":"2024-12-11T06:21:33Z","published":"2024-12-11T06:21:33Z","title":"Progressive Multi-granular Alignments for Grounded Reasoning in Large\n  Vision-Language Models","summary":"  Existing Large Vision-Language Models (LVLMs) excel at matching concepts\nacross multi-modal inputs but struggle with compositional concepts and\nhigh-level relationships between entities. This paper introduces Progressive\nmulti-granular Vision-Language alignments (PromViL), a novel framework to\nenhance LVLMs' ability in performing grounded compositional visual reasoning\ntasks. Our approach constructs a hierarchical structure of multi-modal\nalignments, ranging from simple to complex concepts. By progressively aligning\ntextual descriptions with corresponding visual regions, our model learns to\nleverage contextual information from lower levels to inform higher-level\nreasoning. To facilitate this learning process, we introduce a data generation\nprocess that creates a novel dataset derived from Visual Genome, providing a\nwide range of nested compositional vision-language pairs. Experimental results\ndemonstrate that our PromViL framework significantly outperforms baselines on\nvarious visual grounding and compositional question answering tasks.\n","authors":["Quang-Hung Le","Long Hoang Dang","Ngan Le","Truyen Tran","Thao Minh Le"],"pdf_url":"https://arxiv.org/pdf/2412.08125v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08120v1","updated":"2024-12-11T06:13:38Z","published":"2024-12-11T06:13:38Z","title":"Dense Depth from Event Focal Stack","summary":"  We propose a method for dense depth estimation from an event stream generated\nwhen sweeping the focal plane of the driving lens attached to an event camera.\nIn this method, a depth map is inferred from an ``event focal stack'' composed\nof the event stream using a convolutional neural network trained with\nsynthesized event focal stacks. The synthesized event stream is created from a\nfocal stack generated by Blender for any arbitrary 3D scene. This allows for\ntraining on scenes with diverse structures. Additionally, we explored methods\nto eliminate the domain gap between real event streams and synthetic event\nstreams. Our method demonstrates superior performance over a depth-from-defocus\nmethod in the image domain on synthetic and real datasets.\n","authors":["Kenta Horikawa","Mariko Isogawa","Hideo Saito","Shohei Mori"],"pdf_url":"https://arxiv.org/pdf/2412.08120v1.pdf","comment":"Accepted at WACV2025"},{"id":"http://arxiv.org/abs/2412.04955v2","updated":"2024-12-11T06:00:52Z","published":"2024-12-06T11:17:25Z","title":"MixedGaussianAvatar: Realistically and Geometrically Accurate Head\n  Avatar via Mixed 2D-3D Gaussian Splatting","summary":"  Reconstructing high-fidelity 3D head avatars is crucial in various\napplications such as virtual reality. The pioneering methods reconstruct\nrealistic head avatars with Neural Radiance Fields (NeRF), which have been\nlimited by training and rendering speed. Recent methods based on 3D Gaussian\nSplatting (3DGS) significantly improve the efficiency of training and\nrendering. However, the surface inconsistency of 3DGS results in subpar\ngeometric accuracy; later, 2DGS uses 2D surfels to enhance geometric accuracy\nat the expense of rendering fidelity. To leverage the benefits of both 2DGS and\n3DGS, we propose a novel method named MixedGaussianAvatar for realistically and\ngeometrically accurate head avatar reconstruction. Our main idea is to utilize\n2D Gaussians to reconstruct the surface of the 3D head, ensuring geometric\naccuracy. We attach the 2D Gaussians to the triangular mesh of the FLAME model\nand connect additional 3D Gaussians to those 2D Gaussians where the rendering\nquality of 2DGS is inadequate, creating a mixed 2D-3D Gaussian representation.\nThese 2D-3D Gaussians can then be animated using FLAME parameters. We further\nintroduce a progressive training strategy that first trains the 2D Gaussians\nand then fine-tunes the mixed 2D-3D Gaussians. We demonstrate the superiority\nof MixedGaussianAvatar through comprehensive experiments. The code will be\nreleased at: https://github.com/ChenVoid/MGA/.\n","authors":["Peng Chen","Xiaobao Wei","Qingpo Wuwu","Xinyi Wang","Xingyu Xiao","Ming Lu"],"pdf_url":"https://arxiv.org/pdf/2412.04955v2.pdf","comment":"Project: https://chenvoid.github.io/MGA/"},{"id":"http://arxiv.org/abs/2412.08116v1","updated":"2024-12-11T05:50:33Z","published":"2024-12-11T05:50:33Z","title":"DAKD: Data Augmentation and Knowledge Distillation using Diffusion\n  Models for SAR Oil Spill Segmentation","summary":"  Oil spills in the ocean pose severe environmental risks, making early\ndetection essential. Synthetic aperture radar (SAR) based oil spill\nsegmentation offers robust monitoring under various conditions but faces\nchallenges due to the limited labeled data and inherent speckle noise in SAR\nimagery. To address these issues, we propose (i) a diffusion-based Data\nAugmentation and Knowledge Distillation (DAKD) pipeline and (ii) a novel SAR\noil spill segmentation network, called SAROSS-Net. In our DAKD pipeline, we\npresent a diffusion-based SAR-JointNet that learns to generate realistic SAR\nimages and their labels for segmentation, by effectively modeling joint\ndistribution with balancing two modalities. The DAKD pipeline augments the\ntraining dataset and distills knowledge from SAR-JointNet by utilizing\ngenerated soft labels (pixel-wise probability maps) to supervise our\nSAROSS-Net. The SAROSS-Net is designed to selectively transfer high-frequency\nfeatures from noisy SAR images, by employing novel Context-Aware Feature\nTransfer blocks along skip connections. We demonstrate our SAR-JointNet can\ngenerate realistic SAR images and well-aligned segmentation labels, providing\nthe augmented data to train SAROSS-Net with enhanced generalizability. Our\nSAROSS-Net trained with the DAKD pipeline significantly outperforms existing\nSAR oil spill segmentation methods with large margins.\n","authors":["Jaeho Moon","Jeonghwan Yun","Jaehyun Kim","Jaehyup Lee","Munchurl Kim"],"pdf_url":"https://arxiv.org/pdf/2412.08116v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08111v1","updated":"2024-12-11T05:37:04Z","published":"2024-12-11T05:37:04Z","title":"Seeing Syntax: Uncovering Syntactic Learning Limitations in\n  Vision-Language Models","summary":"  Vision-language models (VLMs), serve as foundation models for multi-modal\napplications such as image captioning and text-to-image generation. Recent\nstudies have highlighted limitations in VLM text encoders, particularly in\nareas like compositionality and semantic understanding, though the underlying\nreasons for these limitations remain unclear. In this work, we aim to address\nthis gap by analyzing the syntactic information, one of the fundamental\nlinguistic properties, encoded by the text encoders of VLMs. We perform a\nthorough analysis comparing VLMs with different objective functions, parameter\nsize and training data size, and with uni-modal language models (ULMs) in their\nability to encode syntactic knowledge. Our findings suggest that ULM text\nencoders acquire syntactic information more effectively than those in VLMs. The\nsyntactic information learned by VLM text encoders is shaped primarily by the\npre-training objective, which plays a more crucial role than other factors such\nas model architecture, model size, or the volume of pre-training data. Models\nexhibit different layer-wise trends where CLIP performance dropped across\nlayers while for other models, middle layers are rich in encoding syntactic\nknowledge.\n","authors":["Sri Harsha Dumpala","David Arps","Sageev Oore","Laura Kallmeyer","Hassan Sajjad"],"pdf_url":"https://arxiv.org/pdf/2412.08111v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08110v1","updated":"2024-12-11T05:36:18Z","published":"2024-12-11T05:36:18Z","title":"Barking Up The Syntactic Tree: Enhancing VLM Training with Syntactic\n  Losses","summary":"  Vision-Language Models (VLMs) achieved strong performance on a variety of\ntasks (e.g., image-text retrieval, visual question answering). However, most\nVLMs rely on coarse-grained image-caption pairs for alignment, relying on data\nvolume to resolve ambiguities and ground linguistic concepts in images. The\nricher semantic and syntactic structure within text is largely overlooked. To\naddress this, we propose HIerarchically STructured Learning (HIST) that\nenhances VLM training without any additional supervision, by hierarchically\ndecomposing captions into the constituent Subject, Noun Phrases, and Composite\nPhrases. Entailment between these constituent components allows us to formulate\nadditional regularization constraints on the VLM attention maps. Specifically,\nwe introduce two novel loss functions: (1) Subject Loss, which aligns image\ncontent with the subject of corresponding phrase, acting as an entailment of\nstandard contrastive/matching losses at the Phrase level; (2) Addition Loss, to\nbalance attention across multiple objects. HIST is general, and can be applied\nto any VLM for which attention between vision and language can be computed; we\nillustrate its efficacy on BLIP and ALBEF. HIST outperforms baseline VLMs,\nachieving up to +9.8% improvement in visual grounding, +6.3% in multi-object\nreferring segmentation, +1.1% in image-text retrieval, and +0.2% in visual\nquestion answering, underscoring the value of structuring learning in VLMs.\n","authors":["Jiayun Luo","Mir Rayat Imtiaz Hossain","Boyang Li","Leonid Sigal"],"pdf_url":"https://arxiv.org/pdf/2412.08110v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14053v2","updated":"2024-12-11T05:28:48Z","published":"2024-11-21T11:59:04Z","title":"Stereo Anything: Unifying Stereo Matching with Large-Scale Mixed Data","summary":"  Stereo matching has been a pivotal component in 3D vision, aiming to find\ncorresponding points between pairs of stereo images to recover depth\ninformation. In this work, we introduce StereoAnything, a highly practical\nsolution for robust stereo matching. Rather than focusing on a specialized\nmodel, our goal is to develop a versatile foundational model capable of\nhandling stereo images across diverse environments. To this end, we scale up\nthe dataset by collecting labeled stereo images and generating synthetic stereo\npairs from unlabeled monocular images. To further enrich the model's ability to\ngeneralize across different conditions, we introduce a novel synthetic dataset\nthat complements existing data by adding variability in baselines, camera\nangles, and scene types. We extensively evaluate the zero-shot capabilities of\nour model on five public datasets, showcasing its impressive ability to\ngeneralize to new, unseen data. Code will be available at\n\\url{https://github.com/XiandaGuo/OpenStereo}.\n","authors":["Xianda Guo","Chenming Zhang","Youmin Zhang","Dujun Nie","Ruilin Wang","Wenzhao Zheng","Matteo Poggi","Long Chen"],"pdf_url":"https://arxiv.org/pdf/2411.14053v2.pdf","comment":"Code will be available at\n  \\url{https://github.com/XiandaGuo/OpenStereo}"},{"id":"http://arxiv.org/abs/2412.08108v1","updated":"2024-12-11T05:23:34Z","published":"2024-12-11T05:23:34Z","title":"Doubly-Universal Adversarial Perturbations: Deceiving Vision-Language\n  Models Across Both Images and Text with a Single Perturbation","summary":"  Large Vision-Language Models (VLMs) have demonstrated remarkable performance\nacross multimodal tasks by integrating vision encoders with large language\nmodels (LLMs). However, these models remain vulnerable to adversarial attacks.\nAmong such attacks, Universal Adversarial Perturbations (UAPs) are especially\npowerful, as a single optimized perturbation can mislead the model across\nvarious input images. In this work, we introduce a novel UAP specifically\ndesigned for VLMs: the Doubly-Universal Adversarial Perturbation (Doubly-UAP),\ncapable of universally deceiving VLMs across both image and text inputs. To\nsuccessfully disrupt the vision encoder's fundamental process, we analyze the\ncore components of the attention mechanism. After identifying value vectors in\nthe middle-to-late layers as the most vulnerable, we optimize Doubly-UAP in a\nlabel-free manner with a frozen model. Despite being developed as a black-box\nto the LLM, Doubly-UAP achieves high attack success rates on VLMs, consistently\noutperforming baseline methods across vision-language tasks. Extensive ablation\nstudies and analyses further demonstrate the robustness of Doubly-UAP and\nprovide insights into how it influences internal attention mechanisms.\n","authors":["Hee-Seon Kim","Minbeom Kim","Changick Kim"],"pdf_url":"https://arxiv.org/pdf/2412.08108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08101v1","updated":"2024-12-11T04:57:53Z","published":"2024-12-11T04:57:53Z","title":"Generative Zoo","summary":"  The model-based estimation of 3D animal pose and shape from images enables\ncomputational modeling of animal behavior. Training models for this purpose\nrequires large amounts of labeled image data with precise pose and shape\nannotations. However, capturing such data requires the use of multi-view or\nmarker-based motion-capture systems, which are impractical to adapt to wild\nanimals in situ and impossible to scale across a comprehensive set of animal\nspecies. Some have attempted to address the challenge of procuring training\ndata by pseudo-labeling individual real-world images through manual 2D\nannotation, followed by 3D-parameter optimization to those labels. While this\napproach may produce silhouette-aligned samples, the obtained pose and shape\nparameters are often implausible due to the ill-posed nature of the monocular\nfitting problem. Sidestepping real-world ambiguity, others have designed\ncomplex synthetic-data-generation pipelines leveraging video-game engines and\ncollections of artist-designed 3D assets. Such engines yield perfect\nground-truth annotations but are often lacking in visual realism and require\nconsiderable manual effort to adapt to new species or environments. Motivated\nby these shortcomings, we propose an alternative approach to synthetic-data\ngeneration: rendering with a conditional image-generation model. We introduce a\npipeline that samples a diverse set of poses and shapes for a variety of\nmammalian quadrupeds and generates realistic images with corresponding\nground-truth pose and shape parameters. To demonstrate the scalability of our\napproach, we introduce GenZoo, a synthetic dataset containing one million\nimages of distinct subjects. We train a 3D pose and shape regressor on GenZoo,\nwhich achieves state-of-the-art performance on a real-world animal pose and\nshape estimation benchmark, despite being trained solely on synthetic data.\nhttps://genzoo.is.tue.mpg.de\n","authors":["Tomasz Niewiadomski","Anastasios Yiannakidis","Hanz Cuevas-Velasquez","Soubhik Sanyal","Michael J. Black","Silvia Zuffi","Peter Kulits"],"pdf_url":"https://arxiv.org/pdf/2412.08101v1.pdf","comment":"12 pages; project page: https://genzoo.is.tue.mpg.de"},{"id":"http://arxiv.org/abs/2406.17840v2","updated":"2024-12-11T04:37:15Z","published":"2024-06-25T17:46:28Z","title":"Human-Object Interaction from Human-Level Instructions","summary":"  Intelligent agents must autonomously interact with the environments to\nperform daily tasks based on human-level instructions. They need a foundational\nunderstanding of the world to accurately interpret these instructions, along\nwith precise low-level movement and interaction skills to execute the derived\nactions. In this work, we propose the first complete system for synthesizing\nphysically plausible, long-horizon human-object interactions for object\nmanipulation in contextual environments, driven by human-level instructions. We\nleverage large language models (LLMs) to interpret the input instructions into\ndetailed execution plans. Unlike prior work, our system is capable of\ngenerating detailed finger-object interactions, in seamless coordination with\nfull-body movements. We also train a policy to track generated motions in\nphysics simulation via reinforcement learning (RL) to ensure physical\nplausibility of the motion. Our experiments demonstrate the effectiveness of\nour system in synthesizing realistic interactions with diverse objects in\ncomplex environments, highlighting its potential for real-world applications.\n","authors":["Zhen Wu","Jiaman Li","Pei Xu","C. Karen Liu"],"pdf_url":"https://arxiv.org/pdf/2406.17840v2.pdf","comment":"project page: https://hoifhli.github.io/"},{"id":"http://arxiv.org/abs/2402.16825v4","updated":"2024-12-11T04:10:08Z","published":"2024-02-26T18:51:15Z","title":"Efficient 3D affinely equivariant CNNs with adaptive fusion of augmented\n  spherical Fourier-Bessel bases","summary":"  Filter-decomposition-based group equivariant convolutional neural networks\n(CNNs) have shown promising stability and data efficiency for 3D image feature\nextraction. However, these networks, which rely on parameter sharing and\ndiscrete transformation groups, often underperform in modern deep neural\nnetwork architectures for processing volumetric images, such as the common 3D\nmedical images. To address these limitations, this paper presents an efficient\nnon-parameter-sharing continuous 3D affine group equivariant neural network for\nvolumetric images. This network uses an adaptive aggregation of Monte Carlo\naugmented spherical Fourier-Bessel filter bases to improve the efficiency and\nflexibility of 3D group equivariant CNNs for volumetric data. Unlike existing\nmethods that focus only on angular orthogonality in filter bases, the\nintroduced spherical Bessel Fourier filter base incorporates both angular and\nradial orthogonality to improve feature extraction. Experiments on four medical\nimage segmentation datasets show that the proposed methods achieve better\naffine group equivariance and superior segmentation accuracy than existing 3D\ngroup equivariant convolutional neural network layers, significantly improving\nthe training stability and data efficiency of conventional CNN layers (at 0.05\nsignificance level). The code is available at\nhttps://github.com/ZhaoWenzhao/WMCSFB.\n","authors":["Wenzhao Zhao","Steffen Albert","Barbara D. Wichtmann","Angelika Maurer","Ulrike Attenberger","Frank G. Zöllner","Jürgen Hesser"],"pdf_url":"https://arxiv.org/pdf/2402.16825v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08082v1","updated":"2024-12-11T04:00:17Z","published":"2024-12-11T04:00:17Z","title":"FaceTracer: Unveiling Source Identities from Swapped Face Images and\n  Videos for Fraud Prevention","summary":"  Face-swapping techniques have advanced rapidly with the evolution of deep\nlearning, leading to widespread use and growing concerns about potential\nmisuse, especially in cases of fraud. While many efforts have focused on\ndetecting swapped face images or videos, these methods are insufficient for\ntracing the malicious users behind fraudulent activities. Intrusive\nwatermark-based approaches also fail to trace unmarked identities, limiting\ntheir practical utility. To address these challenges, we introduce FaceTracer,\nthe first non-intrusive framework specifically designed to trace the identity\nof the source person from swapped face images or videos. Specifically,\nFaceTracer leverages a disentanglement module that effectively suppresses\nidentity information related to the target person while isolating the identity\nfeatures of the source person. This allows us to extract robust identity\ninformation that can directly link the swapped face back to the original\nindividual, aiding in uncovering the actors behind fraudulent activities.\nExtensive experiments demonstrate FaceTracer's effectiveness across various\nface-swapping techniques, successfully identifying the source person in swapped\ncontent and enabling the tracing of malicious actors involved in fraudulent\nactivities. Additionally, FaceTracer shows strong transferability to unseen\nface-swapping methods including commercial applications and robustness against\ntransmission distortions and adaptive attacks.\n","authors":["Zhongyi Zhang","Jie Zhang","Wenbo Zhou","Xinghui Zhou","Qing Guo","Weiming Zhang","Tianwei Zhang","Nenghai Yu"],"pdf_url":"https://arxiv.org/pdf/2412.08082v1.pdf","comment":"17 pages, 18 figures, under review"},{"id":"http://arxiv.org/abs/2412.08081v1","updated":"2024-12-11T03:59:05Z","published":"2024-12-11T03:59:05Z","title":"How to select slices for annotation to train best-performing deep\n  learning segmentation models for cross-sectional medical images?","summary":"  Automated segmentation of medical images highly depends on the availability\nof accurate manual image annotations. Such annotations are very time-consuming\nand costly to generate, and often require specialized expertise, particularly\nfor cross-sectional images which contain many slices for each patient. It is\ncrucial to ensure the best use of annotation resources. In this paper, we\nsystematically answer the question of how to select slices of cross-sectional\nmedical images in order to maximize performance of the resulting deep learning\nsegmentation models. We conducted experiments on 4 medical imaging segmentation\ntasks with varying annotation budgets, numbers of annotated cases, numbers of\nannotated slices per volume, slice selection techniques, and mask\ninterpolations. We found that:\n  1) It is almost always preferable to annotate fewer slices per volume and\nmore volumes given an annotation budget. 2) Selecting slices for annotation by\nunsupervised active learning (UAL) is not superior to selecting slices randomly\nor at fixed intervals, provided that each volume is allocated the same number\nof annotated slices. 3) Interpolating masks between annotated slices rarely\nenhances model performance, with exceptions of some specific configuration for\n3D models.\n","authors":["Yixin Zhang","Kevin Kramer","Maciej A. Mazurowski"],"pdf_url":"https://arxiv.org/pdf/2412.08081v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.05871v3","updated":"2024-12-11T03:58:47Z","published":"2024-12-08T09:34:23Z","title":"MID: A Comprehensive Shore-Based Dataset for Multi-Scale Dense Ship\n  Occlusion and Interaction Scenarios","summary":"  This paper introduces the Maritime Ship Navigation Behavior Dataset (MID),\ndesigned to address challenges in ship detection within complex maritime\nenvironments using Oriented Bounding Boxes (OBB). MID contains 5,673 images\nwith 135,884 finely annotated target instances, supporting both supervised and\nsemi-supervised learning. It features diverse maritime scenarios such as ship\nencounters under varying weather, docking maneuvers, small target clustering,\nand partial occlusions, filling critical gaps in datasets like HRSID, SSDD, and\nNWPU-10. MID's images are sourced from high-definition video clips of\nreal-world navigation across 43 water areas, with varied weather and lighting\nconditions (e.g., rain, fog). Manually curated annotations enhance the\ndataset's variety, ensuring its applicability to real-world demands in busy\nports and dense maritime regions. This diversity equips models trained on MID\nto better handle complex, dynamic environments, supporting advancements in\nmaritime situational awareness. To validate MID's utility, we evaluated 10\ndetection algorithms, providing an in-depth analysis of the dataset, detection\nresults from various models, and a comparative study of baseline algorithms,\nwith a focus on handling occlusions and dense target clusters. The results\nhighlight MID's potential to drive innovation in intelligent maritime traffic\nmonitoring and autonomous navigation systems. The dataset will be made publicly\navailable at https://github.com/VirtualNew/MID_DataSet.\n","authors":["Yugang Chang","Hongyu Chen","Fei Wang","Chengcheng Chen","Weiming Zeng"],"pdf_url":"https://arxiv.org/pdf/2412.05871v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08074v1","updated":"2024-12-11T03:43:18Z","published":"2024-12-11T03:43:18Z","title":"EM-Net: Gaze Estimation with Expectation Maximization Algorithm","summary":"  In recent years, the accuracy of gaze estimation techniques has gradually\nimproved, but existing methods often rely on large datasets or large models to\nimprove performance, which leads to high demands on computational resources. In\nterms of this issue, this paper proposes a lightweight gaze estimation model\nEM-Net based on deep learning and traditional machine learning algorithms\nExpectation Maximization algorithm. First, the proposed Global Attention\nMechanism(GAM) is added to extract features related to gaze estimation to\nimprove the model's ability to capture global dependencies and thus improve its\nperformance. Second, by learning hierarchical feature representations through\nthe EM module, the model has strong generalization ability, which reduces the\nneed for sample size. Experiments have confirmed that, on the premise of using\nonly 50% of the training data, EM-Net improves the performance of Gaze360,\nMPIIFaceGaze, and RT-Gene datasets by 2.2%, 2.02%, and 2.03%, respectively,\ncompared with GazeNAS-ETH. It also shows good robustness in the face of\nGaussian noise interference.\n","authors":["Zhang Cheng","Yanxia Wang","Guoyu Xia"],"pdf_url":"https://arxiv.org/pdf/2412.08074v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08073v1","updated":"2024-12-11T03:42:31Z","published":"2024-12-11T03:42:31Z","title":"Visible and Infrared Image Fusion Using Encoder-Decoder Network","summary":"  The aim of multispectral image fusion is to combine object or scene features\nof images with different spectral characteristics to increase the perceptual\nquality. In this paper, we present a novel learning-based solution to image\nfusion problem focusing on infrared and visible spectrum images. The proposed\nsolution utilizes only convolution and pooling layers together with a loss\nfunction using no-reference quality metrics. The analysis is performed\nqualitatively and quantitatively on various datasets. The results show better\nperformance than state-of-the-art methods. Also, the size of our network\nenables real-time performance on embedded devices. Project codes can be found\nat \\url{https://github.com/ferhatcan/pyFusionSR}.\n","authors":["Ferhat Can Ataman","Gözde Bozdaği Akar"],"pdf_url":"https://arxiv.org/pdf/2412.08073v1.pdf","comment":"5 pages, published at ICIP 2021"},{"id":"http://arxiv.org/abs/2312.02021v4","updated":"2024-12-11T03:37:28Z","published":"2023-12-04T16:46:38Z","title":"Strong but simple: A Baseline for Domain Generalized Dense Perception by\n  CLIP-based Transfer Learning","summary":"  Domain generalization (DG) remains a significant challenge for perception\nbased on deep neural networks (DNNs), where domain shifts occur due to\nsynthetic data, lighting, weather, or location changes. Vision-language models\n(VLMs) marked a large step for the generalization capabilities and have been\nalready applied to various tasks. Very recently, first approaches utilized VLMs\nfor domain generalized segmentation and object detection and obtained strong\ngeneralization. However, all these approaches rely on complex modules, feature\naugmentation frameworks or additional models. Surprisingly and in contrast to\nthat, we found that simple fine-tuning of vision-language pre-trained models\nyields competitive or even stronger generalization results while being\nextremely simple to apply. Moreover, we found that vision-language pre-training\nconsistently provides better generalization than the previous standard of\nvision-only pre-training. This challenges the standard of using ImageNet-based\ntransfer learning for domain generalization. Fully fine-tuning a\nvision-language pre-trained model is capable of reaching the domain\ngeneralization SOTA when training on the synthetic GTA5 dataset. Moreover, we\nconfirm this observation for object detection on a novel synthetic-to-real\nbenchmark. We further obtain superior generalization capabilities by reaching\n77.9% mIoU on the popular Cityscapes-to-ACDC benchmark. We also found improved\nin-domain generalization, leading to an improved SOTA of 86.4% mIoU on the\nCityscapes test set marking the first place on the leaderboard.\n","authors":["Christoph Hümmer","Manuel Schwonberg","Liangwei Zhou","Hu Cao","Alois Knoll","Hanno Gottschalk"],"pdf_url":"https://arxiv.org/pdf/2312.02021v4.pdf","comment":"Accepted to ACCV 2024; Project Page: https://vltseg.github.io/"},{"id":"http://arxiv.org/abs/2308.03492v2","updated":"2024-12-11T03:34:48Z","published":"2023-08-07T11:34:27Z","title":"Learning Photometric Feature Transform for Free-form Object Scan","summary":"  We propose a novel framework to automatically learn to aggregate and\ntransform photometric measurements from multiple unstructured views into\nspatially distinctive and view-invariant low-level features, which are\nsubsequently fed to a multi-view stereo pipeline to enhance 3D reconstruction.\nThe illumination conditions during acquisition and the feature transform are\njointly trained on a large amount of synthetic data. We further build a system\nto reconstruct both the geometry and anisotropic reflectance of a variety of\nchallenging objects from hand-held scans. The effectiveness of the system is\ndemonstrated with a lightweight prototype, consisting of a camera and an array\nof LEDs, as well as an off-the-shelf tablet. Our results are validated against\nreconstructions from a professional 3D scanner and photographs, and compare\nfavorably with state-of-the-art techniques.\n","authors":["Xiang Feng","Kaizhang Kang","Fan Pei","Huakeng Ding","Jinjiang You","Ping Tan","Kun Zhou","Hongzhi Wu"],"pdf_url":"https://arxiv.org/pdf/2308.03492v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07377v2","updated":"2024-12-11T03:27:12Z","published":"2024-12-10T10:22:17Z","title":"CADSpotting: Robust Panoptic Symbol Spotting on Large-Scale CAD Drawings","summary":"  We introduce CADSpotting, an efficient method for panoptic symbol spotting in\nlarge-scale architectural CAD drawings. Existing approaches struggle with the\ndiversity of symbols, scale variations, and overlapping elements in CAD\ndesigns. CADSpotting overcomes these challenges by representing each primitive\nwith dense points instead of a single primitive point, described by essential\nattributes like coordinates and color. Building upon a unified 3D point cloud\nmodel for joint semantic, instance, and panoptic segmentation, CADSpotting\nlearns robust feature representations. To enable accurate segmentation in\nlarge, complex drawings, we further propose a novel Sliding Window Aggregation\n(SWA) technique, combining weighted voting and Non-Maximum Suppression (NMS).\nMoreover, we introduce a large-scale CAD dataset named LS-CAD to support our\nexperiments. Each floorplan in LS-CAD has an average coverage of 1,000 square\nmeter(versus 100 square meter in the existing dataset), providing a valuable\nbenchmark for symbol spotting research. Experimental results on FloorPlanCAD\nand LS-CAD datasets demonstrate that CADSpotting outperforms existing methods,\nshowcasing its robustness and scalability for real-world CAD applications.\n","authors":["Jiazuo Mu","Fuyi Yang","Yanshun Zhang","Junxiong Zhang","Yongjian Luo","Lan Xu","Yujiao Shi","Jingyi Yu","Yingliang Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.07377v2.pdf","comment":"16pages, 12 figures, Project web-page:\n  https://dgeneai.github.io/cadspotting-pages/"},{"id":"http://arxiv.org/abs/2412.07768v2","updated":"2024-12-11T03:04:20Z","published":"2024-12-10T18:59:32Z","title":"Test-time Correction with Human Feedback: An Online 3D Detection System\n  via Visual Prompting","summary":"  This paper introduces Test-time Correction (TTC) system, a novel online 3D\ndetection system designated for online correction of test-time errors via human\nfeedback, to guarantee the safety of deployed autonomous driving systems.\nUnlike well-studied offline 3D detectors frozen at inference, TTC explores the\ncapability of instant online error rectification. By leveraging user feedback\nwith interactive prompts at a frame, e.g., a simple click or draw of boxes, TTC\ncould immediately update the corresponding detection results for future\nstreaming inputs, even though the model is deployed with fixed parameters. This\nenables autonomous driving systems to adapt to new scenarios immediately and\ndecrease deployment risks reliably without additional expensive training. To\nachieve such TTC system, we equip existing 3D detectors with Online Adapter\n(OA) module, a prompt-driven query generator for online correction. At the core\nof OA module are visual prompts, images of missed object-of-interest for\nguiding the corresponding detection and subsequent tracking. Those visual\nprompts, belonging to missed objects through online inference, are maintained\nby the visual prompt buffer for continuous error correction in subsequent\nframes. By doing so, TTC consistently detects online missed objects and\nimmediately lowers driving risks. It achieves reliable, versatile, and adaptive\ndriving autonomy. Extensive experiments demonstrate significant gain on instant\nerror rectification over pre-trained 3D detectors, even in challenging\nscenarios with limited labels, zero-shot detection, and adverse conditions. We\nhope this work would inspire the community to investigate online rectification\nsystems for autonomous driving post-deployment. Code would be publicly shared.\n","authors":["Zetong Yang","Hanxue Zhang","Yanan Sun","Li Chen","Fei Xia","Fatma Güney","Hongyang Li"],"pdf_url":"https://arxiv.org/pdf/2412.07768v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08053v1","updated":"2024-12-11T03:00:15Z","published":"2024-12-11T03:00:15Z","title":"DynamicPAE: Generating Scene-Aware Physical Adversarial Examples in\n  Real-Time","summary":"  Physical adversarial examples (PAEs) are regarded as \"whistle-blowers\" of\nreal-world risks in deep-learning applications. However, current PAE generation\nstudies show limited adaptive attacking ability to diverse and varying scenes.\nThe key challenges in generating dynamic PAEs are exploring their patterns\nunder noisy gradient feedback and adapting the attack to agnostic scenario\nnatures. To address the problems, we present DynamicPAE, the first generative\nframework that enables scene-aware real-time physical attacks beyond static\nattacks. Specifically, to train the dynamic PAE generator under noisy gradient\nfeedback, we introduce the residual-driven sample trajectory guidance\ntechnique, which redefines the training task to break the limited feedback\ninformation restriction that leads to the degeneracy problem. Intuitively, it\nallows the gradient feedback to be passed to the generator through a low-noise\nauxiliary task, thereby guiding the optimization away from degenerate solutions\nand facilitating a more comprehensive and stable exploration of feasible PAEs.\nTo adapt the generator to agnostic scenario natures, we introduce the\ncontext-aligned scene expectation simulation process, consisting of the\nconditional-uncertainty-aligned data module and the skewness-aligned objective\nre-weighting module. The former enhances robustness in the context of\nincomplete observation by employing a conditional probabilistic model for\ndomain randomization, while the latter facilitates consistent stealth control\nacross different attack targets by automatically reweighting losses based on\nthe skewness indicator. Extensive digital and physical evaluations demonstrate\nthe superior attack performance of DynamicPAE, attaining a 1.95 $\\times$ boost\n(65.55% average AP drop under attack) on representative object detectors (e.g.,\nYolo-v8) over state-of-the-art static PAE generating methods.\n","authors":["Jin Hu","Xianglong Liu","Jiakai Wang","Junkai Zhang","Xianqi Yang","Haotong Qin","Yuqing Ma","Ke Xu"],"pdf_url":"https://arxiv.org/pdf/2412.08053v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2412.08050v1","updated":"2024-12-11T02:56:23Z","published":"2024-12-11T02:56:23Z","title":"BSAFusion: A Bidirectional Stepwise Feature Alignment Network for\n  Unaligned Medical Image Fusion","summary":"  If unaligned multimodal medical images can be simultaneously aligned and\nfused using a single-stage approach within a unified processing framework, it\nwill not only achieve mutual promotion of dual tasks but also help reduce the\ncomplexity of the model. However, the design of this model faces the challenge\nof incompatible requirements for feature fusion and alignment; specifically,\nfeature alignment requires consistency among corresponding features, whereas\nfeature fusion requires the features to be complementary to each other. To\naddress this challenge, this paper proposes an unaligned medical image fusion\nmethod called Bidirectional Stepwise Feature Alignment and Fusion (BSFA-F)\nstrategy. To reduce the negative impact of modality differences on cross-modal\nfeature matching, we incorporate the Modal Discrepancy-Free Feature\nRepresentation (MDF-FR) method into BSFA-F. MDF-FR utilizes a Modality Feature\nRepresentation Head (MFRH) to integrate the global information of the input\nimage. By injecting the information contained in MFRH of the current image into\nother modality images, it effectively reduces the impact of modality\ndifferences on feature alignment while preserving the complementary information\ncarried by different images. In terms of feature alignment, BSFA-F employs a\nbidirectional stepwise alignment deformation field prediction strategy based on\nthe path independence of vector displacement between two points. This strategy\nsolves the problem of large spans and inaccurate deformation field prediction\nin single-step alignment. Finally, Multi-Modal Feature Fusion block achieves\nthe fusion of aligned features. The experimental results across multiple\ndatasets demonstrate the effectiveness of our method. The source code is\navailable at https://github.com/slrl123/BSAFusion.\n","authors":["Huafeng Li","Dayong Su","Qing Cai","Yafei Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.08050v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2410.10306v2","updated":"2024-12-11T02:55:31Z","published":"2024-10-14T09:06:55Z","title":"Animate-X: Universal Character Image Animation with Enhanced Motion\n  Representation","summary":"  Character image animation, which generates high-quality videos from a\nreference image and target pose sequence, has seen significant progress in\nrecent years. However, most existing methods only apply to human figures, which\nusually do not generalize well on anthropomorphic characters commonly used in\nindustries like gaming and entertainment. Our in-depth analysis suggests to\nattribute this limitation to their insufficient modeling of motion, which is\nunable to comprehend the movement pattern of the driving video, thus imposing a\npose sequence rigidly onto the target character. To this end, this paper\nproposes Animate-X, a universal animation framework based on LDM for various\ncharacter types (collectively named X), including anthropomorphic characters.\nTo enhance motion representation, we introduce the Pose Indicator, which\ncaptures comprehensive motion pattern from the driving video through both\nimplicit and explicit manner. The former leverages CLIP visual features of a\ndriving video to extract its gist of motion, like the overall movement pattern\nand temporal relations among motions, while the latter strengthens the\ngeneralization of LDM by simulating possible inputs in advance that may arise\nduring inference. Moreover, we introduce a new Animated Anthropomorphic\nBenchmark (A^2Bench) to evaluate the performance of Animate-X on universal and\nwidely applicable animation images. Extensive experiments demonstrate the\nsuperiority and effectiveness of Animate-X compared to state-of-the-art\nmethods.\n","authors":["Shuai Tan","Biao Gong","Xiang Wang","Shiwei Zhang","Dandan Zheng","Ruobing Zheng","Kecheng Zheng","Jingdong Chen","Ming Yang"],"pdf_url":"https://arxiv.org/pdf/2410.10306v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08048v1","updated":"2024-12-11T02:54:21Z","published":"2024-12-11T02:54:21Z","title":"Surveying Facial Recognition Models for Diverse Indian Demographics: A\n  Comparative Analysis on LFW and Custom Dataset","summary":"  Facial recognition technology has made significant advances, yet its\neffectiveness across diverse ethnic backgrounds, particularly in specific\nIndian demographics, is less explored. This paper presents a detailed\nevaluation of both traditional and deep learning-based facial recognition\nmodels using the established LFW dataset and our newly developed IITJ Faces of\nAcademia Dataset (JFAD), which comprises images of students from IIT Jodhpur.\nThis unique dataset is designed to reflect the ethnic diversity of India,\nproviding a critical test bed for assessing model performance in a focused\nacademic environment. We analyze models ranging from holistic approaches like\nEigenfaces and SIFT to advanced hybrid models that integrate CNNs with Gabor\nfilters, Laplacian transforms, and segmentation techniques. Our findings reveal\nsignificant insights into the models' ability to adapt to the ethnic\nvariability within Indian demographics and suggest modifications to enhance\naccuracy and inclusivity in real-world applications. The JFAD not only serves\nas a valuable resource for further research but also highlights the need for\ndeveloping facial recognition systems that perform equitably across diverse\npopulations.\n","authors":["Pranav Pant","Niharika Dadu","Harsh V. Singh","Anshul Thakur"],"pdf_url":"https://arxiv.org/pdf/2412.08048v1.pdf","comment":"Research Project - Computer Vision"},{"id":"http://arxiv.org/abs/2412.06172v2","updated":"2024-12-11T02:51:34Z","published":"2024-12-09T03:06:10Z","title":"Robust Noisy Correspondence Learning via Self-Drop and Dual-Weight","summary":"  Many researchers collect data from the internet through crowd-sourcing or web\ncrawling to alleviate the data-hungry challenge associated with cross-modal\nmatching. Although such practice does not require expensive annotations, it\ninevitably introduces mismatched pairs and results in a noisy correspondence\nproblem. Current approaches leverage the memorization effect of deep neural\nnetworks to distinguish noise and perform re-weighting. However, briefly\nlowering the weight of noisy pairs cannot eliminate the negative impact of\nnoisy correspondence in the training process. In this paper, we propose a novel\nself-drop and dual-weight approach, which achieves elaborate data processing by\nqua-partitioning the data. Specifically, our approach partitions all data into\nfour types: clean and significant, clean yet insignificant, vague, and noisy.\nWe analyze the effect of noisy and clean data pairs and find that for\nvision-language pre-training models, a small number of clean samples is more\nvaluable than a majority of noisy ones. Based on this observation, we employ\nself-drop to discard noisy samples to effectively mitigate the impact of noise.\nIn addition, we adopt a dual-weight strategy to ensure that the model focuses\nmore on significant samples while appropriately leveraging vague samples.\nCompared to the prior works, our approach is more robust and demonstrates\nrelatively more stable performance on noisy datasets, especially under a high\nnoise ratio. Extensive experiments on three widely used datasets, including\nFlickr30K, MS-COCO, and Conceptual Captions, validate the effectiveness of our\napproach.\n","authors":["Fan Liu","Chenwei Dong","Chuanyi Zhang","Hualiang Zhou","Jun Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.06172v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17778v2","updated":"2024-12-11T02:50:25Z","published":"2024-09-26T12:16:11Z","title":"Taming Diffusion Prior for Image Super-Resolution with Domain Shift SDEs","summary":"  Diffusion-based image super-resolution (SR) models have attracted substantial\ninterest due to their powerful image restoration capabilities. However,\nprevailing diffusion models often struggle to strike an optimal balance between\nefficiency and performance. Typically, they either neglect to exploit the\npotential of existing extensive pretrained models, limiting their generative\ncapacity, or they necessitate a dozens of forward passes starting from random\nnoises, compromising inference efficiency. In this paper, we present DoSSR, a\nDomain Shift diffusion-based SR model that capitalizes on the generative powers\nof pretrained diffusion models while significantly enhancing efficiency by\ninitiating the diffusion process with low-resolution (LR) images. At the core\nof our approach is a domain shift equation that integrates seamlessly with\nexisting diffusion models. This integration not only improves the use of\ndiffusion prior but also boosts inference efficiency. Moreover, we advance our\nmethod by transitioning the discrete shift process to a continuous formulation,\ntermed as DoS-SDEs. This advancement leads to the fast and customized solvers\nthat further enhance sampling efficiency. Empirical results demonstrate that\nour proposed method achieves state-of-the-art performance on synthetic and\nreal-world datasets, while notably requiring only 5 sampling steps. Compared to\nprevious diffusion prior based methods, our approach achieves a remarkable\nspeedup of 5-7 times, demonstrating its superior efficiency. Code:\nhttps://github.com/QinpengCui/DoSSR.\n","authors":["Qinpeng Cui","Yixuan Liu","Xinyi Zhang","Qiqi Bao","Qingmin Liao","Li Wang","Tian Lu","Zicheng Liu","Zhongdao Wang","Emad Barsoum"],"pdf_url":"https://arxiv.org/pdf/2409.17778v2.pdf","comment":"This paper is accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2408.12245v3","updated":"2024-12-11T02:50:08Z","published":"2024-08-22T09:27:49Z","title":"Scalable Autoregressive Image Generation with Mamba","summary":"  We introduce AiM, an autoregressive (AR) image generative model based on\nMamba architecture. AiM employs Mamba, a novel state-space model characterized\nby its exceptional performance for long-sequence modeling with linear time\ncomplexity, to supplant the commonly utilized Transformers in AR image\ngeneration models, aiming to achieve both superior generation quality and\nenhanced inference speed. Unlike existing methods that adapt Mamba to handle\ntwo-dimensional signals via multi-directional scan, AiM directly utilizes the\nnext-token prediction paradigm for autoregressive image generation. This\napproach circumvents the need for extensive modifications to enable Mamba to\nlearn 2D spatial representations. By implementing straightforward yet\nstrategically targeted modifications for visual generative tasks, we preserve\nMamba's core structure, fully exploiting its efficient long-sequence modeling\ncapabilities and scalability. We provide AiM models in various scales, with\nparameter counts ranging from 148M to 1.3B. On the ImageNet1K 256*256\nbenchmark, our best AiM model achieves a FID of 2.21, surpassing all existing\nAR models of comparable parameter counts and demonstrating significant\ncompetitiveness against diffusion models, with 2 to 10 times faster inference\nspeed. Code is available at https://github.com/hp-l33/AiM\n","authors":["Haopeng Li","Jinyue Yang","Kexin Wang","Xuerui Qiu","Yuhong Chou","Xin Li","Guoqi Li"],"pdf_url":"https://arxiv.org/pdf/2408.12245v3.pdf","comment":"9 pages, 8 figures"},{"id":"http://arxiv.org/abs/2310.16045v2","updated":"2024-12-11T02:46:29Z","published":"2023-10-24T17:58:07Z","title":"Woodpecker: Hallucination Correction for Multimodal Large Language\n  Models","summary":"  Hallucination is a big shadow hanging over the rapidly evolving Multimodal\nLarge Language Models (MLLMs), referring to the phenomenon that the generated\ntext is inconsistent with the image content. In order to mitigate\nhallucinations, existing studies mainly resort to an instruction-tuning manner\nthat requires retraining the models with specific data. In this paper, we pave\na different way, introducing a training-free method named Woodpecker. Like a\nwoodpecker heals trees, it picks out and corrects hallucinations from the\ngenerated text. Concretely, Woodpecker consists of five stages: key concept\nextraction, question formulation, visual knowledge validation, visual claim\ngeneration, and hallucination correction. Implemented in a post-remedy manner,\nWoodpecker can easily serve different MLLMs, while being interpretable by\naccessing intermediate outputs of the five stages. We evaluate Woodpecker both\nquantitatively and qualitatively and show the huge potential of this new\nparadigm. On the POPE benchmark, our method obtains a 30.66%/24.33% improvement\nin accuracy over the baseline MiniGPT-4/mPLUG-Owl. The source code is released\nat https://github.com/BradyFU/Woodpecker.\n","authors":["Shukang Yin","Chaoyou Fu","Sirui Zhao","Tong Xu","Hao Wang","Dianbo Sui","Yunhang Shen","Ke Li","Xing Sun","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2310.16045v2.pdf","comment":"Accepted by Science China Information Sciences (SCIS)"},{"id":"http://arxiv.org/abs/2412.08034v1","updated":"2024-12-11T02:29:51Z","published":"2024-12-11T02:29:51Z","title":"Static-Dynamic Class-level Perception Consistency in Video Semantic\n  Segmentation","summary":"  Video semantic segmentation(VSS) has been widely employed in lots of fields,\nsuch as simultaneous localization and mapping, autonomous driving and\nsurveillance. Its core challenge is how to leverage temporal information to\nachieve better segmentation. Previous efforts have primarily focused on\npixel-level static-dynamic contexts matching, utilizing techniques such as\noptical flow and attention mechanisms. Instead, this paper rethinks\nstatic-dynamic contexts at the class level and proposes a novel static-dynamic\nclass-level perceptual consistency (SD-CPC) framework. In this framework, we\npropose multivariate class prototype with contrastive learning and a\nstatic-dynamic semantic alignment module. The former provides class-level\nconstraints for the model, obtaining personalized inter-class features and\ndiversified intra-class features. The latter first establishes intra-frame\nspatial multi-scale and multi-level correlations to achieve static semantic\nalignment. Then, based on cross-frame static perceptual differences, it\nperforms two-stage cross-frame selective aggregation to achieve dynamic\nsemantic alignment. Meanwhile, we propose a window-based attention map\ncalculation method that leverages the sparsity of attention points during\ncross-frame aggregation to reduce computation cost. Extensive experiments on\nVSPW and Cityscapes datasets show that the proposed approach outperforms\nstate-of-the-art methods. Our implementation will be open-sourced on GitHub.\n","authors":["Zhigang Cen","Ningyan Guo","Wenjing Xu","Zhiyong Feng","Danlan Huang"],"pdf_url":"https://arxiv.org/pdf/2412.08034v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.05280v2","updated":"2024-12-11T02:27:18Z","published":"2024-12-06T18:59:56Z","title":"Stag-1: Towards Realistic 4D Driving Simulation with Video Generation\n  Model","summary":"  4D driving simulation is essential for developing realistic autonomous\ndriving simulators. Despite advancements in existing methods for generating\ndriving scenes, significant challenges remain in view transformation and\nspatial-temporal dynamic modeling. To address these limitations, we propose a\nSpatial-Temporal simulAtion for drivinG (Stag-1) model to reconstruct\nreal-world scenes and design a controllable generative network to achieve 4D\nsimulation. Stag-1 constructs continuous 4D point cloud scenes using\nsurround-view data from autonomous vehicles. It decouples spatial-temporal\nrelationships and produces coherent keyframe videos. Additionally, Stag-1\nleverages video generation models to obtain photo-realistic and controllable 4D\ndriving simulation videos from any perspective. To expand the range of view\ngeneration, we train vehicle motion videos based on decomposed camera poses,\nenhancing modeling capabilities for distant scenes. Furthermore, we reconstruct\nvehicle camera trajectories to integrate 3D points across consecutive views,\nenabling comprehensive scene understanding along the temporal dimension.\nFollowing extensive multi-level scene training, Stag-1 can simulate from any\ndesired viewpoint and achieve a deep understanding of scene evolution under\nstatic spatial-temporal conditions. Compared to existing methods, our approach\nshows promising performance in multi-view scene consistency, background\ncoherence, and accuracy, and contributes to the ongoing advancements in\nrealistic autonomous driving simulation. Code: https://github.com/wzzheng/Stag.\n","authors":["Lening Wang","Wenzhao Zheng","Dalong Du","Yunpeng Zhang","Yilong Ren","Han Jiang","Zhiyong Cui","Haiyang Yu","Jie Zhou","Jiwen Lu","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.05280v2.pdf","comment":"Code is available at: https://github.com/wzzheng/Stag"},{"id":"http://arxiv.org/abs/2412.08029v1","updated":"2024-12-11T02:17:33Z","published":"2024-12-11T02:17:33Z","title":"NeRF-NQA: No-Reference Quality Assessment for Scenes Generated by NeRF\n  and Neural View Synthesis Methods","summary":"  Neural View Synthesis (NVS) has demonstrated efficacy in generating\nhigh-fidelity dense viewpoint videos using a image set with sparse views.\nHowever, existing quality assessment methods like PSNR, SSIM, and LPIPS are not\ntailored for the scenes with dense viewpoints synthesized by NVS and NeRF\nvariants, thus, they often fall short in capturing the perceptual quality,\nincluding spatial and angular aspects of NVS-synthesized scenes. Furthermore,\nthe lack of dense ground truth views makes the full reference quality\nassessment on NVS-synthesized scenes challenging. For instance, datasets such\nas LLFF provide only sparse images, insufficient for complete full-reference\nassessments. To address the issues above, we propose NeRF-NQA, the first\nno-reference quality assessment method for densely-observed scenes synthesized\nfrom the NVS and NeRF variants. NeRF-NQA employs a joint quality assessment\nstrategy, integrating both viewwise and pointwise approaches, to evaluate the\nquality of NVS-generated scenes. The viewwise approach assesses the spatial\nquality of each individual synthesized view and the overall inter-views\nconsistency, while the pointwise approach focuses on the angular qualities of\nscene surface points and their compound inter-point quality. Extensive\nevaluations are conducted to compare NeRF-NQA with 23 mainstream visual quality\nassessment methods (from fields of image, video, and light-field assessment).\nThe results demonstrate NeRF-NQA outperforms the existing assessment methods\nsignificantly and it shows substantial superiority on assessing NVS-synthesized\nscenes without references. An implementation of this paper are available at\nhttps://github.com/VincentQQu/NeRF-NQA.\n","authors":["Qiang Qu","Hanxue Liang","Xiaoming Chen","Yuk Ying Chung","Yiran Shen"],"pdf_url":"https://arxiv.org/pdf/2412.08029v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08020v1","updated":"2024-12-11T02:00:25Z","published":"2024-12-11T02:00:25Z","title":"Intelligent Control of Robotic X-ray Devices using a Language-promptable\n  Digital Twin","summary":"  Natural language offers a convenient, flexible interface for controlling\nrobotic C-arm X-ray systems, making advanced functionality and controls\naccessible. However, enabling language interfaces requires specialized AI\nmodels that interpret X-ray images to create a semantic representation for\nreasoning. The fixed outputs of such AI models limit the functionality of\nlanguage controls. Incorporating flexible, language-aligned AI models prompted\nthrough language enables more versatile interfaces for diverse tasks and\nprocedures. Using a language-aligned foundation model for X-ray image\nsegmentation, our system continually updates a patient digital twin based on\nsparse reconstructions of desired anatomical structures. This supports\nautonomous capabilities such as visualization, patient-specific viewfinding,\nand automatic collimation from novel viewpoints, enabling commands 'Focus in on\nthe lower lumbar vertebrae.' In a cadaver study, users visualized, localized,\nand collimated structures across the torso using verbal commands, achieving 84%\nend-to-end success. Post hoc analysis of randomly oriented images showed our\npatient digital twin could localize 35 commonly requested structures to within\n51.68 mm, enabling localization and isolation from arbitrary orientations. Our\nresults demonstrate how intelligent robotic X-ray systems can incorporate\nphysicians' expressed intent directly. While existing foundation models for\nintra-operative X-ray analysis exhibit failure modes, as they improve, they can\nfacilitate highly flexible, intelligent robotic C-arms.\n","authors":["Benjamin D. Killeen","Anushri Suresh","Catalina Gomez","Blanca Inigo","Christopher Bailey","Mathias Unberath"],"pdf_url":"https://arxiv.org/pdf/2412.08020v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03013v3","updated":"2024-12-11T01:54:12Z","published":"2024-11-05T11:25:19Z","title":"CRT-Fusion: Camera, Radar, Temporal Fusion Using Motion Information for\n  3D Object Detection","summary":"  Accurate and robust 3D object detection is a critical component in autonomous\nvehicles and robotics. While recent radar-camera fusion methods have made\nsignificant progress by fusing information in the bird's-eye view (BEV)\nrepresentation, they often struggle to effectively capture the motion of\ndynamic objects, leading to limited performance in real-world scenarios. In\nthis paper, we introduce CRT-Fusion, a novel framework that integrates temporal\ninformation into radar-camera fusion to address this challenge. Our approach\ncomprises three key modules: Multi-View Fusion (MVF), Motion Feature Estimator\n(MFE), and Motion Guided Temporal Fusion (MGTF). The MVF module fuses radar and\nimage features within both the camera view and bird's-eye view, thereby\ngenerating a more precise unified BEV representation. The MFE module conducts\ntwo simultaneous tasks: estimation of pixel-wise velocity information and BEV\nsegmentation. Based on the velocity and the occupancy score map obtained from\nthe MFE module, the MGTF module aligns and fuses feature maps across multiple\ntimestamps in a recurrent manner. By considering the motion of dynamic objects,\nCRT-Fusion can produce robust BEV feature maps, thereby improving detection\naccuracy and robustness. Extensive evaluations on the challenging nuScenes\ndataset demonstrate that CRT-Fusion achieves state-of-the-art performance for\nradar-camera-based 3D object detection. Our approach outperforms the previous\nbest method in terms of NDS by +1.7%, while also surpassing the leading\napproach in mAP by +1.4%. These significant improvements in both metrics\nshowcase the effectiveness of our proposed fusion strategy in enhancing the\nreliability and accuracy of 3D object detection.\n","authors":["Jisong Kim","Minjae Seong","Jun Won Choi"],"pdf_url":"https://arxiv.org/pdf/2411.03013v3.pdf","comment":"Accepted at NeurIPS2024"},{"id":"http://arxiv.org/abs/2412.08014v1","updated":"2024-12-11T01:41:19Z","published":"2024-12-11T01:41:19Z","title":"MAGIC: Mastering Physical Adversarial Generation in Context through\n  Collaborative LLM Agents","summary":"  Physical adversarial attacks in driving scenarios can expose critical\nvulnerabilities in visual perception models. However, developing such attacks\nremains challenging due to diverse real-world backgrounds and the requirement\nfor maintaining visual naturality. Building upon this challenge, we reformulate\nphysical adversarial attacks as a one-shot patch-generation problem. Our\napproach generates adversarial patches through a deep generative model that\nconsiders the specific scene context, enabling direct physical deployment in\nmatching environments. The primary challenge lies in simultaneously achieving\ntwo objectives: generating adversarial patches that effectively mislead object\ndetection systems while determining contextually appropriate placement within\nthe scene. We propose MAGIC (Mastering Physical Adversarial Generation In\nContext), a novel framework powered by multi-modal LLM agents to address these\nchallenges. MAGIC automatically understands scene context and orchestrates\nadversarial patch generation through the synergistic interaction of language\nand vision capabilities. MAGIC orchestrates three specialized LLM agents: The\nadv-patch generation agent (GAgent) masters the creation of deceptive patches\nthrough strategic prompt engineering for text-to-image models. The adv-patch\ndeployment agent (DAgent) ensures contextual coherence by determining optimal\nplacement strategies based on scene understanding. The self-examination agent\n(EAgent) completes this trilogy by providing critical oversight and iterative\nrefinement of both processes. We validate our method on both digital and\nphysical level, \\ie, nuImage and manually captured real scenes, where both\nstatistical and visual results prove that our MAGIC is powerful and effectively\nfor attacking wide-used object detection systems.\n","authors":["Yun Xing","Nhat Chung","Jie Zhang","Yue Cao","Ivor Tsang","Yang Liu","Lei Ma","Qing Guo"],"pdf_url":"https://arxiv.org/pdf/2412.08014v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17740v2","updated":"2024-12-11T00:59:09Z","published":"2024-06-25T17:26:05Z","title":"Structured Unrestricted-Rank Matrices for Parameter Efficient\n  Fine-tuning","summary":"  Recent efforts to scale Transformer models have demonstrated rapid progress\nacross a wide range of tasks (Wei et al., 2022). However, fine-tuning these\nmodels for downstream tasks is expensive due to their large parameter counts.\nParameter-efficient fine-tuning (PEFT) approaches have emerged as a viable\nalternative by allowing us to fine-tune models by updating only a small number\nof parameters. In this work, we propose a general framework for parameter\nefficient fine-tuning (PEFT), based on structured unrestricted-rank matrices\n(SURM) which can serve as a drop-in replacement for popular approaches such as\nAdapters and LoRA. Unlike other methods like LoRA, SURMs provides more\nflexibility in finding the right balance between compactness and\nexpressiveness. This is achieved by using low displacement rank matrices\n(LDRMs), which hasn't been used in this context before. SURMs remain\ncompetitive with baselines, often providing significant quality improvements\nwhile using a smaller parameter budget. SURMs achieve 5-7% accuracy gains on\nvarious image classification tasks while replacing low-rank matrices in LoRA.\nIt also results in up to 12x reduction of the number of parameters in adapters\n(with virtually no loss in quality) on the GLUE benchmark.\n","authors":["Arijit Sehanobish","Avinava Dubey","Krzysztof Choromanski","Somnath Basu Roy Chowdhury","Deepali Jain","Vikas Sindhwani","Snigdha Chaturvedi"],"pdf_url":"https://arxiv.org/pdf/2406.17740v2.pdf","comment":"Accepted at NeurIPS 2024. Updated draft at:\n  https://openreview.net/pdf?id=MXOzgjlWDF"},{"id":"http://arxiv.org/abs/2410.01061v2","updated":"2024-12-11T00:43:16Z","published":"2024-10-01T20:43:20Z","title":"Pose Estimation of Buried Deep-Sea Objects using 3D Vision Deep Learning\n  Models","summary":"  We present an approach for pose and burial fraction estimation of debris\nfield barrels found on the seabed in the Southern California San Pedro Basin.\nOur computational workflow leverages recent advances in foundation models for\nsegmentation and a vision transformer-based approach to estimate the point\ncloud which defines the geometry of the barrel. We propose BarrelNet for\nestimating the 6-DOF pose and radius of buried barrels from the barrel point\nclouds as input. We train BarrelNet using synthetically generated barrel point\nclouds, and qualitatively demonstrate the potential of our approach using\nremotely operated vehicle (ROV) video footage of barrels found at a historic\ndump site. We compare our method to a traditional least squares fitting\napproach and show significant improvement according to our defined benchmarks.\n","authors":["Jerry Yan","Chinmay Talegaonkar","Nicholas Antipa","Eric Terrill","Sophia Merrifield"],"pdf_url":"https://arxiv.org/pdf/2410.01061v2.pdf","comment":"Submitted to OCEANS 2024 Halifax"},{"id":"http://arxiv.org/abs/2412.07996v1","updated":"2024-12-11T00:38:35Z","published":"2024-12-11T00:38:35Z","title":"Enhancing Remote Adversarial Patch Attacks on Face Detectors with Tiling\n  and Scaling","summary":"  This paper discusses the attack feasibility of Remote Adversarial Patch (RAP)\ntargeting face detectors. The RAP that targets face detectors is similar to the\nRAP that targets general object detectors, but the former has multiple issues\nin the attack process the latter does not. (1) It is possible to detect objects\nof various scales. In particular, the area of small objects that are convolved\nduring feature extraction by CNN is small,so the area that affects the\ninference results is also small. (2) It is a two-class classification, so there\nis a large gap in characteristics between the classes. This makes it difficult\nto attack the inference results by directing them to a different class. In this\npaper, we propose a new patch placement method and loss function for each\nproblem. The patches targeting the proposed face detector showed superior\ndetection obstruct effects compared to the patches targeting the general object\ndetector.\n","authors":["Masora Okano","Koichi Ito","Masakatsu Nishigaki","Tetsushi Ohki"],"pdf_url":"https://arxiv.org/pdf/2412.07996v1.pdf","comment":"Accepted and Presented at APSIPA ASC 2024"},{"id":"http://arxiv.org/abs/2302.04865v3","updated":"2024-12-11T22:55:09Z","published":"2023-02-09T18:59:41Z","title":"ELBA: Learning by Asking for Embodied Visual Navigation and Task\n  Completion","summary":"  The research community has shown increasing interest in designing intelligent\nembodied agents that can assist humans in accomplishing tasks. Although there\nhave been significant advancements in related vision-language benchmarks, most\nprior work has focused on building agents that follow instructions rather than\nendowing agents the ability to ask questions to actively resolve ambiguities\narising naturally in embodied environments. To address this gap, we propose an\nEmbodied Learning-By-Asking (ELBA) model that learns when and what questions to\nask to dynamically acquire additional information for completing the task. We\nevaluate ELBA on the TEACh vision-dialog navigation and task completion\ndataset. Experimental results show that the proposed method achieves improved\ntask performance compared to baseline models without question-answering\ncapabilities.\n","authors":["Ying Shen","Daniel Bis","Cynthia Lu","Ismini Lourentzou"],"pdf_url":"https://arxiv.org/pdf/2302.04865v3.pdf","comment":"14 pages, 10 figures, WACV 2025"},{"id":"http://arxiv.org/abs/2412.07774v2","updated":"2024-12-11T22:51:08Z","published":"2024-12-10T18:59:55Z","title":"UniReal: Universal Image Generation and Editing via Learning Real-world\n  Dynamics","summary":"  We introduce UniReal, a unified framework designed to address various image\ngeneration and editing tasks. Existing solutions often vary by tasks, yet share\nfundamental principles: preserving consistency between inputs and outputs while\ncapturing visual variations. Inspired by recent video generation models that\neffectively balance consistency and variation across frames, we propose a\nunifying approach that treats image-level tasks as discontinuous video\ngeneration. Specifically, we treat varying numbers of input and output images\nas frames, enabling seamless support for tasks such as image generation,\nediting, customization, composition, etc. Although designed for image-level\ntasks, we leverage videos as a scalable source for universal supervision.\nUniReal learns world dynamics from large-scale videos, demonstrating advanced\ncapability in handling shadows, reflections, pose variation, and object\ninteraction, while also exhibiting emergent capability for novel applications.\n","authors":["Xi Chen","Zhifei Zhang","He Zhang","Yuqian Zhou","Soo Ye Kim","Qing Liu","Yijun Li","Jianming Zhang","Nanxuan Zhao","Yilin Wang","Hui Ding","Zhe Lin","Hengshuang Zhao"],"pdf_url":"https://arxiv.org/pdf/2412.07774v2.pdf","comment":"webpage: https://xavierchen34.github.io/UniReal-Page/"},{"id":"http://arxiv.org/abs/2412.08806v1","updated":"2024-12-11T22:43:41Z","published":"2024-12-11T22:43:41Z","title":"DALI: Domain Adaptive LiDAR Object Detection via Distribution-level and\n  Instance-level Pseudo Label Denoising","summary":"  Object detection using LiDAR point clouds relies on a large amount of\nhuman-annotated samples when training the underlying detectors' deep neural\nnetworks. However, generating 3D bounding box annotation for a large-scale\ndataset could be costly and time-consuming. Alternatively, unsupervised domain\nadaptation (UDA) enables a given object detector to operate on a novel new\ndata, with unlabeled training dataset, by transferring the knowledge learned\nfrom training labeled \\textit{source domain} data to the new unlabeled\n\\textit{target domain}. Pseudo label strategies, which involve training the 3D\nobject detector using target-domain predicted bounding boxes from a pre-trained\nmodel, are commonly used in UDA. However, these pseudo labels often introduce\nnoise, impacting performance. In this paper, we introduce the Domain Adaptive\nLIdar (DALI) object detection framework to address noise at both distribution\nand instance levels. Firstly, a post-training size normalization (PTSN)\nstrategy is developed to mitigate bias in pseudo label size distribution by\nidentifying an unbiased scale after network training. To address instance-level\nnoise between pseudo labels and corresponding point clouds, two pseudo point\nclouds generation (PPCG) strategies, ray-constrained and constraint-free, are\ndeveloped to generate pseudo point clouds for each instance, ensuring the\nconsistency between pseudo labels and pseudo points during training. We\ndemonstrate the effectiveness of our method on the publicly available and\npopular datasets KITTI, Waymo, and nuScenes. We show that the proposed DALI\nframework achieves state-of-the-art results and outperforms leading approaches\non most of the domain adaptation tasks. Our code is available at\n\\href{https://github.com/xiaohulugo/T-RO2024-DALI}{https://github.com/xiaohulugo/T-RO2024-DALI}.\n","authors":["Xiaohu Lu","Hayder Radha"],"pdf_url":"https://arxiv.org/pdf/2412.08806v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11940v4","updated":"2024-12-11T22:32:49Z","published":"2024-02-19T08:27:23Z","title":"AICAttack: Adversarial Image Captioning Attack with Attention-Based\n  Optimization","summary":"  Recent advances in deep learning research have shown remarkable achievements\nacross many tasks in computer vision (CV) and natural language processing\n(NLP). At the intersection of CV and NLP is the problem of image captioning,\nwhere the related models' robustness against adversarial attacks has not been\nwell studied. This paper presents a novel adversarial attack strategy,\nAICAttack (Attention-based Image Captioning Attack), designed to attack image\ncaptioning models through subtle perturbations on images. Operating within a\nblack-box attack scenario, our algorithm requires no access to the target\nmodel's architecture, parameters, or gradient information. We introduce an\nattention-based candidate selection mechanism that identifies the optimal\npixels to attack, followed by a customised differential evolution method to\noptimise the perturbations of pixels' RGB values. We demonstrate AICAttack's\neffectiveness through extensive experiments on benchmark datasets against\nmultiple victim models. The experimental results demonstrate that our method\noutperforms current leading-edge techniques by achieving consistently higher\nattack success rates.\n","authors":["Jiyao Li","Mingze Ni","Yifei Dong","Tianqing Zhu","Wei Liu"],"pdf_url":"https://arxiv.org/pdf/2402.11940v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08802v1","updated":"2024-12-11T22:28:12Z","published":"2024-12-11T22:28:12Z","title":"jina-clip-v2: Multilingual Multimodal Embeddings for Text and Images","summary":"  Contrastive Language-Image Pretraining (CLIP) is a highly effective method\nfor aligning images and texts in a shared embedding space. These models are\nwidely used for tasks such as cross-modal information retrieval and multi-modal\nunderstanding. However, CLIP models often struggle with text-only tasks,\nunderperforming compared to specialized text models. This performance disparity\nforces retrieval systems to rely on separate models for text-only and\nmulti-modal tasks. In this work, we build upon our previous model,\njina-clip-v1, by introducing a refined framework that utilizes multi-task,\nmulti-stage contrastive learning across multiple languages, coupled with an\nimproved training recipe to enhance text-only retrieval. The resulting model,\njina-clip-v2, outperforms its predecessor on text-only and multimodal tasks,\nwhile adding multilingual support, better understanding of complex visual\ndocuments and efficiency gains thanks to Matryoshka Representation Learning and\nvector truncation. The model performs comparably to the state-of-the-art in\nboth multilingual-multimodal and multilingual text retrieval benchmarks,\naddressing the challenge of unifying text-only and multi-modal retrieval\nsystems.\n","authors":["Andreas Koukounas","Georgios Mastrapas","Bo Wang","Mohammad Kalim Akram","Sedigheh Eslami","Michael Günther","Isabelle Mohr","Saba Sturua","Scott Martens","Nan Wang","Han Xiao"],"pdf_url":"https://arxiv.org/pdf/2412.08802v1.pdf","comment":"21 pages, 1-10 main paper, 10-12 refs, 12-21 benchmarks"},{"id":"http://arxiv.org/abs/2412.08800v1","updated":"2024-12-11T22:12:21Z","published":"2024-12-11T22:12:21Z","title":"A Hybrid Framework for Statistical Feature Selection and Image-Based\n  Noise-Defect Detection","summary":"  In industrial imaging, accurately detecting and distinguishing surface\ndefects from noise is critical and challenging, particularly in complex\nenvironments with noisy data. This paper presents a hybrid framework that\nintegrates both statistical feature selection and classification techniques to\nimprove defect detection accuracy while minimizing false positives. The\nmotivation of the system is based on the generation of scalar scores that\nrepresent the likelihood that a region of interest (ROI) is classified as a\ndefect or noise. We present around 55 distinguished features that are extracted\nfrom industrial images, which are then analyzed using statistical methods such\nas Fisher separation, chi-squared test, and variance analysis. These techniques\nidentify the most discriminative features, focusing on maximizing the\nseparation between true defects and noise. Fisher's criterion ensures robust,\nreal-time performance for automated systems. This statistical framework opens\nup multiple avenues for application, functioning as a standalone assessment\nmodule or as an a posteriori enhancement to machine learning classifiers. The\nframework can be implemented as a black-box module that applies to existing\nclassifiers, providing an adaptable layer of quality control and optimizing\npredictions by leveraging intuitive feature extraction strategies, emphasizing\nthe rationale behind feature significance and the statistical rigor of feature\nselection. By integrating these methods with flexible machine learning\napplications, the proposed framework improves detection accuracy and reduces\nfalse positives and misclassifications, especially in complex, noisy\nenvironments.\n","authors":["Alejandro Garnung Menéndez"],"pdf_url":"https://arxiv.org/pdf/2412.08800v1.pdf","comment":"23 pages, 17 figures"},{"id":"http://arxiv.org/abs/2408.08313v3","updated":"2024-12-11T21:42:14Z","published":"2024-08-15T17:59:57Z","title":"Can Large Language Models Understand Symbolic Graphics Programs?","summary":"  Against the backdrop of enthusiasm for large language models (LLMs), there is\nan urgent need to scientifically assess their capabilities and shortcomings.\nThis is nontrivial in part because it is difficult to find tasks which the\nmodels have not encountered during training. Utilizing symbolic graphics\nprograms, we propose a domain well-suited to test multiple spatial-semantic\nreasoning skills of LLMs. Popular in computer graphics, these programs\nprocedurally generate visual data. While LLMs exhibit impressive skills in\ngeneral program synthesis and analysis, symbolic graphics programs offer a new\nlayer of evaluation: they allow us to test an LLM's ability to answer\ndifferent-grained semantic-level questions of the images or 3D geometries\nwithout a vision encoder. To semantically understand the symbolic programs,\nLLMs would need to possess the ability to \"imagine\" and reason how the\ncorresponding graphics content would look with only the symbolic description.\nWe use this task to evaluate LLMs by creating a large benchmark for the\nsemantic visual understanding of symbolic graphics programs, built procedurally\nwith minimal human effort. Particular emphasis is placed on transformations of\nimages that leave the image level semantics invariant while introducing\nsignificant changes to the underlying program. We evaluate commercial and\nopen-source LLMs on our benchmark to assess their ability to reason about\nvisual output of programs, finding that LLMs considered stronger at reasoning\ngenerally perform better. Lastly, we introduce a novel method to improve this\nability -- Symbolic Instruction Tuning (SIT), in which the LLM is finetuned\nwith pre-collected instruction data on symbolic graphics programs.\nInterestingly, we find that SIT not only improves LLM's understanding on\nsymbolic programs, but it also improves general reasoning ability on various\nother benchmarks.\n","authors":["Zeju Qiu","Weiyang Liu","Haiwen Feng","Zhen Liu","Tim Z. Xiao","Katherine M. Collins","Joshua B. Tenenbaum","Adrian Weller","Michael J. Black","Bernhard Schölkopf"],"pdf_url":"https://arxiv.org/pdf/2408.08313v3.pdf","comment":"Technical Report v3 (47 pages, 26 figures, project page:\n  https://sgp-bench.github.io/, added visual illusion examples)"},{"id":"http://arxiv.org/abs/2412.08781v1","updated":"2024-12-11T21:23:24Z","published":"2024-12-11T21:23:24Z","title":"Generative Modeling with Explicit Memory","summary":"  Recent studies indicate that the denoising process in deep generative\ndiffusion models implicitly learns and memorizes semantic information from the\ndata distribution. These findings suggest that capturing more complex data\ndistributions requires larger neural networks, leading to a substantial\nincrease in computational demands, which in turn become the primary bottleneck\nin both training and inference of diffusion models. To this end, we introduce\n\\textbf{G}enerative \\textbf{M}odeling with \\textbf{E}xplicit \\textbf{M}emory\n(GMem), leveraging an external memory bank in both training and sampling phases\nof diffusion models. This approach preserves semantic information from data\ndistributions, reducing reliance on neural network capacity for learning and\ngeneralizing across diverse datasets. The results are significant: our GMem\nenhances both training, sampling efficiency, and generation quality. For\ninstance, on ImageNet at $256 \\times 256$ resolution, GMem accelerates SiT\ntraining by over $46.7\\times$, achieving the performance of a SiT model trained\nfor $7M$ steps in fewer than $150K$ steps. Compared to the most efficient\nexisting method, REPA, GMem still offers a $16\\times$ speedup, attaining an FID\nscore of 5.75 within $250K$ steps, whereas REPA requires over $4M$ steps.\nAdditionally, our method achieves state-of-the-art generation quality, with an\nFID score of {3.56} without classifier-free guidance on ImageNet\n$256\\times256$. Our code is available at\n\\url{https://github.com/LINs-lab/GMem}.\n","authors":["Yi Tang","Peng Sun","Zhenglin Cheng","Tao Lin"],"pdf_url":"https://arxiv.org/pdf/2412.08781v1.pdf","comment":"10 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2412.04189v3","updated":"2024-12-11T21:13:49Z","published":"2024-12-05T14:29:10Z","title":"Instructional Video Generation","summary":"  Despite the recent strides in video generation, state-of-the-art methods\nstill struggle with elements of visual detail. One particularly challenging\ncase is the class of egocentric instructional videos in which the intricate\nmotion of the hand coupled with a mostly stable and non-distracting environment\nis necessary to convey the appropriate visual action instruction. To address\nthese challenges, we introduce a new method for instructional video generation.\nOur diffusion-based method incorporates two distinct innovations. First, we\npropose an automatic method to generate the expected region of motion, guided\nby both the visual context and the action text. Second, we introduce a critical\nhand structure loss to guide the diffusion model to focus on smooth and\nconsistent hand poses. We evaluate our method on augmented instructional\ndatasets based on EpicKitchens and Ego4D, demonstrating significant\nimprovements over state-of-the-art methods in terms of instructional clarity,\nespecially of the hand motion in the target region, across diverse environments\nand actions. Video results can be found in\nhttps://excitedbutter.github.io/Instructional-Video-Generation/\n","authors":["Yayuan Li","Zhi Cao","Jason J. Corso"],"pdf_url":"https://arxiv.org/pdf/2412.04189v3.pdf","comment":"14 pages, 5 figures and 4 tables"},{"id":"http://arxiv.org/abs/2412.03928v2","updated":"2024-12-11T21:06:05Z","published":"2024-12-05T07:07:35Z","title":"MT3DNet: Multi-Task learning Network for 3D Surgical Scene\n  Reconstruction","summary":"  In image-assisted minimally invasive surgeries (MIS), understanding surgical\nscenes is vital for real-time feedback to surgeons, skill evaluation, and\nimproving outcomes through collaborative human-robot procedures. Within this\ncontext, the challenge lies in accurately detecting, segmenting, and estimating\nthe depth of surgical scenes depicted in high-resolution images, while\nsimultaneously reconstructing the scene in 3D and providing segmentation of\nsurgical instruments along with detection labels for each instrument. To\naddress this challenge, a novel Multi-Task Learning (MTL) network is proposed\nfor performing these tasks concurrently. A key aspect of this approach involves\novercoming the optimization hurdles associated with handling multiple tasks\nconcurrently by integrating a Adversarial Weight Update into the MTL framework,\nthe proposed MTL model achieves 3D reconstruction through the integration of\nsegmentation, depth estimation, and object detection, thereby enhancing the\nunderstanding of surgical scenes, which marks a significant advancement\ncompared to existing studies that lack 3D capabilities. Comprehensive\nexperiments on the EndoVis2018 benchmark dataset underscore the adeptness of\nthe model in efficiently addressing all three tasks, demonstrating the efficacy\nof the proposed techniques.\n","authors":["Mithun Parab","Pranay Lendave","Jiyoung Kim","Thi Quynh Dan Nguyen","Palash Ingle"],"pdf_url":"https://arxiv.org/pdf/2412.03928v2.pdf","comment":"1. Notation Update: Added * for equal contribution, ensuring proper\n  attribution. 2. Subsection Fix: Removed the `subsection` tag for Section 3.1\n  (no 3.2 existed), maintaining content but fixing hierarchy. 3. Text\n  Additions: Added lines in Section 5 and Subsection 4.2 for clarity, with\n  references for better context"},{"id":"http://arxiv.org/abs/2407.13517v4","updated":"2024-12-11T21:01:39Z","published":"2024-07-18T13:48:52Z","title":"Mask2Map: Vectorized HD Map Construction Using Bird's Eye View\n  Segmentation Masks","summary":"  In this paper, we introduce Mask2Map, a novel end-to-end online HD map\nconstruction method designed for autonomous driving applications. Our approach\nfocuses on predicting the class and ordered point set of map instances within a\nscene, represented in the bird's eye view (BEV). Mask2Map consists of two\nprimary components: the Instance-Level Mask Prediction Network (IMPNet) and the\nMask-Driven Map Prediction Network (MMPNet). IMPNet generates Mask-Aware\nQueries and BEV Segmentation Masks to capture comprehensive semantic\ninformation globally. Subsequently, MMPNet enhances these query features using\nlocal contextual information through two submodules: the Positional Query\nGenerator (PQG) and the Geometric Feature Extractor (GFE). PQG extracts\ninstance-level positional queries by embedding BEV positional information into\nMask-Aware Queries, while GFE utilizes BEV Segmentation Masks to generate\npoint-level geometric features. However, we observed limited performance in\nMask2Map due to inter-network inconsistency stemming from different predictions\nto Ground Truth (GT) matching between IMPNet and MMPNet. To tackle this\nchallenge, we propose the Inter-network Denoising Training method, which guides\nthe model to denoise the output affected by both noisy GT queries and perturbed\nGT Segmentation Masks. Our evaluation conducted on nuScenes and Argoverse2\nbenchmarks demonstrates that Mask2Map achieves remarkable performance\nimprovements over previous state-of-the-art methods, with gains of 10.1% mAP\nand 4.1 mAP, respectively. Our code can be found at\nhttps://github.com/SehwanChoi0307/Mask2Map.\n","authors":["Sehwan Choi","Jungho Kim","Hongjae Shin","Jun Won Choi"],"pdf_url":"https://arxiv.org/pdf/2407.13517v4.pdf","comment":"Accepted to European Conference on Computer Vision (ECCV) 2024, 20\n  pages, 9 figures"},{"id":"http://arxiv.org/abs/2412.08774v1","updated":"2024-12-11T20:55:21Z","published":"2024-12-11T20:55:21Z","title":"ProtoOcc: Accurate, Efficient 3D Occupancy Prediction Using Dual Branch\n  Encoder-Prototype Query Decoder","summary":"  In this paper, we introduce ProtoOcc, a novel 3D occupancy prediction model\ndesigned to predict the occupancy states and semantic classes of 3D voxels\nthrough a deep semantic understanding of scenes. ProtoOcc consists of two main\ncomponents: the Dual Branch Encoder (DBE) and the Prototype Query Decoder\n(PQD). The DBE produces a new 3D voxel representation by combining 3D voxel and\nBEV representations across multiple scales through a dual branch structure.\nThis design enhances both performance and computational efficiency by providing\na large receptive field for the BEV representation while maintaining a smaller\nreceptive field for the voxel representation. The PQD introduces Prototype\nQueries to accelerate the decoding process. Scene-Adaptive Prototypes are\nderived from the 3D voxel features of input sample, while Scene-Agnostic\nPrototypes are computed by applying Scene-Adaptive Prototypes to an Exponential\nMoving Average during the training phase. By using these prototype-based\nqueries for decoding, we can directly predict 3D occupancy in a single step,\neliminating the need for iterative Transformer decoding. Additionally, we\npropose the Robust Prototype Learning, which injects noise into prototype\ngeneration process and trains the model to denoise during the training phase.\nProtoOcc achieves state-of-the-art performance with 45.02% mIoU on the\nOcc3D-nuScenes benchmark. For single-frame method, it reaches 39.56% mIoU with\nan inference speed of 12.83 FPS on an NVIDIA RTX 3090. Our code can be found at\nhttps://github.com/SPA-junghokim/ProtoOcc.\n","authors":["Jungho Kim","Changwon Kang","Dongyoung Lee","Sehwan Choi","Jun Won Choi"],"pdf_url":"https://arxiv.org/pdf/2412.08774v1.pdf","comment":"Accepted to AAAI Conference on Artificial Intelligence 2025, 9 pages,\n  5 figures"},{"id":"http://arxiv.org/abs/2412.08771v1","updated":"2024-12-11T20:46:06Z","published":"2024-12-11T20:46:06Z","title":"LLaVA-Zip: Adaptive Visual Token Compression with Intrinsic Image\n  Information","summary":"  Multi-modal large language models (MLLMs) utilizing instruction-following\ndata, such as LLaVA, have achieved great progress in the industry. A major\nlimitation in these models is that visual tokens consume a substantial portion\nof the maximum token limit in large language models (LLMs), leading to\nincreased computational demands and decreased performance when prompts include\nmultiple images or videos. Industry solutions often mitigate this issue by\nincreasing computational power, but this approach is less feasible in academic\nenvironments with limited resources. In this study, we propose Dynamic Feature\nMap Reduction (DFMR) based on LLaVA-1.5 to address the challenge of visual\ntoken overload. DFMR dynamically compresses the visual tokens, freeing up token\ncapacity. Our experimental results demonstrate that integrating DFMR into\nLLaVA-1.5 significantly improves the performance of LLaVA in varied visual\ntoken lengths, offering a promising solution for extending LLaVA to handle\nmulti-image and video scenarios in resource-constrained academic environments\nand it can also be applied in industry settings for data augmentation to help\nmitigate the scarcity of open-domain image-text pair datasets in the continued\npretraining stage.\n","authors":["Ke Wang","Hong Xuan"],"pdf_url":"https://arxiv.org/pdf/2412.08771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08763v1","updated":"2024-12-11T20:28:42Z","published":"2024-12-11T20:28:42Z","title":"Beyond Knowledge Silos: Task Fingerprinting for Democratization of\n  Medical Imaging AI","summary":"  The field of medical imaging AI is currently undergoing rapid\ntransformations, with methodical research increasingly translated into clinical\npractice. Despite these successes, research suffers from knowledge silos,\nhindering collaboration and progress: Existing knowledge is scattered across\npublications and many details remain unpublished, while privacy regulations\nrestrict data sharing. In the spirit of democratizing of AI, we propose a\nframework for secure knowledge transfer in the field of medical image analysis.\nThe key to our approach is dataset \"fingerprints\", structured representations\nof feature distributions, that enable quantification of task similarity. We\ntested our approach across 71 distinct tasks and 12 medical imaging modalities\nby transferring neural architectures, pretraining, augmentation policies, and\nmulti-task learning. According to comprehensive analyses, our method\noutperforms traditional methods for identifying relevant knowledge and\nfacilitates collaborative model training. Our framework fosters the\ndemocratization of AI in medical imaging and could become a valuable tool for\npromoting faster scientific advancement.\n","authors":["Patrick Godau","Akriti Srivastava","Tim Adler","Lena Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2412.08763v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22366v2","updated":"2024-12-11T20:01:40Z","published":"2024-10-28T19:01:18Z","title":"Unpacking SDXL Turbo: Interpreting Text-to-Image Models with Sparse\n  Autoencoders","summary":"  Sparse autoencoders (SAEs) have become a core ingredient in the reverse\nengineering of large-language models (LLMs). For LLMs, they have been shown to\ndecompose intermediate representations that often are not interpretable\ndirectly into sparse sums of interpretable features, facilitating better\ncontrol and subsequent analysis. However, similar analyses and approaches have\nbeen lacking for text-to-image models. We investigated the possibility of using\nSAEs to learn interpretable features for a few-step text-to-image diffusion\nmodels, such as SDXL Turbo. To this end, we train SAEs on the updates performed\nby transformer blocks within SDXL Turbo's denoising U-net. We find that their\nlearned features are interpretable, causally influence the generation process,\nand reveal specialization among the blocks. In particular, we find one block\nthat deals mainly with image composition, one that is mainly responsible for\nadding local details, and one for color, illumination, and style. Therefore,\nour work is an important first step towards better understanding the internals\nof generative text-to-image models like SDXL Turbo and showcases the potential\nof features learned by SAEs for the visual domain.\n  Code is available at https://github.com/surkovv/sdxl-unbox\n","authors":["Viacheslav Surkov","Chris Wendler","Mikhail Terekhov","Justin Deschenaux","Robert West","Caglar Gulcehre"],"pdf_url":"https://arxiv.org/pdf/2410.22366v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09918v5","updated":"2024-12-11T19:55:33Z","published":"2024-03-14T23:31:41Z","title":"Attention-based Class-Conditioned Alignment for Multi-Source Domain\n  Adaptation of Object Detectors","summary":"  Domain adaptation methods for object detection (OD) strive to mitigate the\nimpact of distribution shifts by promoting feature alignment across source and\ntarget domains. Multi-source domain adaptation (MSDA) allows leveraging\nmultiple annotated source datasets and unlabeled target data to improve the\naccuracy and robustness of the detection model. Most state-of-the-art MSDA\nmethods for OD perform feature alignment in a class-agnostic manner. This is\nchallenging since the objects have unique modality information due to\nvariations in object appearance across domains. A recent prototype-based\napproach proposed a class-wise alignment, yet it suffers from error\naccumulation caused by noisy pseudo-labels that can negatively affect\nadaptation with imbalanced data. To overcome these limitations, we propose an\nattention-based class-conditioned alignment method for MSDA, designed to align\ninstances of each object category across domains. In particular, an attention\nmodule combined with an adversarial domain classifier allows learning\ndomain-invariant and class-specific instance representations. Experimental\nresults on multiple benchmarking MSDA datasets indicate that our method\noutperforms state-of-the-art methods and exhibits robustness to class\nimbalance, achieved through a conceptually simple class-conditioning strategy.\nOur code is available at: https://github.com/imatif17/ACIA.\n","authors":["Atif Belal","Akhil Meethal","Francisco Perdigon Romero","Marco Pedersoli","Eric Granger"],"pdf_url":"https://arxiv.org/pdf/2403.09918v5.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2309.14950"},{"id":"http://arxiv.org/abs/2412.08755v1","updated":"2024-12-11T19:54:14Z","published":"2024-12-11T19:54:14Z","title":"Proactive Adversarial Defense: Harnessing Prompt Tuning in\n  Vision-Language Models to Detect Unseen Backdoored Images","summary":"  Backdoor attacks pose a critical threat by embedding hidden triggers into\ninputs, causing models to misclassify them into target labels. While extensive\nresearch has focused on mitigating these attacks in object recognition models\nthrough weight fine-tuning, much less attention has been given to detecting\nbackdoored samples directly. Given the vast datasets used in training, manual\ninspection for backdoor triggers is impractical, and even state-of-the-art\ndefense mechanisms fail to fully neutralize their impact. To address this gap,\nwe introduce a groundbreaking method to detect unseen backdoored images during\nboth training and inference. Leveraging the transformative success of prompt\ntuning in Vision Language Models (VLMs), our approach trains learnable text\nprompts to differentiate clean images from those with hidden backdoor triggers.\nExperiments demonstrate the exceptional efficacy of this method, achieving an\nimpressive average accuracy of 86% across two renowned datasets for detecting\nunseen backdoor triggers, establishing a new standard in backdoor defense.\n","authors":["Kyle Stein","Andrew Arash Mahyari","Guillermo Francia","Eman El-Sheikh"],"pdf_url":"https://arxiv.org/pdf/2412.08755v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08746v1","updated":"2024-12-11T19:35:06Z","published":"2024-12-11T19:35:06Z","title":"DocVLM: Make Your VLM an Efficient Reader","summary":"  Vision-Language Models (VLMs) excel in diverse visual tasks but face\nchallenges in document understanding, which requires fine-grained text\nprocessing. While typical visual tasks perform well with low-resolution inputs,\nreading-intensive applications demand high-resolution, resulting in significant\ncomputational overhead. Using OCR-extracted text in VLM prompts partially\naddresses this issue but underperforms compared to full-resolution counterpart,\nas it lacks the complete visual context needed for optimal performance. We\nintroduce DocVLM, a method that integrates an OCR-based modality into VLMs to\nenhance document processing while preserving original weights. Our approach\nemploys an OCR encoder to capture textual content and layout, compressing these\ninto a compact set of learned queries incorporated into the VLM. Comprehensive\nevaluations across leading VLMs show that DocVLM significantly reduces reliance\non high-resolution images for document understanding. In limited-token regimes\n(448$\\times$448), DocVLM with 64 learned queries improves DocVQA results from\n56.0% to 86.6% when integrated with InternVL2 and from 84.4% to 91.2% with\nQwen2-VL. In LLaVA-OneVision, DocVLM achieves improved results while using 80%\nless image tokens. The reduced token usage allows processing multiple pages\neffectively, showing impressive zero-shot results on DUDE and state-of-the-art\nperformance on MP-DocVQA, highlighting DocVLM's potential for applications\nrequiring high-performance and efficiency.\n","authors":["Mor Shpigel Nacson","Aviad Aberdam","Roy Ganz","Elad Ben Avraham","Alona Golts","Yair Kittenplon","Shai Mazor","Ron Litman"],"pdf_url":"https://arxiv.org/pdf/2412.08746v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.02886v2","updated":"2024-12-11T19:28:10Z","published":"2024-12-03T22:46:09Z","title":"Patchfinder: Leveraging Visual Language Models for Accurate Information\n  Retrieval using Model Uncertainty","summary":"  For decades, corporations and governments have relied on scanned documents to\nrecord vast amounts of information. However, extracting this information is a\nslow and tedious process due to the overwhelming amount of documents. The rise\nof vision language models presents a way to efficiently and accurately extract\nthe information out of these documents. The current automated workflow often\nrequires a two-step approach involving the extraction of information using\noptical character recognition software, and subsequent usage of large language\nmodels for processing this information. Unfortunately, these methods encounter\nsignificant challenges when dealing with noisy scanned documents. The high\ninformation density of such documents often necessitates using computationally\nexpensive language models to effectively reduce noise.\n  In this study, we propose PatchFinder, an algorithm that builds upon Vision\nLanguage Models (VLMs) to address the information extraction task. First, we\ndevise a confidence-based score, called Patch Confidence, based on the Maximum\nSoftmax Probability of the VLMs' output to measure the model's confidence in\nits predictions. Then, PatchFinder utilizes that score to determine a suitable\npatch size, partition the input document into overlapping patches of that size,\nand generate confidence-based predictions for the target information. Our\nexperimental results show that PatchFinder can leverage Phi-3v, a 4.2 billion\nparameter vision language model, to achieve an accuracy of 94% on our dataset\nof 190 noisy scanned documents, surpassing the performance of ChatGPT-4o by\n18.5 percentage points.\n","authors":["Roman Colman","Minh Vu","Manish Bhattarai","Martin Ma","Hari Viswanathan","Daniel O'Malley","Javier E. Santos"],"pdf_url":"https://arxiv.org/pdf/2412.02886v2.pdf","comment":"This paper has been accepted to IEEE/CVF Winter Conference on\n  Applications of Computer Vision (WACV) 2025"},{"id":"http://arxiv.org/abs/2412.08737v1","updated":"2024-12-11T19:12:13Z","published":"2024-12-11T19:12:13Z","title":"Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity\n  Visual Descriptions","summary":"  Multimodal large language models (MLLMs) have made rapid progress in recent\nyears, yet continue to struggle with low-level visual perception (LLVP) --\nparticularly the ability to accurately describe the geometric details of an\nimage. This capability is crucial for applications in areas such as robotics,\nmedical image analysis, and manufacturing. In this paper, we first introduce\nGeoperception, a benchmark designed to evaluate an MLLM's ability to accurately\ntranscribe 2D geometric information from an image. Using this benchmark, we\ndemonstrate the limitations of leading MLLMs, and then conduct a comprehensive\nempirical study to explore strategies for improving their performance on\ngeometric tasks. Our findings highlight the benefits of certain model\narchitectures, training techniques, and data strategies, including the use of\nhigh-fidelity synthetic data and multi-stage training with a data curriculum.\nNotably, we find that a data curriculum enables models to learn challenging\ngeometry understanding tasks which they fail to learn from scratch. Leveraging\nthese insights, we develop Euclid, a family of models specifically optimized\nfor strong low-level geometric perception. Although purely trained on synthetic\nmultimodal data, Euclid shows strong generalization ability to novel geometry\nshapes. For instance, Euclid outperforms the best closed-source model,\nGemini-1.5-Pro, by up to 58.56% on certain Geoperception benchmark tasks and\n10.65% on average across all tasks.\n","authors":["Jiarui Zhang","Ollie Liu","Tianyu Yu","Jinyi Hu","Willie Neiswanger"],"pdf_url":"https://arxiv.org/pdf/2412.08737v1.pdf","comment":"33 pages, 22 figures, 5 tables, 7 algorithms"},{"id":"http://arxiv.org/abs/2412.07041v2","updated":"2024-12-11T19:05:09Z","published":"2024-12-09T23:01:04Z","title":"Generalized Least Squares Kernelized Tensor Factorization","summary":"  Real-world datasets often contain missing or corrupted values. Completing\nmultidimensional tensor-structured data with missing entries is essential for\nnumerous applications. Smoothness-constrained low-rank factorization models\nhave shown superior performance with reduced computational costs. While\neffective at capturing global and long-range correlations, these models\nstruggle to reproduce short-scale, high-frequency variations in the data. In\nthis paper, we introduce the Generalized Least Squares Kernelized Tensor\nFactorization (GLSKF) framework for tensor completion. GLSKF integrates\nsmoothness-constrained low-rank factorization with a locally correlated\nresidual process; the resulting additive structure can effectively characterize\nboth global dependencies and local variations. In particular, we define the\ncovariance norm to enforce the smoothness of factor matrices in the global\nlow-rank factorization, and use structured covariance/kernel functions to model\nthe local processes. For model estimation, we develop an alternating least\nsquares (ALS) procedure with closed-form solutions for each subproblem. To\nefficiently handle missing data, GLSKF utilizes projection matrices that\npreserve the Kronecker structure of covariances, facilitating fast computations\nthrough conjugate gradient (CG) and preconditioned conjugate gradient (PCG)\nalgorithms. The proposed framework is evaluated on four real-world datasets\nacross diverse tasks: traffic speed imputation, color image inpainting, video\ncompletion, and MRI image reconstruction. Experimental results confirm that\nGLSKF delivers superior effectiveness and scalability, establishing it as a\nrobust solution for multidimensional tensor completion.\n","authors":["Mengying Lei","Lijun Sun"],"pdf_url":"https://arxiv.org/pdf/2412.07041v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15491v2","updated":"2024-12-11T19:03:37Z","published":"2024-05-24T12:16:28Z","title":"GSDeformer: Direct, Real-time and Extensible Cage-based Deformation for\n  3D Gaussian Splatting","summary":"  We present GSDeformer, a method that achieves cage-based deformation on 3D\nGaussian Splatting (3DGS). Our method bridges cage-based deformation and 3DGS\nusing a proxy point cloud representation. The point cloud is created from 3DGS,\nand deformations on the point cloud translate to transformations on the 3D\nGaussians that comprise 3DGS. To handle potential bending from deformation, we\nemploy a splitting process to approximate it. Our method does not extend or\nmodify the core architecture of 3DGS; thus, it can work with any existing\ntrained vanilla 3DGS as well as its variants. We also automated cage\nconstruction from 3DGS for convenience. Experiments show that GSDeformer\nproduces superior deformation results than current methods, is robust under\nextreme deformations, does not require retraining for editing, runs in\nreal-time(60FPS), and can extend to other 3DGS variants.\n","authors":["Jiajun Huang","Shuolin Xu","Hongchuan Yu","Jian Jun Zhang","Hammadi Nait Charif"],"pdf_url":"https://arxiv.org/pdf/2405.15491v2.pdf","comment":"Project Page: https://jhuangbu.github.io/gsdeformer, Video:\n  https://www.youtube.com/watch?v=-ecrj48-MqM"},{"id":"http://arxiv.org/abs/2412.08687v1","updated":"2024-12-11T18:59:46Z","published":"2024-12-11T18:59:46Z","title":"VisionArena: 230K Real World User-VLM Conversations with Preference\n  Labels","summary":"  With the growing adoption and capabilities of vision-language models (VLMs)\ncomes the need for benchmarks that capture authentic user-VLM interactions. In\nresponse, we create VisionArena, a dataset of 230K real-world conversations\nbetween users and VLMs. Collected from Chatbot Arena - an open-source platform\nwhere users interact with VLMs and submit preference votes - VisionArena spans\n73K unique users, 45 VLMs, and 138 languages. Our dataset contains three\nsubsets: VisionArena-Chat, 200k single and multi-turn conversations between a\nuser and a VLM; VisionArena-Battle, 30K conversations comparing two anonymous\nVLMs with user preference votes; and VisionArena-Bench, an automatic benchmark\nof 500 diverse user prompts that efficiently approximate the live Chatbot Arena\nmodel rankings. Additionally, we highlight the types of question asked by\nusers, the influence of response style on preference, and areas where models\noften fail. We find open-ended tasks like captioning and humor are highly\nstyle-dependent, and current VLMs struggle with spatial reasoning and planning\ntasks. Lastly, we show finetuning the same base model on VisionArena-Chat\noutperforms Llava-Instruct-158K, with a 17-point gain on MMMU and a 46-point\ngain on the WildVision benchmark. Dataset at https://huggingface.co/lmarena-ai\n","authors":["Christopher Chou","Lisa Dunlap","Koki Mashita","Krishna Mandal","Trevor Darrell","Ion Stoica","Joseph E. Gonzalez","Wei-Lin Chiang"],"pdf_url":"https://arxiv.org/pdf/2412.08687v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08685v1","updated":"2024-12-11T18:58:48Z","published":"2024-12-11T18:58:48Z","title":"ChatDyn: Language-Driven Multi-Actor Dynamics Generation in Street\n  Scenes","summary":"  Generating realistic and interactive dynamics of traffic participants\naccording to specific instruction is critical for street scene simulation.\nHowever, there is currently a lack of a comprehensive method that generates\nrealistic dynamics of different types of participants including vehicles and\npedestrians, with different kinds of interactions between them. In this paper,\nwe introduce ChatDyn, the first system capable of generating interactive,\ncontrollable and realistic participant dynamics in street scenes based on\nlanguage instructions. To achieve precise control through complex language,\nChatDyn employs a multi-LLM-agent role-playing approach, which utilizes natural\nlanguage inputs to plan the trajectories and behaviors for different traffic\nparticipants. To generate realistic fine-grained dynamics based on the\nplanning, ChatDyn designs two novel executors: the PedExecutor, a unified\nmulti-task executor that generates realistic pedestrian dynamics under\ndifferent task plannings; and the VehExecutor, a physical transition-based\npolicy that generates physically plausible vehicle dynamics. Extensive\nexperiments show that ChatDyn can generate realistic driving scene dynamics\nwith multiple vehicles and pedestrians, and significantly outperforms previous\nmethods on subtasks. Code and model will be available at\nhttps://vfishc.github.io/chatdyn.\n","authors":["Yuxi Wei","Jingbo Wang","Yuwen Du","Dingju Wang","Liang Pan","Chenxin Xu","Yao Feng","Bo Dai","Siheng Chen"],"pdf_url":"https://arxiv.org/pdf/2412.08685v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08684v1","updated":"2024-12-11T18:57:24Z","published":"2024-12-11T18:57:24Z","title":"Coherent3D: Coherent 3D Portrait Video Reconstruction via Triplane\n  Fusion","summary":"  Recent breakthroughs in single-image 3D portrait reconstruction have enabled\ntelepresence systems to stream 3D portrait videos from a single camera in\nreal-time, democratizing telepresence. However, per-frame 3D reconstruction\nexhibits temporal inconsistency and forgets the user's appearance. On the other\nhand, self-reenactment methods can render coherent 3D portraits by driving a 3D\navatar built from a single reference image, but fail to faithfully preserve the\nuser's per-frame appearance (e.g., instantaneous facial expression and\nlighting). As a result, none of these two frameworks is an ideal solution for\ndemocratized 3D telepresence. In this work, we address this dilemma and propose\na novel solution that maintains both coherent identity and dynamic per-frame\nappearance to enable the best possible realism. To this end, we propose a new\nfusion-based method that takes the best of both worlds by fusing a canonical 3D\nprior from a reference view with dynamic appearance from per-frame input views,\nproducing temporally stable 3D videos with faithful reconstruction of the\nuser's per-frame appearance. Trained only using synthetic data produced by an\nexpression-conditioned 3D GAN, our encoder-based method achieves both\nstate-of-the-art 3D reconstruction and temporal consistency on in-studio and\nin-the-wild datasets. https://research.nvidia.com/labs/amri/projects/coherent3d\n","authors":["Shengze Wang","Xueting Li","Chao Liu","Matthew Chan","Michael Stengel","Henry Fuchs","Shalini De Mello","Koki Nagano"],"pdf_url":"https://arxiv.org/pdf/2412.08684v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2405.00794"},{"id":"http://arxiv.org/abs/2412.08683v1","updated":"2024-12-11T18:52:39Z","published":"2024-12-11T18:52:39Z","title":"Emotional Vietnamese Speech-Based Depression Diagnosis Using Dynamic\n  Attention Mechanism","summary":"  Major depressive disorder is a prevalent and serious mental health condition\nthat negatively impacts your emotions, thoughts, actions, and overall\nperception of the world. It is complicated to determine whether a person is\ndepressed due to the symptoms of depression not apparent. However, their voice\ncan be one of the factor from which we can acknowledge signs of depression.\nPeople who are depressed express discomfort, sadness and they may speak slowly,\ntrembly, and lose emotion in their voices. In this study, we proposed the\nDynamic Convolutional Block Attention Module (Dynamic-CBAM) to utilized with in\nan Attention-GRU Network to classify the emotions by analyzing the audio signal\nof humans. Based on the results, we can diagnose which patients are depressed\nor prone to depression then so that treatment and prevention can be started as\nsoon as possible. The research delves into the intricate computational steps\ninvolved in implementing a Attention-GRU deep learning architecture. Through\nexperimentation, the model has achieved an impressive recognition with\nUnweighted Accuracy (UA) rate of 0.87 and 0.86 Weighted Accuracy (WA) rate and\nF1 rate of 0.87 in the VNEMOS dataset. Training code is released in\nhttps://github.com/fiyud/Emotional-Vietnamese-Speech-Based-Depression-Diagnosis-Using-Dynamic-Attention-Mechanism\n","authors":["Quang-Anh N. D.","Manh-Hung Ha","Thai Kim Dinh","Minh-Duc Pham","Ninh Nguyen Van"],"pdf_url":"https://arxiv.org/pdf/2412.08683v1.pdf","comment":"9 Page, 5 Figures"},{"id":"http://arxiv.org/abs/2412.08671v1","updated":"2024-12-11T03:40:46Z","published":"2024-12-11T03:40:46Z","title":"A Deep Semantic Segmentation Network with Semantic and Contextual\n  Refinements","summary":"  Semantic segmentation is a fundamental task in multimedia processing, which\ncan be used for analyzing, understanding, editing contents of images and\nvideos, among others. To accelerate the analysis of multimedia data, existing\nsegmentation researches tend to extract semantic information by progressively\nreducing the spatial resolutions of feature maps. However, this approach\nintroduces a misalignment problem when restoring the resolution of high-level\nfeature maps. In this paper, we design a Semantic Refinement Module (SRM) to\naddress this issue within the segmentation network. Specifically, SRM is\ndesigned to learn a transformation offset for each pixel in the upsampled\nfeature maps, guided by high-resolution feature maps and neighboring offsets.\nBy applying these offsets to the upsampled feature maps, SRM enhances the\nsemantic representation of the segmentation network, particularly for pixels\naround object boundaries. Furthermore, a Contextual Refinement Module (CRM) is\npresented to capture global context information across both spatial and channel\ndimensions. To balance dimensions between channel and space, we aggregate the\nsemantic maps from all four stages of the backbone to enrich channel context\ninformation. The efficacy of these proposed modules is validated on three\nwidely used datasets-Cityscapes, Bdd100K, and ADE20K-demonstrating superior\nperformance compared to state-of-the-art methods. Additionally, this paper\nextends these modules to a lightweight segmentation network, achieving an mIoU\nof 82.5% on the Cityscapes validation set with only 137.9 GFLOPs.\n","authors":["Zhiyan Wang","Deyin Liu","Lin Yuanbo Wu","Song Wang","Xin Guo","Lin Qi"],"pdf_url":"https://arxiv.org/pdf/2412.08671v1.pdf","comment":"Accept by tmm"},{"id":"http://arxiv.org/abs/2412.08670v1","updated":"2024-12-11T03:31:20Z","published":"2024-12-11T03:31:20Z","title":"A feature refinement module for light-weight semantic segmentation\n  network","summary":"  Low computational complexity and high segmentation accuracy are both\nessential to the real-world semantic segmentation tasks. However, to speed up\nthe model inference, most existing approaches tend to design light-weight\nnetworks with a very limited number of parameters, leading to a considerable\ndegradation in accuracy due to the decrease of the representation ability of\nthe networks. To solve the problem, this paper proposes a novel semantic\nsegmentation method to improve the capacity of obtaining semantic information\nfor the light-weight network. Specifically, a feature refinement module (FRM)\nis proposed to extract semantics from multi-stage feature maps generated by the\nbackbone and capture non-local contextual information by utilizing a\ntransformer block. On Cityscapes and Bdd100K datasets, the experimental results\ndemonstrate that the proposed method achieves a promising trade-off between\naccuracy and computational cost, especially for Cityscapes test set where 80.4%\nmIoU is achieved and only 214.82 GFLOPs are required.\n","authors":["Zhiyan Wang","Xin Guo","Song Wang","Peixiao Zheng","Lin Qi"],"pdf_url":"https://arxiv.org/pdf/2412.08670v1.pdf","comment":"Accept by icip 2023"},{"id":"http://arxiv.org/abs/2412.09649v1","updated":"2024-12-11T09:05:05Z","published":"2024-12-11T09:05:05Z","title":"Pole-based Vehicle Localization with Vector Maps: A Camera-LiDAR\n  Comparative Study","summary":"  For autonomous navigation, accurate localization with respect to a map is\nneeded. In urban environments, infrastructure such as buildings or bridges\ncause major difficulties to Global Navigation Satellite Systems (GNSS) and,\ndespite advances in inertial navigation, it is necessary to support them with\nother sources of exteroceptive information. In road environments, many common\nfurniture such as traffic signs, traffic lights and street lights take the form\nof poles. By georeferencing these features in vector maps, they can be used\nwithin a localization filter that includes a detection pipeline and a data\nassociation method. Poles, having discriminative vertical structures, can be\nextracted from 3D geometric information using LiDAR sensors. Alternatively,\ndeep neural networks can be employed to detect them from monocular cameras. The\nlack of depth information induces challenges in associating camera detections\nwith map features. Yet, multi-camera integration provides a cost-efficient\nsolution. This paper quantitatively evaluates the efficacy of these approaches\nin terms of localization. It introduces a real-time method for camera-based\npole detection using a lightweight neural network trained on automatically\nannotated images. The proposed methods' efficiency is assessed on a challenging\nsequence with a vector map. The results highlight the high accuracy of the\nvision-based approach in open road conditions.\n","authors":["Maxime Noizet","Philippe Xu","Philippe Bonnifait"],"pdf_url":"https://arxiv.org/pdf/2412.09649v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09648v1","updated":"2024-12-11T07:32:17Z","published":"2024-12-11T07:32:17Z","title":"DSplats: 3D Generation by Denoising Splats-Based Multiview Diffusion\n  Models","summary":"  Generating high-quality 3D content requires models capable of learning robust\ndistributions of complex scenes and the real-world objects within them. Recent\nGaussian-based 3D reconstruction techniques have achieved impressive results in\nrecovering high-fidelity 3D assets from sparse input images by predicting 3D\nGaussians in a feed-forward manner. However, these techniques often lack the\nextensive priors and expressiveness offered by Diffusion Models. On the other\nhand, 2D Diffusion Models, which have been successfully applied to denoise\nmultiview images, show potential for generating a wide range of photorealistic\n3D outputs but still fall short on explicit 3D priors and consistency. In this\nwork, we aim to bridge these two approaches by introducing DSplats, a novel\nmethod that directly denoises multiview images using Gaussian Splat-based\nReconstructors to produce a diverse array of realistic 3D assets. To harness\nthe extensive priors of 2D Diffusion Models, we incorporate a pretrained Latent\nDiffusion Model into the reconstructor backbone to predict a set of 3D\nGaussians. Additionally, the explicit 3D representation embedded in the\ndenoising network provides a strong inductive bias, ensuring geometrically\nconsistent novel view generation. Our qualitative and quantitative experiments\ndemonstrate that DSplats not only produces high-quality, spatially consistent\noutputs, but also sets a new standard in single-image to 3D reconstruction.\nWhen evaluated on the Google Scanned Objects dataset, DSplats achieves a PSNR\nof 20.38, an SSIM of 0.842, and an LPIPS of 0.109.\n","authors":["Kevin Miao","Harsh Agrawal","Qihang Zhang","Federico Semeraro","Marco Cavallo","Jiatao Gu","Alexander Toshev"],"pdf_url":"https://arxiv.org/pdf/2412.09648v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09647v1","updated":"2024-12-11T06:35:18Z","published":"2024-12-11T06:35:18Z","title":"Bench2Drive-R: Turning Real World Data into Reactive Closed-Loop\n  Autonomous Driving Benchmark by Generative Model","summary":"  For end-to-end autonomous driving (E2E-AD), the evaluation system remains an\nopen problem. Existing closed-loop evaluation protocols usually rely on\nsimulators like CARLA being less realistic; while NAVSIM using real-world\nvision data, yet is limited to fixed planning trajectories in short horizon and\nassumes other agents are not reactive.\n  We introduce Bench2Drive-R, a generative framework that enables reactive\nclosed-loop evaluation. Unlike existing video generative models for AD, the\nproposed designs are tailored for interactive simulation, where sensor\nrendering and behavior rollout are decoupled by applying a separate behavioral\ncontroller to simulate the reactions of surrounding agents. As a result, the\nrenderer could focus on image fidelity, control adherence, and spatial-temporal\ncoherence. For temporal consistency, due to the step-wise interaction nature of\nsimulation, we design a noise modulating temporal encoder with Gaussian\nblurring to encourage long-horizon autoregressive rollout of image sequences\nwithout deteriorating distribution shifts. For spatial consistency, a retrieval\nmechanism, which takes the spatially nearest images as references, is\nintroduced to to ensure scene-level rendering fidelity during the generation\nprocess. The spatial relations between target and reference are explicitly\nmodeled with 3D relative position encodings and the potential over-reliance of\nreference images is mitigated with hierarchical sampling and classifier-free\nguidance.\n  We compare the generation quality of Bench2Drive-R with existing generative\nmodels and achieve state-of-the-art performance. We further integrate\nBench2Drive-R into nuPlan and evaluate the generative qualities with\nclosed-loop simulation results. We will open source our code.\n","authors":["Junqi You","Xiaosong Jia","Zhiyuan Zhang","Yutao Zhu","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2412.09647v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09646v1","updated":"2024-12-11T06:23:14Z","published":"2024-12-11T06:23:14Z","title":"RealOSR: Latent Unfolding Boosting Diffusion-based Real-world\n  Omnidirectional Image Super-Resolution","summary":"  Omnidirectional image super-resolution (ODISR) aims to upscale low-resolution\n(LR) omnidirectional images (ODIs) to high-resolution (HR), addressing the\ngrowing demand for detailed visual content across a\n$180^{\\circ}\\times360^{\\circ}$ viewport. Existing methods are limited by simple\ndegradation assumptions (e.g., bicubic downsampling), which fail to capture the\ncomplex, unknown real-world degradation processes. Recent diffusion-based\napproaches suffer from slow inference due to their hundreds of sampling steps\nand frequent pixel-latent space conversions. To tackle these challenges, in\nthis paper, we propose RealOSR, a novel diffusion-based approach for real-world\nODISR (Real-ODISR) with single-step diffusion denoising. To sufficiently\nexploit the input information, RealOSR introduces a lightweight domain\nalignment module, which facilitates the efficient injection of LR ODI into the\nsingle-step latent denoising. Additionally, to better utilize the rich semantic\nand multi-scale feature modeling ability of denoising UNet, we develop a latent\nunfolding module that simulates the gradient descent process directly in latent\nspace. Experimental results demonstrate that RealOSR outperforms previous\nmethods in both ODI recovery quality and efficiency. Compared to the recent\nstate-of-the-art diffusion-based ODISR method, OmniSSR, RealOSR achieves\nsignificant improvements in visual quality and over \\textbf{200$\\times$}\ninference acceleration. Our code and models will be released.\n","authors":["Xuhan Sheng","Runyi Li","Bin Chen","Weiqi Li","Xu Jiang","Jian Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.09646v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11946v1","updated":"2024-12-11T23:11:50Z","published":"2024-12-11T23:11:50Z","title":"Physics Meets Pixels: PDE Models in Image Processing","summary":"  Partial Differential Equations (PDEs) have long been recognized as powerful\ntools for image processing and analysis, providing a framework to model and\nexploit structural and geometric properties inherent in visual data. Over the\nyears, numerous PDE-based models have been developed and refined, inspired by\nnatural analogies between physical phenomena and image spaces. These methods\nhave proven highly effective in a wide range of applications, including\ndenoising, deblurring, sharpening, inpainting, feature extraction, and others.\nThis work provides a theoretical and computational exploration of both\nfundamental and innovative PDE models applied to image processing, accompanied\nby extensive numerical experimentation and objective and subjective analysis.\nBuilding upon well-established techniques, we introduce novel physical-based\nPDE models specifically designed for various image processing tasks. These\nmodels incorporate mathematical principles and approaches that, to the best of\nour knowledge, have not been previously applied in this domain, showcasing\ntheir potential to address challenges beyond the capabilities of traditional\nand existing PDE methods. By formulating and solving these mathematical models,\nwe demonstrate their effectiveness in advancing image processing tasks while\nretaining a rigorous connection to their theoretical underpinnings. This work\nseeks to bridge foundational concepts and cutting-edge innovations,\ncontributing to the evolution of PDE methodologies in digital image processing\nand related interdisciplinary fields.\n","authors":["Alejandro Garnung Menéndez"],"pdf_url":"https://arxiv.org/pdf/2412.11946v1.pdf","comment":"19 pages, 15 figures, 4 tables"},{"id":"http://arxiv.org/abs/2412.10448v1","updated":"2024-12-11T20:24:15Z","published":"2024-12-11T20:24:15Z","title":"Unlocking Visual Secrets: Inverting Features with Diffusion Priors for\n  Image Reconstruction","summary":"  Inverting visual representations within deep neural networks (DNNs) presents\na challenging and important problem in the field of security and privacy for\ndeep learning. The main goal is to invert the features of an unidentified\ntarget image generated by a pre-trained DNN, aiming to reconstruct the original\nimage. Feature inversion holds particular significance in understanding the\nprivacy leakage inherent in contemporary split DNN execution techniques, as\nwell as in various applications based on the extracted DNN features.\n  In this paper, we explore the use of diffusion models, a promising technique\nfor image synthesis, to enhance feature inversion quality. We also investigate\nthe potential of incorporating alternative forms of prior knowledge, such as\ntextual prompts and cross-frame temporal correlations, to further improve the\nquality of inverted features. Our findings reveal that diffusion models can\neffectively leverage hidden information from the DNN features, resulting in\nsuperior reconstruction performance compared to previous methods. This research\noffers valuable insights into how diffusion models can enhance privacy and\nsecurity within applications that are reliant on DNN features.\n","authors":["Sai Qian Zhang","Ziyun Li","Chuan Guo","Saeed Mahloujifar","Deeksha Dangwal","Edward Suh","Barbara De Salvo","Chiao Liu"],"pdf_url":"https://arxiv.org/pdf/2412.10448v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12165v1","updated":"2024-12-11T19:58:31Z","published":"2024-12-11T19:58:31Z","title":"Multimodal Approaches to Fair Image Classification: An Ethical\n  Perspective","summary":"  In the rapidly advancing field of artificial intelligence, machine perception\nis becoming paramount to achieving increased performance. Image classification\nsystems are becoming increasingly integral to various applications, ranging\nfrom medical diagnostics to image generation; however, these systems often\nexhibit harmful biases that can lead to unfair and discriminatory outcomes.\nMachine Learning systems that depend on a single data modality, i.e. only\nimages or only text, can exaggerate hidden biases present in the training data,\nif the data is not carefully balanced and filtered. Even so, these models can\nstill harm underrepresented populations when used in improper contexts, such as\nwhen government agencies reinforce racial bias using predictive policing. This\nthesis explores the intersection of technology and ethics in the development of\nfair image classification models. Specifically, I focus on improving fairness\nand methods of using multiple modalities to combat harmful demographic bias.\nIntegrating multimodal approaches, which combine visual data with additional\nmodalities such as text and metadata, allows this work to enhance the fairness\nand accuracy of image classification systems. The study critically examines\nexisting biases in image datasets and classification algorithms, proposes\ninnovative methods for mitigating these biases, and evaluates the ethical\nimplications of deploying such systems in real-world scenarios. Through\ncomprehensive experimentation and analysis, the thesis demonstrates how\nmultimodal techniques can contribute to more equitable and ethical AI\nsolutions, ultimately advocating for responsible AI practices that prioritize\nfairness.\n","authors":["Javon Hickmon"],"pdf_url":"https://arxiv.org/pdf/2412.12165v1.pdf","comment":"Bachelor's thesis"},{"id":"http://arxiv.org/abs/2412.10447v1","updated":"2024-12-11T18:54:22Z","published":"2024-12-11T18:54:22Z","title":"TidyBot++: An Open-Source Holonomic Mobile Manipulator for Robot\n  Learning","summary":"  Exploiting the promise of recent advances in imitation learning for mobile\nmanipulation will require the collection of large numbers of human-guided\ndemonstrations. This paper proposes an open-source design for an inexpensive,\nrobust, and flexible mobile manipulator that can support arbitrary arms,\nenabling a wide range of real-world household mobile manipulation tasks.\nCrucially, our design uses powered casters to enable the mobile base to be\nfully holonomic, able to control all planar degrees of freedom independently\nand simultaneously. This feature makes the base more maneuverable and\nsimplifies many mobile manipulation tasks, eliminating the kinematic\nconstraints that create complex and time-consuming motions in nonholonomic\nbases. We equip our robot with an intuitive mobile phone teleoperation\ninterface to enable easy data acquisition for imitation learning. In our\nexperiments, we use this interface to collect data and show that the resulting\nlearned policies can successfully perform a variety of common household mobile\nmanipulation tasks.\n","authors":["Jimmy Wu","William Chong","Robert Holmberg","Aaditya Prasad","Yihuai Gao","Oussama Khatib","Shuran Song","Szymon Rusinkiewicz","Jeannette Bohg"],"pdf_url":"https://arxiv.org/pdf/2412.10447v1.pdf","comment":"Conference on Robot Learning (CoRL), 2024. Project page:\n  https://tidybot2.github.io"},{"id":"http://arxiv.org/abs/2412.10446v1","updated":"2024-12-11T18:20:53Z","published":"2024-12-11T18:20:53Z","title":"Disentanglement and Compositionality of Letter Identity and Letter\n  Position in Variational Auto-Encoder Vision Models","summary":"  Human readers can accurately count how many letters are in a word (e.g., 7 in\n``buffalo''), remove a letter from a given position (e.g., ``bufflo'') or add a\nnew one. The human brain of readers must have therefore learned to disentangle\ninformation related to the position of a letter and its identity. Such\ndisentanglement is necessary for the compositional, unbounded, ability of\nhumans to create and parse new strings, with any combination of letters\nappearing in any positions. Do modern deep neural models also possess this\ncrucial compositional ability? Here, we tested whether neural models that\nachieve state-of-the-art on disentanglement of features in visual input can\nalso disentangle letter position and letter identity when trained on images of\nwritten words. Specifically, we trained beta variational autoencoder\n($\\beta$-VAE) to reconstruct images of letter strings and evaluated their\ndisentanglement performance using CompOrth - a new benchmark that we created\nfor studying compositional learning and zero-shot generalization in visual\nmodels for orthography. The benchmark suggests a set of tests, of increasing\ncomplexity, to evaluate the degree of disentanglement between orthographic\nfeatures of written words in deep neural models. Using CompOrth, we conducted a\nset of experiments to analyze the generalization ability of these models, in\nparticular, to unseen word length and to unseen combinations of letter\nidentities and letter positions. We found that while models effectively\ndisentangle surface features, such as horizontal and vertical `retinal'\nlocations of words within an image, they dramatically fail to disentangle\nletter position and letter identity and lack any notion of word length.\nTogether, this study demonstrates the shortcomings of state-of-the-art\n$\\beta$-VAE models compared to humans and proposes a new challenge and a\ncorresponding benchmark to evaluate neural models.\n","authors":["Bruno Bianchi","Aakash Agrawal","Stanislas Dehaene","Emmanuel Chemla","Yair Lakretz"],"pdf_url":"https://arxiv.org/pdf/2412.10446v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10444v1","updated":"2024-12-11T16:11:13Z","published":"2024-12-11T16:11:13Z","title":"Boundary Exploration of Next Best View Policy in 3D Robotic Scanning","summary":"  The Next Best View (NBV) problem is a pivotal challenge in 3D robotic\nscanning, with the potential to greatly improve the efficiency of object\ncapture and reconstruction. Current methods for determining the NBV often\noverlook view overlaps, assume a virtual origin point for the camera's focus,\nand rely on voxel representations of 3D data. To address these issues and\nimprove the practicality of scanning unknown objects, we propose an NBV policy\nin which the next view explores the boundary of the scanned point cloud, and\nthe overlap is intrinsically considered. The scanning distance or camera\nworking distance is adjustable and flexible. To this end, a model-based\napproach is proposed where the next sensor positions are searched iteratively\nbased on a reference model. A score is calculated by considering the overlaps\nbetween newly scanned and existing data, as well as the final convergence.\nAdditionally, following the boundary exploration idea, a deep learning network,\nBoundary Exploration NBV network (BENBV-Net), is designed and proposed, which\ncan be used to predict the NBV directly from the scanned data without requiring\nthe reference model. It predicts the scores for given boundaries, and the\nboundary with the highest score is selected as the target point of the next\nbest view. BENBV-Net improves the speed of NBV generation while maintaining the\nperformance of the model-based approach. Our proposed methods are evaluated and\ncompared with existing approaches on the ShapeNet, ModelNet, and 3D Repository\ndatasets. Experimental results demonstrate that our approach outperforms others\nin terms of scanning efficiency and overlap, both of which are crucial for\npractical 3D scanning applications. The related code is released at\n\\url{github.com/leihui6/BENBV}.\n","authors":["Leihui Li","Xuping Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.10444v1.pdf","comment":"Will be submitted to IROS 2025"},{"id":"http://arxiv.org/abs/2412.10441v1","updated":"2024-12-11T11:29:53Z","published":"2024-12-11T11:29:53Z","title":"Novel 3D Binary Indexed Tree for Volume Computation of 3D Reconstructed\n  Models from Volumetric Data","summary":"  In the burgeoning field of medical imaging, precise computation of 3D volume\nholds a significant importance for subsequent qualitative analysis of 3D\nreconstructed objects. Combining multivariate calculus, marching cube\nalgorithm, and binary indexed tree data structure, we developed an algorithm\nfor efficient computation of intrinsic volume of any volumetric data recovered\nfrom computed tomography (CT) or magnetic resonance (MR). We proposed the 30\nconfigurations of volume values based on the polygonal mesh generation method.\nOur algorithm processes the data in scan-line order simultaneously with\nreconstruction algorithm to create a Fenwick tree, ensuring query time much\nfaster and assisting users' edition of slicing or transforming model. We tested\nthe algorithm's accuracy on simple 3D objects (e.g., sphere, cylinder) to\ncomplicated structures (e.g., lungs, cardiac chambers). The result deviated\nwithin $\\pm 0.004 \\text{cm}^3$ and there is still room for further improvement.\n","authors":["Quoc-Bao Nguyen-Le","Tuan-Hy Le","Anh-Triet Do"],"pdf_url":"https://arxiv.org/pdf/2412.10441v1.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2412.10440v1","updated":"2024-12-11T10:26:17Z","published":"2024-12-11T10:26:17Z","title":"Multi-level Matching Network for Multimodal Entity Linking","summary":"  Multimodal entity linking (MEL) aims to link ambiguous mentions within\nmultimodal contexts to corresponding entities in a multimodal knowledge base.\nMost existing approaches to MEL are based on representation learning or\nvision-and-language pre-training mechanisms for exploring the complementary\neffect among multiple modalities. However, these methods suffer from two\nlimitations. On the one hand, they overlook the possibility of considering\nnegative samples from the same modality. On the other hand, they lack\nmechanisms to capture bidirectional cross-modal interaction. To address these\nissues, we propose a Multi-level Matching network for Multimodal Entity Linking\n(M3EL). Specifically, M3EL is composed of three different modules: (i) a\nMultimodal Feature Extraction module, which extracts modality-specific\nrepresentations with a multimodal encoder and introduces an intra-modal\ncontrastive learning sub-module to obtain better discriminative embeddings\nbased on uni-modal differences; (ii) an Intra-modal Matching Network module,\nwhich contains two levels of matching granularity: Coarse-grained\nGlobal-to-Global and Fine-grained Global-to-Local, to achieve local and global\nlevel intra-modal interaction; (iii) a Cross-modal Matching Network module,\nwhich applies bidirectional strategies, Textual-to-Visual and Visual-to-Textual\nmatching, to implement bidirectional cross-modal interaction. Extensive\nexperiments conducted on WikiMEL, RichpediaMEL, and WikiDiverse datasets\ndemonstrate the outstanding performance of M3EL when compared to the\nstate-of-the-art baselines.\n","authors":["Zhiwei Hu","Víctor Gutiérrez-Basulto","Ru Li","Jeff Z. Pan"],"pdf_url":"https://arxiv.org/pdf/2412.10440v1.pdf","comment":"Accepted at KDD'25"},{"id":"http://arxiv.org/abs/2412.10439v1","updated":"2024-12-11T09:50:35Z","published":"2024-12-11T09:50:35Z","title":"CogNav: Cognitive Process Modeling for Object Goal Navigation with LLMs","summary":"  Object goal navigation (ObjectNav) is a fundamental task of embodied AI that\nrequires the agent to find a target object in unseen environments. This task is\nparticularly challenging as it demands both perceptual and cognitive processes\nfor effective perception and decision-making. While perception has gained\nsignificant progress powered by the rapidly developed visual foundation models,\nthe progress on the cognitive side remains limited to either implicitly\nlearning from massive navigation demonstrations or explicitly leveraging\npre-defined heuristic rules. Inspired by neuroscientific evidence that humans\nconsistently update their cognitive states while searching for objects in\nunseen environments, we present CogNav, which attempts to model this cognitive\nprocess with the help of large language models. Specifically, we model the\ncognitive process with a finite state machine composed of cognitive states\nranging from exploration to identification. The transitions between the states\nare determined by a large language model based on an online built heterogeneous\ncognitive map containing spatial and semantic information of the scene being\nexplored. Extensive experiments on both synthetic and real-world environments\ndemonstrate that our cognitive modeling significantly improves ObjectNav\nefficiency, with human-like navigation behaviors. In an open-vocabulary and\nzero-shot setting, our method advances the SOTA of the HM3D benchmark from\n69.3% to 87.2%. The code and data will be released.\n","authors":["Yihan Cao","Jiazhao Zhang","Zhinan Yu","Shuzhen Liu","Zheng Qin","Qin Zou","Bo Du","Kai Xu"],"pdf_url":"https://arxiv.org/pdf/2412.10439v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10438v1","updated":"2024-12-11T09:06:52Z","published":"2024-12-11T09:06:52Z","title":"Automatic Image Annotation for Mapped Features Detection","summary":"  Detecting road features is a key enabler for autonomous driving and\nlocalization. For instance, a reliable detection of poles which are widespread\nin road environments can improve localization. Modern deep learning-based\nperception systems need a significant amount of annotated data. Automatic\nannotation avoids time-consuming and costly manual annotation. Because\nautomatic methods are prone to errors, managing annotation uncertainty is\ncrucial to ensure a proper learning process. Fusing multiple annotation sources\non the same dataset can be an efficient way to reduce the errors. This not only\nimproves the quality of annotations, but also improves the learning of\nperception models. In this paper, we consider the fusion of three automatic\nannotation methods in images: feature projection from a high accuracy vector\nmap combined with a lidar, image segmentation and lidar segmentation. Our\nexperimental results demonstrate the significant benefits of multi-modal\nautomatic annotation for pole detection through a comparative evaluation on\nmanually annotated images. Finally, the resulting multi-modal fusion is used to\nfine-tune an object detection model for pole base detection using unlabeled\ndata, showing overall improvements achieved by enhancing network\nspecialization. The dataset is publicly available.\n","authors":["Maxime Noizet","Philippe Xu","Philippe Bonnifait"],"pdf_url":"https://arxiv.org/pdf/2412.10438v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10437v1","updated":"2024-12-11T09:02:25Z","published":"2024-12-11T09:02:25Z","title":"SVGFusion: Scalable Text-to-SVG Generation via Vector Space Diffusion","summary":"  The generation of Scalable Vector Graphics (SVG) assets from textual data\nremains a significant challenge, largely due to the scarcity of high-quality\nvector datasets and the limitations in scalable vector representations required\nfor modeling intricate graphic distributions. This work introduces SVGFusion, a\nText-to-SVG model capable of scaling to real-world SVG data without reliance on\na text-based discrete language model or prolonged SDS optimization. The essence\nof SVGFusion is to learn a continuous latent space for vector graphics with a\npopular Text-to-Image framework. Specifically, SVGFusion consists of two\nmodules: a Vector-Pixel Fusion Variational Autoencoder (VP-VAE) and a Vector\nSpace Diffusion Transformer (VS-DiT). VP-VAE takes both the SVGs and\ncorresponding rasterizations as inputs and learns a continuous latent space,\nwhereas VS-DiT learns to generate a latent code within this space based on the\ntext prompt. Based on VP-VAE, a novel rendering sequence modeling strategy is\nproposed to enable the latent space to embed the knowledge of construction\nlogics in SVGs. This empowers the model to achieve human-like design\ncapabilities in vector graphics, while systematically preventing occlusion in\ncomplex graphic compositions. Moreover, our SVGFusion's ability can be\ncontinuously improved by leveraging the scalability of the VS-DiT by adding\nmore VS-DiT blocks. A large-scale SVG dataset is collected to evaluate the\neffectiveness of our proposed method. Extensive experimentation has confirmed\nthe superiority of our SVGFusion over existing SVG generation methods,\nachieving enhanced quality and generalizability, thereby establishing a novel\nframework for SVG content creation. Code, model, and data will be released at:\n\\href{https://ximinng.github.io/SVGFusionProject/}{https://ximinng.github.io/SVGFusionProject/}\n","authors":["Ximing Xing","Juncheng Hu","Jing Zhang","Dong Xu","Qian Yu"],"pdf_url":"https://arxiv.org/pdf/2412.10437v1.pdf","comment":"project page:\n  \\href{https://ximinng.github.io/SVGFusionProject/}{https://ximinng.github.io/SVGFusionProject/}"},{"id":"http://arxiv.org/abs/2412.10436v1","updated":"2024-12-11T08:10:46Z","published":"2024-12-11T08:10:46Z","title":"Benchmarking Federated Learning for Semantic Datasets: Federated Scene\n  Graph Generation","summary":"  Federated learning (FL) has recently garnered attention as a\ndata-decentralized training framework that enables the learning of deep models\nfrom locally distributed samples while keeping data privacy. Built upon the\nframework, immense efforts have been made to establish FL benchmarks, which\nprovide rigorous evaluation settings that control data heterogeneity across\nclients. Prior efforts have mainly focused on handling relatively simple\nclassification tasks, where each sample is annotated with a one-hot label, such\nas MNIST, CIFAR, LEAF benchmark, etc. However, little attention has been paid\nto demonstrating an FL benchmark that handles complicated semantics, where each\nsample encompasses diverse semantic information from multiple labels, such as\nPanoptic Scene Graph Generation (PSG) with objects, subjects, and relations\nbetween them. Because the existing benchmark is designed to distribute data in\na narrow view of a single semantic, e.g., a one-hot label, managing the\ncomplicated semantic heterogeneity across clients when formalizing FL\nbenchmarks is non-trivial. In this paper, we propose a benchmark process to\nestablish an FL benchmark with controllable semantic heterogeneity across\nclients: two key steps are i) data clustering with semantics and ii) data\ndistributing via controllable semantic heterogeneity across clients. As a proof\nof concept, we first construct a federated PSG benchmark, demonstrating the\nefficacy of the existing PSG methods in an FL setting with controllable\nsemantic heterogeneity of scene graphs. We also present the effectiveness of\nour benchmark by applying robust federated learning algorithms to data\nheterogeneity to show increased performance. Our code is available at\nhttps://github.com/Seung-B/FL-PSG.\n","authors":["SeungBum Ha","Taehwan Lee","Jiyoun Lim","Sung Whan Yoon"],"pdf_url":"https://arxiv.org/pdf/2412.10436v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2412.10435v1","updated":"2024-12-11T08:10:32Z","published":"2024-12-11T08:10:32Z","title":"COEF-VQ: Cost-Efficient Video Quality Understanding through a Cascaded\n  Multimodal LLM Framework","summary":"  Recently, with the emergence of recent Multimodal Large Language Model (MLLM)\ntechnology, it has become possible to exploit its video understanding\ncapability on different classification tasks. In practice, we face the\ndifficulty of huge requirements for GPU resource if we need to deploy MLLMs\nonline. In this paper, we propose COEF-VQ, a novel cascaded MLLM framework for\nbetter video quality understanding on TikTok. To this end, we first propose a\nMLLM fusing all visual, textual and audio signals, and then develop a cascade\nframework with a lightweight model as pre-filtering stage and MLLM as\nfine-consideration stage, significantly reducing the need for GPU resource,\nwhile retaining the performance demonstrated solely by MLLM. To demonstrate the\neffectiveness of COEF-VQ, we deployed this new framework onto the video\nmanagement platform (VMP) at TikTok, and performed a series of detailed\nexperiments on two in-house tasks related to video quality understanding. We\nshow that COEF-VQ leads to substantial performance gains with limit resource\nconsumption in these two tasks.\n","authors":["Xin Dong","Sen Jia","Hongyu Xiong"],"pdf_url":"https://arxiv.org/pdf/2412.10435v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12150v1","updated":"2024-12-11T05:29:54Z","published":"2024-12-11T05:29:54Z","title":"Rethinking Comprehensive Benchmark for Chart Understanding: A\n  Perspective from Scientific Literature","summary":"  Scientific Literature charts often contain complex visual elements, including\nmulti-plot figures, flowcharts, structural diagrams and etc. Evaluating\nmultimodal models using these authentic and intricate charts provides a more\naccurate assessment of their understanding abilities. However, existing\nbenchmarks face limitations: a narrow range of chart types, overly simplistic\ntemplate-based questions and visual elements, and inadequate evaluation\nmethods. These shortcomings lead to inflated performance scores that fail to\nhold up when models encounter real-world scientific charts. To address these\nchallenges, we introduce a new benchmark, Scientific Chart QA (SCI-CQA), which\nemphasizes flowcharts as a critical yet often overlooked category. To overcome\nthe limitations of chart variety and simplistic visual elements, we curated a\ndataset of 202,760 image-text pairs from 15 top-tier computer science\nconferences papers over the past decade. After rigorous filtering, we refined\nthis to 37,607 high-quality charts with contextual information. SCI-CQA also\nintroduces a novel evaluation framework inspired by human exams, encompassing\n5,629 carefully curated questions, both objective and open-ended. Additionally,\nwe propose an efficient annotation pipeline that significantly reduces data\nannotation costs. Finally, we explore context-based chart understanding,\nhighlighting the crucial role of contextual information in solving previously\nunanswerable questions.\n","authors":["Lingdong Shen"," Qigqi","Kun Ding","Gaofeng Meng","Shiming Xiang"],"pdf_url":"https://arxiv.org/pdf/2412.12150v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10433v1","updated":"2024-12-11T03:22:00Z","published":"2024-12-11T03:22:00Z","title":"Implicit Neural Compression of Point Clouds","summary":"  Point clouds have gained prominence in numerous applications due to their\nability to accurately depict 3D objects and scenes. However, compressing\nunstructured, high-precision point cloud data effectively remains a significant\nchallenge. In this paper, we propose NeRC$^{\\textbf{3}}$, a novel point cloud\ncompression framework leveraging implicit neural representations to handle both\ngeometry and attributes. Our approach employs two coordinate-based neural\nnetworks to implicitly represent a voxelized point cloud: the first determines\nthe occupancy status of a voxel, while the second predicts the attributes of\noccupied voxels. By feeding voxel coordinates into these networks, the receiver\ncan efficiently reconstructs the original point cloud's geometry and\nattributes. The neural network parameters are quantized and compressed\nalongside auxiliary information required for reconstruction. Additionally, we\nextend our method to dynamic point cloud compression with techniques to reduce\ntemporal redundancy, including a 4D spatial-temporal representation termed\n4D-NeRC$^{\\textbf{3}}$. Experimental results validate the effectiveness of our\napproach: for static point clouds, NeRC$^{\\textbf{3}}$ outperforms octree-based\nmethods in the latest G-PCC standard. For dynamic point clouds,\n4D-NeRC$^{\\textbf{3}}$ demonstrates superior geometry compression compared to\nstate-of-the-art G-PCC and V-PCC standards and achieves competitive results for\njoint geometry and attribute compression.\n","authors":["Hongning Ruan","Yulin Shao","Qianqian Yang","Liang Zhao","Zhaoyang Zhang","Dusit Niyato"],"pdf_url":"https://arxiv.org/pdf/2412.10433v1.pdf","comment":"16 pages, 8 figures"},{"id":"http://arxiv.org/abs/2412.10431v1","updated":"2024-12-11T03:11:44Z","published":"2024-12-11T03:11:44Z","title":"CUPS: Improving Human Pose-Shape Estimators with Conformalized Deep\n  Uncertainty","summary":"  We introduce CUPS, a novel method for learning sequence-to-sequence 3D human\nshapes and poses from RGB videos with uncertainty quantification. To improve on\ntop of prior work, we develop a method to generate and score multiple\nhypotheses during training, effectively integrating uncertainty quantification\ninto the learning process. This process results in a deep uncertainty function\nthat is trained end-to-end with the 3D pose estimator. Post-training, the\nlearned deep uncertainty model is used as the conformity score, which can be\nused to calibrate a conformal predictor in order to assess the quality of the\noutput prediction. Since the data in human pose-shape learning is not fully\nexchangeable, we also present two practical bounds for the coverage gap in\nconformal prediction, developing theoretical backing for the uncertainty bound\nof our model. Our results indicate that by taking advantage of deep uncertainty\nwith conformal prediction, our method achieves state-of-the-art performance\nacross various metrics and datasets while inheriting the probabilistic\nguarantees of conformal prediction.\n","authors":["Harry Zhang","Luca Carlone"],"pdf_url":"https://arxiv.org/pdf/2412.10431v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12149v1","updated":"2024-12-11T02:59:57Z","published":"2024-12-11T02:59:57Z","title":"MHSA: A Multi-scale Hypergraph Network for Mild Cognitive Impairment\n  Detection via Synchronous and Attentive Fusion","summary":"  The precise detection of mild cognitive impairment (MCI) is of significant\nimportance in preventing the deterioration of patients in a timely manner.\nAlthough hypergraphs have enhanced performance by learning and analyzing brain\nnetworks, they often only depend on vector distances between features at a\nsingle scale to infer interactions. In this paper, we deal with a more arduous\nchallenge, hypergraph modelling with synchronization between brain regions, and\ndesign a novel framework, i.e., A Multi-scale Hypergraph Network for MCI\nDetection via Synchronous and Attentive Fusion (MHSA), to tackle this\nchallenge. Specifically, our approach employs the Phase-Locking Value (PLV) to\ncalculate the phase synchronization relationship in the spectrum domain of\nregions of interest (ROIs) and designs a multi-scale feature fusion mechanism\nto integrate dynamic connectivity features of functional magnetic resonance\nimaging (fMRI) from both the temporal and spectrum domains. To evaluate and\noptimize the direct contribution of each ROI to phase synchronization in the\ntemporal domain, we structure the PLV coefficients dynamically adjust strategy,\nand the dynamic hypergraph is modelled based on a comprehensive\ntemporal-spectrum fusion matrix. Experiments on the real-world dataset indicate\nthe effectiveness of our strategy. The code is available at\nhttps://github.com/Jia-Weiming/MHSA.\n","authors":["Manman Yuan","Weiming Jia","Xiong Luo","Jiazhen Ye","Peican Zhu","Junlin Li"],"pdf_url":"https://arxiv.org/pdf/2412.12149v1.pdf","comment":"International Conference on Bioinformatics and Biomedicine 2024(BIBM\n  2024)"},{"id":"http://arxiv.org/abs/2412.10430v1","updated":"2024-12-11T01:49:10Z","published":"2024-12-11T01:49:10Z","title":"Unsupervised Cross-Domain Regression for Fine-grained 3D Game Character\n  Reconstruction","summary":"  With the rise of the ``metaverse'' and the rapid development of games, it has\nbecome more and more critical to reconstruct characters in the virtual world\nfaithfully. The immersive experience is one of the most central themes of the\n``metaverse'', while the reducibility of the avatar is the crucial point.\nMeanwhile, the game is the carrier of the metaverse, in which players can\nfreely edit the facial appearance of the game character. In this paper, we\npropose a simple but powerful cross-domain framework that can reconstruct\nfine-grained 3D game characters from single-view images in an end-to-end\nmanner. Different from the previous methods, which do not resolve the\ncross-domain gap, we propose an effective regressor that can greatly reduce the\ndiscrepancy between the real-world domain and the game domain. To figure out\nthe drawbacks of no ground truth, our unsupervised framework has accomplished\nthe knowledge transfer of the target domain. Additionally, an innovative\ncontrastive loss is proposed to solve the instance-wise disparity, which keeps\nthe person-specific details of the reconstructed character. In contrast, an\nauxiliary 3D identity-aware extractor is activated to make the results of our\nmodel more impeccable. Then a large set of physically meaningful facial\nparameters is generated robustly and exquisitely. Experiments demonstrate that\nour method yields state-of-the-art performance in 3D game character\nreconstruction.\n","authors":["Qi Wen","Xiang Wen","Hao Jiang","Siqi Yang","Bingfeng Han","Tianlei Hu","Gang Chen","Shuang Li"],"pdf_url":"https://arxiv.org/pdf/2412.10430v1.pdf","comment":"12 pages, 10 figures"},{"id":"http://arxiv.org/abs/2412.10429v1","updated":"2024-12-11T00:42:44Z","published":"2024-12-11T00:42:44Z","title":"GPTDrawer: Enhancing Visual Synthesis through ChatGPT","summary":"  In the burgeoning field of AI-driven image generation, the quest for\nprecision and relevance in response to textual prompts remains paramount. This\npaper introduces GPTDrawer, an innovative pipeline that leverages the\ngenerative prowess of GPT-based models to enhance the visual synthesis process.\nOur methodology employs a novel algorithm that iteratively refines input\nprompts using keyword extraction, semantic analysis, and image-text congruence\nevaluation. By integrating ChatGPT for natural language processing and Stable\nDiffusion for image generation, GPTDrawer produces a batch of images that\nundergo successive refinement cycles, guided by cosine similarity metrics until\na threshold of semantic alignment is attained. The results demonstrate a marked\nimprovement in the fidelity of images generated in accordance with user-defined\nprompts, showcasing the system's ability to interpret and visualize complex\nsemantic constructs. The implications of this work extend to various\napplications, from creative arts to design automation, setting a new benchmark\nfor AI-assisted creative processes.\n","authors":["Kun Li","Xinwei Chen","Tianyou Song","Hansong Zhang","Wenzhe Zhang","Qing Shan"],"pdf_url":"https://arxiv.org/pdf/2412.10429v1.pdf","comment":null}],"Robotics":[{"id":"http://arxiv.org/abs/2412.08643v1","updated":"2024-12-11T18:59:51Z","published":"2024-12-11T18:59:51Z","title":"GPD-1: Generative Pre-training for Driving","summary":"  Modeling the evolutions of driving scenarios is important for the evaluation\nand decision-making of autonomous driving systems. Most existing methods focus\non one aspect of scene evolution such as map generation, motion prediction, and\ntrajectory planning. In this paper, we propose a unified Generative\nPre-training for Driving (GPD-1) model to accomplish all these tasks altogether\nwithout additional fine-tuning. We represent each scene with ego, agent, and\nmap tokens and formulate autonomous driving as a unified token generation\nproblem. We adopt the autoregressive transformer architecture and use a\nscene-level attention mask to enable intra-scene bi-directional interactions.\nFor the ego and agent tokens, we propose a hierarchical positional tokenizer to\neffectively encode both 2D positions and headings. For the map tokens, we train\na map vector-quantized autoencoder to efficiently compress ego-centric semantic\nmaps into discrete tokens. We pre-train our GPD-1 on the large-scale nuPlan\ndataset and conduct extensive experiments to evaluate its effectiveness. With\ndifferent prompts, our GPD-1 successfully generalizes to various tasks without\nfinetuning, including scene generation, traffic simulation, closed-loop\nsimulation, map prediction, and motion planning. Code:\nhttps://github.com/wzzheng/GPD.\n","authors":["Zixun Xie","Sicheng Zuo","Wenzhao Zheng","Yunpeng Zhang","Dalong Du","Jie Zhou","Jiwen Lu","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.08643v1.pdf","comment":"Code is available at: https://github.com/wzzheng/GPD"},{"id":"http://arxiv.org/abs/2412.08644v1","updated":"2024-12-11T18:59:51Z","published":"2024-12-11T18:59:51Z","title":"Bilevel Learning for Dual-Quadruped Collaborative Transportation under\n  Kinematic and Anisotropic Velocity Constraints","summary":"  Multi-robot collaborative transportation is a critical capability that has\nattracted significant attention over recent years. To reliably transport a\nkinematically constrained payload, a team of robots must closely collaborate\nand coordinate their individual velocities to achieve the desired payload\nmotion. For quadruped robots, a key challenge is caused by their anisotropic\nvelocity limits, where forward and backward movement is faster and more stable\nthan lateral motion. In order to enable dual-quadruped collaborative\ntransportation and address the above challenges, we propose a novel Bilevel\nLearning for Collaborative Transportation (BLCT) approach. In the upper-level,\nBLCT learns a team collaboration policy for the two quadruped robots to move\nthe payload to the goal position, while accounting for the kinematic\nconstraints imposed by their connection to the payload. In the lower-level,\nBLCT optimizes velocity controls of each individual robot to closely follow the\ncollaboration policy while satisfying the anisotropic velocity constraints and\navoiding obstacles. Experiments demonstrate that our BLCT approach well enables\ncollaborative transportation in challenging scenarios and outperforms baseline\napproaches.\n","authors":["Williard Joshua Jose","Hao Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.08644v1.pdf","comment":"8 pages, 5 figures, project website:\n  https://hcrlab.gitlab.io/project/blct"},{"id":"http://arxiv.org/abs/2412.08591v1","updated":"2024-12-11T18:10:21Z","published":"2024-12-11T18:10:21Z","title":"RoomTour3D: Geometry-Aware Video-Instruction Tuning for Embodied\n  Navigation","summary":"  Vision-and-Language Navigation (VLN) suffers from the limited diversity and\nscale of training data, primarily constrained by the manual curation of\nexisting simulators. To address this, we introduce RoomTour3D, a\nvideo-instruction dataset derived from web-based room tour videos that capture\nreal-world indoor spaces and human walking demonstrations. Unlike existing VLN\ndatasets, RoomTour3D leverages the scale and diversity of online videos to\ngenerate open-ended human walking trajectories and open-world navigable\ninstructions. To compensate for the lack of navigation data in online videos,\nwe perform 3D reconstruction and obtain 3D trajectories of walking paths\naugmented with additional information on the room types, object locations and\n3D shape of surrounding scenes. Our dataset includes $\\sim$100K open-ended\ndescription-enriched trajectories with $\\sim$200K instructions, and 17K\naction-enriched trajectories from 1847 room tour environments. We demonstrate\nexperimentally that RoomTour3D enables significant improvements across multiple\nVLN tasks including CVDN, SOON, R2R, and REVERIE. Moreover, RoomTour3D\nfacilitates the development of trainable zero-shot VLN agents, showcasing the\npotential and challenges of advancing towards open-world navigation.\n","authors":["Mingfei Han","Liang Ma","Kamila Zhumakhanova","Ekaterina Radionova","Jingyi Zhang","Xiaojun Chang","Xiaodan Liang","Ivan Laptev"],"pdf_url":"https://arxiv.org/pdf/2412.08591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08568v1","updated":"2024-12-11T17:33:51Z","published":"2024-12-11T17:33:51Z","title":"Real-Time Trajectory Generation for Soft Robot Manipulators Using\n  Differential Flatness","summary":"  Soft robots have the potential to interact with sensitive environments and\nperform complex tasks effectively. However, motion plans and trajectories for\nsoft manipulators are challenging to calculate due to their deformable nature\nand nonlinear dynamics. This article introduces a fast real-time trajectory\ngeneration approach for soft robot manipulators, which creates\ndynamically-feasible motions for arbitrary kinematically-feasible paths of the\nrobot's end effector. Our insight is that piecewise constant curvature (PCC)\ndynamics models of soft robots can be differentially flat, therefore control\ninputs can be calculated algebraically rather than through a nonlinear\ndifferential equation. We prove this flatness under certain conditions, with\nthe curvatures of the robot as the flat outputs. Our two-step trajectory\ngeneration approach uses an inverse kinematics procedure to calculate a motion\nplan of robot curvatures per end-effector position, then, our flatness\ndiffeomorphism generates corresponding control inputs that respect velocity. We\nvalidate our approach through simulations of our representative soft robot\nmanipulator along three different trajectories, demonstrating a margin of 23x\nfaster than real-time at a frequency of 100 Hz. This approach could allow fast\nverifiable replanning of soft robots' motions in safety-critical physical\nenvironments, crucial for deployment in the real world.\n","authors":["Akua Dickson","Juan C. Pacheco Garcia","Ran Jing","Meredith L. Anderson","Andrew P. Sabelhaus"],"pdf_url":"https://arxiv.org/pdf/2412.08568v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08562v1","updated":"2024-12-11T17:30:08Z","published":"2024-12-11T17:30:08Z","title":"An End-to-End Collaborative Learning Approach for Connected Autonomous\n  Vehicles in Occluded Scenarios","summary":"  Collaborative navigation becomes essential in situations of occluded\nscenarios in autonomous driving where independent driving policies are likely\nto lead to collisions. One promising approach to address this issue is through\nthe use of Vehicle-to-Vehicle (V2V) networks that allow for the sharing of\nperception information with nearby agents, preventing catastrophic accidents.\nIn this article, we propose a collaborative control method based on a V2V\nnetwork for sharing compressed LiDAR features and employing Proximal Policy\nOptimisation to train safe and efficient navigation policies. Unlike previous\napproaches that rely on expert data (behaviour cloning), our proposed approach\nlearns the multi-agent policies directly from experience in the occluded\nenvironment, while effectively meeting bandwidth limitations. The proposed\nmethod first prepossesses LiDAR point cloud data to obtain meaningful features\nthrough a convolutional neural network and then shares them with nearby CAVs to\nalert for potentially dangerous situations. To evaluate the proposed method, we\ndeveloped an occluded intersection gym environment based on the CARLA\nautonomous driving simulator, allowing real-time data sharing among agents. Our\nexperimental results demonstrate the consistent superiority of our\ncollaborative control method over an independent reinforcement learning method\nand a cooperative early fusion method.\n","authors":["Leandro Parada","Hanlin Tian","Jose Escribano","Panagiotis Angeloudis"],"pdf_url":"https://arxiv.org/pdf/2412.08562v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.06985v2","updated":"2024-12-11T17:29:15Z","published":"2024-12-09T20:49:26Z","title":"Ground Perturbation Detection via Lower-Limb Kinematic States During\n  Locomotion","summary":"  Falls during daily ambulation activities are a leading cause of injury in\nolder adults due to delayed physiological responses to disturbances of balance.\nLower-limb exoskeletons have the potential to mitigate fall incidents by\ndetecting and reacting to perturbations before the user. Although commonly\nused, the standard metric for perturbation detection, whole-body angular\nmomentum, is poorly suited for exoskeleton applications due to computational\ndelays and additional tunings. To address this, we developed a novel ground\nperturbation detector using lower-limb kinematic states during locomotion. To\nidentify perturbations, we tracked deviations in the kinematic states from\ntheir nominal steady-state trajectories. Using a data-driven approach, we\nfurther optimized our detector with an open-source ground perturbation\nbiomechanics dataset. A pilot experimental validation with five able-bodied\nsubjects demonstrated that our model detected ground perturbations with 97.8%\naccuracy and only a delay of 23.1% within the gait cycle, outperforming the\nbenchmark by 46.8% in detection accuracy. The results of our study offer\nexciting promise for our detector and its potential utility to enhance the\ncontrollability of robotic assistive exoskeletons.\n","authors":["Maria T. Tagliaferri","Leonardo Campeggi","Owen N. Beck","Inseung Kang"],"pdf_url":"https://arxiv.org/pdf/2412.06985v2.pdf","comment":"6 pages, 6 figures"},{"id":"http://arxiv.org/abs/2310.05239v2","updated":"2024-12-11T16:50:42Z","published":"2023-10-08T17:13:01Z","title":"Lan-grasp: Using Large Language Models for Semantic Object Grasping","summary":"  In this paper, we propose Lan-grasp, a novel approach towards more\nappropriate semantic grasping. We use foundation models to provide the robot\nwith a deeper understanding of the objects, the right place to grasp an object,\nor even the parts to avoid. This allows our robot to grasp and utilize objects\nin a more meaningful and safe manner. We leverage the combination of a Large\nLanguage Model, a Vision Language Model, and a traditional grasp planner to\ngenerate grasps demonstrating a deeper semantic understanding of the objects.\nWe first prompt the Large Language Model about which object part is appropriate\nfor grasping. Next, the Vision Language Model identifies the corresponding part\nin the object image. Finally, we generate grasp proposals in the region\nproposed by the Vision Language Model. Building on foundation models provides\nus with a zero-shot grasp method that can handle a wide range of objects\nwithout the need for further training or fine-tuning. We evaluated our method\nin real-world experiments on a custom object data set. We present the results\nof a survey that asks the participants to choose an object part appropriate for\ngrasping. The results show that the grasps generated by our method are\nconsistently ranked higher by the participants than those generated by a\nconventional grasping planner and a recent semantic grasping approach. In\naddition, we propose a Visual Chain-of-Thought feedback loop to assess grasp\nfeasibility in complex scenarios. This mechanism enables dynamic reasoning and\ngenerates alternative grasp strategies when needed, ensuring safer and more\neffective grasping outcomes.\n","authors":["Reihaneh Mirjalili","Michael Krawez","Simone Silenzi","Yannik Blei","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2310.05239v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08522v1","updated":"2024-12-11T16:35:21Z","published":"2024-12-11T16:35:21Z","title":"Subspace-wise Hybrid RL for Articulated Object Manipulation","summary":"  Articulated object manipulation is a challenging task, requiring constrained\nmotion and adaptive control to handle the unknown dynamics of the manipulated\nobjects. While reinforcement learning (RL) has been widely employed to tackle\nvarious scenarios and types of articulated objects, the complexity of these\ntasks, stemming from multiple intertwined objectives makes learning a control\npolicy in the full task space highly difficult. To address this issue, we\npropose a Subspace-wise hybrid RL (SwRL) framework that learns policies for\neach divided task space, or subspace, based on independent objectives. This\napproach enables adaptive force modulation to accommodate the unknown dynamics\nof objects. Additionally, it effectively leverages the previously underlooked\nredundant subspace, thereby maximizing the robot's dexterity. Our method\nenhances both learning efficiency and task execution performance, as validated\nthrough simulations and real-world experiments. Supplementary video is\navailable at https://youtu.be/PkNxv0P8Atk\n","authors":["Yujin Kim","Sol Choi","Bum-Jae You","Keunwoo Jang","Yisoo Lee"],"pdf_url":"https://arxiv.org/pdf/2412.08522v1.pdf","comment":"14 pages, 10 figures, Submitted to Robotics and Autonomous Systems"},{"id":"http://arxiv.org/abs/2409.17562v2","updated":"2024-12-11T16:17:55Z","published":"2024-09-26T06:21:52Z","title":"Software for the SpaceDREAM Robotic Arm","summary":"  Impedance-controlled robots are widely used on Earth to perform\ninteraction-rich tasks and will be a key enabler for In-Space Servicing,\nAssembly and Manufacturing (ISAM) activities. This paper introduces the\nsoftware architecture used on the On-Board Computer (OBC) for the planned\nSpaceDREAM mission aiming to validate such robotic arm in Lower Earth Orbit\n(LEO) conducted by the German Aerospace Center (DLR) in cooperation with\nKINETIK Space GmbH and the Technical University of Munich (TUM). During the\nmission several free motion as well as contact tasks are to be performed in\norder to verify proper functionality of the robot in position and impedance\ncontrol on joint level as well as in cartesian control. The tasks are selected\nto be representative for subsequent servicing missions e.g. requiring interface\ndocking or precise manipulation.\n  The software on the OBC commands the robot's joints via SpaceWire to perform\nthose mission tasks, reads camera images and data from additional sensors and\nsends telemetry data through an Ethernet link via the spacecraft down to Earth.\nIt is set up to execute a predefined mission after receiving a start signal\nfrom the spacecraft while it should be extendable to receive commands from\nEarth for later missions. Core design principle was to reuse as much existing\nsoftware and to stay as close as possible to existing robot software stacks at\nDLR. This allowed for a quick full operational start of the robot arm compared\nto a custom development of all robot software, a lower entry barrier for\nsoftware developers as well as a reuse of existing libraries. While not every\nline of code can be tested with this design, most of the software has already\nproven its functionality through daily execution on multiple robot systems.\n","authors":["Maximilian Mühlbauer","Maxime Chalon","Maximilian Ulmer","Alin Albu-Schäffer"],"pdf_url":"https://arxiv.org/pdf/2409.17562v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08496v1","updated":"2024-12-11T15:59:46Z","published":"2024-12-11T15:59:46Z","title":"Drift-free Visual SLAM using Digital Twins","summary":"  Globally-consistent localization in urban environments is crucial for\nautonomous systems such as self-driving vehicles and drones, as well as\nassistive technologies for visually impaired people. Traditional\nVisual-Inertial Odometry (VIO) and Visual Simultaneous Localization and Mapping\n(VSLAM) methods, though adequate for local pose estimation, suffer from drift\nin the long term due to reliance on local sensor data. While GPS counteracts\nthis drift, it is unavailable indoors and often unreliable in urban areas. An\nalternative is to localize the camera to an existing 3D map using\nvisual-feature matching. This can provide centimeter-level accurate\nlocalization but is limited by the visual similarities between the current view\nand the map. This paper introduces a novel approach that achieves accurate and\nglobally-consistent localization by aligning the sparse 3D point cloud\ngenerated by the VIO/VSLAM system to a digital twin using point-to-plane\nmatching; no visual data association is needed. The proposed method provides a\n6-DoF global measurement tightly integrated into the VIO/VSLAM system.\nExperiments run on a high-fidelity GPS simulator and real-world data collected\nfrom a drone demonstrate that our approach outperforms state-of-the-art VIO-GPS\nsystems and offers superior robustness against viewpoint changes compared to\nthe state-of-the-art Visual SLAM systems.\n","authors":["Roxane Merat","Giovanni Cioffi","Leonard Bauersfeld","Davide Scaramuzza"],"pdf_url":"https://arxiv.org/pdf/2412.08496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08468v1","updated":"2024-12-11T15:33:35Z","published":"2024-12-11T15:33:35Z","title":"Multi-GraspLLM: A Multimodal LLM for Multi-Hand Semantic Guided Grasp\n  Generation","summary":"  Multi-hand semantic grasp generation aims to generate feasible and\nsemantically appropriate grasp poses for different robotic hands based on\nnatural language instructions. Although the task is highly valuable, due to the\nlack of multi-hand grasp datasets with fine-grained contact description between\nrobotic hands and objects, it is still a long-standing difficult task. In this\npaper, we present Multi-GraspSet, the first large-scale multi-hand grasp\ndataset with automatically contact annotations. Based on Multi-GraspSet, we\npropose Multi-GraspLLM, a unified language-guided grasp generation framework.\nIt leverages large language models (LLM) to handle variable-length sequences,\ngenerating grasp poses for diverse robotic hands in a single unified\narchitecture. Multi-GraspLLM first aligns the encoded point cloud features and\ntext features into a unified semantic space. It then generates grasp bin tokens\nwhich are subsequently converted into grasp pose for each robotic hand via\nhand-aware linear mapping. The experimental results demonstrate that our\napproach significantly outperforms existing methods on Multi-GraspSet. More\ninformation can be found on our project page https://multi-graspllm.github.io.\n","authors":["Haosheng Li","Weixin Mao","Weipeng Deng","Chenyu Meng","Haoqiang Fan","Tiancai Wang","Ping Tan","Hongan Wang","Xiaoming Deng"],"pdf_url":"https://arxiv.org/pdf/2412.08468v1.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.08428v1","updated":"2024-12-11T14:48:19Z","published":"2024-12-11T14:48:19Z","title":"SwarmGPT-Primitive: A Language-Driven Choreographer for Drone Swarms\n  Using Safe Motion Primitive Composition","summary":"  Catalyzed by advancements in hardware and software, drone performances are\nincreasingly making their mark in the entertainment industry. However,\ndesigning smooth and safe choreographies for drone swarms is complex and often\nrequires expert domain knowledge. In this work, we introduce\nSwarmGPT-Primitive, a language-based choreographer that integrates the\nreasoning capabilities of large language models (LLMs) with safe motion\nplanning to facilitate deployable drone swarm choreographies. The LLM composes\nchoreographies for a given piece of music by utilizing a library of motion\nprimitives; the language-based choreographer is augmented with an\noptimization-based safety filter, which certifies the choreography for\nreal-world deployment by making minimal adjustments when feasibility and safety\nconstraints are violated. The overall SwarmGPT-Primitive framework decouples\nchoreographic design from safe motion planning, which allows non-expert users\nto re-prompt and refine compositions without concerns about compliance with\nconstraints such as avoiding collisions or downwash effects or satisfying\nactuation limits. We demonstrate our approach through simulations and\nexperiments with swarms of up to 20 drones performing choreographies designed\nbased on various songs, highlighting the system's ability to generate effective\nand synchronized drone choreographies for real-world deployment.\n","authors":["Vedant Vyas","Martin Schuck","Dinushka O. Dahanaggamaarachchi","Siqi Zhou","Angela P. Schoellig"],"pdf_url":"https://arxiv.org/pdf/2412.08428v1.pdf","comment":"Submitted to ICRA 2025"},{"id":"http://arxiv.org/abs/2412.08398v1","updated":"2024-12-11T14:17:17Z","published":"2024-12-11T14:17:17Z","title":"Grasp Diffusion Network: Learning Grasp Generators from Partial Point\n  Clouds with Diffusion Models in SO(3)xR3","summary":"  Grasping objects successfully from a single-view camera is crucial in many\nrobot manipulation tasks. An approach to solve this problem is to leverage\nsimulation to create large datasets of pairs of objects and grasp poses, and\nthen learn a conditional generative model that can be prompted quickly during\ndeployment. However, the grasp pose data is highly multimodal since there are\nseveral ways to grasp an object. Hence, in this work, we learn a grasp\ngenerative model with diffusion models to sample candidate grasp poses given a\npartial point cloud of an object. A novel aspect of our method is to consider\ndiffusion in the manifold space of rotations and to propose a\ncollision-avoidance cost guidance to improve the grasp success rate during\ninference. To accelerate grasp sampling we use recent techniques from the\ndiffusion literature to achieve faster inference times. We show in simulation\nand real-world experiments that our approach can grasp several objects from raw\ndepth images with $90\\%$ success rate and benchmark it against several\nbaselines.\n","authors":["Joao Carvalho","An T. Le","Philipp Jahr","Qiao Sun","Julen Urain","Dorothea Koert","Jan Peters"],"pdf_url":"https://arxiv.org/pdf/2412.08398v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.06991v3","updated":"2024-12-11T12:57:56Z","published":"2024-05-11T11:45:19Z","title":"PIPE: Process Informed Parameter Estimation, a learning based approach\n  to task generalized system identification","summary":"  We address the problem of robot guided assembly tasks, by using a\nlearning-based approach to identify contact model parameters for known and\nnovel parts. First, a Variational Autoencoder (VAE) is used to extract\ngeometric features of assembly parts. Then, we combine the extracted features\nwith physical knowledge to derive the parameters of a contact model using our\nnewly proposed neural network structure. The measured force from real\nexperiments is used to supervise the predicted forces, thus avoiding the need\nfor ground truth model parameters. Although trained only on a small set of\nassembly parts, good contact model estimation for unknown objects were\nachieved. Our main contribution is the network structure that allows us to\nestimate contact models of assembly tasks depending on the geometry of the part\nto be joined. Where current system identification processes have to record new\ndata for a new assembly process, our method only requires the 3D model of the\nassembly part. We evaluate our method by estimating contact models for\nrobot-guided assembly tasks of pin connectors as well as electronic plugs and\ncompare the results with real experiments.\n","authors":["Constantin Schempp","Christian Friedrich"],"pdf_url":"https://arxiv.org/pdf/2405.06991v3.pdf","comment":"\\copyright 2024 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2412.08346v1","updated":"2024-12-11T12:41:06Z","published":"2024-12-11T12:41:06Z","title":"Grasping by parallel shape matching","summary":"  Grasping is essential in robotic manipulation, yet challenging due to object\nand gripper diversity and real-world complexities. Traditional analytic\napproaches often have long optimization times, while data-driven methods\nstruggle with unseen objects. This paper formulates the problem as a rigid\nshape matching between gripper and object, which optimizes with Annealed Stein\nIterative Closest Point (AS-ICP) and leverages GPU-based parallelization. By\nincorporating the gripper's tool center point and the object's center of mass\ninto the cost function and using a signed distance field of the gripper for\ncollision checking, our method achieves robust grasps with low computational\ntime. Experiments with the Kinova KG3 gripper show an 87.3% success rate and\n0.926 s computation time across various objects and settings, highlighting its\npotential for real-world applications.\n","authors":["Wenzheng Zhang","Fahira Afzal Maken","Tin Lai","Fabio Ramos"],"pdf_url":"https://arxiv.org/pdf/2412.08346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.18562v3","updated":"2024-12-11T11:48:44Z","published":"2024-11-27T18:03:26Z","title":"DexHandDiff: Interaction-aware Diffusion Planning for Adaptive Dexterous\n  Manipulation","summary":"  Dexterous manipulation with contact-rich interactions is crucial for advanced\nrobotics. While recent diffusion-based planning approaches show promise for\nsimpler manipulation tasks, they often produce unrealistic ghost states (e.g.,\nthe object automatically moves without hand contact) or lack adaptability when\nhandling complex sequential interactions. In this work, we introduce\nDexHandDiff, an interaction-aware diffusion planning framework for adaptive\ndexterous manipulation. DexHandDiff models joint state-action dynamics through\na dual-phase diffusion process which consists of pre-interaction contact\nalignment and post-contact goal-directed control, enabling goal-adaptive\ngeneralizable dexterous manipulation. Additionally, we incorporate dynamics\nmodel-based dual guidance and leverage large language models for automated\nguidance function generation, enhancing generalizability for physical\ninteractions and facilitating diverse goal adaptation through language cues.\nExperiments on physical interaction tasks such as door opening, pen and block\nre-orientation, and hammer striking demonstrate DexHandDiff's effectiveness on\ngoals outside training distributions, achieving over twice the average success\nrate (59.2% vs. 29.5%) compared to existing methods. Our framework achieves\n70.0% success on 30-degree door opening, 40.0% and 36.7% on pen and block\nhalf-side re-orientation respectively, and 46.7% on hammer nail half drive,\nhighlighting its robustness and flexibility in contact-rich manipulation.\n","authors":["Zhixuan Liang","Yao Mu","Yixiao Wang","Tianxing Chen","Wenqi Shao","Wei Zhan","Masayoshi Tomizuka","Ping Luo","Mingyu Ding"],"pdf_url":"https://arxiv.org/pdf/2411.18562v3.pdf","comment":"27 pages (new name). Project page: https://dexdiffuser.github.io/"},{"id":"http://arxiv.org/abs/2311.18355v2","updated":"2024-12-11T11:26:43Z","published":"2023-11-30T08:45:19Z","title":"Enabling Robots to Identify Missing Steps in Robot Tasks for Guided\n  Learning from Demonstration","summary":"  Learning from Demonstration (LfD) systems are commonly used to teach robots\nnew tasks by generating a set of skills from user-provided demonstrations.\nThese skills can then be sequenced by planning algorithms to execute complex\ntasks. However, LfD systems typically require a full demonstration of the\nentire task, even when parts of it are already known to the robot. This\nlimitation comes from the system's inability to recognize which sub-tasks are\nalready familiar, leading to a repetitive and burdensome demonstration process\nfor users. In this paper, we introduce a new method for guided demonstrations\nthat reduces this burden, by helping the robot to identify which parts of the\ntask it already knows, considering the overall task goal and the robot's\nexisting skills. In particular, through a combinatorial search, the method\nfinds the smallest necessary change in the initial task conditions that allows\nthe robot to solve the task with its current knowledge. This state is referred\nto as the excuse state. The human demonstrator is then only required to teach\nhow to reach the excuse state (missing sub-task), rather than demonstrating the\nentire task. Empirical results and a pilot user study show that our method\nreduces demonstration time by 61% and decreases the size of demonstrations by\n72%.\n","authors":["Maximilian Diehl","Tathagata Chakraborti","Karinne Ramirez-Amaro"],"pdf_url":"https://arxiv.org/pdf/2311.18355v2.pdf","comment":"To appear at IEEE/SICE International Symposium on System Integrations\n  (SII 2025)"},{"id":"http://arxiv.org/abs/2407.17905v2","updated":"2024-12-11T11:07:02Z","published":"2024-07-25T09:51:09Z","title":"StreamMOS: Streaming Moving Object Segmentation with Multi-View\n  Perception and Dual-Span Memory","summary":"  Moving object segmentation based on LiDAR is a crucial and challenging task\nfor autonomous driving and mobile robotics. Most approaches explore\nspatio-temporal information from LiDAR sequences to predict moving objects in\nthe current frame. However, they often focus on transferring temporal cues in a\nsingle inference and regard every prediction as independent of others. This may\ncause inconsistent segmentation results for the same object in different\nframes. To overcome this issue, we propose a streaming network with a memory\nmechanism, called StreamMOS, to build the association of features and\npredictions among multiple inferences. Specifically, we utilize a short-term\nmemory to convey historical features, which can be regarded as spatial prior of\nmoving objects and adopted to enhance current inference by temporal fusion.\nMeanwhile, we build a long-term memory to store previous predictions and\nexploit them to refine the present forecast at voxel and instance levels\nthrough voting. Besides, we present multi-view encoder with cascade projection\nand asymmetric convolution to extract motion feature of objects in different\nrepresentations. Extensive experiments validate that our algorithm gets\ncompetitive performance on SemanticKITTI and Sipailou Campus datasets. Code\nwill be released at https://github.com/NEU-REAL/StreamMOS.git.\n","authors":["Zhiheng Li","Yubo Cui","Jiexi Zhong","Zheng Fang"],"pdf_url":"https://arxiv.org/pdf/2407.17905v2.pdf","comment":"Accepted for publication at IEEE Robotics and Automation Letters\n  (RAL)"},{"id":"http://arxiv.org/abs/2412.08275v1","updated":"2024-12-11T10:48:17Z","published":"2024-12-11T10:48:17Z","title":"Environmentally Adaptive Control Including Variance Minimization Using\n  Stochastic Predictive Network with Parametric Bias: Application to Mobile\n  Robots","summary":"  In this study, we propose a predictive model composed of a recurrent neural\nnetwork including parametric bias and stochastic elements, and an\nenvironmentally adaptive robot control method including variance minimization\nusing the model. Robots which have flexible bodies or whose states can only be\npartially observed are difficult to modelize, and their predictive models often\nhave stochastic behaviors. In addition, the physical state of the robot and the\nsurrounding environment change sequentially, and so the predictive model can\nchange online. Therefore, in this study, we construct a learning-based\nstochastic predictive model implemented in a neural network embedded with such\ninformation from the experience of the robot, and develop a control method for\nthe robot to avoid unstable motion with large variance while adapting to the\ncurrent environment. This method is verified through a mobile robot in\nsimulation and to the actual robot Fetch.\n","authors":["Kento Kawaharazuka","Koki Shinjo","Yoichiro Kawamura","Kei Okada","Masayuki Inaba"],"pdf_url":"https://arxiv.org/pdf/2412.08275v1.pdf","comment":"Accepted at IROS2021"},{"id":"http://arxiv.org/abs/2412.08270v1","updated":"2024-12-11T10:40:52Z","published":"2024-12-11T10:40:52Z","title":"Task-specific Self-body Controller Acquisition by Musculoskeletal\n  Humanoids: Application to Pedal Control in Autonomous Driving","summary":"  The musculoskeletal humanoid has many benefits that human beings have, but\nthe modeling of its complex flexible body is difficult. Although we have\ndeveloped an online acquisition method of the nonlinear relationship between\njoints and muscles, we could not completely match the actual robot and its\nself-body image. When realizing a certain task, the direct relationship between\nthe control input and task state needs to be learned. So, we construct a neural\nnetwork representing the time-series relationship between the control input and\ntask state, and realize the intended task state by applying the network to a\nreal-time control. In this research, we conduct accelerator pedal control\nexperiments as one application, and verify the effectiveness of this study.\n","authors":["Kento Kawaharazuka","Kei Tsuzuki","Shogo Makino","Moritaka Onitsuka","Koki Shinjo","Yuki Asano","Kei Okada","Koji Kawasaki","Masayuki Inaba"],"pdf_url":"https://arxiv.org/pdf/2412.08270v1.pdf","comment":"Accepted at IROS2019"},{"id":"http://arxiv.org/abs/2412.08261v1","updated":"2024-12-11T10:17:00Z","published":"2024-12-11T10:17:00Z","title":"FLIP: Flow-Centric Generative Planning for General-Purpose Manipulation\n  Tasks","summary":"  We aim to develop a model-based planning framework for world models that can\nbe scaled with increasing model and data budgets for general-purpose\nmanipulation tasks with only language and vision inputs. To this end, we\npresent FLow-centric generative Planning (FLIP), a model-based planning\nalgorithm on visual space that features three key modules: 1. a multi-modal\nflow generation model as the general-purpose action proposal module; 2. a\nflow-conditioned video generation model as the dynamics module; and 3. a\nvision-language representation learning model as the value module. Given an\ninitial image and language instruction as the goal, FLIP can progressively\nsearch for long-horizon flow and video plans that maximize the discounted\nreturn to accomplish the task. FLIP is able to synthesize long-horizon plans\nacross objects, robots, and tasks with image flows as the general action\nrepresentation, and the dense flow information also provides rich guidance for\nlong-horizon video generation. In addition, the synthesized flow and video\nplans can guide the training of low-level control policies for robot execution.\nExperiments on diverse benchmarks demonstrate that FLIP can improve both the\nsuccess rates and quality of long-horizon video plan synthesis and has the\ninteractive world model property, opening up wider applications for future\nworks.\n","authors":["Chongkai Gao","Haozhuo Zhang","Zhixuan Xu","Zhehao Cai","Lin Shao"],"pdf_url":"https://arxiv.org/pdf/2412.08261v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07344v2","updated":"2024-12-11T09:40:48Z","published":"2024-12-10T09:37:25Z","title":"Virtual Reflections on a Dynamic 2D Eye Model Improve Spatial Reference\n  Identification","summary":"  The visible orientation of human eyes creates some transparency about\npeople's spatial attention and other mental states. This leads to a dual role\nfor the eyes as a means of sensing and communication. Accordingly, artificial\neye models are being explored as communication media in human-machine\ninteraction scenarios. One challenge in the use of eye models for communication\nconsists of resolving spatial reference ambiguities, especially for\nscreen-based models. Here, we introduce an approach for overcoming this\nchallenge through the introduction of reflection-like features that are\ncontingent on artificial eye movements. We conducted a user study with 30\nparticipants in which participants had to use spatial references provided by\ndynamic eye models to advance in a fast-paced group interaction task. Compared\nto a non-reflective eye model and a pure reflection mode, their combination in\nthe new approach resulted in a higher identification accuracy and user\nexperience, suggesting a synergistic benefit.\n","authors":["Matti Krüger","Yutaka Oshima","Yu Fang"],"pdf_url":"https://arxiv.org/pdf/2412.07344v2.pdf","comment":"15 pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2412.08195v1","updated":"2024-12-11T08:36:36Z","published":"2024-12-11T08:36:36Z","title":"Semantic Scene Completion Based 3D Traversability Estimation for\n  Off-Road Terrains","summary":"  Off-road environments present significant challenges for autonomous ground\nvehicles due to the absence of structured roads and the presence of complex\nobstacles, such as uneven terrain, vegetation, and occlusions. Traditional\nperception algorithms, designed primarily for structured environments, often\nfail under these conditions, leading to inaccurate traversability estimations.\nIn this paper, ORDformer, a novel multimodal method that combines LiDAR point\nclouds with monocular images, is proposed to generate dense traversable\noccupancy predictions from a forward-facing perspective. By integrating\nmultimodal data, environmental feature extraction is enhanced, which is crucial\nfor accurate occupancy estimation in complex terrains. Furthermore, RELLIS-OCC,\na dataset with 3D traversable occupancy annotations, is introduced,\nincorporating geometric features such as step height, slope, and unevenness.\nThrough a comprehensive analysis of vehicle obstacle-crossing conditions and\nthe incorporation of vehicle body structure constraints, four traversability\ncost labels are generated: lethal, medium-cost, low-cost, and free.\nExperimental results demonstrate that ORDformer outperforms existing approaches\nin 3D traversable area recognition, particularly in off-road environments with\nirregular geometries and partial occlusions. Specifically, ORDformer achieves\nover a 20\\% improvement in scene completion IoU compared to other models. The\nproposed framework is scalable and adaptable to various vehicle platforms,\nallowing for adjustments to occupancy grid parameters and the integration of\nadvanced dynamic models for traversability cost estimation.\n","authors":["Zitong Chen","Chao Sun","Shida Nie","Chen Min","Changjiu Ning","Haoyu Li","Bo Wang"],"pdf_url":"https://arxiv.org/pdf/2412.08195v1.pdf","comment":"12 pages,14 figures"},{"id":"http://arxiv.org/abs/2402.17504v2","updated":"2024-12-11T08:02:17Z","published":"2024-02-27T13:43:55Z","title":"Towards Aerial Collaborative Stereo: Real-Time Cross-Camera Feature\n  Association and Relative Pose Estimation for UAVs","summary":"  The collaborative visual perception of multiple Unmanned Aerial Vehicles\n(UAVs) has increasingly become a research hotspot. Compared to a single UAV\nequipped with a short-baseline stereo camera, multi-UAV collaborative vision\noffers a wide and variable baseline, providing potential benefits in flexible\nand large-scale depth perception. In this paper, we propose the concept of a\ncollaborative stereo camera, where the left and right cameras are mounted on\ntwo UAVs that share an overlapping FOV. Considering the dynamic flight of two\nUAVs in the real world, the FOV and relative pose of the left and right cameras\nare continuously changing. Compared to fixed-baseline stereo cameras, this\naerial collaborative stereo system introduces two challenges, which are highly\nreal-time requirements for dynamic cross-camera stereo feature association and\nrelative pose estimation of left and right cameras. To address these\nchallenges, we first propose a real-time dual-channel feature association\nalgorithm with a guidance-prediction structure. Then, we propose a Relative\nMulti-State Constrained Kalman Filter (Rel-MSCKF) algorithm to estimate the\nrelative pose by fusing co-visual features and UAVs' visual-inertial odometry\n(VIO). Extensive experiments are performed on the popular onboard computer\nNVIDIA NX. Results on the resource-constrained platform show that the real-time\nperformance of the dual-channel feature association is significantly superior\nto traditional methods. The convergence of Rel-MSCKF is assessed under\ndifferent initial baseline errors. In the end, we present a potential\napplication of aerial collaborative stereo for remote mapping obstacles in\nurban scenarios. We hope this work can serve as a foundational study for more\nmulti-UAV collaborative vision research. Online video:\nhttps://youtu.be/avxMuOf5Qcw\n","authors":["Zhaoying Wang","Wei Dong"],"pdf_url":"https://arxiv.org/pdf/2402.17504v2.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2412.08135v1","updated":"2024-12-11T06:44:22Z","published":"2024-12-11T06:44:22Z","title":"DOGE: An Extrinsic Orientation and Gyroscope Bias Estimation for\n  Visual-Inertial Odometry Initialization","summary":"  Most existing visual-inertial odometry (VIO) initialization methods rely on\naccurate pre-calibrated extrinsic parameters. However, during long-term use,\nirreversible structural deformation caused by temperature changes, mechanical\nsqueezing, etc. will cause changes in extrinsic parameters, especially in the\nrotational part. Existing initialization methods that simultaneously estimate\nextrinsic parameters suffer from poor robustness, low precision, and long\ninitialization latency due to the need for sufficient translational motion. To\naddress these problems, we propose a novel VIO initialization method, which\njointly considers extrinsic orientation and gyroscope bias within the normal\nepipolar constraints, achieving higher precision and better robustness without\ndelayed rotational calibration. First, a rotation-only constraint is designed\nfor extrinsic orientation and gyroscope bias estimation, which tightly couples\ngyroscope measurements and visual observations and can be solved in\npure-rotation cases. Second, we propose a weighting strategy together with a\nfailure detection strategy to enhance the precision and robustness of the\nestimator. Finally, we leverage Maximum A Posteriori to refine the results\nbefore enough translation parallax comes. Extensive experiments have\ndemonstrated that our method outperforms the state-of-the-art methods in both\naccuracy and robustness while maintaining competitive efficiency.\n","authors":["Zewen Xu","Yijia He","Hao Wei","Yihong Wu"],"pdf_url":"https://arxiv.org/pdf/2412.08135v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08133v1","updated":"2024-12-11T06:41:51Z","published":"2024-12-11T06:41:51Z","title":"Intelligent Electric Power Steering: Artificial Intelligence Integration\n  Enhances Vehicle Safety and Performance","summary":"  Electric Power Steering (EPS) systems utilize electric motors to aid users in\nsteering their vehicles, which provide additional precise control and reduced\nenergy consumption compared to traditional hydraulic systems. EPS technology\nprovides safety,control and efficiency.. This paper explains the integration of\nArtificial Intelligence (AI) into Electric Power Steering (EPS) systems,\nfocusing on its role in enhancing the safety, and adaptability across diverse\ndriving conditions. We explore significant development in AI-driven EPS,\nincluding predictive control algorithms, adaptive torque management systems,\nand data-driven diagnostics. The paper presents case studies of AI applications\nin EPS, such as Lane centering control (LCC), Automated Parking Systems, and\nAutonomous Vehicle Steering, while considering the challenges, limitations, and\nfuture prospects of this technology. This article discusses current\ndevelopments in AI-driven EPS, emphasizing on the benefits of improved safety,\nadaptive control, and predictive maintenance. Challenges in integrating AI in\nEPS systems. This paper addresses cybersecurity risks, ethical concerns, and\ntechnical limitations,, along with next steps for research and implementation\nin autonomous, and connected vehicles.\n","authors":["Vikas Vyas","Sneha Sudhir Shetiya"],"pdf_url":"https://arxiv.org/pdf/2412.08133v1.pdf","comment":"IEEE Summit on Reliability, Availability and Serviceability, 2024"},{"id":"http://arxiv.org/abs/2412.08121v1","updated":"2024-12-11T06:14:50Z","published":"2024-12-11T06:14:50Z","title":"DTAA: A Detect, Track and Avoid Architecture for navigation in spaces\n  with Multiple Velocity Objects","summary":"  Proactive collision avoidance measures are imperative in environments where\nhumans and robots coexist. Moreover, the introduction of high quality legged\nrobots into workplaces highlighted the crucial role of a robust, fully\nautonomous safety solution for robots to be viable in shared spaces or in\nco-existence with humans. This article establishes for the first time ever an\ninnovative Detect-Track-and-Avoid Architecture (DTAA) to enhance safety and\noverall mission performance. The proposed novel architectyre has the merit ot\nintegrating object detection using YOLOv8, utilizing Ultralytics embedded\nobject tracking, and state estimation of tracked objects through Kalman\nfilters. Moreover, a novel heuristic clustering is employed to facilitate\nactive avoidance of multiple closely positioned objects with similar\nvelocities, creating sets of unsafe spaces for the Nonlinear Model Predictive\nController (NMPC) to navigate around. The NMPC identifies the most hazardous\nunsafe space, considering not only their current positions but also their\npredicted future locations. In the sequel, the NMPC calculates maneuvers to\nguide the robot along a path planned by D$^{*}_{+}$ towards its intended\ndestination, while maintaining a safe distance to all identified obstacles. The\nefficacy of the novelly suggested DTAA framework is being validated by\nReal-life experiments featuring a Boston Dynamics Spot robot that demonstrates\nthe robot's capability to consistently maintain a safe distance from humans in\ndynamic subterranean, urban indoor, and outdoor environments.\n","authors":["Samuel Nordström","Björn Lindquist","George Nikolakopoulos"],"pdf_url":"https://arxiv.org/pdf/2412.08121v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08120v1","updated":"2024-12-11T06:13:38Z","published":"2024-12-11T06:13:38Z","title":"Dense Depth from Event Focal Stack","summary":"  We propose a method for dense depth estimation from an event stream generated\nwhen sweeping the focal plane of the driving lens attached to an event camera.\nIn this method, a depth map is inferred from an ``event focal stack'' composed\nof the event stream using a convolutional neural network trained with\nsynthesized event focal stacks. The synthesized event stream is created from a\nfocal stack generated by Blender for any arbitrary 3D scene. This allows for\ntraining on scenes with diverse structures. Additionally, we explored methods\nto eliminate the domain gap between real event streams and synthetic event\nstreams. Our method demonstrates superior performance over a depth-from-defocus\nmethod in the image domain on synthetic and real datasets.\n","authors":["Kenta Horikawa","Mariko Isogawa","Hideo Saito","Shohei Mori"],"pdf_url":"https://arxiv.org/pdf/2412.08120v1.pdf","comment":"Accepted at WACV2025"},{"id":"http://arxiv.org/abs/2412.08102v1","updated":"2024-12-11T05:05:16Z","published":"2024-12-11T05:05:16Z","title":"Verification and Validation of a Vision-Based Landing System for\n  Autonomous VTOL Air Taxis","summary":"  Autonomous air taxis are poised to revolutionize urban mass transportation,\nhowever, ensuring their safety and reliability remains an open challenge.\nValidating autonomy solutions on air taxis in the real world presents\ncomplexities, risks, and costs that further convolute this challenge.\nVerification and Validation (V&V) frameworks play a crucial role in the design\nand development of highly reliable systems by formally verifying safety\nproperties and validating algorithm behavior across diverse operational\nscenarios. Advancements in high-fidelity simulators have significantly enhanced\ntheir capability to emulate real-world conditions, encouraging their use for\nvalidating autonomous air taxi solutions, especially during early development\nstages. This evolution underscores the growing importance of simulation\nenvironments, not only as complementary tools to real-world testing but as\nessential platforms for evaluating algorithms in a controlled, reproducible,\nand scalable manner.\n  This work presents a V&V framework for a vision-based landing system for air\ntaxis with vertical take-off and landing (VTOL) capabilities. Specifically, we\nuse Verse, a tool for formal verification, to model and verify the safety of\nthe system by obtaining and analyzing the reachable sets. To conduct this\nanalysis, we utilize a photorealistic simulation environment. The simulation\nenvironment, built on Unreal Engine, provides realistic terrain, weather, and\nsensor characteristics to emulate real-world conditions with high fidelity. To\nvalidate the safety analysis results, we conduct extensive scenario-based\ntesting to assess the reachability set and robustness of the landing algorithm\nin various conditions. This approach showcases the representativeness of\nhigh-fidelity simulators, offering an effective means to analyze and refine\nalgorithms before real-world deployment.\n","authors":["Ayoosh Bansal","Duo Wang","Mikael Yeghiazaryan","Yangge Li","Chuyuan Tao","Hyung-Jin Yoon","Prateek Arora","Christos Papachristos","Petros Voulgaris","Sayan Mitra","Lui Sha","Naira Hovakimyan"],"pdf_url":"https://arxiv.org/pdf/2412.08102v1.pdf","comment":"To be published in AIAA SciTech 2025 Forum"},{"id":"http://arxiv.org/abs/2412.08096v1","updated":"2024-12-11T04:37:15Z","published":"2024-12-11T04:37:15Z","title":"THUD++: Large-Scale Dynamic Indoor Scene Dataset and Benchmark for\n  Mobile Robots","summary":"  Most existing mobile robotic datasets primarily capture static scenes,\nlimiting their utility for evaluating robotic performance in dynamic\nenvironments. To address this, we present a mobile robot oriented large-scale\nindoor dataset, denoted as THUD++ (TsingHua University Dynamic) robotic\ndataset, for dynamic scene understanding. Our current dataset includes 13\nlarge-scale dynamic scenarios, combining both real-world and synthetic data\ncollected with a real robot platform and a physical simulation platform,\nrespectively. The RGB-D dataset comprises over 90K image frames, 20M 2D/3D\nbounding boxes of static and dynamic objects, camera poses, and IMU. The\ntrajectory dataset covers over 6,000 pedestrian trajectories in indoor scenes.\nAdditionally, the dataset is augmented with a Unity3D-based simulation\nplatform, allowing researchers to create custom scenes and test algorithms in a\ncontrolled environment. We evaluate state-of-the-art methods on THUD++ across\nmainstream indoor scene understanding tasks, e.g., 3D object detection,\nsemantic segmentation, relocalization, pedestrian trajectory prediction, and\nnavigation. Our experiments highlight the challenges mobile robots encounter in\nindoor environments, especially when navigating in complex, crowded, and\ndynamic scenes. By sharing this dataset, we aim to accelerate the development\nand testing of mobile robot algorithms, contributing to real-world robotic\napplications.\n","authors":["Zeshun Li","Fuhao Li","Wanting Zhang","Zijie Zheng","Xueping Liu","Yongjin Liu","Long Zeng"],"pdf_url":"https://arxiv.org/pdf/2412.08096v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2406.19791"},{"id":"http://arxiv.org/abs/2411.09942v3","updated":"2024-12-11T04:08:26Z","published":"2024-11-15T04:42:15Z","title":"ALPHA-$α$ and Bi-ACT Are All You Need: Importance of Position and\n  Force Information/Control for Imitation Learning of Unimanual and Bimanual\n  Robotic Manipulation with Low-Cost System","summary":"  Autonomous manipulation in everyday tasks requires flexible action generation\nto handle complex, diverse real-world environments, such as objects with\nvarying hardness and softness. Imitation Learning (IL) enables robots to learn\ncomplex tasks from expert demonstrations. However, a lot of existing methods\nrely on position/unilateral control, leaving challenges in tasks that require\nforce information/control, like carefully grasping fragile or varying-hardness\nobjects. As the need for diverse controls increases, there are demand for\nlow-cost bimanual robots that consider various motor inputs. To address these\nchallenges, we introduce Bilateral Control-Based Imitation Learning via Action\nChunking with Transformers(Bi-ACT) and\"A\" \"L\"ow-cost \"P\"hysical \"Ha\"rdware\nConsidering Diverse Motor Control Modes for Research in Everyday Bimanual\nRobotic Manipulation (ALPHA-$\\alpha$). Bi-ACT leverages bilateral control to\nutilize both position and force information, enhancing the robot's adaptability\nto object characteristics such as hardness, shape, and weight. The concept of\nALPHA-$\\alpha$ is affordability, ease of use, repairability, ease of assembly,\nand diverse control modes (position, velocity, torque), allowing\nresearchers/developers to freely build control systems using ALPHA-$\\alpha$. In\nour experiments, we conducted a detailed analysis of Bi-ACT in unimanual\nmanipulation tasks, confirming its superior performance and adaptability\ncompared to Bi-ACT without force control. Based on these results, we applied\nBi-ACT to bimanual manipulation tasks. Experimental results demonstrated high\nsuccess rates in coordinated bimanual operations across multiple tasks. The\neffectiveness of the Bi-ACT and ALPHA-$\\alpha$ can be seen through\ncomprehensive real-world experiments. Video available at:\nhttps://mertcookimg.github.io/alpha-biact/\n","authors":["Masato Kobayashi","Thanpimon Buamanee","Takumi Kobayashi"],"pdf_url":"https://arxiv.org/pdf/2411.09942v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05493v2","updated":"2024-12-11T02:31:56Z","published":"2024-09-09T10:43:18Z","title":"DexDiff: Towards Extrinsic Dexterity Manipulation of Ungraspable Objects\n  in Unrestricted Environments","summary":"  Grasping large and flat objects (e.g. a book or a pan) is often regarded as\nan ungraspable task, which poses significant challenges due to the unreachable\ngrasping poses. Previous works leverage Extrinsic Dexterity like walls or table\nedges to grasp such objects. However, they are limited to task-specific\npolicies and lack task planning to find pre-grasp conditions. This makes it\ndifficult to adapt to various environments and extrinsic dexterity constraints.\nTherefore, we present DexDiff, a robust robotic manipulation method for\nlong-horizon planning with extrinsic dexterity. Specifically, we utilize a\nvision-language model (VLM) to perceive the environmental state and generate\nhigh-level task plans, followed by a goal-conditioned action diffusion (GCAD)\nmodel to predict the sequence of low-level actions. This model learns the\nlow-level policy from offline data with the cumulative reward guided by\nhigh-level planning as the goal condition, which allows for improved prediction\nof robot actions. Experimental results demonstrate that our method not only\neffectively performs ungraspable tasks but also generalizes to previously\nunseen objects. It outperforms baselines by a 47% higher success rate in\nsimulation and facilitates efficient deployment and manipulation in real-world\nscenarios.\n","authors":["Chengzhong Ma","Houxue Yang","Hanbo Zhang","Zeyang Liu","Chao Zhao","Jian Tang","Xuguang Lan","Nanning Zheng"],"pdf_url":"https://arxiv.org/pdf/2409.05493v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08020v1","updated":"2024-12-11T02:00:25Z","published":"2024-12-11T02:00:25Z","title":"Intelligent Control of Robotic X-ray Devices using a Language-promptable\n  Digital Twin","summary":"  Natural language offers a convenient, flexible interface for controlling\nrobotic C-arm X-ray systems, making advanced functionality and controls\naccessible. However, enabling language interfaces requires specialized AI\nmodels that interpret X-ray images to create a semantic representation for\nreasoning. The fixed outputs of such AI models limit the functionality of\nlanguage controls. Incorporating flexible, language-aligned AI models prompted\nthrough language enables more versatile interfaces for diverse tasks and\nprocedures. Using a language-aligned foundation model for X-ray image\nsegmentation, our system continually updates a patient digital twin based on\nsparse reconstructions of desired anatomical structures. This supports\nautonomous capabilities such as visualization, patient-specific viewfinding,\nand automatic collimation from novel viewpoints, enabling commands 'Focus in on\nthe lower lumbar vertebrae.' In a cadaver study, users visualized, localized,\nand collimated structures across the torso using verbal commands, achieving 84%\nend-to-end success. Post hoc analysis of randomly oriented images showed our\npatient digital twin could localize 35 commonly requested structures to within\n51.68 mm, enabling localization and isolation from arbitrary orientations. Our\nresults demonstrate how intelligent robotic X-ray systems can incorporate\nphysicians' expressed intent directly. While existing foundation models for\nintra-operative X-ray analysis exhibit failure modes, as they improve, they can\nfacilitate highly flexible, intelligent robotic C-arms.\n","authors":["Benjamin D. Killeen","Anushri Suresh","Catalina Gomez","Blanca Inigo","Christopher Bailey","Mathias Unberath"],"pdf_url":"https://arxiv.org/pdf/2412.08020v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08019v1","updated":"2024-12-11T01:56:47Z","published":"2024-12-11T01:56:47Z","title":"Ask1: Development and Reinforcement Learning-Based Control of a Custom\n  Quadruped Robot","summary":"  In this work, we present the design, development, and experimental validation\nof a custom-built quadruped robot, Ask1. The Ask1 robot shares similar\nmorphology with the Unitree Go1, but features custom hardware components and a\ndifferent control architecture. We transfer and extend previous reinforcement\nlearning (RL)-based control methods to the Ask1 robot, demonstrating the\napplicability of our approach in real-world scenarios. By eliminating the need\nfor Adversarial Motion Priors (AMP) and reference trajectories, we introduce a\nnovel reward function to guide the robot's motion style. We demonstrate the\ngeneralization capability of the proposed RL algorithm by training it on both\nthe Go1 and Ask1 robots. Simulation and real-world experiments validate the\neffectiveness of this method, showing that Ask1, like the Go1, is capable of\nnavigating various rugged terrains.\n","authors":["Yuxing Lu","Yufei Xue","Guiyang Xin","Chenkun Qi","Yan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2412.08019v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07990v1","updated":"2024-12-11T00:02:48Z","published":"2024-12-11T00:02:48Z","title":"Adaptive Querying for Reward Learning from Human Feedback","summary":"  Learning from human feedback is a popular approach to train robots to adapt\nto user preferences and improve safety. Existing approaches typically consider\na single querying (interaction) format when seeking human feedback and do not\nleverage multiple modes of user interaction with a robot. We examine how to\nlearn a penalty function associated with unsafe behaviors, such as side\neffects, using multiple forms of human feedback, by optimizing the query state\nand feedback format. Our framework for adaptive feedback selection enables\nquerying for feedback in critical states in the most informative format, while\naccounting for the cost and probability of receiving feedback in a certain\nformat. We employ an iterative, two-phase approach which first selects critical\nstates for querying, and then uses information gain to select a feedback format\nfor querying across the sampled critical states. Our evaluation in simulation\ndemonstrates the sample efficiency of our approach.\n","authors":["Yashwanthi Anand","Sandhya Saisubramanian"],"pdf_url":"https://arxiv.org/pdf/2412.07990v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.11380v3","updated":"2024-12-11T20:36:26Z","published":"2024-05-18T19:58:44Z","title":"Meta-Control: Automatic Model-based Control Synthesis for Heterogeneous\n  Robot Skills","summary":"  The requirements for real-world manipulation tasks are diverse and often\nconflicting; some tasks require precise motion while others require force\ncompliance; some tasks require avoidance of certain regions, while others\nrequire convergence to certain states. Satisfying these varied requirements\nwith a fixed state-action representation and control strategy is challenging,\nimpeding the development of a universal robotic foundation model. In this work,\nwe propose Meta-Control, the first LLM-enabled automatic control synthesis\napproach that creates customized state representations and control strategies\ntailored to specific tasks. Our core insight is that a meta-control system can\nbe built to automate the thought process that human experts use to design\ncontrol systems. Specifically, human experts heavily use a model-based,\nhierarchical (from abstract to concrete) thought model, then compose various\ndynamic models and controllers together to form a control system. Meta-Control\nmimics the thought model and harnesses LLM's extensive control knowledge with\nSocrates' \"art of midwifery\" to automate the thought process. Meta-Control\nstands out for its fully model-based nature, allowing rigorous analysis,\ngeneralizability, robustness, efficient parameter tuning, and reliable\nreal-time execution.\n","authors":["Tianhao Wei","Liqian Ma","Rui Chen","Weiye Zhao","Changliu Liu"],"pdf_url":"https://arxiv.org/pdf/2405.11380v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08757v1","updated":"2024-12-11T19:55:38Z","published":"2024-12-11T19:55:38Z","title":"Vision-based indoor localization of nano drones in controlled\n  environment with its applications","summary":"  Navigating unmanned aerial vehicles in environments where GPS signals are\nunavailable poses a compelling and intricate challenge. This challenge is\nfurther heightened when dealing with Nano Aerial Vehicles (NAVs) due to their\ncompact size, payload restrictions, and computational capabilities. This paper\nproposes an approach for localization using off-board computing, an off-board\nmonocular camera, and modified open-source algorithms. The proposed method uses\nthree parallel proportional-integral-derivative controllers on the off-board\ncomputer to provide velocity corrections via wireless communication,\nstabilizing the NAV in a custom-controlled environment. Featuring a 3.1cm\nlocalization error and a modest setup cost of 50 USD, this approach proves\noptimal for environments where cost considerations are paramount. It is\nespecially well-suited for applications like teaching drone control in academic\ninstitutions, where the specified error margin is deemed acceptable. Various\napplications are designed to validate the proposed technique, such as landing\nthe NAV on a moving ground vehicle, path planning in a 3D space, and localizing\nmulti-NAVs. The created package is openly available at\nhttps://github.com/simmubhangu/eyantra_drone to foster research in this field.\n","authors":["Simranjeet Singh","Amit Kumar","Fayyaz Pocker Chemban","Vikrant Fernandes","Lohit Penubaku","Kavi Arya"],"pdf_url":"https://arxiv.org/pdf/2412.08757v1.pdf","comment":"26 pages. Submitted to Cyber-Physical Systems journal"},{"id":"http://arxiv.org/abs/2412.09647v1","updated":"2024-12-11T06:35:18Z","published":"2024-12-11T06:35:18Z","title":"Bench2Drive-R: Turning Real World Data into Reactive Closed-Loop\n  Autonomous Driving Benchmark by Generative Model","summary":"  For end-to-end autonomous driving (E2E-AD), the evaluation system remains an\nopen problem. Existing closed-loop evaluation protocols usually rely on\nsimulators like CARLA being less realistic; while NAVSIM using real-world\nvision data, yet is limited to fixed planning trajectories in short horizon and\nassumes other agents are not reactive.\n  We introduce Bench2Drive-R, a generative framework that enables reactive\nclosed-loop evaluation. Unlike existing video generative models for AD, the\nproposed designs are tailored for interactive simulation, where sensor\nrendering and behavior rollout are decoupled by applying a separate behavioral\ncontroller to simulate the reactions of surrounding agents. As a result, the\nrenderer could focus on image fidelity, control adherence, and spatial-temporal\ncoherence. For temporal consistency, due to the step-wise interaction nature of\nsimulation, we design a noise modulating temporal encoder with Gaussian\nblurring to encourage long-horizon autoregressive rollout of image sequences\nwithout deteriorating distribution shifts. For spatial consistency, a retrieval\nmechanism, which takes the spatially nearest images as references, is\nintroduced to to ensure scene-level rendering fidelity during the generation\nprocess. The spatial relations between target and reference are explicitly\nmodeled with 3D relative position encodings and the potential over-reliance of\nreference images is mitigated with hierarchical sampling and classifier-free\nguidance.\n  We compare the generation quality of Bench2Drive-R with existing generative\nmodels and achieve state-of-the-art performance. We further integrate\nBench2Drive-R into nuPlan and evaluate the generative qualities with\nclosed-loop simulation results. We will open source our code.\n","authors":["Junqi You","Xiaosong Jia","Zhiyuan Zhang","Yutao Zhu","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2412.09647v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10447v1","updated":"2024-12-11T18:54:22Z","published":"2024-12-11T18:54:22Z","title":"TidyBot++: An Open-Source Holonomic Mobile Manipulator for Robot\n  Learning","summary":"  Exploiting the promise of recent advances in imitation learning for mobile\nmanipulation will require the collection of large numbers of human-guided\ndemonstrations. This paper proposes an open-source design for an inexpensive,\nrobust, and flexible mobile manipulator that can support arbitrary arms,\nenabling a wide range of real-world household mobile manipulation tasks.\nCrucially, our design uses powered casters to enable the mobile base to be\nfully holonomic, able to control all planar degrees of freedom independently\nand simultaneously. This feature makes the base more maneuverable and\nsimplifies many mobile manipulation tasks, eliminating the kinematic\nconstraints that create complex and time-consuming motions in nonholonomic\nbases. We equip our robot with an intuitive mobile phone teleoperation\ninterface to enable easy data acquisition for imitation learning. In our\nexperiments, we use this interface to collect data and show that the resulting\nlearned policies can successfully perform a variety of common household mobile\nmanipulation tasks.\n","authors":["Jimmy Wu","William Chong","Robert Holmberg","Aaditya Prasad","Yihuai Gao","Oussama Khatib","Shuran Song","Szymon Rusinkiewicz","Jeannette Bohg"],"pdf_url":"https://arxiv.org/pdf/2412.10447v1.pdf","comment":"Conference on Robot Learning (CoRL), 2024. Project page:\n  https://tidybot2.github.io"},{"id":"http://arxiv.org/abs/2412.10444v1","updated":"2024-12-11T16:11:13Z","published":"2024-12-11T16:11:13Z","title":"Boundary Exploration of Next Best View Policy in 3D Robotic Scanning","summary":"  The Next Best View (NBV) problem is a pivotal challenge in 3D robotic\nscanning, with the potential to greatly improve the efficiency of object\ncapture and reconstruction. Current methods for determining the NBV often\noverlook view overlaps, assume a virtual origin point for the camera's focus,\nand rely on voxel representations of 3D data. To address these issues and\nimprove the practicality of scanning unknown objects, we propose an NBV policy\nin which the next view explores the boundary of the scanned point cloud, and\nthe overlap is intrinsically considered. The scanning distance or camera\nworking distance is adjustable and flexible. To this end, a model-based\napproach is proposed where the next sensor positions are searched iteratively\nbased on a reference model. A score is calculated by considering the overlaps\nbetween newly scanned and existing data, as well as the final convergence.\nAdditionally, following the boundary exploration idea, a deep learning network,\nBoundary Exploration NBV network (BENBV-Net), is designed and proposed, which\ncan be used to predict the NBV directly from the scanned data without requiring\nthe reference model. It predicts the scores for given boundaries, and the\nboundary with the highest score is selected as the target point of the next\nbest view. BENBV-Net improves the speed of NBV generation while maintaining the\nperformance of the model-based approach. Our proposed methods are evaluated and\ncompared with existing approaches on the ShapeNet, ModelNet, and 3D Repository\ndatasets. Experimental results demonstrate that our approach outperforms others\nin terms of scanning efficiency and overlap, both of which are crucial for\npractical 3D scanning applications. The related code is released at\n\\url{github.com/leihui6/BENBV}.\n","authors":["Leihui Li","Xuping Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.10444v1.pdf","comment":"Will be submitted to IROS 2025"},{"id":"http://arxiv.org/abs/2412.10439v1","updated":"2024-12-11T09:50:35Z","published":"2024-12-11T09:50:35Z","title":"CogNav: Cognitive Process Modeling for Object Goal Navigation with LLMs","summary":"  Object goal navigation (ObjectNav) is a fundamental task of embodied AI that\nrequires the agent to find a target object in unseen environments. This task is\nparticularly challenging as it demands both perceptual and cognitive processes\nfor effective perception and decision-making. While perception has gained\nsignificant progress powered by the rapidly developed visual foundation models,\nthe progress on the cognitive side remains limited to either implicitly\nlearning from massive navigation demonstrations or explicitly leveraging\npre-defined heuristic rules. Inspired by neuroscientific evidence that humans\nconsistently update their cognitive states while searching for objects in\nunseen environments, we present CogNav, which attempts to model this cognitive\nprocess with the help of large language models. Specifically, we model the\ncognitive process with a finite state machine composed of cognitive states\nranging from exploration to identification. The transitions between the states\nare determined by a large language model based on an online built heterogeneous\ncognitive map containing spatial and semantic information of the scene being\nexplored. Extensive experiments on both synthetic and real-world environments\ndemonstrate that our cognitive modeling significantly improves ObjectNav\nefficiency, with human-like navigation behaviors. In an open-vocabulary and\nzero-shot setting, our method advances the SOTA of the HM3D benchmark from\n69.3% to 87.2%. The code and data will be released.\n","authors":["Yihan Cao","Jiazhao Zhang","Zhinan Yu","Shuzhen Liu","Zheng Qin","Qin Zou","Bo Du","Kai Xu"],"pdf_url":"https://arxiv.org/pdf/2412.10439v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10431v1","updated":"2024-12-11T03:11:44Z","published":"2024-12-11T03:11:44Z","title":"CUPS: Improving Human Pose-Shape Estimators with Conformalized Deep\n  Uncertainty","summary":"  We introduce CUPS, a novel method for learning sequence-to-sequence 3D human\nshapes and poses from RGB videos with uncertainty quantification. To improve on\ntop of prior work, we develop a method to generate and score multiple\nhypotheses during training, effectively integrating uncertainty quantification\ninto the learning process. This process results in a deep uncertainty function\nthat is trained end-to-end with the 3D pose estimator. Post-training, the\nlearned deep uncertainty model is used as the conformity score, which can be\nused to calibrate a conformal predictor in order to assess the quality of the\noutput prediction. Since the data in human pose-shape learning is not fully\nexchangeable, we also present two practical bounds for the coverage gap in\nconformal prediction, developing theoretical backing for the uncertainty bound\nof our model. Our results indicate that by taking advantage of deep uncertainty\nwith conformal prediction, our method achieves state-of-the-art performance\nacross various metrics and datasets while inheriting the probabilistic\nguarantees of conformal prediction.\n","authors":["Harry Zhang","Luca Carlone"],"pdf_url":"https://arxiv.org/pdf/2412.10431v1.pdf","comment":null}]},"2024-12-12T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2412.09625v1","updated":"2024-12-12T18:59:59Z","published":"2024-12-12T18:59:59Z","title":"Illusion3D: 3D Multiview Illusion with 2D Diffusion Priors","summary":"  Automatically generating multiview illusions is a compelling challenge, where\na single piece of visual content offers distinct interpretations from different\nviewing perspectives. Traditional methods, such as shadow art and wire art,\ncreate interesting 3D illusions but are limited to simple visual outputs (i.e.,\nfigure-ground or line drawing), restricting their artistic expressiveness and\npractical versatility. Recent diffusion-based illusion generation methods can\ngenerate more intricate designs but are confined to 2D images. In this work, we\npresent a simple yet effective approach for creating 3D multiview illusions\nbased on user-provided text prompts or images. Our method leverages a\npre-trained text-to-image diffusion model to optimize the textures and geometry\nof neural 3D representations through differentiable rendering. When viewed from\nmultiple angles, this produces different interpretations. We develop several\ntechniques to improve the quality of the generated 3D multiview illusions. We\ndemonstrate the effectiveness of our approach through extensive experiments and\nshowcase illusion generation with diverse 3D forms.\n","authors":["Yue Feng","Vaibhav Sanjay","Spencer Lutz","Badour AlBahar","Songwei Ge","Jia-Bin Huang"],"pdf_url":"https://arxiv.org/pdf/2412.09625v1.pdf","comment":"Project page: https://3d-multiview-illusion.github.io/"},{"id":"http://arxiv.org/abs/2412.09626v1","updated":"2024-12-12T18:59:59Z","published":"2024-12-12T18:59:59Z","title":"FreeScale: Unleashing the Resolution of Diffusion Models via Tuning-Free\n  Scale Fusion","summary":"  Visual diffusion models achieve remarkable progress, yet they are typically\ntrained at limited resolutions due to the lack of high-resolution data and\nconstrained computation resources, hampering their ability to generate\nhigh-fidelity images or videos at higher resolutions. Recent efforts have\nexplored tuning-free strategies to exhibit the untapped potential\nhigher-resolution visual generation of pre-trained models. However, these\nmethods are still prone to producing low-quality visual content with repetitive\npatterns. The key obstacle lies in the inevitable increase in high-frequency\ninformation when the model generates visual content exceeding its training\nresolution, leading to undesirable repetitive patterns deriving from the\naccumulated errors. To tackle this challenge, we propose FreeScale, a\ntuning-free inference paradigm to enable higher-resolution visual generation\nvia scale fusion. Specifically, FreeScale processes information from different\nreceptive scales and then fuses it by extracting desired frequency components.\nExtensive experiments validate the superiority of our paradigm in extending the\ncapabilities of higher-resolution visual generation for both image and video\nmodels. Notably, compared with the previous best-performing method, FreeScale\nunlocks the generation of 8k-resolution images for the first time.\n","authors":["Haonan Qiu","Shiwei Zhang","Yujie Wei","Ruihang Chu","Hangjie Yuan","Xiang Wang","Yingya Zhang","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2412.09626v1.pdf","comment":"Project Page: http://haonanqiu.com/projects/FreeScale.html"},{"id":"http://arxiv.org/abs/2412.09627v1","updated":"2024-12-12T18:59:59Z","published":"2024-12-12T18:59:59Z","title":"Doe-1: Closed-Loop Autonomous Driving with Large World Model","summary":"  End-to-end autonomous driving has received increasing attention due to its\npotential to learn from large amounts of data. However, most existing methods\nare still open-loop and suffer from weak scalability, lack of high-order\ninteractions, and inefficient decision-making. In this paper, we explore a\nclosed-loop framework for autonomous driving and propose a large Driving wOrld\nmodEl (Doe-1) for unified perception, prediction, and planning. We formulate\nautonomous driving as a next-token generation problem and use multi-modal\ntokens to accomplish different tasks. Specifically, we use free-form texts\n(i.e., scene descriptions) for perception and generate future predictions\ndirectly in the RGB space with image tokens. For planning, we employ a\nposition-aware tokenizer to effectively encode action into discrete tokens. We\ntrain a multi-modal transformer to autoregressively generate perception,\nprediction, and planning tokens in an end-to-end and unified manner.\nExperiments on the widely used nuScenes dataset demonstrate the effectiveness\nof Doe-1 in various tasks including visual question-answering,\naction-conditioned video generation, and motion planning. Code:\nhttps://github.com/wzzheng/Doe.\n","authors":["Wenzhao Zheng","Zetian Xia","Yuanhui Huang","Sicheng Zuo","Jie Zhou","Jiwen Lu"],"pdf_url":"https://arxiv.org/pdf/2412.09627v1.pdf","comment":"Code is available at: https://github.com/wzzheng/Doe"},{"id":"http://arxiv.org/abs/2412.09624v1","updated":"2024-12-12T18:59:57Z","published":"2024-12-12T18:59:57Z","title":"GenEx: Generating an Explorable World","summary":"  Understanding, navigating, and exploring the 3D physical real world has long\nbeen a central challenge in the development of artificial intelligence. In this\nwork, we take a step toward this goal by introducing GenEx, a system capable of\nplanning complex embodied world exploration, guided by its generative\nimagination that forms priors (expectations) about the surrounding\nenvironments. GenEx generates an entire 3D-consistent imaginative environment\nfrom as little as a single RGB image, bringing it to life through panoramic\nvideo streams. Leveraging scalable 3D world data curated from Unreal Engine,\nour generative model is rounded in the physical world. It captures a continuous\n360-degree environment with little effort, offering a boundless landscape for\nAI agents to explore and interact with. GenEx achieves high-quality world\ngeneration, robust loop consistency over long trajectories, and demonstrates\nstrong 3D capabilities such as consistency and active 3D mapping. Powered by\ngenerative imagination of the world, GPT-assisted agents are equipped to\nperform complex embodied tasks, including both goal-agnostic exploration and\ngoal-driven navigation. These agents utilize predictive expectation regarding\nunseen parts of the physical world to refine their beliefs, simulate different\noutcomes based on potential decisions, and make more informed choices. In\nsummary, we demonstrate that GenEx provides a transformative platform for\nadvancing embodied AI in imaginative spaces and brings potential for extending\nthese capabilities to real-world exploration.\n","authors":["Taiming Lu","Tianmin Shu","Junfei Xiao","Luoxin Ye","Jiahao Wang","Cheng Peng","Chen Wei","Daniel Khashabi","Rama Chellappa","Alan Yuille","Jieneng Chen"],"pdf_url":"https://arxiv.org/pdf/2412.09624v1.pdf","comment":"Website: GenEx.world"},{"id":"http://arxiv.org/abs/2412.09623v1","updated":"2024-12-12T18:59:56Z","published":"2024-12-12T18:59:56Z","title":"OmniDrag: Enabling Motion Control for Omnidirectional Image-to-Video\n  Generation","summary":"  As virtual reality gains popularity, the demand for controllable creation of\nimmersive and dynamic omnidirectional videos (ODVs) is increasing. While\nprevious text-to-ODV generation methods achieve impressive results, they\nstruggle with content inaccuracies and inconsistencies due to reliance solely\non textual inputs. Although recent motion control techniques provide\nfine-grained control for video generation, directly applying these methods to\nODVs often results in spatial distortion and unsatisfactory performance,\nespecially with complex spherical motions. To tackle these challenges, we\npropose OmniDrag, the first approach enabling both scene- and object-level\nmotion control for accurate, high-quality omnidirectional image-to-video\ngeneration. Building on pretrained video diffusion models, we introduce an\nomnidirectional control module, which is jointly fine-tuned with temporal\nattention layers to effectively handle complex spherical motion. In addition,\nwe develop a novel spherical motion estimator that accurately extracts\nmotion-control signals and allows users to perform drag-style ODV generation by\nsimply drawing handle and target points. We also present a new dataset, named\nMove360, addressing the scarcity of ODV data with large scene and object\nmotions. Experiments demonstrate the significant superiority of OmniDrag in\nachieving holistic scene-level and fine-grained object-level control for ODV\ngeneration. The project page is available at\nhttps://lwq20020127.github.io/OmniDrag.\n","authors":["Weiqi Li","Shijie Zhao","Chong Mou","Xuhan Sheng","Zhenyu Zhang","Qian Wang","Junlin Li","Li Zhang","Jian Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.09623v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09622v1","updated":"2024-12-12T18:59:55Z","published":"2024-12-12T18:59:55Z","title":"LoRACLR: Contrastive Adaptation for Customization of Diffusion Models","summary":"  Recent advances in text-to-image customization have enabled high-fidelity,\ncontext-rich generation of personalized images, allowing specific concepts to\nappear in a variety of scenarios. However, current methods struggle with\ncombining multiple personalized models, often leading to attribute entanglement\nor requiring separate training to preserve concept distinctiveness. We present\nLoRACLR, a novel approach for multi-concept image generation that merges\nmultiple LoRA models, each fine-tuned for a distinct concept, into a single,\nunified model without additional individual fine-tuning. LoRACLR uses a\ncontrastive objective to align and merge the weight spaces of these models,\nensuring compatibility while minimizing interference. By enforcing distinct yet\ncohesive representations for each concept, LoRACLR enables efficient, scalable\nmodel composition for high-quality, multi-concept image synthesis. Our results\nhighlight the effectiveness of LoRACLR in accurately merging multiple concepts,\nadvancing the capabilities of personalized image generation.\n","authors":["Enis Simsar","Thomas Hofmann","Federico Tombari","Pinar Yanardag"],"pdf_url":"https://arxiv.org/pdf/2412.09622v1.pdf","comment":"Project page: https://loraclr.github.io/"},{"id":"http://arxiv.org/abs/2412.09620v1","updated":"2024-12-12T18:59:54Z","published":"2024-12-12T18:59:54Z","title":"Learning Camera Movement Control from Real-World Drone Videos","summary":"  This study seeks to automate camera movement control for filming existing\nsubjects into attractive videos, contrasting with the creation of non-existent\ncontent by directly generating the pixels. We select drone videos as our test\ncase due to their rich and challenging motion patterns, distinctive viewing\nangles, and precise controls. Existing AI videography methods struggle with\nlimited appearance diversity in simulation training, high costs of recording\nexpert operations, and difficulties in designing heuristic-based goals to cover\nall scenarios. To avoid these issues, we propose a scalable method that\ninvolves collecting real-world training data to improve diversity, extracting\ncamera trajectories automatically to minimize annotation costs, and training an\neffective architecture that does not rely on heuristics. Specifically, we\ncollect 99k high-quality trajectories by running 3D reconstruction on online\nvideos, connecting camera poses from consecutive frames to formulate 3D camera\npaths, and using Kalman filter to identify and remove low-quality data.\nMoreover, we introduce DVGFormer, an auto-regressive transformer that leverages\nthe camera path and images from all past frames to predict camera movement in\nthe next frame. We evaluate our system across 38 synthetic natural scenes and 7\nreal city 3D scans. We show that our system effectively learns to perform\nchallenging camera movements such as navigating through obstacles, maintaining\nlow altitude to increase perceived speed, and orbiting towers and buildings,\nwhich are very useful for recording high-quality videos. Data and code are\navailable at dvgformer.github.io.\n","authors":["Yunzhong Hou","Liang Zheng","Philip Torr"],"pdf_url":"https://arxiv.org/pdf/2412.09620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09621v1","updated":"2024-12-12T18:59:54Z","published":"2024-12-12T18:59:54Z","title":"Stereo4D: Learning How Things Move in 3D from Internet Stereo Videos","summary":"  Learning to understand dynamic 3D scenes from imagery is crucial for\napplications ranging from robotics to scene reconstruction. Yet, unlike other\nproblems where large-scale supervised training has enabled rapid progress,\ndirectly supervising methods for recovering 3D motion remains challenging due\nto the fundamental difficulty of obtaining ground truth annotations. We present\na system for mining high-quality 4D reconstructions from internet stereoscopic,\nwide-angle videos. Our system fuses and filters the outputs of camera pose\nestimation, stereo depth estimation, and temporal tracking methods into\nhigh-quality dynamic 3D reconstructions. We use this method to generate\nlarge-scale data in the form of world-consistent, pseudo-metric 3D point clouds\nwith long-term motion trajectories. We demonstrate the utility of this data by\ntraining a variant of DUSt3R to predict structure and 3D motion from real-world\nimage pairs, showing that training on our reconstructed data enables\ngeneralization to diverse real-world scenes. Project page:\nhttps://stereo4d.github.io\n","authors":["Linyi Jin","Richard Tucker","Zhengqi Li","David Fouhey","Noah Snavely","Aleksander Holynski"],"pdf_url":"https://arxiv.org/pdf/2412.09621v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09619v1","updated":"2024-12-12T18:59:53Z","published":"2024-12-12T18:59:53Z","title":"SnapGen: Taming High-Resolution Text-to-Image Models for Mobile Devices\n  with Efficient Architectures and Training","summary":"  Existing text-to-image (T2I) diffusion models face several limitations,\nincluding large model sizes, slow runtime, and low-quality generation on mobile\ndevices. This paper aims to address all of these challenges by developing an\nextremely small and fast T2I model that generates high-resolution and\nhigh-quality images on mobile platforms. We propose several techniques to\nachieve this goal. First, we systematically examine the design choices of the\nnetwork architecture to reduce model parameters and latency, while ensuring\nhigh-quality generation. Second, to further improve generation quality, we\nemploy cross-architecture knowledge distillation from a much larger model,\nusing a multi-level approach to guide the training of our model from scratch.\nThird, we enable a few-step generation by integrating adversarial guidance with\nknowledge distillation. For the first time, our model SnapGen, demonstrates the\ngeneration of 1024x1024 px images on a mobile device around 1.4 seconds. On\nImageNet-1K, our model, with only 372M parameters, achieves an FID of 2.06 for\n256x256 px generation. On T2I benchmarks (i.e., GenEval and DPG-Bench), our\nmodel with merely 379M parameters, surpasses large-scale models with billions\nof parameters at a significantly smaller size (e.g., 7x smaller than SDXL, 14x\nsmaller than IF-XL).\n","authors":["Dongting Hu","Jierun Chen","Xijie Huang","Huseyin Coskun","Arpit Sahni","Aarush Gupta","Anujraaj Goyal","Dishani Lahiri","Rajesh Singh","Yerlan Idelbayev","Junli Cao","Yanyu Li","Kwang-Ting Cheng","S. -H. Gary Chan","Mingming Gong","Sergey Tulyakov","Anil Kag","Yanwu Xu","Jian Ren"],"pdf_url":"https://arxiv.org/pdf/2412.09619v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09618v1","updated":"2024-12-12T18:59:48Z","published":"2024-12-12T18:59:48Z","title":"EasyRef: Omni-Generalized Group Image Reference for Diffusion Models via\n  Multimodal LLM","summary":"  Significant achievements in personalization of diffusion models have been\nwitnessed. Conventional tuning-free methods mostly encode multiple reference\nimages by averaging their image embeddings as the injection condition, but such\nan image-independent operation cannot perform interaction among images to\ncapture consistent visual elements within multiple references. Although the\ntuning-based Low-Rank Adaptation (LoRA) can effectively extract consistent\nelements within multiple images through the training process, it necessitates\nspecific finetuning for each distinct image group. This paper introduces\nEasyRef, a novel plug-and-play adaptation method that enables diffusion models\nto be conditioned on multiple reference images and the text prompt. To\neffectively exploit consistent visual elements within multiple images, we\nleverage the multi-image comprehension and instruction-following capabilities\nof the multimodal large language model (MLLM), prompting it to capture\nconsistent visual elements based on the instruction. Besides, injecting the\nMLLM's representations into the diffusion process through adapters can easily\ngeneralize to unseen domains, mining the consistent visual elements within\nunseen data. To mitigate computational costs and enhance fine-grained detail\npreservation, we introduce an efficient reference aggregation strategy and a\nprogressive training scheme. Finally, we introduce MRBench, a new\nmulti-reference image generation benchmark. Experimental results demonstrate\nEasyRef surpasses both tuning-free methods like IP-Adapter and tuning-based\nmethods like LoRA, achieving superior aesthetic quality and robust zero-shot\ngeneralization across diverse domains.\n","authors":["Zhuofan Zong","Dongzhi Jiang","Bingqi Ma","Guanglu Song","Hao Shao","Dazhong Shen","Yu Liu","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2412.09618v1.pdf","comment":"Tech report"},{"id":"http://arxiv.org/abs/2412.09616v1","updated":"2024-12-12T18:59:46Z","published":"2024-12-12T18:59:46Z","title":"V2PE: Improving Multimodal Long-Context Capability of Vision-Language\n  Models with Variable Visual Position Encoding","summary":"  Vision-Language Models (VLMs) have shown promising capabilities in handling\nvarious multimodal tasks, yet they struggle in long-context scenarios,\nparticularly in tasks involving videos, high-resolution images, or lengthy\nimage-text documents. In our work, we first conduct an empirical analysis of\nthe long-context capabilities of VLMs using our augmented long-context\nmultimodal datasets. Our findings reveal that directly applying the positional\nencoding mechanism used for textual tokens to visual tokens is suboptimal, and\nVLM performance degrades sharply when the position encoding exceeds the model's\ncontext window. To address this, we propose Variable Visual Position Encoding\n(V2PE), a novel positional encoding approach that employs variable and smaller\nincrements for visual tokens, enabling more efficient management of long\nmultimodal sequences. Our experiments demonstrate the effectiveness of V2PE to\nenhances VLMs' ability to effectively understand and reason over long\nmultimodal contexts. We further integrate V2PE with our augmented long-context\nmultimodal datasets to fine-tune the open-source VLM, InternVL2. The fine-tuned\nmodel achieves strong performance on both standard and long-context multimodal\ntasks. Notably, when the sequence length of the training dataset is increased\nto 256K tokens, the model is capable of processing multimodal sequences up to\n1M tokens, highlighting its potential for real-world long-context applications.\n","authors":["Junqi Ge","Ziyi Chen","Jintao Lin","Jinguo Zhu","Xihui Liu","Jifeng Dai","Xizhou Zhu"],"pdf_url":"https://arxiv.org/pdf/2412.09616v1.pdf","comment":"The code and models will be available at\n  https://github.com/OpenGVLab/V2PE"},{"id":"http://arxiv.org/abs/2412.09614v1","updated":"2024-12-12T18:59:41Z","published":"2024-12-12T18:59:41Z","title":"Context Canvas: Enhancing Text-to-Image Diffusion Models with Knowledge\n  Graph-Based RAG","summary":"  We introduce a novel approach to enhance the capabilities of text-to-image\nmodels by incorporating a graph-based RAG. Our system dynamically retrieves\ndetailed character information and relational data from the knowledge graph,\nenabling the generation of visually accurate and contextually rich images. This\ncapability significantly improves upon the limitations of existing T2I models,\nwhich often struggle with the accurate depiction of complex or culturally\nspecific subjects due to dataset constraints. Furthermore, we propose a novel\nself-correcting mechanism for text-to-image models to ensure consistency and\nfidelity in visual outputs, leveraging the rich context from the graph to guide\ncorrections. Our qualitative and quantitative experiments demonstrate that\nContext Canvas significantly enhances the capabilities of popular models such\nas Flux, Stable Diffusion, and DALL-E, and improves the functionality of\nControlNet for fine-grained image editing tasks. To our knowledge, Context\nCanvas represents the first application of graph-based RAG in enhancing T2I\nmodels, representing a significant advancement for producing high-fidelity,\ncontext-aware multi-faceted images.\n","authors":["Kavana Venkatesh","Yusuf Dalva","Ismini Lourentzou","Pinar Yanardag"],"pdf_url":"https://arxiv.org/pdf/2412.09614v1.pdf","comment":"Project Page: https://context-canvas.github.io/"},{"id":"http://arxiv.org/abs/2412.09611v1","updated":"2024-12-12T18:59:40Z","published":"2024-12-12T18:59:40Z","title":"FluxSpace: Disentangled Semantic Editing in Rectified Flow Transformers","summary":"  Rectified flow models have emerged as a dominant approach in image\ngeneration, showcasing impressive capabilities in high-quality image synthesis.\nHowever, despite their effectiveness in visual generation, rectified flow\nmodels often struggle with disentangled editing of images. This limitation\nprevents the ability to perform precise, attribute-specific modifications\nwithout affecting unrelated aspects of the image. In this paper, we introduce\nFluxSpace, a domain-agnostic image editing method leveraging a representation\nspace with the ability to control the semantics of images generated by\nrectified flow transformers, such as Flux. By leveraging the representations\nlearned by the transformer blocks within the rectified flow models, we propose\na set of semantically interpretable representations that enable a wide range of\nimage editing tasks, from fine-grained image editing to artistic creation. This\nwork offers a scalable and effective image editing approach, along with its\ndisentanglement capabilities.\n","authors":["Yusuf Dalva","Kavana Venkatesh","Pinar Yanardag"],"pdf_url":"https://arxiv.org/pdf/2412.09611v1.pdf","comment":"Project Page: https://fluxspace.github.io"},{"id":"http://arxiv.org/abs/2412.09612v1","updated":"2024-12-12T18:59:40Z","published":"2024-12-12T18:59:40Z","title":"Olympus: A Universal Task Router for Computer Vision Tasks","summary":"  We introduce Olympus, a new approach that transforms Multimodal Large\nLanguage Models (MLLMs) into a unified framework capable of handling a wide\narray of computer vision tasks. Utilizing a controller MLLM, Olympus delegates\nover 20 specialized tasks across images, videos, and 3D objects to dedicated\nmodules. This instruction-based routing enables complex workflows through\nchained actions without the need for training heavy generative models. Olympus\neasily integrates with existing MLLMs, expanding their capabilities with\ncomparable performance. Experimental results demonstrate that Olympus achieves\nan average routing accuracy of 94.75% across 20 tasks and precision of 91.82%\nin chained action scenarios, showcasing its effectiveness as a universal task\nrouter that can solve a diverse range of computer vision tasks. Project page:\nhttps://github.com/yuanze-lin/Olympus_page\n","authors":["Yuanze Lin","Yunsheng Li","Dongdong Chen","Weijian Xu","Ronald Clark","Philip H. S. Torr"],"pdf_url":"https://arxiv.org/pdf/2412.09612v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2412.09613v1","updated":"2024-12-12T18:59:40Z","published":"2024-12-12T18:59:40Z","title":"PVC: Progressive Visual Token Compression for Unified Image and Video\n  Processing in Large Vision-Language Models","summary":"  Large Vision-Language Models (VLMs) have been extended to understand both\nimages and videos. Visual token compression is leveraged to reduce the\nconsiderable token length of visual inputs. To meet the needs of different\ntasks, existing high-performance models usually process images and videos\nseparately with different token compression strategies, limiting the\ncapabilities of combining images and videos. To this end, we extend each image\ninto a \"static\" video and introduce a unified token compression strategy called\nProgressive Visual Token Compression (PVC), where the tokens of each frame are\nprogressively encoded and adaptively compressed to supplement the information\nnot extracted from previous frames. Video tokens are efficiently compressed\nwith exploiting the inherent temporal redundancy. Images are repeated as static\nvideos, and the spatial details can be gradually supplemented in multiple\nframes. PVC unifies the token compressing of images and videos. With a limited\nnumber of tokens per frame (64 tokens by default), spatial details and temporal\nchanges can still be preserved. Experiments show that our model achieves\nstate-of-the-art performance across various video understanding benchmarks,\nincluding long video tasks and fine-grained short video tasks. Meanwhile, our\nunified token compression strategy incurs no performance loss on image\nbenchmarks, particularly in detail-sensitive tasks.\n","authors":["Chenyu Yang","Xuan Dong","Xizhou Zhu","Weijie Su","Jiahao Wang","Hao Tian","Zhe Chen","Wenhai Wang","Lewei Lu","Jifeng Dai"],"pdf_url":"https://arxiv.org/pdf/2412.09613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09608v1","updated":"2024-12-12T18:59:34Z","published":"2024-12-12T18:59:34Z","title":"Representing Long Volumetric Video with Temporal Gaussian Hierarchy","summary":"  This paper aims to address the challenge of reconstructing long volumetric\nvideos from multi-view RGB videos. Recent dynamic view synthesis methods\nleverage powerful 4D representations, like feature grids or point cloud\nsequences, to achieve high-quality rendering results. However, they are\ntypically limited to short (1~2s) video clips and often suffer from large\nmemory footprints when dealing with longer videos. To solve this issue, we\npropose a novel 4D representation, named Temporal Gaussian Hierarchy, to\ncompactly model long volumetric videos. Our key observation is that there are\ngenerally various degrees of temporal redundancy in dynamic scenes, which\nconsist of areas changing at different speeds. Motivated by this, our approach\nbuilds a multi-level hierarchy of 4D Gaussian primitives, where each level\nseparately describes scene regions with different degrees of content change,\nand adaptively shares Gaussian primitives to represent unchanged scene content\nover different temporal segments, thus effectively reducing the number of\nGaussian primitives. In addition, the tree-like structure of the Gaussian\nhierarchy allows us to efficiently represent the scene at a particular moment\nwith a subset of Gaussian primitives, leading to nearly constant GPU memory\nusage during the training or rendering regardless of the video length.\nExtensive experimental results demonstrate the superiority of our method over\nalternative methods in terms of training cost, rendering speed, and storage\nusage. To our knowledge, this work is the first approach capable of efficiently\nhandling minutes of volumetric video data while maintaining state-of-the-art\nrendering quality. Our project page is available at:\nhttps://zju3dv.github.io/longvolcap.\n","authors":["Zhen Xu","Yinghao Xu","Zhiyuan Yu","Sida Peng","Jiaming Sun","Hujun Bao","Xiaowei Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.09608v1.pdf","comment":"SIGGRAPH Asia 2024 (TOG). Project page:\n  https://zju3dv.github.io/longvolcap"},{"id":"http://arxiv.org/abs/2412.09607v1","updated":"2024-12-12T18:59:31Z","published":"2024-12-12T18:59:31Z","title":"Spectral Image Tokenizer","summary":"  Image tokenizers map images to sequences of discrete tokens, and are a\ncrucial component of autoregressive transformer-based image generation. The\ntokens are typically associated with spatial locations in the input image,\narranged in raster scan order, which is not ideal for autoregressive modeling.\nIn this paper, we propose to tokenize the image spectrum instead, obtained from\na discrete wavelet transform (DWT), such that the sequence of tokens represents\nthe image in a coarse-to-fine fashion. Our tokenizer brings several advantages:\n1) it leverages that natural images are more compressible at high frequencies,\n2) it can take and reconstruct images of different resolutions without\nretraining, 3) it improves the conditioning for next-token prediction --\ninstead of conditioning on a partial line-by-line reconstruction of the image,\nit takes a coarse reconstruction of the full image, 4) it enables partial\ndecoding where the first few generated tokens can reconstruct a coarse version\nof the image, 5) it enables autoregressive models to be used for image\nupsampling. We evaluate the tokenizer reconstruction metrics as well as\nmultiscale image generation, text-guided image upsampling and editing.\n","authors":["Carlos Esteves","Mohammed Suhail","Ameesh Makadia"],"pdf_url":"https://arxiv.org/pdf/2412.09607v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09606v1","updated":"2024-12-12T18:59:28Z","published":"2024-12-12T18:59:28Z","title":"Feat2GS: Probing Visual Foundation Models with Gaussian Splatting","summary":"  Given that visual foundation models (VFMs) are trained on extensive datasets\nbut often limited to 2D images, a natural question arises: how well do they\nunderstand the 3D world? With the differences in architecture and training\nprotocols (i.e., objectives, proxy tasks), a unified framework to fairly and\ncomprehensively probe their 3D awareness is urgently needed. Existing works on\n3D probing suggest single-view 2.5D estimation (e.g., depth and normal) or\ntwo-view sparse 2D correspondence (e.g., matching and tracking). Unfortunately,\nthese tasks ignore texture awareness, and require 3D data as ground-truth,\nwhich limits the scale and diversity of their evaluation set. To address these\nissues, we introduce Feat2GS, which readout 3D Gaussians attributes from VFM\nfeatures extracted from unposed images. This allows us to probe 3D awareness\nfor geometry and texture via novel view synthesis, without requiring 3D data.\nAdditionally, the disentanglement of 3DGS parameters - geometry\n($\\boldsymbol{x}, \\alpha, \\Sigma$) and texture ($\\boldsymbol{c}$) - enables\nseparate analysis of texture and geometry awareness. Under Feat2GS, we conduct\nextensive experiments to probe the 3D awareness of several VFMs, and\ninvestigate the ingredients that lead to a 3D aware VFM. Building on these\nfindings, we develop several variants that achieve state-of-the-art across\ndiverse datasets. This makes Feat2GS useful for probing VFMs, and as a\nsimple-yet-effective baseline for novel-view synthesis. Code and data will be\nmade available at https://fanegg.github.io/Feat2GS/.\n","authors":["Yue Chen","Xingyu Chen","Anpei Chen","Gerard Pons-Moll","Yuliang Xiu"],"pdf_url":"https://arxiv.org/pdf/2412.09606v1.pdf","comment":"Project Page: https://fanegg.github.io/Feat2GS/"},{"id":"http://arxiv.org/abs/2412.09604v1","updated":"2024-12-12T18:59:26Z","published":"2024-12-12T18:59:26Z","title":"SynerGen-VL: Towards Synergistic Image Understanding and Generation with\n  Vision Experts and Token Folding","summary":"  The remarkable success of Large Language Models (LLMs) has extended to the\nmultimodal domain, achieving outstanding performance in image understanding and\ngeneration. Recent efforts to develop unified Multimodal Large Language Models\n(MLLMs) that integrate these capabilities have shown promising results.\nHowever, existing approaches often involve complex designs in model\narchitecture or training pipeline, increasing the difficulty of model training\nand scaling. In this paper, we propose SynerGen-VL, a simple yet powerful\nencoder-free MLLM capable of both image understanding and generation. To\naddress challenges identified in existing encoder-free unified MLLMs, we\nintroduce the token folding mechanism and the vision-expert-based progressive\nalignment pretraining strategy, which effectively support high-resolution image\nunderstanding while reducing training complexity. After being trained on\nlarge-scale mixed image-text data with a unified next-token prediction\nobjective, SynerGen-VL achieves or surpasses the performance of existing\nencoder-free unified MLLMs with comparable or smaller parameter sizes, and\nnarrows the gap with task-specific state-of-the-art models, highlighting a\npromising path toward future unified MLLMs. Our code and models shall be\nreleased.\n","authors":["Hao Li","Changyao Tian","Jie Shao","Xizhou Zhu","Zhaokai Wang","Jinguo Zhu","Wenhan Dou","Xiaogang Wang","Hongsheng Li","Lewei Lu","Jifeng Dai"],"pdf_url":"https://arxiv.org/pdf/2412.09604v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09603v1","updated":"2024-12-12T18:59:25Z","published":"2024-12-12T18:59:25Z","title":"Do Multimodal Large Language Models See Like Humans?","summary":"  Multimodal Large Language Models (MLLMs) have achieved impressive results on\nvarious vision tasks, leveraging recent advancements in large language models.\nHowever, a critical question remains unaddressed: do MLLMs perceive visual\ninformation similarly to humans? Current benchmarks lack the ability to\nevaluate MLLMs from this perspective. To address this challenge, we introduce\nHVSBench, a large-scale benchmark designed to assess the alignment between\nMLLMs and the human visual system (HVS) on fundamental vision tasks that mirror\nhuman vision. HVSBench curated over 85K multimodal samples, spanning 13\ncategories and 5 fields in HVS, including Prominence, Subitizing, Prioritizing,\nFree-Viewing, and Searching. Extensive experiments demonstrate the\neffectiveness of our benchmark in providing a comprehensive evaluation of\nMLLMs. Specifically, we evaluate 13 MLLMs, revealing that even the best models\nshow significant room for improvement, with most achieving only moderate\nresults. Our experiments reveal that HVSBench presents a new and significant\nchallenge for cutting-edge MLLMs. We believe that HVSBench will facilitate\nresearch on human-aligned and explainable MLLMs, marking a key step in\nunderstanding how MLLMs perceive and process visual information.\n","authors":["Jiaying Lin","Shuquan Ye","Rynson W. H. Lau"],"pdf_url":"https://arxiv.org/pdf/2412.09603v1.pdf","comment":"Project page: https://jiaying.link/HVSBench/"},{"id":"http://arxiv.org/abs/2412.09602v1","updated":"2024-12-12T18:59:13Z","published":"2024-12-12T18:59:13Z","title":"Hidden Biases of End-to-End Driving Datasets","summary":"  End-to-end driving systems have made rapid progress, but have so far not been\napplied to the challenging new CARLA Leaderboard 2.0. Further, while there is a\nlarge body of literature on end-to-end architectures and training strategies,\nthe impact of the training dataset is often overlooked. In this work, we make a\nfirst attempt at end-to-end driving for Leaderboard 2.0. Instead of\ninvestigating architectures, we systematically analyze the training dataset,\nleading to new insights: (1) Expert style significantly affects downstream\npolicy performance. (2) In complex data sets, the frames should not be weighted\non the basis of simplistic criteria such as class frequencies. (3) Instead,\nestimating whether a frame changes the target labels compared to previous\nframes can reduce the size of the dataset without removing important\ninformation. By incorporating these findings, our model ranks first and second\nrespectively on the map and sensors tracks of the 2024 CARLA Challenge, and\nsets a new state-of-the-art on the Bench2Drive test routes. Finally, we uncover\na design flaw in the current evaluation metrics and propose a modification for\nfuture challenges. Our dataset, code, and pre-trained models are publicly\navailable at https://github.com/autonomousvision/carla_garage.\n","authors":["Julian Zimmerlin","Jens Beißwenger","Bernhard Jaeger","Andreas Geiger","Kashyap Chitta"],"pdf_url":"https://arxiv.org/pdf/2412.09602v1.pdf","comment":"Technical report for the CVPR 2024 Workshop on Foundation Models for\n  Autonomous Systems. Runner-up of the track 'CARLA Autonomous Driving\n  Challenge' in the 2024 Autonomous Grand Challenge\n  (https://opendrivelab.com/challenge2024/)"},{"id":"http://arxiv.org/abs/2412.09601v1","updated":"2024-12-12T18:59:11Z","published":"2024-12-12T18:59:11Z","title":"TimeRefine: Temporal Grounding with Time Refining Video LLM","summary":"  Video temporal grounding aims to localize relevant temporal boundaries in a\nvideo given a textual prompt. Recent work has focused on enabling Video LLMs to\nperform video temporal grounding via next-token prediction of temporal\ntimestamps. However, accurately localizing timestamps in videos remains\nchallenging for Video LLMs when relying solely on temporal token prediction.\nOur proposed TimeRefine addresses this challenge in two ways. First, instead of\ndirectly predicting the start and end timestamps, we reformulate the temporal\ngrounding task as a temporal refining task: the model first makes rough\npredictions and then refines them by predicting offsets to the target segment.\nThis refining process is repeated multiple times, through which the model\nprogressively self-improves its temporal localization accuracy. Second, to\nenhance the model's temporal perception capabilities, we incorporate an\nauxiliary prediction head that penalizes the model more if a predicted segment\ndeviates further from the ground truth, thus encouraging the model to make\ncloser and more accurate predictions. Our plug-and-play method can be\nintegrated into most LLM-based temporal grounding approaches. The experimental\nresults demonstrate that TimeRefine achieves 3.6% and 5.0% mIoU improvements on\nthe ActivityNet and Charades-STA datasets, respectively. Code and pretrained\nmodels will be released.\n","authors":["Xizi Wang","Feng Cheng","Ziyang Wang","Huiyu Wang","Md Mohaiminul Islam","Lorenzo Torresani","Mohit Bansal","Gedas Bertasius","David Crandall"],"pdf_url":"https://arxiv.org/pdf/2412.09601v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09600v1","updated":"2024-12-12T18:59:01Z","published":"2024-12-12T18:59:01Z","title":"Owl-1: Omni World Model for Consistent Long Video Generation","summary":"  Video generation models (VGMs) have received extensive attention recently and\nserve as promising candidates for general-purpose large vision models. While\nthey can only generate short videos each time, existing methods achieve long\nvideo generation by iteratively calling the VGMs, using the last-frame output\nas the condition for the next-round generation. However, the last frame only\ncontains short-term fine-grained information about the scene, resulting in\ninconsistency in the long horizon. To address this, we propose an Omni World\nmodeL (Owl-1) to produce long-term coherent and comprehensive conditions for\nconsistent long video generation. As videos are observations of the underlying\nevolving world, we propose to model the long-term developments in a latent\nspace and use VGMs to film them into videos. Specifically, we represent the\nworld with a latent state variable which can be decoded into explicit video\nobservations. These observations serve as a basis for anticipating temporal\ndynamics which in turn update the state variable. The interaction between\nevolving dynamics and persistent state enhances the diversity and consistency\nof the long videos. Extensive experiments show that Owl-1 achieves comparable\nperformance with SOTA methods on VBench-I2V and VBench-Long, validating its\nability to generate high-quality video observations. Code:\nhttps://github.com/huang-yh/Owl.\n","authors":["Yuanhui Huang","Wenzhao Zheng","Yuan Gao","Xin Tao","Pengfei Wan","Di Zhang","Jie Zhou","Jiwen Lu"],"pdf_url":"https://arxiv.org/pdf/2412.09600v1.pdf","comment":"Code is available at: https://github.com/huang-yh/Owl"},{"id":"http://arxiv.org/abs/2412.09599v1","updated":"2024-12-12T18:59:00Z","published":"2024-12-12T18:59:00Z","title":"RatBodyFormer: Rodent Body Surface from Keypoints","summary":"  Rat behavior modeling goes to the heart of many scientific studies, yet the\ntextureless body surface evades automatic analysis as it literally has no\nkeypoints that detectors can find. The movement of the body surface, however,\nis a rich source of information for deciphering the rat behavior. We introduce\ntwo key contributions to automatically recover densely 3D sampled rat body\nsurface points, passively. The first is RatDome, a novel multi-camera system\nfor rat behavior capture, and a large-scale dataset captured with it that\nconsists of pairs of 3D keypoints and 3D body surface points. The second is\nRatBodyFormer, a novel network to transform detected keypoints to 3D body\nsurface points. RatBodyFormer is agnostic to the exact locations of the 3D body\nsurface points in the training data and is trained with masked-learning. We\nexperimentally validate our framework with a number of real-world experiments.\nOur results collectively serve as a novel foundation for automated rat behavior\nanalysis and will likely have far-reaching implications for biomedical and\nneuroscientific research.\n","authors":["Ayaka Higami","Karin Oshima","Tomoyo Isoguchi Shiramatsu","Hirokazu Takahashi","Shohei Nobuhara","Ko Nishino"],"pdf_url":"https://arxiv.org/pdf/2412.09599v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09597v1","updated":"2024-12-12T18:58:42Z","published":"2024-12-12T18:58:42Z","title":"LiftImage3D: Lifting Any Single Image to 3D Gaussians with Video\n  Generation Priors","summary":"  Single-image 3D reconstruction remains a fundamental challenge in computer\nvision due to inherent geometric ambiguities and limited viewpoint information.\nRecent advances in Latent Video Diffusion Models (LVDMs) offer promising 3D\npriors learned from large-scale video data. However, leveraging these priors\neffectively faces three key challenges: (1) degradation in quality across large\ncamera motions, (2) difficulties in achieving precise camera control, and (3)\ngeometric distortions inherent to the diffusion process that damage 3D\nconsistency. We address these challenges by proposing LiftImage3D, a framework\nthat effectively releases LVDMs' generative priors while ensuring 3D\nconsistency. Specifically, we design an articulated trajectory strategy to\ngenerate video frames, which decomposes video sequences with large camera\nmotions into ones with controllable small motions. Then we use robust neural\nmatching models, i.e. MASt3R, to calibrate the camera poses of generated frames\nand produce corresponding point clouds. Finally, we propose a distortion-aware\n3D Gaussian splatting representation, which can learn independent distortions\nbetween frames and output undistorted canonical Gaussians. Extensive\nexperiments demonstrate that LiftImage3D achieves state-of-the-art performance\non two challenging datasets, i.e. LLFF, DL3DV, and Tanks and Temples, and\ngeneralizes well to diverse in-the-wild images, from cartoon illustrations to\ncomplex real-world scenes.\n","authors":["Yabo Chen","Chen Yang","Jiemin Fang","Xiaopeng Zhang","Lingxi Xie","Wei Shen","Wenrui Dai","Hongkai Xiong","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2412.09597v1.pdf","comment":"Project page: https://liftimage3d.github.io/"},{"id":"http://arxiv.org/abs/2406.09390v2","updated":"2024-12-12T18:58:34Z","published":"2024-06-13T17:59:05Z","title":"LLAVIDAL: A Large LAnguage VIsion Model for Daily Activities of Living","summary":"  Current Large Language Vision Models (LLVMs) trained on web videos perform\nwell in general video understanding but struggle with fine-grained details,\ncomplex human-object interactions (HOI), and view-invariant representation\nlearning essential for Activities of Daily Living (ADL). This limitation stems\nfrom a lack of specialized ADL video instruction-tuning datasets and\ninsufficient modality integration to capture discriminative action\nrepresentations. To address this, we propose a semi-automated framework for\ncurating ADL datasets, creating ADL-X, a multiview, multimodal RGBS\ninstruction-tuning dataset. Additionally, we introduce LLAVIDAL, an LLVM\nintegrating videos, 3D skeletons, and HOIs to model ADL's complex\nspatiotemporal relationships. For training LLAVIDAL a simple joint alignment of\nall modalities yields suboptimal results; thus, we propose a Multimodal\nProgressive (MMPro) training strategy, incorporating modalities in stages\nfollowing a curriculum. We also establish ADL MCQ and video description\nbenchmarks to assess LLVM performance in ADL tasks. Trained on ADL-X, LLAVIDAL\nachieves state-of-the-art performance across ADL benchmarks. Code and data will\nbe made publicly available at: https://adl-x.github.io/.\n","authors":["Dominick Reilly","Rajatsubhra Chakraborty","Arkaprava Sinha","Manish Kumar Govind","Pu Wang","Francois Bremond","Le Xue","Srijan Das"],"pdf_url":"https://arxiv.org/pdf/2406.09390v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09596v1","updated":"2024-12-12T18:58:30Z","published":"2024-12-12T18:58:30Z","title":"InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for\n  Long-term Streaming Video and Audio Interactions","summary":"  Creating AI systems that can interact with environments over long periods,\nsimilar to human cognition, has been a longstanding research goal. Recent\nadvancements in multimodal large language models (MLLMs) have made significant\nstrides in open-world understanding. However, the challenge of continuous and\nsimultaneous streaming perception, memory, and reasoning remains largely\nunexplored. Current MLLMs are constrained by their sequence-to-sequence\narchitecture, which limits their ability to process inputs and generate\nresponses simultaneously, akin to being unable to think while perceiving.\nFurthermore, relying on long contexts to store historical data is impractical\nfor long-term interactions, as retaining all information becomes costly and\ninefficient. Therefore, rather than relying on a single foundation model to\nperform all functions, this project draws inspiration from the concept of the\nSpecialized Generalist AI and introduces disentangled streaming perception,\nreasoning, and memory mechanisms, enabling real-time interaction with streaming\nvideo and audio input. The proposed framework InternLM-XComposer2.5-OmniLive\n(IXC2.5-OL) consists of three key modules: (1) Streaming Perception Module:\nProcesses multimodal information in real-time, storing key details in memory\nand triggering reasoning in response to user queries. (2) Multi-modal Long\nMemory Module: Integrates short-term and long-term memory, compressing\nshort-term memories into long-term ones for efficient retrieval and improved\naccuracy. (3) Reasoning Module: Responds to queries and executes reasoning\ntasks, coordinating with the perception and memory modules. This project\nsimulates human-like cognition, enabling multimodal large language models to\nprovide continuous and adaptive service over time.\n","authors":["Pan Zhang","Xiaoyi Dong","Yuhang Cao","Yuhang Zang","Rui Qian","Xilin Wei","Lin Chen","Yifei Li","Junbo Niu","Shuangrui Ding","Qipeng Guo","Haodong Duan","Xin Chen","Han Lv","Zheng Nie","Min Zhang","Bin Wang","Wenwei Zhang","Xinyue Zhang","Jiaye Ge","Wei Li","Jingwen Li","Zhongying Tu","Conghui He","Xingcheng Zhang","Kai Chen","Yu Qiao","Dahua Lin","Jiaqi Wang"],"pdf_url":"https://arxiv.org/pdf/2412.09596v1.pdf","comment":"Github Repo:\n  https://github.com/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-2.5-OmniLive"},{"id":"http://arxiv.org/abs/2412.09593v1","updated":"2024-12-12T18:58:09Z","published":"2024-12-12T18:58:09Z","title":"Neural LightRig: Unlocking Accurate Object Normal and Material\n  Estimation with Multi-Light Diffusion","summary":"  Recovering the geometry and materials of objects from a single image is\nchallenging due to its under-constrained nature. In this paper, we present\nNeural LightRig, a novel framework that boosts intrinsic estimation by\nleveraging auxiliary multi-lighting conditions from 2D diffusion priors.\nSpecifically, 1) we first leverage illumination priors from large-scale\ndiffusion models to build our multi-light diffusion model on a synthetic\nrelighting dataset with dedicated designs. This diffusion model generates\nmultiple consistent images, each illuminated by point light sources in\ndifferent directions. 2) By using these varied lighting images to reduce\nestimation uncertainty, we train a large G-buffer model with a U-Net backbone\nto accurately predict surface normals and materials. Extensive experiments\nvalidate that our approach significantly outperforms state-of-the-art methods,\nenabling accurate surface normal and PBR material estimation with vivid\nrelighting effects. Code and dataset are available on our project page at\nhttps://projects.zxhezexin.com/neural-lightrig.\n","authors":["Zexin He","Tengfei Wang","Xin Huang","Xingang Pan","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2412.09593v1.pdf","comment":"Project page: https://projects.zxhezexin.com/neural-lightrig"},{"id":"http://arxiv.org/abs/2412.09586v1","updated":"2024-12-12T18:55:30Z","published":"2024-12-12T18:55:30Z","title":"Gaze-LLE: Gaze Target Estimation via Large-Scale Learned Encoders","summary":"  We address the problem of gaze target estimation, which aims to predict where\na person is looking in a scene. Predicting a person's gaze target requires\nreasoning both about the person's appearance and the contents of the scene.\nPrior works have developed increasingly complex, hand-crafted pipelines for\ngaze target estimation that carefully fuse features from separate scene\nencoders, head encoders, and auxiliary models for signals like depth and pose.\nMotivated by the success of general-purpose feature extractors on a variety of\nvisual tasks, we propose Gaze-LLE, a novel transformer framework that\nstreamlines gaze target estimation by leveraging features from a frozen DINOv2\nencoder. We extract a single feature representation for the scene, and apply a\nperson-specific positional prompt to decode gaze with a lightweight module. We\ndemonstrate state-of-the-art performance across several gaze benchmarks and\nprovide extensive analysis to validate our design choices. Our code is\navailable at: http://github.com/fkryan/gazelle .\n","authors":["Fiona Ryan","Ajay Bati","Sangmin Lee","Daniel Bolya","Judy Hoffman","James M. Rehg"],"pdf_url":"https://arxiv.org/pdf/2412.09586v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09585v1","updated":"2024-12-12T18:55:18Z","published":"2024-12-12T18:55:18Z","title":"OLA-VLM: Elevating Visual Perception in Multimodal LLMs with Auxiliary\n  Embedding Distillation","summary":"  The standard practice for developing contemporary MLLMs is to feed features\nfrom vision encoder(s) into the LLM and train with natural language\nsupervision. In this work, we posit an overlooked opportunity to optimize the\nintermediate LLM representations through a vision perspective (objective),\ni.e., solely natural language supervision is sub-optimal for the MLLM's visual\nunderstanding ability. To that end, we propose OLA-VLM, the first approach\ndistilling knowledge into the LLM's hidden representations from a set of target\nvisual representations. Firstly, we formulate the objective during the\npretraining stage in MLLMs as a coupled optimization of predictive visual\nembedding and next text-token prediction. Secondly, we investigate MLLMs\ntrained solely with natural language supervision and identify a positive\ncorrelation between the quality of visual representations within these models\nand their downstream performance. Moreover, upon probing our OLA-VLM, we\nobserve improved representation quality owing to the embedding optimization.\nThirdly, we demonstrate that our OLA-VLM outperforms the single and\nmulti-encoder baselines, proving our approach's superiority over explicitly\nfeeding the corresponding features to the LLM. Particularly, OLA-VLM boosts\nperformance by an average margin of up to 2.5% on various benchmarks, with a\nnotable improvement of 8.7% on the Depth task in CV-Bench. Our code is\nopen-sourced at https://github.com/SHI-Labs/OLA-VLM .\n","authors":["Jitesh Jain","Zhengyuan Yang","Humphrey Shi","Jianfeng Gao","Jianwei Yang"],"pdf_url":"https://arxiv.org/pdf/2412.09585v1.pdf","comment":"Project Page: https://praeclarumjj3.github.io/ola_vlm/"},{"id":"http://arxiv.org/abs/2412.09582v1","updated":"2024-12-12T18:54:48Z","published":"2024-12-12T18:54:48Z","title":"Neptune: The Long Orbit to Benchmarking Long Video Understanding","summary":"  This paper describes a semi-automatic pipeline to generate challenging\nquestion-answer-decoy sets for understanding long videos. Many existing video\ndatasets and models are focused on short clips (10s-30s). While some long video\ndatasets do exist, they can often be solved by powerful image models applied\nper frame (and often to very few frames) in a video, and are usually manually\nannotated at high cost. In order to mitigate both these problems, we propose a\nscalable dataset creation pipeline which leverages large models (VLMs and\nLLMs), to automatically generate dense, time-aligned video captions, as well as\ntough question answer decoy sets for video segments (up to 15 minutes in\nlength). Our dataset Neptune covers a broad range of long video reasoning\nabilities and consists of a subset that emphasizes multimodal reasoning. Since\nexisting metrics for open-ended question answering are either rule-based or may\nrely on proprietary models, we provide a new open source model-based metric GEM\nto score open-ended responses on Neptune. Benchmark evaluations reveal that\nmost current open-source long video models perform poorly on Neptune,\nparticularly on questions testing temporal ordering, counting and state\nchanges. Through Neptune, we aim to spur the development of more advanced\nmodels capable of understanding long videos. The dataset is available at\nhttps://github.com/google-deepmind/neptune\n","authors":["Arsha Nagrani","Mingda Zhang","Ramin Mehran","Rachel Hornung","Nitesh Bharadwaj Gundavarapu","Nilpa Jha","Austin Myers","Xingyi Zhou","Boqing Gong","Cordelia Schmid","Mikhail Sirotenko","Yukun Zhu","Tobias Weyand"],"pdf_url":"https://arxiv.org/pdf/2412.09582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09573v1","updated":"2024-12-12T18:52:53Z","published":"2024-12-12T18:52:53Z","title":"FreeSplatter: Pose-free Gaussian Splatting for Sparse-view 3D\n  Reconstruction","summary":"  Existing sparse-view reconstruction models heavily rely on accurate known\ncamera poses. However, deriving camera extrinsics and intrinsics from\nsparse-view images presents significant challenges. In this work, we present\nFreeSplatter, a highly scalable, feed-forward reconstruction framework capable\nof generating high-quality 3D Gaussians from uncalibrated sparse-view images\nand recovering their camera parameters in mere seconds. FreeSplatter is built\nupon a streamlined transformer architecture, comprising sequential\nself-attention blocks that facilitate information exchange among multi-view\nimage tokens and decode them into pixel-wise 3D Gaussian primitives. The\npredicted Gaussian primitives are situated in a unified reference frame,\nallowing for high-fidelity 3D modeling and instant camera parameter estimation\nusing off-the-shelf solvers. To cater to both object-centric and scene-level\nreconstruction, we train two model variants of FreeSplatter on extensive\ndatasets. In both scenarios, FreeSplatter outperforms state-of-the-art\nbaselines in terms of reconstruction quality and pose estimation accuracy.\nFurthermore, we showcase FreeSplatter's potential in enhancing the productivity\nof downstream applications, such as text/image-to-3D content creation.\n","authors":["Jiale Xu","Shenghua Gao","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2412.09573v1.pdf","comment":"Project page: https://bluestyle97.github.io/projects/freesplatter/"},{"id":"http://arxiv.org/abs/2409.19069v3","updated":"2024-12-12T18:48:25Z","published":"2024-09-27T18:11:00Z","title":"Localizing Memorization in SSL Vision Encoders","summary":"  Recent work on studying memorization in self-supervised learning (SSL)\nsuggests that even though SSL encoders are trained on millions of images, they\nstill memorize individual data points. While effort has been put into\ncharacterizing the memorized data and linking encoder memorization to\ndownstream utility, little is known about where the memorization happens inside\nSSL encoders. To close this gap, we propose two metrics for localizing\nmemorization in SSL encoders on a per-layer (layermem) and per-unit basis\n(unitmem). Our localization methods are independent of the downstream task, do\nnot require any label information, and can be performed in a forward pass. By\nlocalizing memorization in various encoder architectures (convolutional and\ntransformer-based) trained on diverse datasets with contrastive and\nnon-contrastive SSL frameworks, we find that (1) while SSL memorization\nincreases with layer depth, highly memorizing units are distributed across the\nentire encoder, (2) a significant fraction of units in SSL encoders experiences\nsurprisingly high memorization of individual data points, which is in contrast\nto models trained under supervision, (3) atypical (or outlier) data points\ncause much higher layer and unit memorization than standard data points, and\n(4) in vision transformers, most memorization happens in the fully-connected\nlayers. Finally, we show that localizing memorization in SSL has the potential\nto improve fine-tuning and to inform pruning strategies.\n","authors":["Wenhao Wang","Adam Dziedzic","Michael Backes","Franziska Boenisch"],"pdf_url":"https://arxiv.org/pdf/2409.19069v3.pdf","comment":"Accepted at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2412.08486v2","updated":"2024-12-12T18:43:39Z","published":"2024-12-11T15:51:14Z","title":"Learning Flow Fields in Attention for Controllable Person Image\n  Generation","summary":"  Controllable person image generation aims to generate a person image\nconditioned on reference images, allowing precise control over the person's\nappearance or pose. However, prior methods often distort fine-grained textural\ndetails from the reference image, despite achieving high overall image quality.\nWe attribute these distortions to inadequate attention to corresponding regions\nin the reference image. To address this, we thereby propose learning flow\nfields in attention (Leffa), which explicitly guides the target query to attend\nto the correct reference key in the attention layer during training.\nSpecifically, it is realized via a regularization loss on top of the attention\nmap within a diffusion-based baseline. Our extensive experiments show that\nLeffa achieves state-of-the-art performance in controlling appearance (virtual\ntry-on) and pose (pose transfer), significantly reducing fine-grained detail\ndistortion while maintaining high image quality. Additionally, we show that our\nloss is model-agnostic and can be used to improve the performance of other\ndiffusion models.\n","authors":["Zijian Zhou","Shikun Liu","Xiao Han","Haozhe Liu","Kam Woh Ng","Tian Xie","Yuren Cong","Hang Li","Mengmeng Xu","Juan-Manuel Pérez-Rúa","Aditya Patel","Tao Xiang","Miaojing Shi","Sen He"],"pdf_url":"https://arxiv.org/pdf/2412.08486v2.pdf","comment":"github: https://github.com/franciszzj/Leffa, demo:\n  https://huggingface.co/spaces/franciszzj/Leffa, model:\n  https://huggingface.co/franciszzj/Leffa"},{"id":"http://arxiv.org/abs/2405.04211v3","updated":"2024-12-12T18:42:37Z","published":"2024-05-07T11:24:37Z","title":"Leveraging Medical Foundation Model Features in Graph Neural\n  Network-Based Retrieval of Breast Histopathology Images","summary":"  Breast cancer is the most common cancer type in women worldwide. Early\ndetection and appropriate treatment can significantly reduce its impact. While\nhistopathology examinations play a vital role in rapid and accurate diagnosis,\nthey often require experienced medical experts for proper recognition and\ncancer grading. Automated image retrieval systems have the potential to assist\npathologists in identifying cancerous tissues, thereby accelerating the\ndiagnostic process. Nevertheless, proposing an accurate image retrieval model\nis challenging due to considerable variability among the tissue and cell\npatterns in histological images. In this work, we leverage the features from\nfoundation models in a novel attention-based adversarially regularized\nvariational graph autoencoder model for breast histological image retrieval.\nOur results confirm the superior performance of models trained with foundation\nmodel features compared to those using pre-trained convolutional neural\nnetworks (up to 7.7% and 15.5% for mAP and mMV, respectively), with the\npre-trained general-purpose self-supervised model for computational pathology\n(UNI) delivering the best overall performance. By evaluating two publicly\navailable histology image datasets of breast cancer, our top-performing model,\ntrained with UNI features, achieved average mAP/mMV scores of 96.7%/91.5% and\n97.6%/94.2% for the BreakHis and BACH datasets, respectively. Our proposed\nretrieval model has the potential to be used in clinical settings to enhance\ndiagnostic performance and ultimately benefit patients.\n","authors":["Nematollah Saeidi","Hossein Karshenas","Bijan Shoushtarian","Sepideh Hatamikia","Ramona Woitek","Amirreza Mahbod"],"pdf_url":"https://arxiv.org/pdf/2405.04211v3.pdf","comment":"29 pages"},{"id":"http://arxiv.org/abs/2412.09551v1","updated":"2024-12-12T18:41:20Z","published":"2024-12-12T18:41:20Z","title":"Video Creation by Demonstration","summary":"  We explore a novel video creation experience, namely Video Creation by\nDemonstration. Given a demonstration video and a context image from a different\nscene, we generate a physically plausible video that continues naturally from\nthe context image and carries out the action concepts from the demonstration.\nTo enable this capability, we present $\\delta$-Diffusion, a self-supervised\ntraining approach that learns from unlabeled videos by conditional future frame\nprediction. Unlike most existing video generation controls that are based on\nexplicit signals, we adopts the form of implicit latent control for maximal\nflexibility and expressiveness required by general videos. By leveraging a\nvideo foundation model with an appearance bottleneck design on top, we extract\naction latents from demonstration videos for conditioning the generation\nprocess with minimal appearance leakage. Empirically, $\\delta$-Diffusion\noutperforms related baselines in terms of both human preference and large-scale\nmachine evaluations, and demonstrates potentials towards interactive world\nsimulation. Sampled video generation results are available at\nhttps://delta-diffusion.github.io/.\n","authors":["Yihong Sun","Hao Zhou","Liangzhe Yuan","Jennifer J. Sun","Yandong Li","Xuhui Jia","Hartwig Adam","Bharath Hariharan","Long Zhao","Ting Liu"],"pdf_url":"https://arxiv.org/pdf/2412.09551v1.pdf","comment":"Project page at https://delta-diffusion.github.io/"},{"id":"http://arxiv.org/abs/2412.09549v1","updated":"2024-12-12T18:40:20Z","published":"2024-12-12T18:40:20Z","title":"Exemplar Masking for Multimodal Incremental Learning","summary":"  Multimodal incremental learning needs to digest the information from multiple\nmodalities while concurrently learning new knowledge without forgetting the\npreviously learned information. There are numerous challenges for this task,\nmainly including the larger storage size of multimodal data in exemplar-based\nmethods and the computational requirement of finetuning on huge multimodal\nmodels. In this paper, we leverage the parameter-efficient tuning scheme to\nreduce the burden of fine-tuning and propose the exemplar masking framework to\nefficiently replay old knowledge. Specifically, the non-important tokens are\nmasked based on the attention weights and the correlation across different\nmodalities, significantly reducing the storage size of an exemplar and\nconsequently saving more exemplars under the same memory buffer. Moreover, we\ndesign a multimodal data augmentation technique to diversify exemplars for\nreplaying prior knowledge. In experiments, we not only evaluate our method in\nexisting multimodal datasets but also extend the ImageNet-R dataset to a\nmultimodal dataset as a real-world application, where captions are generated by\nquerying multimodal large language models (e.g., InstructBLIP). Extensive\nexperiments show that our exemplar masking framework is more efficient and\nrobust to catastrophic forgetting under the same limited memory buffer. Code is\navailable at https://github.com/YiLunLee/Exemplar_Masking_MCIL.\n","authors":["Yi-Lun Lee","Chen-Yu Lee","Wei-Chen Chiu","Yi-Hsuan Tsai"],"pdf_url":"https://arxiv.org/pdf/2412.09549v1.pdf","comment":"Project page: https://github.com/YiLunLee/Exemplar_Masking_MCIL"},{"id":"http://arxiv.org/abs/2412.09548v1","updated":"2024-12-12T18:38:42Z","published":"2024-12-12T18:38:42Z","title":"Meshtron: High-Fidelity, Artist-Like 3D Mesh Generation at Scale","summary":"  Meshes are fundamental representations of 3D surfaces. However, creating\nhigh-quality meshes is a labor-intensive task that requires significant time\nand expertise in 3D modeling. While a delicate object often requires over\n$10^4$ faces to be accurately modeled, recent attempts at generating\nartist-like meshes are limited to $1.6$K faces and heavy discretization of\nvertex coordinates. Hence, scaling both the maximum face count and vertex\ncoordinate resolution is crucial to producing high-quality meshes of realistic,\ncomplex 3D objects. We present Meshtron, a novel autoregressive mesh generation\nmodel able to generate meshes with up to 64K faces at 1024-level coordinate\nresolution --over an order of magnitude higher face count and $8{\\times}$\nhigher coordinate resolution than current state-of-the-art methods. Meshtron's\nscalability is driven by four key components: (1) an hourglass neural\narchitecture, (2) truncated sequence training, (3) sliding window inference,\n(4) a robust sampling strategy that enforces the order of mesh sequences. This\nresults in over $50{\\%}$ less training memory, $2.5{\\times}$ faster throughput,\nand better consistency than existing works. Meshtron generates meshes of\ndetailed, complex 3D objects at unprecedented levels of resolution and\nfidelity, closely resembling those created by professional artists, and opening\nthe door to more realistic generation of detailed 3D assets for animation,\ngaming, and virtual environments.\n","authors":["Zekun Hao","David W. Romero","Tsung-Yi Lin","Ming-Yu Liu"],"pdf_url":"https://arxiv.org/pdf/2412.09548v1.pdf","comment":"Project page: https://research.nvidia.com/labs/dir/meshtron/"},{"id":"http://arxiv.org/abs/2412.09545v1","updated":"2024-12-12T18:35:26Z","published":"2024-12-12T18:35:26Z","title":"SimAvatar: Simulation-Ready Avatars with Layered Hair and Clothing","summary":"  We introduce SimAvatar, a framework designed to generate simulation-ready\nclothed 3D human avatars from a text prompt. Current text-driven human avatar\ngeneration methods either model hair, clothing, and the human body using a\nunified geometry or produce hair and garments that are not easily adaptable for\nsimulation within existing simulation pipelines. The primary challenge lies in\nrepresenting the hair and garment geometry in a way that allows leveraging\nestablished prior knowledge from foundational image diffusion models (e.g.,\nStable Diffusion) while being simulation-ready using either physics or neural\nsimulators. To address this task, we propose a two-stage framework that\ncombines the flexibility of 3D Gaussians with simulation-ready hair strands and\ngarment meshes. Specifically, we first employ three text-conditioned 3D\ngenerative models to generate garment mesh, body shape and hair strands from\nthe given text prompt. To leverage prior knowledge from foundational diffusion\nmodels, we attach 3D Gaussians to the body mesh, garment mesh, as well as hair\nstrands and learn the avatar appearance through optimization. To drive the\navatar given a pose sequence, we first apply physics simulators onto the\ngarment meshes and hair strands. We then transfer the motion onto 3D Gaussians\nthrough carefully designed mechanisms for each body part. As a result, our\nsynthesized avatars have vivid texture and realistic dynamic motion. To the\nbest of our knowledge, our method is the first to produce highly realistic,\nfully simulation-ready 3D avatars, surpassing the capabilities of current\napproaches.\n","authors":["Xueting Li","Ye Yuan","Shalini De Mello","Gilles Daviet","Jonathan Leaf","Miles Macklin","Jan Kautz","Umar Iqbal"],"pdf_url":"https://arxiv.org/pdf/2412.09545v1.pdf","comment":"Project website: https://nvlabs.github.io/SimAvatar/"},{"id":"http://arxiv.org/abs/2410.17251v2","updated":"2024-12-12T18:26:45Z","published":"2024-10-22T17:59:57Z","title":"Altogether: Image Captioning via Re-aligning Alt-text","summary":"  This paper focuses on creating synthetic data to improve the quality of image\ncaptions. Existing works typically have two shortcomings. First, they caption\nimages from scratch, ignoring existing alt-text metadata, and second, lack\ntransparency if the captioners' training data (e.g. GPT) is unknown. In this\npaper, we study a principled approach Altogether based on the key idea to edit\nand re-align existing alt-texts associated with the images. To generate\ntraining data, we perform human annotation where annotators start with the\nexisting alt-text and re-align it to the image content in multiple rounds,\nconsequently constructing captions with rich visual concepts. This differs from\nprior work that carries out human annotation as a one-time description task\nsolely based on images and annotator knowledge. We train a captioner on this\ndata that generalizes the process of re-aligning alt-texts at scale. Our\nresults show our Altogether approach leads to richer image captions that also\nimprove text-to-image generation and zero-shot image classification tasks.\n","authors":["Hu Xu","Po-Yao Huang","Xiaoqing Ellen Tan","Ching-Feng Yeh","Jacob Kahn","Christine Jou","Gargi Ghosh","Omer Levy","Luke Zettlemoyer","Wen-tau Yih","Shang-Wen Li","Saining Xie","Christoph Feichtenhofer"],"pdf_url":"https://arxiv.org/pdf/2410.17251v2.pdf","comment":"accepted by EMNLP 2024; Meta CLIP 1.2 Data Engine"},{"id":"http://arxiv.org/abs/2409.01314v2","updated":"2024-12-12T18:21:03Z","published":"2024-09-02T15:16:07Z","title":"Disentangling Mean Embeddings for Better Diagnostics of Image Generators","summary":"  The evaluation of image generators remains a challenge due to the limitations\nof traditional metrics in providing nuanced insights into specific image\nregions. This is a critical problem as not all regions of an image may be\nlearned with similar ease. In this work, we propose a novel approach to\ndisentangle the cosine similarity of mean embeddings into the product of cosine\nsimilarities for individual pixel clusters via central kernel alignment.\nConsequently, we can quantify the contribution of the cluster-wise performance\nto the overall image generation performance. We demonstrate how this enhances\nthe explainability and the likelihood of identifying pixel regions of model\nmisbehavior across various real-world use cases.\n","authors":["Sebastian G. Gruber","Pascal Tobias Ziegler","Florian Buettner"],"pdf_url":"https://arxiv.org/pdf/2409.01314v2.pdf","comment":"Published at Interpretable AI: Past, Present and Future Workshop at\n  NeurIPS 2024"},{"id":"http://arxiv.org/abs/2412.09530v1","updated":"2024-12-12T18:20:41Z","published":"2024-12-12T18:20:41Z","title":"Dynamic-VLM: Simple Dynamic Visual Token Compression for VideoLLM","summary":"  The application of Large Vision-Language Models (LVLMs) for analyzing images\nand videos is an exciting and rapidly evolving field. In recent years, we've\nseen significant growth in high-quality image-text datasets for fine-tuning\nimage understanding, but there is still a lack of comparable datasets for\nvideos. Additionally, many VideoLLMs are extensions of single-image VLMs, which\nmay not efficiently handle the complexities of longer videos. In this study, we\nintroduce a large-scale synthetic dataset created from proprietary models,\nusing carefully designed prompts to tackle a wide range of questions. We also\nexplore a dynamic visual token compression architecture that strikes a balance\nbetween computational efficiency and performance. Our proposed \\model{}\nachieves state-of-the-art results across various video tasks and shows\nimpressive generalization, setting new baselines in multi-image understanding.\nNotably, \\model{} delivers an absolute improvement of 2.7\\% over\nLLaVA-OneVision on VideoMME and 10.7\\% on MuirBench. Codes are available at\nhttps://github.com/Hon-Wong/ByteVideoLLM\n","authors":["Han Wang","Yuxiang Nie","Yongjie Ye","Deng GuanYu","Yanjie Wang","Shuai Li","Haiyang Yu","Jinghui Lu","Can Huang"],"pdf_url":"https://arxiv.org/pdf/2412.09530v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09529v1","updated":"2024-12-12T18:20:16Z","published":"2024-12-12T18:20:16Z","title":"Can Modern LLMs Act as Agent Cores in Radiology~Environments?","summary":"  Advancements in large language models (LLMs) have paved the way for LLM-based\nagent systems that offer enhanced accuracy and interpretability across various\ndomains. Radiology, with its complex analytical requirements, is an ideal field\nfor the application of these agents. This paper aims to investigate the\npre-requisite question for building concrete radiology agents which is, `Can\nmodern LLMs act as agent cores in radiology environments?' To investigate it,\nwe introduce RadABench with three-fold contributions: First, we present\nRadABench-Data, a comprehensive synthetic evaluation dataset for LLM-based\nagents, generated from an extensive taxonomy encompassing 6 anatomies, 5\nimaging modalities, 10 tool categories, and 11 radiology tasks. Second, we\npropose RadABench-EvalPlat, a novel evaluation platform for agents featuring a\nprompt-driven workflow and the capability to simulate a wide range of radiology\ntoolsets. Third, we assess the performance of 7 leading LLMs on our benchmark\nfrom 5 perspectives with multiple metrics. Our findings indicate that while\ncurrent LLMs demonstrate strong capabilities in many areas, they are still not\nsufficiently advanced to serve as the central agent core in a fully operational\nradiology agent system. Additionally, we identify key factors influencing the\nperformance of LLM-based agent cores, offering insights for clinicians on how\nto apply agent systems in real-world radiology practices effectively. All of\nour code and data are open-sourced in\nhttps://github.com/MAGIC-AI4Med/RadABench.\n","authors":["Qiaoyu Zheng","Chaoyi Wu","Pengcheng Qiu","Lisong Dai","Ya Zhang","Yanfeng Wang","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2412.09529v1.pdf","comment":"22 pages,7 figures"},{"id":"http://arxiv.org/abs/2412.04332v2","updated":"2024-12-12T18:08:56Z","published":"2024-12-05T16:48:16Z","title":"Liquid: Language Models are Scalable Multi-modal Generators","summary":"  We present Liquid, an auto-regressive generation paradigm that seamlessly\nintegrates visual comprehension and generation by tokenizing images into\ndiscrete codes and learning these code embeddings alongside text tokens within\na shared feature space for both vision and language. Unlike previous multimodal\nlarge language model (MLLM), Liquid achieves this integration using a single\nlarge language model (LLM), eliminating the need for external pretrained visual\nembeddings such as CLIP. For the first time, Liquid uncovers a scaling law that\nperformance drop unavoidably brought by the unified training of visual and\nlanguage tasks diminishes as the model size increases. Furthermore, the unified\ntoken space enables visual generation and comprehension tasks to mutually\nenhance each other, effectively removing the typical interference seen in\nearlier models. We show that existing LLMs can serve as strong foundations for\nLiquid, saving 100x in training costs while outperforming Chameleon in\nmultimodal capabilities and maintaining language performance comparable to\nmainstream LLMs like LLAMA2. Liquid also outperforms models like SD v2.1 and\nSD-XL (FID of 5.47 on MJHQ-30K), excelling in both vision-language and\ntext-only tasks. This work demonstrates that LLMs such as LLAMA3.2 and GEMMA2\nare powerful multimodal generators, offering a scalable solution for enhancing\nboth vision-language understanding and generation. The code and models will be\nreleased at https://github.com/FoundationVision/Liquid.\n","authors":["Junfeng Wu","Yi Jiang","Chuofan Ma","Yuliang Liu","Hengshuang Zhao","Zehuan Yuan","Song Bai","Xiang Bai"],"pdf_url":"https://arxiv.org/pdf/2412.04332v2.pdf","comment":"Technical report. Project page:\n  https://github.com/FoundationVision/Liquid"},{"id":"http://arxiv.org/abs/2412.09521v1","updated":"2024-12-12T18:07:23Z","published":"2024-12-12T18:07:23Z","title":"Efficient and Comprehensive Feature Extraction in Large Vision-Language\n  Model for Clinical Pathology Analysis","summary":"  Pathological diagnosis is vital for determining disease characteristics,\nguiding treatment, and assessing prognosis, relying heavily on detailed,\nmulti-scale analysis of high-resolution whole slide images (WSI). However,\ntraditional pure vision models face challenges of redundant feature extraction,\nwhereas existing large vision-language models (LVLMs) are limited by input\nresolution constraints, hindering their efficiency and accuracy. To overcome\nthese issues, we propose two innovative strategies: the mixed task-guided\nfeature enhancement, which directs feature extraction toward lesion-related\ndetails across scales, and the prompt-guided detail feature completion, which\nintegrates coarse- and fine-grained features from WSI based on specific prompts\nwithout compromising inference speed. Leveraging a comprehensive dataset of\n490,000 samples from diverse pathology tasks-including cancer detection,\ngrading, vascular and neural invasion identification, and so on-we trained the\npathology-specialized LVLM, OmniPath. Extensive experiments demonstrate that\nthis model significantly outperforms existing methods in diagnostic accuracy\nand efficiency, offering an interactive, clinically aligned approach for\nauxiliary diagnosis in a wide range of pathology applications.\n","authors":["Shengxuming Zhang","Weihan Li","Tianhong Gao","Jiacong Hu","Haoming Luo","Mingli Song","Xiuming Zhang","Zunlei Feng"],"pdf_url":"https://arxiv.org/pdf/2412.09521v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09513v1","updated":"2024-12-12T17:59:28Z","published":"2024-12-12T17:59:28Z","title":"Agent-based Video Trimming","summary":"  As information becomes more accessible, user-generated videos are increasing\nin length, placing a burden on viewers to sift through vast content for\nvaluable insights. This trend underscores the need for an algorithm to extract\nkey video information efficiently. Despite significant advancements in\nhighlight detection, moment retrieval, and video summarization, current\napproaches primarily focus on selecting specific time intervals, often\noverlooking the relevance between segments and the potential for segment\narranging. In this paper, we introduce a novel task called Video Trimming (VT),\nwhich focuses on detecting wasted footage, selecting valuable segments, and\ncomposing them into a final video with a coherent story. To address this task,\nwe propose Agent-based Video Trimming (AVT), structured into three phases:\nVideo Structuring, Clip Filtering, and Story Composition. Specifically, we\nemploy a Video Captioning Agent to convert video slices into structured textual\ndescriptions, a Filtering Module to dynamically discard low-quality footage\nbased on the structured information of each clip, and a Video Arrangement Agent\nto select and compile valid clips into a coherent final narrative. For\nevaluation, we develop a Video Evaluation Agent to assess trimmed videos,\nconducting assessments in parallel with human evaluations. Additionally, we\ncurate a new benchmark dataset for video trimming using raw user videos from\nthe internet. As a result, AVT received more favorable evaluations in user\nstudies and demonstrated superior mAP and precision on the YouTube Highlights,\nTVSum, and our own dataset for the highlight detection task. The code and\nmodels are available at https://ylingfeng.github.io/AVT.\n","authors":["Lingfeng Yang","Zhenyuan Chen","Xiang Li","Peiyang Jia","Liangqu Long","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2412.09513v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09511v1","updated":"2024-12-12T17:59:03Z","published":"2024-12-12T17:59:03Z","title":"GEAL: Generalizable 3D Affordance Learning with Cross-Modal Consistency","summary":"  Identifying affordance regions on 3D objects from semantic cues is essential\nfor robotics and human-machine interaction. However, existing 3D affordance\nlearning methods struggle with generalization and robustness due to limited\nannotated data and a reliance on 3D backbones focused on geometric encoding,\nwhich often lack resilience to real-world noise and data corruption. We propose\nGEAL, a novel framework designed to enhance the generalization and robustness\nof 3D affordance learning by leveraging large-scale pre-trained 2D models. We\nemploy a dual-branch architecture with Gaussian splatting to establish\nconsistent mappings between 3D point clouds and 2D representations, enabling\nrealistic 2D renderings from sparse point clouds. A granularity-adaptive fusion\nmodule and a 2D-3D consistency alignment module further strengthen cross-modal\nalignment and knowledge transfer, allowing the 3D branch to benefit from the\nrich semantics and generalization capacity of 2D models. To holistically assess\nthe robustness, we introduce two new corruption-based benchmarks: PIAD-C and\nLASO-C. Extensive experiments on public datasets and our benchmarks show that\nGEAL consistently outperforms existing methods across seen and novel object\ncategories, as well as corrupted data, demonstrating robust and adaptable\naffordance prediction under diverse conditions. Code and corruption datasets\nhave been made publicly available.\n","authors":["Dongyue Lu","Lingdong Kong","Tianxin Huang","Gim Hee Lee"],"pdf_url":"https://arxiv.org/pdf/2412.09511v1.pdf","comment":"22 pages, 8 figures, 12 tables; Project Page at\n  https://dylanorange.github.io/projects/geal"},{"id":"http://arxiv.org/abs/2412.09507v1","updated":"2024-12-12T17:55:00Z","published":"2024-12-12T17:55:00Z","title":"Vision Transformers for Efficient Indoor Pathloss Radio Map Prediction","summary":"  Vision Transformers (ViTs) have demonstrated remarkable success in achieving\nstate-of-the-art performance across various image-based tasks and beyond. In\nthis study, we employ a ViT-based neural network to address the problem of\nindoor pathloss radio map prediction. The network's generalization ability is\nevaluated across diverse settings, including unseen buildings, frequencies, and\nantennas with varying radiation patterns. By leveraging extensive data\naugmentation techniques and pretrained DINOv2 weights, we achieve promising\nresults, even under the most challenging scenarios.\n","authors":["Edvard Ghukasyan","Hrant Khachatrian","Rafayel Mkrtchyan","Theofanis P. Raptis"],"pdf_url":"https://arxiv.org/pdf/2412.09507v1.pdf","comment":"Work partly supported by the RA Science Committee grant No. 22rl-052\n  (DISTAL) and the EU under Italian National Recovery and Resilience Plan of\n  NextGenerationEU on \"Telecommunications of the Future\" (PE00000001 - program\n  \"RESTART\")"},{"id":"http://arxiv.org/abs/2412.09501v1","updated":"2024-12-12T17:50:39Z","published":"2024-12-12T17:50:39Z","title":"Lyra: An Efficient and Speech-Centric Framework for Omni-Cognition","summary":"  As Multi-modal Large Language Models (MLLMs) evolve, expanding beyond\nsingle-domain capabilities is essential to meet the demands for more versatile\nand efficient AI. However, previous omni-models have insufficiently explored\nspeech, neglecting its integration with multi-modality. We introduce Lyra, an\nefficient MLLM that enhances multimodal abilities, including advanced\nlong-speech comprehension, sound understanding, cross-modality efficiency, and\nseamless speech interaction. To achieve efficiency and speech-centric\ncapabilities, Lyra employs three strategies: (1) leveraging existing\nopen-source large models and a proposed multi-modality LoRA to reduce training\ncosts and data requirements; (2) using a latent multi-modality regularizer and\nextractor to strengthen the relationship between speech and other modalities,\nthereby enhancing model performance; and (3) constructing a high-quality,\nextensive dataset that includes 1.5M multi-modal (language, vision, audio) data\nsamples and 12K long speech samples, enabling Lyra to handle complex long\nspeech inputs and achieve more robust omni-cognition. Compared to other\nomni-methods, Lyra achieves state-of-the-art performance on various\nvision-language, vision-speech, and speech-language benchmarks, while also\nusing fewer computational resources and less training data.\n","authors":["Zhisheng Zhong","Chengyao Wang","Yuqi Liu","Senqiao Yang","Longxiang Tang","Yuechen Zhang","Jingyao Li","Tianyuan Qu","Yanwei Li","Yukang Chen","Shaozuo Yu","Sitong Wu","Eric Lo","Shu Liu","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2412.09501v1.pdf","comment":"Tech report"},{"id":"http://arxiv.org/abs/2412.09492v1","updated":"2024-12-12T17:41:49Z","published":"2024-12-12T17:41:49Z","title":"Video Seal: Open and Efficient Video Watermarking","summary":"  The proliferation of AI-generated content and sophisticated video editing\ntools has made it both important and challenging to moderate digital platforms.\nVideo watermarking addresses these challenges by embedding imperceptible\nsignals into videos, allowing for identification. However, the rare open tools\nand methods often fall short on efficiency, robustness, and flexibility. To\nreduce these gaps, this paper introduces Video Seal, a comprehensive framework\nfor neural video watermarking and a competitive open-sourced model. Our\napproach jointly trains an embedder and an extractor, while ensuring the\nwatermark robustness by applying transformations in-between, e.g., video\ncodecs. This training is multistage and includes image pre-training, hybrid\npost-training and extractor fine-tuning. We also introduce temporal watermark\npropagation, a technique to convert any image watermarking model to an\nefficient video watermarking model without the need to watermark every\nhigh-resolution frame. We present experimental results demonstrating the\neffectiveness of the approach in terms of speed, imperceptibility, and\nrobustness. Video Seal achieves higher robustness compared to strong baselines\nespecially under challenging distortions combining geometric transformations\nand video compression. Additionally, we provide new insights such as the impact\nof video compression during training, and how to compare methods operating on\ndifferent payloads. Contributions in this work - including the codebase,\nmodels, and a public demo - are open-sourced under permissive licenses to\nfoster further research and development in the field.\n","authors":["Pierre Fernandez","Hady Elsahar","I. Zeki Yalniz","Alexandre Mourachko"],"pdf_url":"https://arxiv.org/pdf/2412.09492v1.pdf","comment":"Code available at https://github.com/facebookresearch/videoseal"},{"id":"http://arxiv.org/abs/2412.09475v1","updated":"2024-12-12T17:20:27Z","published":"2024-12-12T17:20:27Z","title":"New keypoint-based approach for recognising British Sign Language (BSL)\n  from sequences","summary":"  In this paper, we present a novel keypoint-based classification model\ndesigned to recognise British Sign Language (BSL) words within continuous\nsigning sequences. Our model's performance is assessed using the BOBSL dataset,\nrevealing that the keypoint-based approach surpasses its RGB-based counterpart\nin computational efficiency and memory usage. Furthermore, it offers expedited\ntraining times and demands fewer computational resources. To the best of our\nknowledge, this is the inaugural application of a keypoint-based model for BSL\nword classification, rendering direct comparisons with existing works\nunavailable.\n","authors":["Oishi Deb","KR Prajwal","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2412.09475v1.pdf","comment":"International Conference on Computer Vision (ICCV) - HANDS Workshop"},{"id":"http://arxiv.org/abs/2412.09465v1","updated":"2024-12-12T17:14:58Z","published":"2024-12-12T17:14:58Z","title":"OFTSR: One-Step Flow for Image Super-Resolution with Tunable\n  Fidelity-Realism Trade-offs","summary":"  Recent advances in diffusion and flow-based generative models have\ndemonstrated remarkable success in image restoration tasks, achieving superior\nperceptual quality compared to traditional deep learning approaches. However,\nthese methods either require numerous sampling steps to generate high-quality\nimages, resulting in significant computational overhead, or rely on model\ndistillation, which usually imposes a fixed fidelity-realism trade-off and thus\nlacks flexibility. In this paper, we introduce OFTSR, a novel flow-based\nframework for one-step image super-resolution that can produce outputs with\ntunable levels of fidelity and realism. Our approach first trains a conditional\nflow-based super-resolution model to serve as a teacher model. We then distill\nthis teacher model by applying a specialized constraint. Specifically, we force\nthe predictions from our one-step student model for same input to lie on the\nsame sampling ODE trajectory of the teacher model. This alignment ensures that\nthe student model's single-step predictions from initial states match the\nteacher's predictions from a closer intermediate state. Through extensive\nexperiments on challenging datasets including FFHQ (256$\\times$256), DIV2K, and\nImageNet (256$\\times$256), we demonstrate that OFTSR achieves state-of-the-art\nperformance for one-step image super-resolution, while having the ability to\nflexibly tune the fidelity-realism trade-off. Code and pre-trained models are\navailable at https://github.com/yuanzhi-zhu/OFTSR and\nhttps://huggingface.co/Yuanzhi/OFTSR, respectively.\n","authors":["Yuanzhi Zhu","Ruiqing Wang","Shilin Lu","Junnan Li","Hanshu Yan","Kai Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.09465v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09445v1","updated":"2024-12-12T16:59:37Z","published":"2024-12-12T16:59:37Z","title":"Embeddings are all you need! Achieving High Performance Medical Image\n  Classification through Training-Free Embedding Analysis","summary":"  Developing artificial intelligence (AI) and machine learning (ML) models for\nmedical imaging typically involves extensive training and testing on large\ndatasets, consuming significant computational time, energy, and resources.\nThere is a need for more efficient methods that can achieve comparable or\nsuperior diagnostic performance without the associated resource burden. We\ninvestigated the feasibility of replacing conventional training procedures with\nan embedding-based approach that leverages concise and semantically meaningful\nrepresentations of medical images. Using pre-trained foundational\nmodels-specifically, convolutional neural networks (CNN) like ResNet and\nmultimodal models like Contrastive Language-Image Pre-training (CLIP)-we\ngenerated image embeddings for multi-class classification tasks. Simple linear\nclassifiers were then applied to these embeddings. The approach was evaluated\nacross diverse medical imaging modalities, including retinal images,\nmammography, dermatoscopic images, and chest radiographs. Performance was\ncompared to benchmark models trained and tested using traditional methods. The\nembedding-based models surpassed the benchmark area under the receiver\noperating characteristic curve (AUC-ROC) scores by up to 87 percentage in\nmulti-class classification tasks across the various medical imaging modalities.\nNotably, CLIP embedding models achieved the highest AUC-ROC scores,\ndemonstrating superior classification performance while significantly reducing\ncomputational demands. Our study indicates that leveraging embeddings from\npre-trained foundational models can effectively replace conventional,\nresource-intensive training and testing procedures in medical image analysis.\nThis embedding-based approach offers a more efficient alternative for image\nsegmentation, classification, and prediction, potentially accelerating AI\ntechnology integration into clinical practice.\n","authors":["Raj Hansini Khoiwal","Alan B. McMillan"],"pdf_url":"https://arxiv.org/pdf/2412.09445v1.pdf","comment":"15 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2412.09441v1","updated":"2024-12-12T16:57:20Z","published":"2024-12-12T16:57:20Z","title":"MOS: Model Surgery for Pre-Trained Model-Based Class-Incremental\n  Learning","summary":"  Class-Incremental Learning (CIL) requires models to continually acquire\nknowledge of new classes without forgetting old ones. Despite Pre-trained\nModels (PTMs) have shown excellent performance in CIL, catastrophic forgetting\nstill occurs as the model learns new concepts. Existing work seeks to utilize\nlightweight components to adjust the PTM, while the forgetting phenomenon still\ncomes from {\\em parameter and retrieval} levels. Specifically, iterative\nupdates of the model result in parameter drift, while mistakenly retrieving\nirrelevant modules leads to the mismatch during inference. To this end, we\npropose MOdel Surgery (MOS) to rescue the model from forgetting previous\nknowledge. By training task-specific adapters, we continually adjust the PTM to\ndownstream tasks. To mitigate parameter-level forgetting, we present an adapter\nmerging approach to learn task-specific adapters, which aims to bridge the gap\nbetween different components while reserve task-specific information. Besides,\nto address retrieval-level forgetting, we introduce a training-free\nself-refined adapter retrieval mechanism during inference, which leverages the\nmodel's inherent ability for better adapter retrieval. By jointly rectifying\nthe model with those steps, MOS can robustly resist catastrophic forgetting in\nthe learning process. Extensive experiments on seven benchmark datasets\nvalidate MOS's state-of-the-art performance. Code is available at:\nhttps://github.com/sun-hailong/AAAI25-MOS\n","authors":["Hai-Long Sun","Da-Wei Zhou","Hanbin Zhao","Le Gan","De-Chuan Zhan","Han-Jia Ye"],"pdf_url":"https://arxiv.org/pdf/2412.09441v1.pdf","comment":"Accepted to AAAI 2025. Code is available at:\n  https://github.com/sun-hailong/AAAI25-MOS"},{"id":"http://arxiv.org/abs/2412.09442v1","updated":"2024-12-12T16:57:20Z","published":"2024-12-12T16:57:20Z","title":"ATPrompt: Textual Prompt Learning with Embedded Attributes","summary":"  Textual-based prompt learning methods primarily employ multiple learnable\nsoft prompts and hard class tokens in a cascading manner as text prompt inputs,\naiming to align image and text (category) spaces for downstream tasks. However,\ncurrent training is restricted to aligning images with predefined known\ncategories and cannot be associated with unknown categories. In this work, we\npropose utilizing universal attributes as a bridge to enhance the alignment\nbetween images and unknown categories. Specifically, we introduce an\nAttribute-embedded Textual Prompt learning method for vision-language models,\nnamed ATPrompt. This approach expands the learning space of soft prompts from\nthe original one-dimensional category level into the multi-dimensional\nattribute level by incorporating multiple universal attribute tokens into the\nlearnable soft prompts. Through this modification, we transform the text prompt\nfrom a category-centric form to an attribute-category hybrid form. To finalize\nthe attributes for downstream tasks, we propose a differentiable attribute\nsearch method that learns to identify representative and suitable attributes\nfrom a candidate pool summarized by a large language model. As an easy-to-use\nplug-in technique, ATPrompt can seamlessly replace the existing prompt format\nof textual-based methods, offering general improvements at a negligible\ncomputational cost. Extensive experiments on 11 datasets demonstrate the\neffectiveness of our method.\n","authors":["Zheng Li","Yibing Song","Penghai Zhao","Ming-Ming Cheng","Xiang Li","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2412.09442v1.pdf","comment":"Technical Report. Project Page: https://zhengli97.github.io/ATPrompt/"},{"id":"http://arxiv.org/abs/2412.09439v1","updated":"2024-12-12T16:50:52Z","published":"2024-12-12T16:50:52Z","title":"Towards Robust and Fair Vision Learning in Open-World Environments","summary":"  The dissertation presents four key contributions toward fairness and\nrobustness in vision learning. First, to address the problem of large-scale\ndata requirements, the dissertation presents a novel Fairness Domain Adaptation\napproach derived from two major novel research findings of Bijective Maximum\nLikelihood and Fairness Adaptation Learning. Second, to enable the capability\nof open-world modeling of vision learning, this dissertation presents a novel\nOpen-world Fairness Continual Learning Framework. The success of this research\ndirection is the result of two research lines, i.e., Fairness Continual\nLearning and Open-world Continual Learning. Third, since visual data are often\ncaptured from multiple camera views, robust vision learning methods should be\ncapable of modeling invariant features across views. To achieve this desired\ngoal, the research in this thesis will present a novel Geometry-based\nCross-view Adaptation framework to learn robust feature representations across\nviews. Finally, with the recent increase in large-scale videos and multimodal\ndata, understanding the feature representations and improving the robustness of\nlarge-scale visual foundation models is critical. Therefore, this thesis will\npresent novel Transformer-based approaches to improve the robust feature\nrepresentations against multimodal and temporal data. Then, a novel Domain\nGeneralization Approach will be presented to improve the robustness of visual\nfoundation models. The research's theoretical analysis and experimental results\nhave shown the effectiveness of the proposed approaches, demonstrating their\nsuperior performance compared to prior studies. The contributions in this\ndissertation have advanced the fairness and robustness of machine vision\nlearning.\n","authors":["Thanh-Dat Truong"],"pdf_url":"https://arxiv.org/pdf/2412.09439v1.pdf","comment":"PhD Dissertation"},{"id":"http://arxiv.org/abs/2410.22101v2","updated":"2024-12-12T16:46:41Z","published":"2024-10-29T14:54:13Z","title":"Hyperspectral Imaging-Based Perception in Autonomous Driving Scenarios:\n  Benchmarking Baseline Semantic Segmentation Models","summary":"  Hyperspectral Imaging (HSI) is known for its advantages over traditional RGB\nimaging in remote sensing, agriculture, and medicine. Recently, it has gained\nattention for enhancing Advanced Driving Assistance Systems (ADAS) perception.\nSeveral HSI datasets such as HyKo, HSI-Drive, HSI-Road, and Hyperspectral City\nhave been made available. However, a comprehensive evaluation of semantic\nsegmentation models (SSM) using these datasets is lacking. To address this gap,\nwe evaluated the available annotated HSI datasets on four deep learning-based\nbaseline SSMs: DeepLab v3+, HRNet, PSPNet, and U-Net, along with its two\nvariants: Coordinate Attention (UNet-CA) and Convolutional Block-Attention\nModule (UNet-CBAM). The original model architectures were adapted to handle the\nvarying spatial and spectral dimensions of the datasets. These baseline SSMs\nwere trained using a class-weighted loss function for individual HSI datasets\nand evaluated using mean-based metrics such as intersection over union (IoU),\nrecall, precision, F1 score, specificity, and accuracy. Our results indicate\nthat UNet-CBAM, which extracts channel-wise features, outperforms other SSMs\nand shows potential to leverage spectral information for enhanced semantic\nsegmentation. This study establishes a baseline SSM benchmark on available\nannotated datasets for future evaluation of HSI-based ADAS perception. However,\nlimitations of current HSI datasets, such as limited dataset size, high class\nimbalance, and lack of fine-grained annotations, remain significant constraints\nfor developing robust SSMs for ADAS applications.\n","authors":["Imad Ali Shah","Jiarong Li","Martin Glavin","Edward Jones","Enda Ward","Brian Deegan"],"pdf_url":"https://arxiv.org/pdf/2410.22101v2.pdf","comment":"Accepted and Presented at IEEE WHISPERS 2024"},{"id":"http://arxiv.org/abs/2412.09428v1","updated":"2024-12-12T16:33:21Z","published":"2024-12-12T16:33:21Z","title":"Multimodal Music Generation with Explicit Bridges and Retrieval\n  Augmentation","summary":"  Multimodal music generation aims to produce music from diverse input\nmodalities, including text, videos, and images. Existing methods use a common\nembedding space for multimodal fusion. Despite their effectiveness in other\nmodalities, their application in multimodal music generation faces challenges\nof data scarcity, weak cross-modal alignment, and limited controllability. This\npaper addresses these issues by using explicit bridges of text and music for\nmultimodal alignment. We introduce a novel method named Visuals Music Bridge\n(VMB). Specifically, a Multimodal Music Description Model converts visual\ninputs into detailed textual descriptions to provide the text bridge; a\nDual-track Music Retrieval module that combines broad and targeted retrieval\nstrategies to provide the music bridge and enable user control. Finally, we\ndesign an Explicitly Conditioned Music Generation framework to generate music\nbased on the two bridges. We conduct experiments on video-to-music,\nimage-to-music, text-to-music, and controllable music generation tasks, along\nwith experiments on controllability. The results demonstrate that VMB\nsignificantly enhances music quality, modality, and customization alignment\ncompared to previous methods. VMB sets a new standard for interpretable and\nexpressive multimodal music generation with applications in various multimedia\nfields. Demos and code are available at https://github.com/wbs2788/VMB.\n","authors":["Baisen Wang","Le Zhuo","Zhaokai Wang","Chenxi Bao","Wu Chengjing","Xuecheng Nie","Jiao Dai","Jizhong Han","Yue Liao","Si Liu"],"pdf_url":"https://arxiv.org/pdf/2412.09428v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09427v1","updated":"2024-12-12T16:33:06Z","published":"2024-12-12T16:33:06Z","title":"A Plug-and-Play Algorithm for 3D Video Super-Resolution of Single-Photon\n  LiDAR data","summary":"  Single-photon avalanche diodes (SPADs) are advanced sensors capable of\ndetecting individual photons and recording their arrival times with picosecond\nresolution using time-correlated Single-Photon Counting detection techniques.\nThey are used in various applications, such as LiDAR, and can capture\nhigh-speed sequences of binary single-photon images, offering great potential\nfor reconstructing 3D environments with high motion dynamics. To complement\nsingle-photon data, they are often paired with conventional passive cameras,\nwhich capture high-resolution (HR) intensity images at a lower frame rate.\nHowever, 3D reconstruction from SPAD data faces challenges. Aggregating\nmultiple binary measurements improves precision and reduces noise but can cause\nmotion blur in dynamic scenes. Additionally, SPAD arrays often have lower\nresolution than passive cameras. To address these issues, we propose a novel\ncomputational imaging algorithm to improve the 3D reconstruction of moving\nscenes from SPAD data by addressing the motion blur and increasing the native\nspatial resolution. We adopt a plug-and-play approach within an optimization\nscheme alternating between guided video super-resolution of the 3D scene, and\nprecise image realignment using optical flow. Experiments on synthetic data\nshow significantly improved image resolutions across various signal-to-noise\nratios and photon levels. We validate our method using real-world SPAD\nmeasurements on three practical situations with dynamic objects. First on\nfast-moving scenes in laboratory conditions at short range; second very low\nresolution imaging of people with a consumer-grade SPAD sensor from\nSTMicroelectronics; and finally, HR imaging of people walking outdoors in\ndaylight at a range of 325 meters under eye-safe illumination conditions using\na short-wave infrared SPAD camera. These results demonstrate the robustness and\nversatility of our approach.\n","authors":["Alice Ruget","Lewis Wilson","Jonathan Leach","Rachael Tobin","Aongus Mccarthy","Gerald S. Buller","Steve Mclaughlin","Abderrahim Halimi"],"pdf_url":"https://arxiv.org/pdf/2412.09427v1.pdf","comment":"14 pages, 10 figures"},{"id":"http://arxiv.org/abs/2409.14747v4","updated":"2024-12-12T16:17:46Z","published":"2024-09-23T06:51:10Z","title":"Distribution-Level Feature Distancing for Machine Unlearning: Towards a\n  Better Trade-off Between Model Utility and Forgetting","summary":"  With the explosive growth of deep learning applications and increasing\nprivacy concerns, the right to be forgotten has become a critical requirement\nin various AI industries. For example, given a facial recognition system, some\nindividuals may wish to remove their personal data that might have been used in\nthe training phase. Unfortunately, deep neural networks sometimes unexpectedly\nleak personal identities, making this removal challenging. While recent machine\nunlearning algorithms aim to enable models to forget specific data, we identify\nan unintended utility drop-correlation collapse-in which the essential\ncorrelations between image features and true labels weaken during the\nforgetting process. To address this challenge, we propose Distribution-Level\nFeature Distancing (DLFD), a novel method that efficiently forgets instances\nwhile preserving task-relevant feature correlations. Our method synthesizes\ndata samples by optimizing the feature distribution to be distinctly different\nfrom that of forget samples, achieving effective results within a single\ntraining epoch. Through extensive experiments on facial recognition datasets,\nwe demonstrate that our approach significantly outperforms state-of-the-art\nmachine unlearning methods in both forgetting performance and model utility\npreservation.\n","authors":["Dasol Choi","Dongbin Na"],"pdf_url":"https://arxiv.org/pdf/2409.14747v4.pdf","comment":"10 pages, 6 figures, AAAI 2025 camera ready version"},{"id":"http://arxiv.org/abs/2412.09405v1","updated":"2024-12-12T16:09:57Z","published":"2024-12-12T16:09:57Z","title":"Learned Compression for Compressed Learning","summary":"  Modern sensors produce increasingly rich streams of high-resolution data. Due\nto resource constraints, machine learning systems discard the vast majority of\nthis information via resolution reduction. Compressed-domain learning allows\nmodels to operate on compact latent representations, allowing higher effective\nresolution for the same budget. However, existing compression systems are not\nideal for compressed learning. Linear transform coding and end-to-end learned\ncompression systems reduce bitrate, but do not uniformly reduce dimensionality;\nthus, they do not meaningfully increase efficiency. Generative autoencoders\nreduce dimensionality, but their adversarial or perceptual objectives lead to\nsignificant information loss. To address these limitations, we introduce WaLLoC\n(Wavelet Learned Lossy Compression), a neural codec architecture that combines\nlinear transform coding with nonlinear dimensionality-reducing autoencoders.\nWaLLoC sandwiches a shallow, asymmetric autoencoder and entropy bottleneck\nbetween an invertible wavelet packet transform. Across several key metrics,\nWaLLoC outperforms the autoencoders used in state-of-the-art latent diffusion\nmodels. WaLLoC does not require perceptual or adversarial losses to represent\nhigh-frequency detail, providing compatibility with modalities beyond RGB\nimages and stereo audio. WaLLoC's encoder consists almost entirely of linear\noperations, making it exceptionally efficient and suitable for mobile\ncomputing, remote sensing, and learning directly from compressed data. We\ndemonstrate WaLLoC's capability for compressed-domain learning across several\ntasks, including image classification, colorization, document understanding,\nand music source separation. Our code, experiments, and pre-trained audio and\nimage codecs are available at https://ut-sysml.org/walloc\n","authors":["Dan Jacobellis","Neeraja J. Yadwadkar"],"pdf_url":"https://arxiv.org/pdf/2412.09405v1.pdf","comment":"Accepted as paper to 2025 IEEE Data Compression Conference"},{"id":"http://arxiv.org/abs/2412.09402v1","updated":"2024-12-12T16:08:43Z","published":"2024-12-12T16:08:43Z","title":"MultiEYE: Dataset and Benchmark for OCT-Enhanced Retinal Disease\n  Recognition from Fundus Images","summary":"  Existing multi-modal learning methods on fundus and OCT images mostly require\nboth modalities to be available and strictly paired for training and testing,\nwhich appears less practical in clinical scenarios. To expand the scope of\nclinical applications, we formulate a novel setting, \"OCT-enhanced disease\nrecognition from fundus images\", that allows for the use of unpaired\nmulti-modal data during the training phase and relies on the widespread fundus\nphotographs for testing. To benchmark this setting, we present the first large\nmulti-modal multi-class dataset for eye disease diagnosis, MultiEYE, and\npropose an OCT-assisted Conceptual Distillation Approach (OCT-CoDA), which\nemploys semantically rich concepts to extract disease-related knowledge from\nOCT images and leverage them into the fundus model. Specifically, we regard the\nimage-concept relation as a link to distill useful knowledge from the OCT\nteacher model to the fundus student model, which considerably improves the\ndiagnostic performance based on fundus images and formulates the cross-modal\nknowledge transfer into an explainable process. Through extensive experiments\non the multi-disease classification task, our proposed OCT-CoDA demonstrates\nremarkable results and interpretability, showing great potential for clinical\napplication. Our dataset and code are available at\nhttps://github.com/xmed-lab/MultiEYE.\n","authors":["Lehan Wang","Chongchong Qi","Chubin Ou","Lin An","Mei Jin","Xiangbin Kong","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2412.09402v1.pdf","comment":"Accepted at IEEE TMI"},{"id":"http://arxiv.org/abs/2412.09401v1","updated":"2024-12-12T16:08:03Z","published":"2024-12-12T16:08:03Z","title":"SLAM3R: Real-Time Dense Scene Reconstruction from Monocular RGB Videos","summary":"  In this paper, we introduce \\textbf{SLAM3R}, a novel and effective monocular\nRGB SLAM system for real-time and high-quality dense 3D reconstruction. SLAM3R\nprovides an end-to-end solution by seamlessly integrating local 3D\nreconstruction and global coordinate registration through feed-forward neural\nnetworks. Given an input video, the system first converts it into overlapping\nclips using a sliding window mechanism. Unlike traditional pose\noptimization-based methods, SLAM3R directly regresses 3D pointmaps from RGB\nimages in each window and progressively aligns and deforms these local\npointmaps to create a globally consistent scene reconstruction - all without\nexplicitly solving any camera parameters. Experiments across datasets\nconsistently show that SLAM3R achieves state-of-the-art reconstruction accuracy\nand completeness while maintaining real-time performance at 20+ FPS. Code and\nweights at: \\url{https://github.com/PKU-VCL-3DV/SLAM3R}.\n","authors":["Yuzheng Liu","Siyan Dong","Shuzhe Wang","Yingda Yin","Yanchao Yang","Qingnan Fan","Baoquan Chen"],"pdf_url":"https://arxiv.org/pdf/2412.09401v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09389v1","updated":"2024-12-12T15:56:26Z","published":"2024-12-12T15:56:26Z","title":"UFO: Enhancing Diffusion-Based Video Generation with a Uniform Frame\n  Organizer","summary":"  Recently, diffusion-based video generation models have achieved significant\nsuccess. However, existing models often suffer from issues like weak\nconsistency and declining image quality over time. To overcome these\nchallenges, inspired by aesthetic principles, we propose a non-invasive plug-in\ncalled Uniform Frame Organizer (UFO), which is compatible with any\ndiffusion-based video generation model. The UFO comprises a series of adaptive\nadapters with adjustable intensities, which can significantly enhance the\nconsistency between the foreground and background of videos and improve image\nquality without altering the original model parameters when integrated. The\ntraining for UFO is simple, efficient, requires minimal resources, and supports\nstylized training. Its modular design allows for the combination of multiple\nUFOs, enabling the customization of personalized video generation models.\nFurthermore, the UFO also supports direct transferability across different\nmodels of the same specification without the need for specific retraining. The\nexperimental results indicate that UFO effectively enhances video generation\nquality and demonstrates its superiority in public video generation benchmarks.\nThe code will be publicly available at https://github.com/Delong-liu-bupt/UFO.\n","authors":["Delong Liu","Zhaohui Hou","Mingjie Zhan","Shihao Han","Zhicheng Zhao","Fei Su"],"pdf_url":"https://arxiv.org/pdf/2412.09389v1.pdf","comment":"Code:https://github.com/Delong-liu-bupt/UFO"},{"id":"http://arxiv.org/abs/2412.09388v1","updated":"2024-12-12T15:56:20Z","published":"2024-12-12T15:56:20Z","title":"All You Need in Knowledge Distillation Is a Tailored Coordinate System","summary":"  Knowledge Distillation (KD) is essential in transferring dark knowledge from\na large teacher to a small student network, such that the student can be much\nmore efficient than the teacher but with comparable accuracy. Existing KD\nmethods, however, rely on a large teacher trained specifically for the target\ntask, which is both very inflexible and inefficient. In this paper, we argue\nthat a SSL-pretrained model can effectively act as the teacher and its dark\nknowledge can be captured by the coordinate system or linear subspace where the\nfeatures lie in. We then need only one forward pass of the teacher, and then\ntailor the coordinate system (TCS) for the student network. Our TCS method is\nteacher-free and applies to diverse architectures, works well for KD and\npractical few-shot learning, and allows cross-architecture distillation with\nlarge capacity gap. Experiments show that TCS achieves significantly higher\naccuracy than state-of-the-art KD methods, while only requiring roughly half of\ntheir training time and GPU memory costs.\n","authors":["Junjie Zhou","Ke Zhu","Jianxin Wu"],"pdf_url":"https://arxiv.org/pdf/2412.09388v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09386v1","updated":"2024-12-12T15:53:14Z","published":"2024-12-12T15:53:14Z","title":"Multi-Stage Segmentation and Cascade Classification Methods for\n  Improving Cardiac MRI Analysis","summary":"  The segmentation and classification of cardiac magnetic resonance imaging are\ncritical for diagnosing heart conditions, yet current approaches face\nchallenges in accuracy and generalizability. In this study, we aim to further\nadvance the segmentation and classification of cardiac magnetic resonance\nimages by introducing a novel deep learning-based approach. Using a multi-stage\nprocess with U-Net and ResNet models for segmentation, followed by Gaussian\nsmoothing, the method improved segmentation accuracy, achieving a Dice\ncoefficient of 0.974 for the left ventricle and 0.947 for the right ventricle.\nFor classification, a cascade of deep learning classifiers was employed to\ndistinguish heart conditions, including hypertrophic cardiomyopathy, myocardial\ninfarction, and dilated cardiomyopathy, achieving an average accuracy of 97.2%.\nThe proposed approach outperformed existing models, enhancing segmentation\naccuracy and classification precision. These advancements show promise for\nclinical applications, though further validation and interpretation across\ndiverse imaging protocols is necessary.\n","authors":["Vitalii Slobodzian","Pavlo Radiuk","Oleksander Barmak","Iurii Krak"],"pdf_url":"https://arxiv.org/pdf/2412.09386v1.pdf","comment":"Cardiac MRI, heart pathology, deep learning, segmentation, Gaussian\n  smoothing, classification, cascade"},{"id":"http://arxiv.org/abs/2411.06908v2","updated":"2024-12-12T15:40:54Z","published":"2024-11-11T12:11:36Z","title":"EVQAScore: Efficient Video Question Answering Data Evaluation","summary":"  Video question-answering (QA) is a core task in video understanding.\nEvaluating the quality of video QA and video caption data quality for training\nvideo large language models (VideoLLMs) is an essential challenge. Although\nvarious methods have been proposed for assessing video caption quality, there\nremains a lack of dedicated evaluation methods for Video QA. To address this\ngap, we introduce EVQAScore, a reference-free method that leverages keyword\nextraction to assess both video caption and video QA data quality.\nAdditionally, we incorporate frame sampling and rescaling techniques to enhance\nthe efficiency and robustness of our evaluation, this enables our score to\nevaluate the quality of extremely long videos. Our approach achieves\nstate-of-the-art (SOTA) performance (32.8 for Kendall correlation and 42.3 for\nSpearman correlation, 4.7 and 5.9 higher than the previous method PAC-S++) on\nthe VATEX-EVAL benchmark for video caption evaluation. Furthermore, by using\nEVQAScore for data selection, we achieved SOTA results with only 12.5\\% of the\noriginal data volume, outperforming the previous SOTA method PAC-S and 100\\% of\ndata.\n","authors":["Hao Liang","Zirong Chen","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.06908v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13809v2","updated":"2024-12-12T15:40:49Z","published":"2024-08-25T11:10:15Z","title":"On the Robustness of Kolmogorov-Arnold Networks: An Adversarial\n  Perspective","summary":"  Kolmogorov-Arnold Networks (KANs) have recently emerged as a novel approach\nto function approximation, demonstrating remarkable potential in various\ndomains. Despite their theoretical promise, the robustness of KANs under\nadversarial conditions has yet to be thoroughly examined. In this paper we\nexplore the adversarial robustness of KANs, with a particular focus on image\nclassification tasks. We assess the performance of KANs against standard white\nbox and black-box adversarial attacks, comparing their resilience to that of\nestablished neural network architectures. Our experimental evaluation\nencompasses a variety of standard image classification benchmark datasets and\ninvestigates both fully connected and convolutional neural network\narchitectures, of three sizes: small, medium, and large. We conclude that\nsmall- and medium-sized KANs (either fully connected or convolutional) are not\nconsistently more robust than their standard counterparts, but that large-sized\nKANs are, by and large, more robust. This comprehensive evaluation of KANs in\nadversarial scenarios offers the first in-depth analysis of KAN security,\nlaying the groundwork for future research in this emerging field.\n","authors":["Tal Alter","Raz Lapid","Moshe Sipper"],"pdf_url":"https://arxiv.org/pdf/2408.13809v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08357v2","updated":"2024-12-12T15:33:55Z","published":"2024-12-11T13:02:09Z","title":"Video Summarization using Denoising Diffusion Probabilistic Model","summary":"  Video summarization aims to eliminate visual redundancy while retaining key\nparts of video to construct concise and comprehensive synopses. Most existing\nmethods use discriminative models to predict the importance scores of video\nframes. However, these methods are susceptible to annotation inconsistency\ncaused by the inherent subjectivity of different annotators when annotating the\nsame video. In this paper, we introduce a generative framework for video\nsummarization that learns how to generate summaries from a probability\ndistribution perspective, effectively reducing the interference of subjective\nannotation noise. Specifically, we propose a novel diffusion summarization\nmethod based on the Denoising Diffusion Probabilistic Model (DDPM), which\nlearns the probability distribution of training data through noise prediction,\nand generates summaries by iterative denoising. Our method is more resistant to\nsubjective annotation noise, and is less prone to overfitting the training data\nthan discriminative methods, with strong generalization ability. Moreover, to\nfacilitate training DDPM with limited data, we employ an unsupervised video\nsummarization model to implement the earlier denoising process. Extensive\nexperiments on various datasets (TVSum, SumMe, and FPVSum) demonstrate the\neffectiveness of our method.\n","authors":["Zirui Shang","Yubo Zhu","Hongxi Li","Shuo Yang","Xinxiao Wu"],"pdf_url":"https://arxiv.org/pdf/2412.08357v2.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.04464v2","updated":"2024-12-12T15:26:07Z","published":"2024-12-05T18:59:48Z","title":"DualPM: Dual Posed-Canonical Point Maps for 3D Shape and Pose\n  Reconstruction","summary":"  The choice of data representation is a key factor in the success of deep\nlearning in geometric tasks. For instance, DUSt3R has recently introduced the\nconcept of viewpoint-invariant point maps, generalizing depth prediction, and\nshowing that one can reduce all the key problems in the 3D reconstruction of\nstatic scenes to predicting such point maps. In this paper, we develop an\nanalogous concept for a very different problem, namely, the reconstruction of\nthe 3D shape and pose of deformable objects. To this end, we introduce the Dual\nPoint Maps (DualPM), where a pair of point maps is extracted from the same\nimage, one associating pixels to their 3D locations on the object, and the\nother to a canonical version of the object at rest pose. We also extend point\nmaps to amodal reconstruction, seeing through self-occlusions to obtain the\ncomplete shape of the object. We show that 3D reconstruction and 3D pose\nestimation reduce to the prediction of the DualPMs. We demonstrate empirically\nthat this representation is a good target for a deep network to predict;\nspecifically, we consider modeling horses, showing that DualPMs can be trained\npurely on 3D synthetic data, consisting of a single model of a horse, while\ngeneralizing very well to real images. With this, we improve by a large margin\nprevious methods for the 3D analysis and reconstruction of this type of\nobjects.\n","authors":["Ben Kaye","Tomas Jakab","Shangzhe Wu","Christian Rupprecht","Andrea Vedaldi"],"pdf_url":"https://arxiv.org/pdf/2412.04464v2.pdf","comment":"First two authors contributed equally. Project page:\n  https://dualpm.github.io"},{"id":"http://arxiv.org/abs/2412.09353v1","updated":"2024-12-12T15:22:03Z","published":"2024-12-12T15:22:03Z","title":"Causal Graphical Models for Vision-Language Compositional Understanding","summary":"  Recent work has empirically shown that Vision-Language Models (VLMs) struggle\nto fully understand the compositional properties of the human language, usually\nmodeling an image caption as a \"bag of words\". As a result, they perform poorly\non compositional tasks, which require a deeper understanding of the different\nentities of a sentence (subject, verb, etc.) jointly with their mutual\nrelationships in order to be solved. In this paper, we model the dependency\nrelations among textual and visual tokens using a Causal Graphical Model (CGM),\nbuilt using a dependency parser, and we train a decoder conditioned by the VLM\nvisual encoder. Differently from standard autoregressive or parallel\npredictions, our decoder's generative process is partially-ordered following\nthe CGM structure. This structure encourages the decoder to learn only the main\ncausal dependencies in a sentence discarding spurious correlations. Using\nextensive experiments on five compositional benchmarks, we show that our method\nsignificantly outperforms all the state-of-the-art compositional approaches by\na large margin, and it also improves over methods trained using much larger\ndatasets.\n","authors":["Fiorenzo Parascandolo","Nicholas Moratelli","Enver Sangineto","Lorenzo Baraldi","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2412.09353v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09349v1","updated":"2024-12-12T15:15:59Z","published":"2024-12-12T15:15:59Z","title":"DisPose: Disentangling Pose Guidance for Controllable Human Image\n  Animation","summary":"  Controllable human image animation aims to generate videos from reference\nimages using driving videos. Due to the limited control signals provided by\nsparse guidance (e.g., skeleton pose), recent works have attempted to introduce\nadditional dense conditions (e.g., depth map) to ensure motion alignment.\nHowever, such strict dense guidance impairs the quality of the generated video\nwhen the body shape of the reference character differs significantly from that\nof the driving video. In this paper, we present DisPose to mine more\ngeneralizable and effective control signals without additional dense input,\nwhich disentangles the sparse skeleton pose in human image animation into\nmotion field guidance and keypoint correspondence. Specifically, we generate a\ndense motion field from a sparse motion field and the reference image, which\nprovides region-level dense guidance while maintaining the generalization of\nthe sparse pose control. We also extract diffusion features corresponding to\npose keypoints from the reference image, and then these point features are\ntransferred to the target pose to provide distinct identity information. To\nseamlessly integrate into existing models, we propose a plug-and-play hybrid\nControlNet that improves the quality and consistency of generated videos while\nfreezing the existing model parameters. Extensive qualitative and quantitative\nexperiments demonstrate the superiority of DisPose compared to current methods.\nCode:\n\\hyperlink{https://github.com/lihxxx/DisPose}{https://github.com/lihxxx/DisPose}.\n","authors":["Hongxiang Li","Yaowei Li","Yuhang Yang","Junjie Cao","Zhihong Zhu","Xuxin Cheng","Long Chen"],"pdf_url":"https://arxiv.org/pdf/2412.09349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09346v1","updated":"2024-12-12T15:13:34Z","published":"2024-12-12T15:13:34Z","title":"Quantitative Evaluation of Motif Sets in Time Series","summary":"  Time Series Motif Discovery (TSMD), which aims at finding recurring patterns\nin time series, is an important task in numerous application domains, and many\nmethods for this task exist. These methods are usually evaluated qualitatively.\nA few metrics for quantitative evaluation, where discovered motifs are compared\nto some ground truth, have been proposed, but they typically make implicit\nassumptions that limit their applicability. This paper introduces PROM, a\nbroadly applicable metric that overcomes those limitations, and TSMD-Bench, a\nbenchmark for quantitative evaluation of time series motif discovery.\nExperiments with PROM and TSMD-Bench show that PROM provides a more\ncomprehensive evaluation than existing metrics, that TSMD-Bench is a more\nchallenging benchmark than earlier ones, and that the combination can help\nunderstand the relative performance of TSMD methods. More generally, the\nproposed approach enables large-scale, systematic performance comparisons in\nthis field.\n","authors":["Daan Van Wesenbeeck","Aras Yurtman","Wannes Meert","Hendrik Blockeel"],"pdf_url":"https://arxiv.org/pdf/2412.09346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09333v1","updated":"2024-12-12T15:01:39Z","published":"2024-12-12T15:01:39Z","title":"MaskTerial: A Foundation Model for Automated 2D Material Flake Detection","summary":"  The detection and classification of exfoliated two-dimensional (2D) material\nflakes from optical microscope images can be automated using computer vision\nalgorithms. This has the potential to increase the accuracy and objectivity of\nclassification and the efficiency of sample fabrication, and it allows for\nlarge-scale data collection. Existing algorithms often exhibit challenges in\nidentifying low-contrast materials and typically require large amounts of\ntraining data. Here, we present a deep learning model, called MaskTerial, that\nuses an instance segmentation network to reliably identify 2D material flakes.\nThe model is extensively pre-trained using a synthetic data generator, that\ngenerates realistic microscopy images from unlabeled data. This results in a\nmodel that can to quickly adapt to new materials with as little as 5 to 10\nimages. Furthermore, an uncertainty estimation model is used to finally\nclassify the predictions based on optical contrast. We evaluate our method on\neight different datasets comprising five different 2D materials and demonstrate\nsignificant improvements over existing techniques in the detection of\nlow-contrast materials such as hexagonal boron nitride.\n","authors":["Jan-Lucas Uslu","Alexey Nekrasov","Alexander Hermans","Bernd Beschoten","Bastian Leibe","Lutz Waldecker","Christoph Stampfer"],"pdf_url":"https://arxiv.org/pdf/2412.09333v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2412.09331v1","updated":"2024-12-12T14:59:56Z","published":"2024-12-12T14:59:56Z","title":"Physics-Driven Autoregressive State Space Models for Medical Image\n  Reconstruction","summary":"  Medical image reconstruction from undersampled acquisitions is an ill-posed\nproblem that involves inversion of the imaging operator linking measurement and\nimage domains. In recent years, physics-driven (PD) models have gained\nprominence in learning-based reconstruction given their enhanced balance\nbetween efficiency and performance. For reconstruction, PD models cascade\ndata-consistency modules that enforce fidelity to acquired data based on the\nimaging operator, with network modules that process feature maps to alleviate\nimage artifacts due to undersampling. Success in artifact suppression\ninevitably depends on the ability of the network modules to tease apart\nartifacts from underlying tissue structures, both of which can manifest\ncontextual relations over broad spatial scales. Convolutional modules that\nexcel at capturing local correlations are relatively insensitive to non-local\ncontext. While transformers promise elevated sensitivity to non-local context,\npractical implementations often suffer from a suboptimal trade-off between\nlocal and non-local sensitivity due to intrinsic model complexity. Here, we\nintroduce a novel physics-driven autoregressive state space model (MambaRoll)\nfor enhanced fidelity in medical image reconstruction. In each cascade of an\nunrolled architecture, MambaRoll employs an autoregressive framework based on\nphysics-driven state space modules (PSSM), where PSSMs efficiently aggregate\ncontextual features at a given spatial scale while maintaining fidelity to\nacquired data, and autoregressive prediction of next-scale feature maps from\nearlier spatial scales enhance capture of multi-scale contextual features.\nDemonstrations on accelerated MRI and sparse-view CT reconstructions indicate\nthat MambaRoll outperforms state-of-the-art PD methods based on convolutional,\ntransformer and conventional SSM modules.\n","authors":["Bilal Kabas","Fuat Arslan","Valiyeh A. Nezhad","Saban Ozturk","Emine U. Saritas","Tolga Çukur"],"pdf_url":"https://arxiv.org/pdf/2412.09331v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2412.09330v1","updated":"2024-12-12T14:59:10Z","published":"2024-12-12T14:59:10Z","title":"Computer-Aided Osteoporosis Diagnosis Using Transfer Learning with\n  Enhanced Features from Stacked Deep Learning Modules","summary":"  Knee osteoporosis weakens the bone tissue in the knee joint, increasing\nfracture risk. Early detection through X-ray images enables timely intervention\nand improved patient outcomes. While some researchers have focused on\ndiagnosing knee osteoporosis through manual radiology evaluation and\ntraditional machine learning using hand-crafted features, these methods often\nstruggle with performance and efficiency due to reliance on manual feature\nextraction and subjective interpretation. In this study, we propose a\ncomputer-aided diagnosis (CAD) system for knee osteoporosis, combining transfer\nlearning with stacked feature enhancement deep learning blocks. Initially, knee\nX-ray images are preprocessed, and features are extracted using a pre-trained\nConvolutional Neural Network (CNN). These features are then enhanced through\nfive sequential Conv-RELU-MaxPooling blocks. The Conv2D layers detect low-level\nfeatures, while the ReLU activations introduce non-linearity, allowing the\nnetwork to learn complex patterns. MaxPooling layers down-sample the features,\nretaining the most important spatial information. This sequential processing\nenables the model to capture complex, high-level features related to bone\nstructure, joint deformation, and osteoporotic markers. The enhanced features\nare passed through a classification module to differentiate between healthy and\nosteoporotic knee conditions. Extensive experiments on three individual\ndatasets and a combined dataset demonstrate that our model achieves 97.32%,\n98.24%, 97.27%, and 98.00% accuracy for OKX Kaggle Binary, KXO-Mendeley\nMulti-Class, OKX Kaggle Multi-Class, and the combined dataset, respectively,\nshowing an improvement of around 2% over existing methods.\n","authors":["Ayesha Siddiqua","Rakibul Hasan","Anichur Rahman","Abu Saleh Musa Miah"],"pdf_url":"https://arxiv.org/pdf/2412.09330v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09324v1","updated":"2024-12-12T14:49:55Z","published":"2024-12-12T14:49:55Z","title":"Are Conditional Latent Diffusion Models Effective for Image Restoration?","summary":"  Recent advancements in image restoration increasingly employ conditional\nlatent diffusion models (CLDMs). While these models have demonstrated notable\nperformance improvements in recent years, this work questions their suitability\nfor IR tasks. CLDMs excel in capturing high-level semantic correlations, making\nthem effective for tasks like text-to-image generation with spatial\nconditioning. However, in IR, where the goal is to enhance image perceptual\nquality, these models face difficulty of modeling the relationship between\ndegraded images and ground truth images using a low-level representation. To\nsupport our claims, we compare state-of-the-art CLDMs with traditional image\nrestoration models through extensive experiments. Results reveal that despite\nthe scaling advantages of CLDMs, they suffer from high distortion and semantic\ndeviation, especially in cases with minimal degradation, where traditional\nmethods outperform them. Additionally, we perform empirical studies to examine\nthe impact of various CLDM design elements on their restoration performance. We\nhope this finding inspires a reexamination of current CLDM-based IR solutions,\nopening up more opportunities in this field.\n","authors":["Yunchen Yuan","Junyuan Xiao","Xinjie Li"],"pdf_url":"https://arxiv.org/pdf/2412.09324v1.pdf","comment":"16 pages, 12 figures, submitted to IEEE / CVF Computer Vision and\n  Pattern Recognition Conference (CVPR 2025)"},{"id":"http://arxiv.org/abs/2412.09323v1","updated":"2024-12-12T14:48:46Z","published":"2024-12-12T14:48:46Z","title":"T-SVG: Text-Driven Stereoscopic Video Generation","summary":"  The advent of stereoscopic videos has opened new horizons in multimedia,\nparticularly in extended reality (XR) and virtual reality (VR) applications,\nwhere immersive content captivates audiences across various platforms. Despite\nits growing popularity, producing stereoscopic videos remains challenging due\nto the technical complexities involved in generating stereo parallax. This\nrefers to the positional differences of objects viewed from two distinct\nperspectives and is crucial for creating depth perception. This complex process\nposes significant challenges for creators aiming to deliver convincing and\nengaging presentations. To address these challenges, this paper introduces the\nText-driven Stereoscopic Video Generation (T-SVG) system. This innovative,\nmodel-agnostic, zero-shot approach streamlines video generation by using text\nprompts to create reference videos. These videos are transformed into 3D point\ncloud sequences, which are rendered from two perspectives with subtle parallax\ndifferences, achieving a natural stereoscopic effect. T-SVG represents a\nsignificant advancement in stereoscopic content creation by integrating\nstate-of-the-art, training-free techniques in text-to-video generation, depth\nestimation, and video inpainting. Its flexible architecture ensures high\nefficiency and user-friendliness, allowing seamless updates with newer models\nwithout retraining. By simplifying the production pipeline, T-SVG makes\nstereoscopic video generation accessible to a broader audience, demonstrating\nits potential to revolutionize the field.\n","authors":["Qiao Jin","Xiaodong Chen","Wu Liu","Tao Mei","Yongdong Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.09323v1.pdf","comment":"5 pages, 4 figures"},{"id":"http://arxiv.org/abs/2412.09319v1","updated":"2024-12-12T14:44:05Z","published":"2024-12-12T14:44:05Z","title":"FAMNet: Frequency-aware Matching Network for Cross-domain Few-shot\n  Medical Image Segmentation","summary":"  Existing few-shot medical image segmentation (FSMIS) models fail to address a\npractical issue in medical imaging: the domain shift caused by different\nimaging techniques, which limits the applicability to current FSMIS tasks. To\novercome this limitation, we focus on the cross-domain few-shot medical image\nsegmentation (CD-FSMIS) task, aiming to develop a generalized model capable of\nadapting to a broader range of medical image segmentation scenarios with\nlimited labeled data from the novel target domain. Inspired by the\ncharacteristics of frequency domain similarity across different domains, we\npropose a Frequency-aware Matching Network (FAMNet), which includes two key\ncomponents: a Frequency-aware Matching (FAM) module and a Multi-Spectral Fusion\n(MSF) module. The FAM module tackles two problems during the meta-learning\nphase: 1) intra-domain variance caused by the inherent support-query bias, due\nto the different appearances of organs and lesions, and 2) inter-domain\nvariance caused by different medical imaging techniques. Additionally, we\ndesign an MSF module to integrate the different frequency features decoupled by\nthe FAM module, and further mitigate the impact of inter-domain variance on the\nmodel's segmentation performance. Combining these two modules, our FAMNet\nsurpasses existing FSMIS models and Cross-domain Few-shot Semantic Segmentation\nmodels on three cross-domain datasets, achieving state-of-the-art performance\nin the CD-FSMIS task.\n","authors":["Yuntian Bo","Yazhou Zhu","Lunbo Li","Haofeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.09319v1.pdf","comment":"Accepted by the 39th Annual AAAI Conference on Artificial\n  Intelligence (AAAI-25)"},{"id":"http://arxiv.org/abs/2408.11447v3","updated":"2024-12-12T14:42:30Z","published":"2024-08-21T09:06:30Z","title":"GaussianOcc: Fully Self-supervised and Efficient 3D Occupancy Estimation\n  with Gaussian Splatting","summary":"  We introduce GaussianOcc, a systematic method that investigates the two\nusages of Gaussian splatting for fully self-supervised and efficient 3D\noccupancy estimation in surround views. First, traditional methods for\nself-supervised 3D occupancy estimation still require ground truth 6D poses\nfrom sensors during training. To address this limitation, we propose Gaussian\nSplatting for Projection (GSP) module to provide accurate scale information for\nfully self-supervised training from adjacent view projection. Additionally,\nexisting methods rely on volume rendering for final 3D voxel representation\nlearning using 2D signals (depth maps, semantic maps), which is both\ntime-consuming and less effective. We propose Gaussian Splatting from Voxel\nspace (GSV) to leverage the fast rendering properties of Gaussian splatting. As\na result, the proposed GaussianOcc method enables fully self-supervised (no\nground truth pose) 3D occupancy estimation in competitive performance with low\ncomputational cost (2.7 times faster in training and 5 times faster in\nrendering). The relevant code is available in\nhttps://github.com/GANWANSHUI/GaussianOcc.git.\n","authors":["Wanshui Gan","Fang Liu","Hongbin Xu","Ningkai Mo","Naoto Yokoya"],"pdf_url":"https://arxiv.org/pdf/2408.11447v3.pdf","comment":"Project page: https://ganwanshui.github.io/GaussianOcc/"},{"id":"http://arxiv.org/abs/2412.09317v1","updated":"2024-12-12T14:42:10Z","published":"2024-12-12T14:42:10Z","title":"Multimodal Sentiment Analysis based on Video and Audio Inputs","summary":"  Despite the abundance of current researches working on the sentiment analysis\nfrom videos and audios, finding the best model that gives the highest accuracy\nrate is still considered a challenge for researchers in this field. The main\nobjective of this paper is to prove the usability of emotion recognition models\nthat take video and audio inputs. The datasets used to train the models are the\nCREMA-D dataset for audio and the RAVDESS dataset for video. The fine-tuned\nmodels that been used are: Facebook/wav2vec2-large for audio and the\nGoogle/vivit-b-16x2-kinetics400 for video. The avarage of the probabilities for\neach emotion generated by the two previous models is utilized in the decision\nmaking framework. After disparity in the results, if one of the models gets\nmuch higher accuracy, another test framework is created. The methods used are\nthe Weighted Average method, the Confidence Level Threshold method, the Dynamic\nWeighting Based on Confidence method, and the Rule-Based Logic method. This\nlimited approach gives encouraging results that make future research into these\nmethods viable.\n","authors":["Antonio Fernandez","Suzan Awinat"],"pdf_url":"https://arxiv.org/pdf/2412.09317v1.pdf","comment":"Presented as a full paper in the 15th International Conference on\n  Emerging Ubiquitous Systems and Pervasive Networks (EUSPN 2024) October\n  28-30, 2024, Leuven, Belgium"},{"id":"http://arxiv.org/abs/2408.08984v2","updated":"2024-12-12T14:37:28Z","published":"2024-08-16T19:25:19Z","title":"Fire Dynamic Vision: Image Segmentation and Tracking for Multi-Scale\n  Fire and Plume Behavior","summary":"  The increasing frequency and severity of wildfires highlight the need for\naccurate fire and plume spread models. We introduce an approach that\neffectively isolates and tracks fire and plume behavior across various spatial\nand temporal scales and image types, identifying physical phenomena in the\nsystem and providing insights useful for developing and validating models. Our\nmethod combines image segmentation and graph theory to delineate fire fronts\nand plume boundaries. We demonstrate that the method effectively distinguishes\nfires and plumes from visually similar objects. Results demonstrate the\nsuccessful isolation and tracking of fire and plume dynamics across various\nimage sources, ranging from synoptic-scale ($10^4$-$10^5$ m) satellite images\nto sub-microscale ($10^0$-$10^1$ m) images captured close to the fire\nenvironment. Furthermore, the methodology leverages image inpainting and\nspatio-temporal dataset generation for use in statistical and machine learning\nmodels.\n","authors":["Daryn Sagel","Bryan Quaife"],"pdf_url":"https://arxiv.org/pdf/2408.08984v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.00727v2","updated":"2024-12-12T14:28:42Z","published":"2024-12-01T08:39:12Z","title":"Perturb and Recover: Fine-tuning for Effective Backdoor Removal from\n  CLIP","summary":"  Vision-Language models like CLIP have been shown to be highly effective at\nlinking visual perception and natural language understanding, enabling\nsophisticated image-text capabilities, including strong retrieval and zero-shot\nclassification performance. Their widespread use, as well as the fact that CLIP\nmodels are trained on image-text pairs from the web, make them both a\nworthwhile and relatively easy target for backdoor attacks. As training\nfoundational models, such as CLIP, from scratch is very expensive, this paper\nfocuses on cleaning potentially poisoned models via fine-tuning. We first show\nthat existing cleaning techniques are not effective against simple structured\ntriggers used in Blended or BadNet backdoor attacks, exposing a critical\nvulnerability for potential real-world deployment of these models. Then, we\nintroduce PAR, Perturb and Recover, a surprisingly simple yet effective\nmechanism to remove backdoors from CLIP models. Through extensive experiments\nacross different encoders and types of backdoor attacks, we show that PAR\nachieves high backdoor removal rate while preserving good standard performance.\nFinally, we illustrate that our approach is effective even only with synthetic\ntext-image pairs, i.e. without access to real training data. The code and\nmodels are available at https://github.com/nmndeep/PerturbAndRecover.\n","authors":["Naman Deep Singh","Francesco Croce","Matthias Hein"],"pdf_url":"https://arxiv.org/pdf/2412.00727v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09311v1","updated":"2024-12-12T14:25:56Z","published":"2024-12-12T14:25:56Z","title":"Advancing Attribution-Based Neural Network Explainability through\n  Relative Absolute Magnitude Layer-Wise Relevance Propagation and\n  Multi-Component Evaluation","summary":"  Recent advancement in deep-neural network performance led to the development\nof new state-of-the-art approaches in numerous areas. However, the black-box\nnature of neural networks often prohibits their use in areas where model\nexplainability and model transparency are crucial. Over the years, researchers\nproposed many algorithms to aid neural network understanding and provide\nadditional information to the human expert. One of the most popular methods\nbeing Layer-Wise Relevance Propagation (LRP). This method assigns local\nrelevance based on the pixel-wise decomposition of nonlinear classifiers. With\nthe rise of attribution method research, there has emerged a pressing need to\nassess and evaluate their performance. Numerous metrics have been proposed,\neach assessing an individual property of attribution methods such as\nfaithfulness, robustness or localization. Unfortunately, no single metric is\ndeemed optimal for every case, and researchers often use several metrics to\ntest the quality of the attribution maps. In this work, we address the\nshortcomings of the current LRP formulations and introduce a novel method for\ndetermining the relevance of input neurons through layer-wise relevance\npropagation. Furthermore, we apply this approach to the recently developed\nVision Transformer architecture and evaluate its performance against existing\nmethods on two image classification datasets, namely ImageNet and PascalVOC.\nOur results clearly demonstrate the advantage of our proposed method.\nFurthermore, we discuss the insufficiencies of current evaluation metrics for\nattribution-based explainability and propose a new evaluation metric that\ncombines the notions of faithfulness, robustness and contrastiveness. We\nutilize this new metric to evaluate the performance of various\nattribution-based methods. Our code is available at:\nhttps://github.com/davor10105/relative-absolute-magnitude-propagation\n","authors":["Davor Vukadin","Petar Afrić","Marin Šilić","Goran Delač"],"pdf_url":"https://arxiv.org/pdf/2412.09311v1.pdf","comment":"30 pages, 16 figures, 13 tables, ACM Transactions on Intelligence\n  Systems and Technology"},{"id":"http://arxiv.org/abs/2412.09296v1","updated":"2024-12-12T14:12:07Z","published":"2024-12-12T14:12:07Z","title":"GoHD: Gaze-oriented and Highly Disentangled Portrait Animation with\n  Rhythmic Poses and Realistic Expression","summary":"  Audio-driven talking head generation necessitates seamless integration of\naudio and visual data amidst the challenges posed by diverse input portraits\nand intricate correlations between audio and facial motions. In response, we\npropose a robust framework GoHD designed to produce highly realistic,\nexpressive, and controllable portrait videos from any reference identity with\nany motion. GoHD innovates with three key modules: Firstly, an animation module\nutilizing latent navigation is introduced to improve the generalization ability\nacross unseen input styles. This module achieves high disentanglement of motion\nand identity, and it also incorporates gaze orientation to rectify unnatural\neye movements that were previously overlooked. Secondly, a conformer-structured\nconditional diffusion model is designed to guarantee head poses that are aware\nof prosody. Thirdly, to estimate lip-synchronized and realistic expressions\nfrom the input audio within limited training data, a two-stage training\nstrategy is devised to decouple frequent and frame-wise lip motion distillation\nfrom the generation of other more temporally dependent but less audio-related\nmotions, e.g., blinks and frowns. Extensive experiments validate GoHD's\nadvanced generalization capabilities, demonstrating its effectiveness in\ngenerating realistic talking face results on arbitrary subjects.\n","authors":["Ziqi Zhou","Weize Quan","Hailin Shi","Wei Li","Lili Wang","Dong-ming Yan"],"pdf_url":"https://arxiv.org/pdf/2412.09296v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.07767v2","updated":"2024-12-12T14:10:43Z","published":"2024-12-10T18:59:31Z","title":"Learning Visual Generative Priors without Text","summary":"  Although text-to-image (T2I) models have recently thrived as visual\ngenerative priors, their reliance on high-quality text-image pairs makes\nscaling up expensive. We argue that grasping the cross-modality alignment is\nnot a necessity for a sound visual generative prior, whose focus should be on\ntexture modeling. Such a philosophy inspires us to study image-to-image (I2I)\ngeneration, where models can learn from in-the-wild images in a self-supervised\nmanner. We first develop a pure vision-based training framework, Lumos, and\nconfirm the feasibility and the scalability of learning I2I models. We then\nfind that, as an upstream task of T2I, our I2I model serves as a more\nfoundational visual prior and achieves on-par or better performance than\nexisting T2I models using only 1/10 text-image pairs for fine-tuning. We\nfurther demonstrate the superiority of I2I priors over T2I priors on some\ntext-irrelevant visual generative tasks, like image-to-3D and image-to-video.\nOur project page is available at https://xiaomabufei.github.io/lumos.\n","authors":["Shuailei Ma","Kecheng Zheng","Ying Wei","Wei Wu","Fan Lu","Yifei Zhang","Chen-Wei Xie","Biao Gong","Jiapeng Zhu","Yujun Shen"],"pdf_url":"https://arxiv.org/pdf/2412.07767v2.pdf","comment":"Project Page: https://xiaomabufei.github.io/lumos"},{"id":"http://arxiv.org/abs/2412.09283v1","updated":"2024-12-12T13:48:40Z","published":"2024-12-12T13:48:40Z","title":"InstanceCap: Improving Text-to-Video Generation via Instance-aware\n  Structured Caption","summary":"  Text-to-video generation has evolved rapidly in recent years, delivering\nremarkable results. Training typically relies on video-caption paired data,\nwhich plays a crucial role in enhancing generation performance. However,\ncurrent video captions often suffer from insufficient details, hallucinations\nand imprecise motion depiction, affecting the fidelity and consistency of\ngenerated videos. In this work, we propose a novel instance-aware structured\ncaption framework, termed InstanceCap, to achieve instance-level and\nfine-grained video caption for the first time. Based on this scheme, we design\nan auxiliary models cluster to convert original video into instances to enhance\ninstance fidelity. Video instances are further used to refine dense prompts\ninto structured phrases, achieving concise yet precise descriptions.\nFurthermore, a 22K InstanceVid dataset is curated for training, and an\nenhancement pipeline that tailored to InstanceCap structure is proposed for\ninference. Experimental results demonstrate that our proposed InstanceCap\nsignificantly outperform previous models, ensuring high fidelity between\ncaptions and videos while reducing hallucinations.\n","authors":["Tiehan Fan","Kepan Nan","Rui Xie","Penghao Zhou","Zhenheng Yang","Chaoyou Fu","Xiang Li","Jian Yang","Ying Tai"],"pdf_url":"https://arxiv.org/pdf/2412.09283v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09278v1","updated":"2024-12-12T13:41:35Z","published":"2024-12-12T13:41:35Z","title":"Towards a Multimodal Large Language Model with Pixel-Level Insight for\n  Biomedicine","summary":"  In recent years, Multimodal Large Language Models (MLLM) have achieved\nnotable advancements, demonstrating the feasibility of developing an\nintelligent biomedical assistant. However, current biomedical MLLMs\npredominantly focus on image-level understanding and restrict interactions to\ntextual commands, thus limiting their capability boundaries and the flexibility\nof usage. In this paper, we introduce a novel end-to-end multimodal large\nlanguage model for the biomedical domain, named MedPLIB, which possesses\npixel-level understanding. Excitingly, it supports visual question answering\n(VQA), arbitrary pixel-level prompts (points, bounding boxes, and free-form\nshapes), and pixel-level grounding. We propose a novel Mixture-of-Experts (MoE)\nmulti-stage training strategy, which divides MoE into separate training phases\nfor a visual-language expert model and a pixel-grounding expert model, followed\nby fine-tuning using MoE. This strategy effectively coordinates multitask\nlearning while maintaining the computational cost at inference equivalent to\nthat of a single expert model. To advance the research of biomedical MLLMs, we\nintroduce the Medical Complex Vision Question Answering Dataset (MeCoVQA),\nwhich comprises an array of 8 modalities for complex medical imaging question\nanswering and image region understanding. Experimental results indicate that\nMedPLIB has achieved state-of-the-art outcomes across multiple medical visual\nlanguage tasks. More importantly, in zero-shot evaluations for the pixel\ngrounding task, MedPLIB leads the best small and large models by margins of\n19.7 and 15.6 respectively on the mDice metric. The codes, data, and model\ncheckpoints will be made publicly available at\nhttps://github.com/ShawnHuang497/MedPLIB.\n","authors":["Xiaoshuang Huang","Lingdong Shen","Jia Liu","Fangxin Shang","Hongxiang Li","Haifeng Huang","Yehui Yang"],"pdf_url":"https://arxiv.org/pdf/2412.09278v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.09276v1","updated":"2024-12-12T13:40:59Z","published":"2024-12-12T13:40:59Z","title":"Text-Video Multi-Grained Integration for Video Moment Montage","summary":"  The proliferation of online short video platforms has driven a surge in user\ndemand for short video editing. However, manually selecting, cropping, and\nassembling raw footage into a coherent, high-quality video remains laborious\nand time-consuming. To accelerate this process, we focus on a user-friendly new\ntask called Video Moment Montage (VMM), which aims to accurately locate the\ncorresponding video segments based on a pre-provided narration text and then\narrange these video clips to create a complete video that aligns with the\ncorresponding descriptions. The challenge lies in extracting precise temporal\nsegments while ensuring intra-sentence and inter-sentence context consistency,\nas a single script sentence may require trimming and assembling multiple video\nclips. To address this problem, we present a novel \\textit{Text-Video\nMulti-Grained Integration} method (TV-MGI) that efficiently fuses text features\nfrom the script with both shot-level and frame-level video features, which\nenables the global and fine-grained alignment between the video content and the\ncorresponding textual descriptions in the script. To facilitate further\nresearch in this area, we introduce the Multiple Sentences with Shots Dataset\n(MSSD), a large-scale dataset designed explicitly for the VMM task. We conduct\nextensive experiments on the MSSD dataset to demonstrate the effectiveness of\nour framework compared to baseline methods.\n","authors":["Zhihui Yin","Ye Ma","Xipeng Cao","Bo Wang","Quan Chen","Peng Jiang"],"pdf_url":"https://arxiv.org/pdf/2412.09276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09262v1","updated":"2024-12-12T13:20:52Z","published":"2024-12-12T13:20:52Z","title":"LatentSync: Audio Conditioned Latent Diffusion Models for Lip Sync","summary":"  We present LatentSync, an end-to-end lip sync framework based on audio\nconditioned latent diffusion models without any intermediate motion\nrepresentation, diverging from previous diffusion-based lip sync methods based\non pixel space diffusion or two-stage generation. Our framework can leverage\nthe powerful capabilities of Stable Diffusion to directly model complex\naudio-visual correlations. Additionally, we found that the diffusion-based lip\nsync methods exhibit inferior temporal consistency due to the inconsistency in\nthe diffusion process across different frames. We propose Temporal\nREPresentation Alignment (TREPA) to enhance temporal consistency while\npreserving lip-sync accuracy. TREPA uses temporal representations extracted by\nlarge-scale self-supervised video models to align the generated frames with the\nground truth frames. Furthermore, we observe the commonly encountered SyncNet\nconvergence issue and conduct comprehensive empirical studies, identifying key\nfactors affecting SyncNet convergence in terms of model architecture, training\nhyperparameters, and data preprocessing methods. We significantly improve the\naccuracy of SyncNet from 91% to 94% on the HDTF test set. Since we did not\nchange the overall training framework of SyncNet, our experience can also be\napplied to other lip sync and audio-driven portrait animation methods that\nutilize SyncNet. Based on the above innovations, our method outperforms\nstate-of-the-art lip sync methods across various metrics on the HDTF and\nVoxCeleb2 datasets.\n","authors":["Chunyu Li","Chao Zhang","Weikai Xu","Jinghui Xie","Weiguo Feng","Bingyue Peng","Weiwei Xing"],"pdf_url":"https://arxiv.org/pdf/2412.09262v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09258v1","updated":"2024-12-12T13:19:05Z","published":"2024-12-12T13:19:05Z","title":"FD2-Net: Frequency-Driven Feature Decomposition Network for\n  Infrared-Visible Object Detection","summary":"  Infrared-visible object detection (IVOD) seeks to harness the complementary\ninformation in infrared and visible images, thereby enhancing the performance\nof detectors in complex environments. However, existing methods often neglect\nthe frequency characteristics of complementary information, such as the\nabundant high-frequency details in visible images and the valuable\nlow-frequency thermal information in infrared images, thus constraining\ndetection performance. To solve this problem, we introduce a novel\nFrequency-Driven Feature Decomposition Network for IVOD, called FD2-Net, which\neffectively captures the unique frequency representations of complementary\ninformation across multimodal visual spaces. Specifically, we propose a feature\ndecomposition encoder, wherein the high-frequency unit (HFU) utilizes discrete\ncosine transform to capture representative high-frequency features, while the\nlow-frequency unit (LFU) employs dynamic receptive fields to model the\nmulti-scale context of diverse objects. Next, we adopt a parameter-free\ncomplementary strengths strategy to enhance multimodal features through\nseamless inter-frequency recoupling. Furthermore, we innovatively design a\nmultimodal reconstruction mechanism that recovers image details lost during\nfeature extraction, further leveraging the complementary information from\ninfrared and visible images to enhance overall representational capacity.\nExtensive experiments demonstrate that FD2-Net outperforms state-of-the-art\n(SOTA) models across various IVOD benchmarks, i.e. LLVIP (96.2% mAP), FLIR\n(82.9% mAP), and M3FD (83.5% mAP).\n","authors":["Ke Li","Di Wang","Zhangyuan Hu","Shaofeng Li","Weiping Ni","Lin Zhao","Quan Wang"],"pdf_url":"https://arxiv.org/pdf/2412.09258v1.pdf","comment":"This work is accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2407.10485v3","updated":"2024-12-12T13:11:11Z","published":"2024-07-15T07:13:27Z","title":"MM-Tracker: Motion Mamba with Margin Loss for UAV-platform Multiple\n  Object Tracking","summary":"  Multiple object tracking (MOT) from unmanned aerial vehicle (UAV) platforms\nrequires efficient motion modeling. This is because UAV-MOT faces both local\nobject motion and global camera motion. Motion blur also increases the\ndifficulty of detecting large moving objects. Previous UAV motion modeling\napproaches either focus only on local motion or ignore motion blurring effects,\nthus limiting their tracking performance and speed. To address these issues, we\npropose the Motion Mamba Module, which explores both local and global motion\nfeatures through cross-correlation and bi-directional Mamba Modules for better\nmotion modeling. To address the detection difficulties caused by motion blur,\nwe also design motion margin loss to effectively improve the detection accuracy\nof motion blurred objects. Based on the Motion Mamba module and motion margin\nloss, our proposed MM-Tracker surpasses the state-of-the-art in two widely\nopen-source UAV-MOT datasets. Code will be available.\n","authors":["Mufeng Yao","Jinlong Peng","Qingdong He","Bo Peng","Hao Chen","Mingmin Chi","Chao Liu","Jon Atli Benediktsson"],"pdf_url":"https://arxiv.org/pdf/2407.10485v3.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2403.16970v4","updated":"2024-12-12T13:06:47Z","published":"2024-03-25T17:31:12Z","title":"A Multi-Stage Framework for Joint Chest X-Ray Diagnosis and Visual\n  Attention Prediction Using Deep Learning","summary":"  Purpose: As visual inspection is an inherent process during radiological\nscreening, the associated eye gaze data can provide valuable insights into\nrelevant clinical decisions. As deep learning has become the state-of-the-art\nfor computer-assisted diagnosis, integrating human behavior, such as eye gaze\ndata, into these systems is instrumental to help align machine predictions with\nclinical diagnostic criteria, thus enhancing the quality of automatic\nradiological diagnosis. Methods: We propose a novel deep learning framework for\njoint disease diagnosis and prediction of corresponding clinical visual\nattention maps for chest X-ray scans. Specifically, we introduce a new\ndual-encoder multi-task UNet, which leverages both a DenseNet201 backbone and a\nResidual and Squeeze-and-Excitation block-based encoder to extract diverse\nfeatures for visual attention map prediction, and a multi-scale feature-fusion\nclassifier to perform disease classification. To tackle the issue of\nasynchronous training schedules of individual tasks in multi-task learning, we\nproposed a multi-stage cooperative learning strategy, with contrastive learning\nfor feature encoder pretraining to boost performance. Results: Our proposed\nmethod is shown to significantly outperform existing techniques for chest X-ray\ndiagnosis (AUC=0.93) and the quality of visual attention map prediction\n(Correlation coefficient=0.58). Conclusion: Benefiting from the proposed\nmulti-task multi-stage cooperative learning, our technique demonstrates the\nbenefit of integrating clinicians' eye gaze into clinical AI systems to boost\nperformance and potentially explainability.\n","authors":["Zirui Qiu","Hassan Rivaz","Yiming Xiao"],"pdf_url":"https://arxiv.org/pdf/2403.16970v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13082v3","updated":"2024-12-12T13:02:17Z","published":"2024-05-21T06:44:40Z","title":"A Survey of Artificial Intelligence in Gait-Based Neurodegenerative\n  Disease Diagnosis","summary":"  Recent years have witnessed an increasing global population affected by\nneurodegenerative diseases (NDs), which traditionally require extensive\nhealthcare resources and human effort for medical diagnosis and monitoring. As\na crucial disease-related motor symptom, human gait can be exploited to\ncharacterize different NDs. The current advances in artificial intelligence\n(AI) models enable automatic gait analysis for NDs identification and\nclassification, opening a new avenue to facilitate faster and more\ncost-effective diagnosis of NDs. In this paper, we provide a comprehensive\nsurvey on recent progress of machine learning and deep learning based AI\ntechniques applied to diagnosis of five typical NDs through gait. We provide an\noverview of the process of AI-assisted NDs diagnosis, and present a systematic\ntaxonomy of existing gait data and AI models. Meanwhile, a novel quality\nevaluation criterion is proposed to quantitatively assess the quality of\nexisting studies. Through an extensive review and analysis of 169 studies, we\npresent recent technical advancements, discuss existing challenges, potential\nsolutions, and future directions in this field. Finally, we envision the\nprospective utilization of 3D skeleton data for human gait representation and\nthe development of more efficient AI models for NDs diagnosis.\n","authors":["Haocong Rao","Minlin Zeng","Xuejiao Zhao","Chunyan Miao"],"pdf_url":"https://arxiv.org/pdf/2405.13082v3.pdf","comment":"Article: 57 pages, citing 290 papers. Appendix: 30 pages. A\n  up-to-date resource (papers, data, etc.) of this survey (AI4NDD) is provided\n  at https://github.com/minlinzeng/AI4NDD-Survey"},{"id":"http://arxiv.org/abs/2412.09240v1","updated":"2024-12-12T12:49:42Z","published":"2024-12-12T12:49:42Z","title":"VLMs meet UDA: Boosting Transferability of Open Vocabulary Segmentation\n  with Unsupervised Domain Adaptation","summary":"  Segmentation models are typically constrained by the categories defined\nduring training. To address this, researchers have explored two independent\napproaches: adapting Vision-Language Models (VLMs) and leveraging synthetic\ndata. However, VLMs often struggle with granularity, failing to disentangle\nfine-grained concepts, while synthetic data-based methods remain limited by the\nscope of available datasets.\n  This paper proposes enhancing segmentation accuracy across diverse domains by\nintegrating Vision-Language reasoning with key strategies for Unsupervised\nDomain Adaptation (UDA). First, we improve the fine-grained segmentation\ncapabilities of VLMs through multi-scale contextual data, robust text\nembeddings with prompt augmentation, and layer-wise fine-tuning in our proposed\nFoundational-Retaining Open Vocabulary Semantic Segmentation (FROVSS)\nframework. Next, we incorporate these enhancements into a UDA framework by\nemploying distillation to stabilize training and cross-domain mixed sampling to\nboost adaptability without compromising generalization. The resulting\nUDA-FROVSS framework is the first UDA approach to effectively adapt across\ndomains without requiring shared categories.\n","authors":["Roberto Alcover-Couso","Marcos Escudero-Viñolo","Juan C. SanMiguel","Jesus Bescos"],"pdf_url":"https://arxiv.org/pdf/2412.09240v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09230v1","updated":"2024-12-12T12:39:07Z","published":"2024-12-12T12:39:07Z","title":"Foundation Models and Adaptive Feature Selection: A Synergistic Approach\n  to Video Question Answering","summary":"  This paper tackles the intricate challenge of video question-answering\n(VideoQA). Despite notable progress, current methods fall short of effectively\nintegrating questions with video frames and semantic object-level abstractions\nto create question-aware video representations. We introduce Local-Global\nQuestion Aware Video Embedding (LGQAVE), which incorporates three major\ninnovations to integrate multi-modal knowledge better and emphasize semantic\nvisual concepts relevant to specific questions. LGQAVE moves beyond traditional\nad-hoc frame sampling by utilizing a cross-attention mechanism that precisely\nidentifies the most relevant frames concerning the questions. It captures the\ndynamics of objects within these frames using distinct graphs, grounding them\nin question semantics with the miniGPT model. These graphs are processed by a\nquestion-aware dynamic graph transformer (Q-DGT), which refines the outputs to\ndevelop nuanced global and local video representations. An additional\ncross-attention module integrates these local and global embeddings to generate\nthe final video embeddings, which a language model uses to generate answers.\nExtensive evaluations across multiple benchmarks demonstrate that LGQAVE\nsignificantly outperforms existing models in delivering accurate multi-choice\nand open-ended answers.\n","authors":["Sai Bhargav Rongali","Mohamad Hassan N C","Ankit Jha","Neha Bhargava","Saurabh Prasad","Biplab Banerjee"],"pdf_url":"https://arxiv.org/pdf/2412.09230v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09229v1","updated":"2024-12-12T12:38:33Z","published":"2024-12-12T12:38:33Z","title":"UADet: A Remarkably Simple Yet Effective Uncertainty-Aware Open-Set\n  Object Detection Framework","summary":"  We tackle the challenging problem of Open-Set Object Detection (OSOD), which\naims to detect both known and unknown objects in unlabelled images. The main\ndifficulty arises from the absence of supervision for these unknown classes,\nmaking it challenging to distinguish them from the background. Existing OSOD\ndetectors either fail to properly exploit or inadequately leverage the abundant\nunlabeled unknown objects in training data, restricting their performance. To\naddress these limitations, we propose UADet, an Uncertainty-Aware Open-Set\nObject Detector that considers appearance and geometric uncertainty. By\nintegrating these uncertainty measures, UADet effectively reduces the number of\nunannotated instances incorrectly utilized or omitted by previous methods.\nExtensive experiments on OSOD benchmarks demonstrate that UADet substantially\noutperforms previous state-of-the-art (SOTA) methods in detecting both known\nand unknown objects, achieving a 1.8x improvement in unknown recall while\nmaintaining high performance on known classes. When extended to Open World\nObject Detection (OWOD), our method shows significant advantages over the\ncurrent SOTA method, with average improvements of 13.8% and 6.9% in unknown\nrecall on M-OWODB and S-OWODB benchmarks, respectively. Extensive results\nvalidate the effectiveness of our uncertainty-aware approach across different\nopen-set scenarios.\n","authors":["Silin Cheng","Yuanpei Liu","Kai Han"],"pdf_url":"https://arxiv.org/pdf/2412.09229v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2407.00574v2","updated":"2024-12-12T12:37:32Z","published":"2024-06-30T03:31:21Z","title":"Humans as Checkerboards: Calibrating Camera Motion Scale for\n  World-Coordinate Human Mesh Recovery","summary":"  Accurate camera motion estimation is essential for recovering global human\nmotion in world coordinates from RGB video inputs. SLAM is widely used for\nestimating camera trajectory and point cloud, but monocular SLAM does so only\nup to an unknown scale factor. Previous works estimate the scale factor through\noptimization, but this is unreliable and time-consuming. This paper presents an\noptimization-free scale calibration framework, Human as Checkerboard (HAC). HAC\ninnovatively leverages the human body predicted by human mesh recovery model as\na calibration reference. Specifically, it uses the absolute depth of\nhuman-scene contact joints as references to calibrate the corresponding\nrelative scene depth from SLAM. HAC benefits from geometric priors encoded in\nhuman mesh recovery models to estimate the SLAM scale and achieves precise\nglobal human motion estimation. Simple yet powerful, our method sets a new\nstate-of-the-art performance for global human mesh estimation tasks, reducing\nmotion errors by 50% over prior local-to-global methods while using 100$\\times$\nless inference time than optimization-based methods. Project page:\nhttps://martayang.github.io/HAC.\n","authors":["Fengyuan Yang","Kerui Gu","Ha Linh Nguyen","Tze Ho Elden Tse","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2407.00574v2.pdf","comment":"13 pages, 11 figures, 6 tables"},{"id":"http://arxiv.org/abs/2108.10201v4","updated":"2024-12-12T12:28:39Z","published":"2021-08-23T14:37:58Z","title":"Improving generative adversarial network inversion via fine-tuning GAN\n  encoders","summary":"  Generative adversarial networks (GANs) can synthesize high-quality (HQ)\nimages, and GAN inversion is a technique that discovers how to invert given\nimages back to latent space. While existing methods perform on StyleGAN\ninversion, they have limited performance and are not generalized to different\nGANs. To address these issues, we proposed a self-supervised method to\npre-train and fine-tune GAN encoders. First, we designed an adaptive block to\nfit different encoder architectures for inverting diverse GANs. Then we\npre-train GAN encoders using synthesized images and emphasize local regions\nthrough cropping images. Finally, we fine-tune the pre-trained GAN encoder for\ninverting real images. Compared with state-of-the-art methods, our method\nachieved better results that reconstructed high-quality images on mainstream\nGANs. Our code and pre-trained models are available at:\nhttps://github.com/disanda/Deep-GAN-Encoders.\n","authors":["Cheng Yu","Wenmin Wang","Roberto Bugiolacchi"],"pdf_url":"https://arxiv.org/pdf/2108.10201v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09224v1","updated":"2024-12-12T12:26:08Z","published":"2024-12-12T12:26:08Z","title":"DASK: Distribution Rehearsing via Adaptive Style Kernel Learning for\n  Exemplar-Free Lifelong Person Re-Identification","summary":"  Lifelong person re-identification (LReID) is an important but challenging\ntask that suffers from catastrophic forgetting due to significant domain gaps\nbetween training steps. Existing LReID approaches typically rely on data replay\nand knowledge distillation to mitigate this issue. However, data replay methods\ncompromise data privacy by storing historical exemplars, while knowledge\ndistillation methods suffer from limited performance due to the cumulative\nforgetting of undistilled knowledge. To overcome these challenges, we propose a\nnovel paradigm that models and rehearses the distribution of the old domains to\nenhance knowledge consolidation during the new data learning, possessing a\nstrong anti-forgetting capacity without storing any exemplars. Specifically, we\nintroduce an exemplar-free LReID method called Distribution Rehearsing via\nAdaptive Style Kernel Learning (DASK). DASK includes a Distribution Rehearser\nLearning mechanism that learns to transform arbitrary distribution data into\nthe current data style at each learning step. To enhance the style transfer\ncapacity of DRL, an Adaptive Kernel Prediction network is explored to achieve\nan instance-specific distribution adjustment. Additionally, we design a\nDistribution Rehearsing-driven LReID Training module, which rehearses old\ndistribution based on the new data via the old AKPNet model, achieving\neffective new-old knowledge accumulation under a joint knowledge consolidation\nscheme. Experimental results show our DASK outperforms the existing methods by\n3.6%-6.8% and 4.5%-6.5% on anti-forgetting and generalization capacity,\nrespectively. Our code is available at\nhttps://github.com/zhoujiahuan1991/AAAI2025-DASK\n","authors":["Kunlun Xu","Chenghao Jiang","Peixi Xiong","Yuxin Peng","Jiahuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.09224v1.pdf","comment":"in Proceedings of the 39th AAAI Conference on Artificial Intelligence\n  (AAAI-25)"},{"id":"http://arxiv.org/abs/2412.09220v1","updated":"2024-12-12T12:20:27Z","published":"2024-12-12T12:20:27Z","title":"USDRL: Unified Skeleton-Based Dense Representation Learning with\n  Multi-Grained Feature Decorrelation","summary":"  Contrastive learning has achieved great success in skeleton-based\nrepresentation learning recently. However, the prevailing methods are\npredominantly negative-based, necessitating additional momentum encoder and\nmemory bank to get negative samples, which increases the difficulty of model\ntraining. Furthermore, these methods primarily concentrate on learning a global\nrepresentation for recognition and retrieval tasks, while overlooking the rich\nand detailed local representations that are crucial for dense prediction tasks.\nTo alleviate these issues, we introduce a Unified Skeleton-based Dense\nRepresentation Learning framework based on feature decorrelation, called USDRL,\nwhich employs feature decorrelation across temporal, spatial, and instance\ndomains in a multi-grained manner to reduce redundancy among dimensions of the\nrepresentations to maximize information extraction from features. Additionally,\nwe design a Dense Spatio-Temporal Encoder (DSTE) to capture fine-grained action\nrepresentations effectively, thereby enhancing the performance of dense\nprediction tasks. Comprehensive experiments, conducted on the benchmarks\nNTU-60, NTU-120, PKU-MMD I, and PKU-MMD II, across diverse downstream tasks\nincluding action recognition, action retrieval, and action detection,\nconclusively demonstrate that our approach significantly outperforms the\ncurrent state-of-the-art (SOTA) approaches. Our code and models are available\nat https://github.com/wengwanjiang/USDRL.\n","authors":["Wanjiang Weng","Hongsong Wang","Junbo He","Lei He","Guosen Xie"],"pdf_url":"https://arxiv.org/pdf/2412.09220v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2410.08926v2","updated":"2024-12-12T12:18:39Z","published":"2024-10-11T15:50:53Z","title":"Zero-Shot Pupil Segmentation with SAM 2: A Case Study of Over 14 Million\n  Images","summary":"  We explore the transformative potential of SAM 2, a vision foundation model,\nin advancing gaze estimation and eye tracking technologies. By significantly\nreducing annotation time, lowering technical barriers through its ease of\ndeployment, and enhancing segmentation accuracy, SAM 2 addresses critical\nchallenges faced by researchers and practitioners. Utilizing its zero-shot\nsegmentation capabilities with minimal user input-a single click per video-we\ntested SAM 2 on over 14 million eye images from diverse datasets, including\nvirtual reality setups and the world's largest unified dataset recorded using\nwearable eye trackers. Remarkably, in pupil segmentation tasks, SAM 2 matches\nthe performance of domain-specific models trained solely on eye images,\nachieving competitive mean Intersection over Union (mIoU) scores of up to 93%\nwithout fine-tuning. Additionally, we provide our code and segmentation masks\nfor these widely used datasets to promote further research.\n","authors":["Virmarie Maquiling","Sean Anthony Byrne","Diederick C. Niehorster","Marco Carminati","Enkelejda Kasneci"],"pdf_url":"https://arxiv.org/pdf/2410.08926v2.pdf","comment":"Virmarie Maquiling and Sean Anthony Byrne contributed equally to this\n  paper, 8 pages, 3 figures, CHI Case Study, pre-print"},{"id":"http://arxiv.org/abs/2412.09213v1","updated":"2024-12-12T12:08:27Z","published":"2024-12-12T12:08:27Z","title":"Enhancing Implicit Neural Representations via Symmetric Power\n  Transformation","summary":"  We propose symmetric power transformation to enhance the capacity of Implicit\nNeural Representation~(INR) from the perspective of data transformation. Unlike\nprior work utilizing random permutation or index rearrangement, our method\nfeatures a reversible operation that does not require additional storage\nconsumption. Specifically, we first investigate the characteristics of data\nthat can benefit the training of INR, proposing the Range-Defined Symmetric\nHypothesis, which posits that specific range and symmetry can improve the\nexpressive ability of INR. Based on this hypothesis, we propose a nonlinear\nsymmetric power transformation to achieve both range-defined and symmetric\nproperties simultaneously. We use the power coefficient to redistribute data to\napproximate symmetry within the target range. To improve the robustness of the\ntransformation, we further design deviation-aware calibration and adaptive soft\nboundary to address issues of extreme deviation boosting and continuity\nbreaking. Extensive experiments are conducted to verify the performance of the\nproposed method, demonstrating that our transformation can reliably improve INR\ncompared with other data transformations. We also conduct 1D audio, 2D image\nand 3D video fitting tasks to demonstrate the effectiveness and applicability\nof our method.\n","authors":["Weixiang Zhang","Shuzhao Xie","Chengwei Ren","Shijia Ge","Mingzi Wang","Zhi Wang"],"pdf_url":"https://arxiv.org/pdf/2412.09213v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2307.03270v2","updated":"2024-12-12T12:05:25Z","published":"2023-07-04T08:29:59Z","title":"A Comprehensive Multi-scale Approach for Speech and Dynamics Synchrony\n  in Talking Head Generation","summary":"  Animating still face images with deep generative models using a speech input\nsignal is an active research topic and has seen important recent\nprogress.However, much of the effort has been put into lip syncing and\nrendering quality while the generation of natural head motion, let alone the\naudio-visual correlation between head motion and speech, has often been\nneglected.In this work, we propose a multi-scale audio-visual synchrony loss\nand a multi-scale autoregressive GAN to better handle short and long-term\ncorrelation between speech and the dynamics of the head and lips.In particular,\nwe train a stack of syncer models on multimodal input pyramids and use these\nmodels as guidance in a multi-scale generator network to produce audio-aligned\nmotion unfolding over diverse time scales.Both the pyramid of audio-visual\nsyncers and the generative models are trained in a low-dimensional space that\nfully preserves dynamics cues.The experiments show significant improvements\nover the state-of-the-art in head motion dynamics quality and especially in\nmulti-scale audio-visual synchrony on a collection of benchmark datasets.\n","authors":["Louis Airale","Dominique Vaufreydaz","Xavier Alameda-Pineda"],"pdf_url":"https://arxiv.org/pdf/2307.03270v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09209v1","updated":"2024-12-12T12:02:23Z","published":"2024-12-12T12:02:23Z","title":"eCARLA-scenes: A synthetically generated dataset for event-based optical\n  flow prediction","summary":"  The joint use of event-based vision and Spiking Neural Networks (SNNs) is\nexpected to have a large impact in robotics in the near future, in tasks such\nas, visual odometry and obstacle avoidance. While researchers have used\nreal-world event datasets for optical flow prediction (mostly captured with\nUnmanned Aerial Vehicles (UAVs)), these datasets are limited in diversity,\nscalability, and are challenging to collect. Thus, synthetic datasets offer a\nscalable alternative by bridging the gap between reality and simulation. In\nthis work, we address the lack of datasets by introducing eWiz, a comprehensive\nlibrary for processing event-based data. It includes tools for data loading,\naugmentation, visualization, encoding, and generation of training data, along\nwith loss functions and performance metrics. We further present a synthetic\nevent-based datasets and data generation pipelines for optical flow prediction\ntasks. Built on top of eWiz, eCARLA-scenes makes use of the CARLA simulator to\nsimulate self-driving car scenarios. The ultimate goal of this dataset is the\ndepiction of diverse environments while laying a foundation for advancing\nevent-based camera applications in autonomous field vehicle navigation, paving\nthe way for using SNNs on neuromorphic hardware such as the Intel Loihi.\n","authors":["Jad Mansour","Hayat Rajani","Rafael Garcia","Nuno Gracias"],"pdf_url":"https://arxiv.org/pdf/2412.09209v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09202v1","updated":"2024-12-12T11:56:24Z","published":"2024-12-12T11:56:24Z","title":"Temporal Action Localization with Cross Layer Task Decoupling and\n  Refinement","summary":"  Temporal action localization (TAL) involves dual tasks to classify and\nlocalize actions within untrimmed videos. However, the two tasks often have\nconflicting requirements for features. Existing methods typically employ\nseparate heads for classification and localization tasks but share the same\ninput feature, leading to suboptimal performance. To address this issue, we\npropose a novel TAL method with Cross Layer Task Decoupling and Refinement\n(CLTDR). Based on the feature pyramid of video, CLTDR strategy integrates\nsemantically strong features from higher pyramid layers and detailed\nboundary-aware boundary features from lower pyramid layers to effectively\ndisentangle the action classification and localization tasks. Moreover, the\nmultiple features from cross layers are also employed to refine and align the\ndisentangled classification and regression results. At last, a lightweight\nGated Multi-Granularity (GMG) module is proposed to comprehensively extract and\naggregate video features at instant, local, and global temporal granularities.\nBenefiting from the CLTDR and GMG modules, our method achieves state-of-the-art\nperformance on five challenging benchmarks: THUMOS14, MultiTHUMOS,\nEPIC-KITCHENS-100, ActivityNet-1.3, and HACS. Our code and pre-trained models\nare publicly available at: https://github.com/LiQiang0307/CLTDR-GMG.\n","authors":["Qiang Li","Di Liu","Jun Kong","Sen Li","Hui Xu","Jianzhong Wang"],"pdf_url":"https://arxiv.org/pdf/2412.09202v1.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2412.09200v1","updated":"2024-12-12T11:53:19Z","published":"2024-12-12T11:53:19Z","title":"Accuracy Improvements for Convolutional and Differential Distance\n  Function Approximations","summary":"  Given a bounded domain, we deal with the problem of estimating the distance\nfunction from the internal points of the domain to the boundary of the domain.\nConvolutional and differential distance estimation schemes are considered and,\nfor both the schemes, accuracy improvements are proposed and evaluated.\nAsymptotics of Laplace integrals and Taylor series extrapolations are used to\nachieve the improvements.\n","authors":["Alexander Belyaev","Pierre-Alain Fayolle"],"pdf_url":"https://arxiv.org/pdf/2412.09200v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09199v1","updated":"2024-12-12T11:49:18Z","published":"2024-12-12T11:49:18Z","title":"MVC-VPR: Mutual Learning of Viewpoint Classification and Visual Place\n  Recognition","summary":"  Visual Place Recognition (VPR) aims to robustly identify locations by\nleveraging image retrieval based on descriptors encoded from environmental\nimages. However, drastic appearance changes of images captured from different\nviewpoints at the same location pose incoherent supervision signals for\ndescriptor learning, which severely hinder the performance of VPR. Previous\nwork proposes classifying images based on manually defined rules or ground\ntruth labels for viewpoints, followed by descriptor training based on the\nclassification results. However, not all datasets have ground truth labels of\nviewpoints and manually defined rules may be suboptimal, leading to degraded\ndescriptor performance.To address these challenges, we introduce the mutual\nlearning of viewpoint self-classification and VPR. Starting from coarse\nclassification based on geographical coordinates, we progress to finer\nclassification of viewpoints using simple clustering techniques. The dataset is\npartitioned in an unsupervised manner while simultaneously training a\ndescriptor extractor for place recognition. Experimental results show that this\napproach almost perfectly partitions the dataset based on viewpoints, thus\nachieving mutually reinforcing effects. Our method even excels state-of-the-art\n(SOTA) methods that partition datasets using ground truth labels.\n","authors":["Qiwen Gu","Xufei Wang","Fenglin Zhang","Junqiao Zhao","Siyue Tao","Chen Ye","Tiantian Feng","Changjun Jiang"],"pdf_url":"https://arxiv.org/pdf/2412.09199v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2410.08490v2","updated":"2024-12-12T11:45:30Z","published":"2024-10-11T03:31:40Z","title":"CAS-GAN for Contrast-free Angiography Synthesis","summary":"  Iodinated contrast agents are widely utilized in numerous interventional\nprocedures, yet posing substantial health risks to patients. This paper\npresents CAS-GAN, a novel GAN framework that serves as a \"virtual contrast\nagent\" to synthesize X-ray angiographies via disentanglement representation\nlearning and vessel semantic guidance, thereby reducing the reliance on\niodinated contrast agents during interventional procedures. Specifically, our\napproach disentangles X-ray angiographies into background and vessel\ncomponents, leveraging medical prior knowledge. A specialized predictor then\nlearns to map the interrelationships between these components. Additionally, a\nvessel semantic-guided generator and a corresponding loss function are\nintroduced to enhance the visual fidelity of generated images. Experimental\nresults on the XCAD dataset demonstrate the state-of-the-art performance of our\nCAS-GAN, achieving a FID of 5.87 and a MMD of 0.016. These promising results\nhighlight {\\tt CAS-GAN}'s potential for clinical applications.\n","authors":["De-Xing Huang","Xiao-Hu Zhou","Mei-Jiang Gui","Xiao-Liang Xie","Shi-Qi Liu","Shuang-Yi Wang","Hao Li","Tian-Yu Xiang","Zeng-Guang Hou"],"pdf_url":"https://arxiv.org/pdf/2410.08490v2.pdf","comment":"IEEE Symposium Series on Computational Intelligence (SSCI 2025)"},{"id":"http://arxiv.org/abs/2412.09193v1","updated":"2024-12-12T11:42:39Z","published":"2024-12-12T11:42:39Z","title":"ExpRDiff: Short-exposure Guided Diffusion Model for Realistic Local\n  Motion Deblurring","summary":"  Removing blur caused by moving objects is challenging, as the moving objects\nare usually significantly blurry while the static background remains clear.\nExisting methods that rely on local blur detection often suffer from\ninaccuracies and cannot generate satisfactory results when focusing solely on\nblurred regions. To overcome these problems, we first design a context-based\nlocal blur detection module that incorporates additional contextual information\nto improve the identification of blurry regions. Considering that modern\nsmartphones are equipped with cameras capable of providing short-exposure\nimages, we develop a blur-aware guided image restoration method that utilizes\nsharp structural details from short-exposure images, facilitating accurate\nreconstruction of heavily blurred regions. Furthermore, to restore images\nrealistically and visually-pleasant, we develop a short-exposure guided\ndiffusion model that explores useful features from short-exposure images and\nblurred regions to better constrain the diffusion process. Finally, we\nformulate the above components into a simple yet effective network, named\nExpRDiff. Experimental results show that ExpRDiff performs favorably against\nstate-of-the-art methods.\n","authors":["Zhongbao Yang","Jiangxin Dong","Jinhui Tang","Jinshan Pan"],"pdf_url":"https://arxiv.org/pdf/2412.09193v1.pdf","comment":"Project website: https://github.com/yzb1997/ExpRDiff"},{"id":"http://arxiv.org/abs/2412.09191v1","updated":"2024-12-12T11:38:46Z","published":"2024-12-12T11:38:46Z","title":"RAD: Region-Aware Diffusion Models for Image Inpainting","summary":"  Diffusion models have achieved remarkable success in image generation, with\napplications broadening across various domains. Inpainting is one such\napplication that can benefit significantly from diffusion models. Existing\nmethods either hijack the reverse process of a pretrained diffusion model or\ncast the problem into a larger framework, \\ie, conditioned generation. However,\nthese approaches often require nested loops in the generation process or\nadditional components for conditioning. In this paper, we present region-aware\ndiffusion models (RAD) for inpainting with a simple yet effective reformulation\nof the vanilla diffusion models. RAD utilizes a different noise schedule for\neach pixel, which allows local regions to be generated asynchronously while\nconsidering the global image context. A plain reverse process requires no\nadditional components, enabling RAD to achieve inference time up to 100 times\nfaster than the state-of-the-art approaches. Moreover, we employ low-rank\nadaptation (LoRA) to fine-tune RAD based on other pretrained diffusion models,\nreducing computational burdens in training as well. Experiments demonstrated\nthat RAD provides state-of-the-art results both qualitatively and\nquantitatively, on the FFHQ, LSUN Bedroom, and ImageNet datasets.\n","authors":["Sora Kim","Sungho Suh","Minsik Lee"],"pdf_url":"https://arxiv.org/pdf/2412.09191v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09182v1","updated":"2024-12-12T11:25:32Z","published":"2024-12-12T11:25:32Z","title":"On the effectiveness of Rotation-Equivariance in U-Net: A Benchmark for\n  Image Segmentation","summary":"  Numerous studies have recently focused on incorporating different variations\nof equivariance in Convolutional Neural Networks (CNNs). In particular,\nrotation-equivariance has gathered significant attention due to its relevance\nin many applications related to medical imaging, microscopic imaging, satellite\nimaging, industrial tasks, etc. While prior research has primarily focused on\nenhancing classification tasks with rotation equivariant CNNs, their impact on\nmore complex architectures, such as U-Net for image segmentation, remains\nscarcely explored. Indeed, previous work interested in integrating\nrotation-equivariance into U-Net architecture have focused on solving specific\napplications with a limited scope. In contrast, this paper aims to provide a\nmore exhaustive evaluation of rotation equivariant U-Net for image segmentation\nacross a broader range of tasks. We benchmark their effectiveness against\nstandard U-Net architectures, assessing improvements in terms of performance\nand sustainability (i.e., computational cost). Our evaluation focuses on\ndatasets whose orientation of objects of interest is arbitrary in the image\n(e.g., Kvasir-SEG), but also on more standard segmentation datasets (such as\nCOCO-Stuff) as to explore the wider applicability of rotation equivariance\nbeyond tasks undoubtedly concerned by rotation equivariance. The main\ncontribution of this work is to provide insights into the trade-offs and\nadvantages of integrating rotation equivariance for segmentation tasks.\n","authors":["Robin Ghyselinck","Valentin Delchevalerie","Bruno Dumas","Benoît Frénay"],"pdf_url":"https://arxiv.org/pdf/2412.09182v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13712v2","updated":"2024-12-12T11:18:51Z","published":"2024-08-25T03:21:48Z","title":"Riemann-based Multi-scale Attention Reasoning Network for Text-3D\n  Retrieval","summary":"  Due to the challenges in acquiring paired Text-3D data and the inherent\nirregularity of 3D data structures, combined representation learning of 3D\npoint clouds and text remains unexplored. In this paper, we propose a novel\nRiemann-based Multi-scale Attention Reasoning Network (RMARN) for text-3D\nretrieval. Specifically, the extracted text and point cloud features are\nrefined by their respective Adaptive Feature Refiner (AFR). Furthermore, we\nintroduce the innovative Riemann Local Similarity (RLS) module and the Global\nPooling Similarity (GPS) module. However, as 3D point cloud data and text data\noften possess complex geometric structures in high-dimensional space, the\nproposed RLS employs a novel Riemann Attention Mechanism to reflect the\nintrinsic geometric relationships of the data. Without explicitly defining the\nmanifold, RMARN learns the manifold parameters to better represent the\ndistances between text-point cloud samples. To address the challenges of\nlacking paired text-3D data, we have created the large-scale Text-3D Retrieval\ndataset T3DR-HIT, which comprises over 3,380 pairs of text and point cloud\ndata. T3DR-HIT contains coarse-grained indoor 3D scenes and fine-grained\nChinese artifact scenes, consisting of 1,380 and over 2,000 text-3D pairs,\nrespectively. Experiments on our custom datasets demonstrate the superior\nperformance of the proposed method. Our code and proposed datasets are\navailable at \\url{https://github.com/liwrui/RMARN}.\n","authors":["Wenrui Li","Wei Han","Yandu Chen","Yeyu Chai","Yidan Lu","Xingtao Wang","Xiaopeng Fan"],"pdf_url":"https://arxiv.org/pdf/2408.13712v2.pdf","comment":"Accepted by AAAI25"},{"id":"http://arxiv.org/abs/2412.09177v1","updated":"2024-12-12T11:09:56Z","published":"2024-12-12T11:09:56Z","title":"Weighted Poisson-disk Resampling on Large-Scale Point Clouds","summary":"  For large-scale point cloud processing, resampling takes the important role\nof controlling the point number and density while keeping the geometric\nconsistency. % in related tasks. However, current methods cannot balance such\ndifferent requirements. Particularly with large-scale point clouds, classical\nmethods often struggle with decreased efficiency and accuracy. To address such\nissues, we propose a weighted Poisson-disk (WPD) resampling method to improve\nthe usability and efficiency for the processing. We first design an initial\nPoisson resampling with a voxel-based estimation strategy. It is able to\nestimate a more accurate radius of the Poisson-disk while maintaining high\nefficiency. Then, we design a weighted tangent smoothing step to further\noptimize the Voronoi diagram for each point. At the same time, sharp features\nare detected and kept in the optimized results with isotropic property.\nFinally, we achieve a resampling copy from the original point cloud with the\nspecified point number, uniform density, and high-quality geometric\nconsistency. Experiments show that our method significantly improves the\nperformance of large-scale point cloud resampling for different applications,\nand provides a highly practical solution.\n","authors":["Xianhe Jiao","Chenlei Lv","Junli Zhao","Ran Yi","Yu-Hui Wen","Zhenkuan Pan","Zhongke Wu","Yong-jin Liu"],"pdf_url":"https://arxiv.org/pdf/2412.09177v1.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2409.16925v2","updated":"2024-12-12T11:06:40Z","published":"2024-09-25T13:33:28Z","title":"Game4Loc: A UAV Geo-Localization Benchmark from Game Data","summary":"  The vision-based geo-localization technology for UAV, serving as a secondary\nsource of GPS information in addition to the global navigation satellite\nsystems (GNSS), can still operate independently in the GPS-denied environment.\nRecent deep learning based methods attribute this as the task of image matching\nand retrieval. By retrieving drone-view images in geo-tagged satellite image\ndatabase, approximate localization information can be obtained. However, due to\nhigh costs and privacy concerns, it is usually difficult to obtain large\nquantities of drone-view images from a continuous area. Existing drone-view\ndatasets are mostly composed of small-scale aerial photography with a strong\nassumption that there exists a perfect one-to-one aligned reference image for\nany query, leaving a significant gap from the practical localization scenario.\nIn this work, we construct a large-range contiguous area UAV geo-localization\ndataset named GTA-UAV, featuring multiple flight altitudes, attitudes, scenes,\nand targets using modern computer games. Based on this dataset, we introduce a\nmore practical UAV geo-localization task including partial matches of\ncross-view paired data, and expand the image-level retrieval to the actual\nlocalization in terms of distance (meters). For the construction of drone-view\nand satellite-view pairs, we adopt a weight-based contrastive learning\napproach, which allows for effective learning while avoiding additional\npost-processing matching steps. Experiments demonstrate the effectiveness of\nour data and training method for UAV geo-localization, as well as the\ngeneralization capabilities to real-world scenarios.\n","authors":["Yuxiang Ji","Boyong He","Zhuoyue Tan","Liaoni Wu"],"pdf_url":"https://arxiv.org/pdf/2409.16925v2.pdf","comment":"AAAI 2025, Project page: https://yux1angji.github.io/game4loc/"},{"id":"http://arxiv.org/abs/2407.09552v2","updated":"2024-12-12T11:03:20Z","published":"2024-06-28T01:31:37Z","title":"Optimized 3D Point Labeling with Leaders Using the Beams Displacement\n  Method","summary":"  In three-dimensional geographical scenes, adding labels with leader lines to\npoint features can significantly improve their visibility. Leadered labels have\na large degree of freedom in position con-figuration, but existing methods are\nmostly based on limited position candidate models, which not only fail to\neffectively utilize the map space but also make it difficult to consider the\nrelative relationships between labels. Therefore, we conceptualize the dynamic\nconfiguration process of computing label positions as akin to solving a map\ndisplacement problem. We use a triangulated graph to delineate spatial\nrelationships among labels and calculate the forces exerted on labels\nconsidering the constraints associated with point feature labels. Then we use\nthe Beams Displacement Method to iteratively calculate new positions for the\nlabels. Our experimental outcomes demonstrate that this method effectively\nmitigates label overlay issues while maintaining minimal average directional\ndeviation between adjacent labels. Furthermore, this method is adaptable to\nvarious types of leader line labels. Meanwhile, we also discuss the block\nprocessing strategy to improve the efficiency of label configuration and\nanalyze the impact of different proximity graphs.\n","authors":["Zhiwei Wei","Nai Yang","Wenjia Xu","Su Ding","Li Minmin","Li You","Guo Renzhong"],"pdf_url":"https://arxiv.org/pdf/2407.09552v2.pdf","comment":"12 pages, in Chinese language, 10 figures"},{"id":"http://arxiv.org/abs/2412.09169v1","updated":"2024-12-12T10:59:44Z","published":"2024-12-12T10:59:44Z","title":"DECOR:Decomposition and Projection of Text Embeddings for Text-to-Image\n  Customization","summary":"  Text-to-image (T2I) models can effectively capture the content or style of\nreference images to perform high-quality customization. A representative\ntechnique for this is fine-tuning using low-rank adaptations (LoRA), which\nenables efficient model customization with reference images. However,\nfine-tuning with a limited number of reference images often leads to\noverfitting, resulting in issues such as prompt misalignment or content\nleakage. These issues prevent the model from accurately following the input\nprompt or generating undesired objects during inference. To address this\nproblem, we examine the text embeddings that guide the diffusion model during\ninference. This study decomposes the text embedding matrix and conducts a\ncomponent analysis to understand the embedding space geometry and identify the\ncause of overfitting. Based on this, we propose DECOR, which projects text\nembeddings onto a vector space orthogonal to undesired token vectors, thereby\nreducing the influence of unwanted semantics in the text embeddings.\nExperimental results demonstrate that DECOR outperforms state-of-the-art\ncustomization models and achieves Pareto frontier performance across text and\nvisual alignment evaluation metrics. Furthermore, it generates images more\nfaithful to the input prompts, showcasing its effectiveness in addressing\noverfitting and enhancing text-to-image customization.\n","authors":["Geonhui Jang","Jin-Hwa Kim","Yong-Hyun Park","Junho Kim","Gayoung Lee","Yonghyun Jeong"],"pdf_url":"https://arxiv.org/pdf/2412.09169v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09168v1","updated":"2024-12-12T10:55:57Z","published":"2024-12-12T10:55:57Z","title":"YingSound: Video-Guided Sound Effects Generation with Multi-modal\n  Chain-of-Thought Controls","summary":"  Generating sound effects for product-level videos, where only a small amount\nof labeled data is available for diverse scenes, requires the production of\nhigh-quality sounds in few-shot settings. To tackle the challenge of limited\nlabeled data in real-world scenes, we introduce YingSound, a foundation model\ndesigned for video-guided sound generation that supports high-quality audio\ngeneration in few-shot settings. Specifically, YingSound consists of two major\nmodules. The first module uses a conditional flow matching transformer to\nachieve effective semantic alignment in sound generation across audio and\nvisual modalities. This module aims to build a learnable audio-visual\naggregator (AVA) that integrates high-resolution visual features with\ncorresponding audio features at multiple stages. The second module is developed\nwith a proposed multi-modal visual-audio chain-of-thought (CoT) approach to\ngenerate finer sound effects in few-shot settings. Finally, an\nindustry-standard video-to-audio (V2A) dataset that encompasses various\nreal-world scenarios is presented. We show that YingSound effectively generates\nhigh-quality synchronized sounds across diverse conditional inputs through\nautomated evaluations and human studies. Project Page:\n\\url{https://giantailab.github.io/yingsound/}\n","authors":["Zihao Chen","Haomin Zhang","Xinhan Di","Haoyu Wang","Sizhe Shan","Junjie Zheng","Yunming Liang","Yihan Fan","Xinfa Zhu","Wenjie Tian","Yihua Wang","Chaofan Ding","Lei Xie"],"pdf_url":"https://arxiv.org/pdf/2412.09168v1.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2412.09160v1","updated":"2024-12-12T10:46:14Z","published":"2024-12-12T10:46:14Z","title":"Pinpoint Counterfactuals: Reducing social bias in foundation models via\n  localized counterfactual generation","summary":"  Foundation models trained on web-scraped datasets propagate societal biases\nto downstream tasks. While counterfactual generation enables bias analysis,\nexisting methods introduce artifacts by modifying contextual elements like\nclothing and background. We present a localized counterfactual generation\nmethod that preserves image context by constraining counterfactual\nmodifications to specific attribute-relevant regions through automated masking\nand guided inpainting. When applied to the Conceptual Captions dataset for\ncreating gender counterfactuals, our method results in higher visual and\nsemantic fidelity than state-of-the-art alternatives, while maintaining the\nperformance of models trained using only real data on non-human-centric tasks.\nModels fine-tuned with our counterfactuals demonstrate measurable bias\nreduction across multiple metrics, including a decrease in gender\nclassification disparity and balanced person preference scores, while\npreserving ImageNet zero-shot performance. The results establish a framework\nfor creating balanced datasets that enable both accurate bias profiling and\neffective mitigation.\n","authors":["Kirill Sirotkin","Marcos Escudero-Viñolo","Pablo Carballeira","Mayug Maniparambil","Catarina Barata","Noel E. O'Connor"],"pdf_url":"https://arxiv.org/pdf/2412.09160v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.09502v2","updated":"2024-12-12T10:39:16Z","published":"2024-11-14T15:13:13Z","title":"Golden Noise for Diffusion Models: A Learning Framework","summary":"  Text-to-image diffusion model is a popular paradigm that synthesizes\npersonalized images by providing a text prompt and a random Gaussian noise.\nWhile people observe that some noises are ``golden noises'' that can achieve\nbetter text-image alignment and higher human preference than others, we still\nlack a machine learning framework to obtain those golden noises. To learn\ngolden noises for diffusion sampling, we mainly make three contributions in\nthis paper. First, we identify a new concept termed the \\textit{noise prompt},\nwhich aims at turning a random Gaussian noise into a golden noise by adding a\nsmall desirable perturbation derived from the text prompt. Following the\nconcept, we first formulate the \\textit{noise prompt learning} framework that\nsystematically learns ``prompted'' golden noise associated with a text prompt\nfor diffusion models. Second, we design a noise prompt data collection pipeline\nand collect a large-scale \\textit{noise prompt dataset}~(NPD) that contains\n100k pairs of random noises and golden noises with the associated text prompts.\nWith the prepared NPD as the training dataset, we trained a small \\textit{noise\nprompt network}~(NPNet) that can directly learn to transform a random noise\ninto a golden noise. The learned golden noise perturbation can be considered as\na kind of prompt for noise, as it is rich in semantic information and tailored\nto the given text prompt. Third, our extensive experiments demonstrate the\nimpressive effectiveness and generalization of NPNet on improving the quality\nof synthesized images across various diffusion models, including SDXL,\nDreamShaper-xl-v2-turbo, and Hunyuan-DiT. Moreover, NPNet is a small and\nefficient controller that acts as a plug-and-play module with very limited\nadditional inference and computational costs, as it just provides a golden\nnoise instead of a random noise without accessing the original pipeline.\n","authors":["Zikai Zhou","Shitong Shao","Lichen Bai","Zhiqiang Xu","Bo Han","Zeke Xie"],"pdf_url":"https://arxiv.org/pdf/2411.09502v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09150v1","updated":"2024-12-12T10:36:26Z","published":"2024-12-12T10:36:26Z","title":"Evaluating Adversarial Attacks on Traffic Sign Classifiers beyond\n  Standard Baselines","summary":"  Adversarial attacks on traffic sign classification models were among the\nfirst successfully tried in the real world. Since then, the research in this\narea has been mainly restricted to repeating baseline models, such as LISA-CNN\nor GTSRB-CNN, and similar experiment settings, including white and black\npatches on traffic signs. In this work, we decouple model architectures from\nthe datasets and evaluate on further generic models to make a fair comparison.\nFurthermore, we compare two attack settings, inconspicuous and visible, which\nare usually regarded without direct comparison. Our results show that standard\nbaselines like LISA-CNN or GTSRB-CNN are significantly more susceptible than\nthe generic ones. We, therefore, suggest evaluating new attacks on a broader\nspectrum of baselines in the future. Our code is available at\n\\url{https://github.com/KASTEL-MobilityLab/attacks-on-traffic-sign-recognition/}.\n","authors":["Svetlana Pavlitska","Leopold Müller","J. Marius Zöllner"],"pdf_url":"https://arxiv.org/pdf/2412.09150v1.pdf","comment":"Accepted for publication at ICMLA 2024"},{"id":"http://arxiv.org/abs/2401.07450v4","updated":"2024-12-12T10:36:14Z","published":"2024-01-15T03:38:57Z","title":"HieraFashDiff: Hierarchical Fashion Design with Multi-stage Diffusion\n  Models","summary":"  Fashion design is a challenging and complex process.Recent works on fashion\ngeneration and editing are all agnostic of the actual fashion design process,\nwhich limits their usage in practice.In this paper, we propose a novel\nhierarchical diffusion-based framework tailored for fashion design, coined as\nHieraFashDiff. Our model is designed to mimic the practical fashion design\nworkflow, by unraveling the denosing process into two successive stages: 1) an\nideation stage that generates design proposals given high-level concepts and 2)\nan iteration stage that continuously refines the proposals using low-level\nattributes. Our model supports fashion design generation and fine-grained local\nediting in a single framework. To train our model, we contribute a new dataset\nof full-body fashion images annotated with hierarchical text descriptions.\nExtensive evaluations show that, as compared to prior approaches, our method\ncan generate fashion designs and edited results with higher fidelity and better\nprompt adherence, showing its promising potential to augment the practical\nfashion design workflow. Code and Dataset are available at\nhttps://github.com/haoli-zbdbc/hierafashdiff.\n","authors":["Zhifeng Xie","Hao Li","Huiming Ding","Mengtian Li","Xinhan Di","Ying Cao"],"pdf_url":"https://arxiv.org/pdf/2401.07450v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.06864v2","updated":"2024-12-12T10:21:16Z","published":"2024-11-11T10:56:40Z","title":"Veri-Car: Towards Open-world Vehicle Information Retrieval","summary":"  Many industrial and service sectors require tools to extract vehicle\ncharacteristics from images. This is a complex task not only by the variety of\nnoise, and large number of classes, but also by the constant introduction of\nnew vehicle models to the market. In this paper, we present Veri-Car, an\ninformation retrieval integrated approach designed to help on this task. It\nleverages supervised learning techniques to accurately identify the make, type,\nmodel, year, color, and license plate of cars. The approach also addresses the\nchallenge of handling open-world problems, where new car models and variations\nfrequently emerge, by employing a sophisticated combination of pre-trained\nmodels, and a hierarchical multi-similarity loss. Veri-Car demonstrates robust\nperformance, achieving high precision and accuracy in classifying both seen and\nunseen data. Additionally, it integrates an ensemble license plate detection,\nand an OCR model to extract license plate numbers with impressive accuracy.\n","authors":["Andrés Muñoz","Nancy Thomas","Annita Vapsi","Daniel Borrajo"],"pdf_url":"https://arxiv.org/pdf/2411.06864v2.pdf","comment":"33 pages, 12 figures"},{"id":"http://arxiv.org/abs/2403.03551v2","updated":"2024-12-12T10:15:41Z","published":"2024-03-06T08:51:09Z","title":"Enhanced Low-Dose CT Image Reconstruction by Domain and Task Shifting\n  Gaussian Denoisers","summary":"  Computed tomography from a low radiation dose (LDCT) is challenging due to\nhigh noise in the projection data. Popular approaches for LDCT image\nreconstruction are two-stage methods, typically consisting of the filtered\nbackprojection (FBP) algorithm followed by a neural network for LDCT image\nenhancement. Two-stage methods are attractive for their simplicity and\npotential for computational efficiency, typically requiring only a single FBP\nand a neural network forward pass for inference. However, the best\nreconstruction quality is currently achieved by unrolled iterative methods\n(Learned Primal-Dual and ItNet), which are more complex and thus have a higher\ncomputational cost for training and inference. We propose a method combining\nthe simplicity and efficiency of two-stage methods with state-of-the-art\nreconstruction quality. Our strategy utilizes a neural network pretrained for\nGaussian noise removal from natural grayscale images, fine-tuned for LDCT image\nenhancement. We call this method FBP-DTSGD (Domain and Task Shifted Gaussian\nDenoisers) as the fine-tuning is a task shift from Gaussian denoising to\nenhancing LDCT images and a domain shift from natural grayscale to LDCT images.\nAn ablation study with three different pretrained Gaussian denoisers indicates\nthat the performance of FBP-DTSGD does not depend on a specific denoising\narchitecture, suggesting future advancements in Gaussian denoising could\nbenefit the method. The study also shows that pretraining on natural images\nenhances LDCT reconstruction quality, especially with limited training data.\nNotably, pretraining involves no additional cost, as existing pretrained models\nare used. The proposed method currently holds the top mean position in the\nLoDoPaB-CT challenge.\n","authors":["Tim Selig","Thomas März","Martin Storath","Andreas Weinmann"],"pdf_url":"https://arxiv.org/pdf/2403.03551v2.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.11210v2","updated":"2024-12-12T10:12:13Z","published":"2024-06-17T05:03:44Z","title":"Zero-Shot Scene Change Detection","summary":"  We present a novel, training-free approach to scene change detection. Our\nmethod leverages tracking models, which inherently perform change detection\nbetween consecutive frames of video by identifying common objects and detecting\nnew or missing objects. Specifically, our method takes advantage of the change\ndetection effect of the tracking model by inputting reference and query images\ninstead of consecutive frames. Furthermore, we focus on the content gap and\nstyle gap between two input images in change detection, and address both issues\nby proposing adaptive content threshold and style bridging layers,\nrespectively. Finally, we extend our approach to video, leveraging rich\ntemporal information to enhance the performance of scene change detection. We\ncompare our approach and baseline through various experiments. While existing\ntrain-based baseline tend to specialize only in the trained domain, our method\nshows consistent performance across various domains, proving the\ncompetitiveness of our approach.\n","authors":["Kyusik Cho","Dong Yeop Kim","Euntai Kim"],"pdf_url":"https://arxiv.org/pdf/2406.11210v2.pdf","comment":"AAAI 2025. Code available at: https://github.com/kyusik-cho/ZSSCD"},{"id":"http://arxiv.org/abs/2411.04956v2","updated":"2024-12-12T10:10:19Z","published":"2024-11-07T18:32:00Z","title":"Uncovering Hidden Subspaces in Video Diffusion Models Using\n  Re-Identification","summary":"  Latent Video Diffusion Models can easily deceive casual observers and domain\nexperts alike thanks to the produced image quality and temporal consistency.\nBeyond entertainment, this creates opportunities around safe data sharing of\nfully synthetic datasets, which are crucial in healthcare, as well as other\ndomains relying on sensitive personal information. However, privacy concerns\nwith this approach have not fully been addressed yet, and models trained on\nsynthetic data for specific downstream tasks still perform worse than those\ntrained on real data. This discrepancy may be partly due to the sampling space\nbeing a subspace of the training videos, effectively reducing the training data\nsize for downstream models. Additionally, the reduced temporal consistency when\ngenerating long videos could be a contributing factor.\n  In this paper, we first show that training privacy-preserving models in\nlatent space is computationally more efficient and generalize better.\nFurthermore, to investigate downstream degradation factors, we propose to use a\nre-identification model, previously employed as a privacy preservation filter.\nWe demonstrate that it is sufficient to train this model on the latent space of\nthe video generator. Subsequently, we use these models to evaluate the subspace\ncovered by synthetic video datasets and thus introduce a new way to measure the\nfaithfulness of generative machine learning models. We focus on a specific\napplication in healthcare echocardiography to illustrate the effectiveness of\nour novel methods. Our findings indicate that only up to 30.8% of the training\nvideos are learned in latent video diffusion models, which could explain the\nlack of performance when training downstream tasks on synthetic data.\n","authors":["Mischa Dombrowski","Hadrien Reynaud","Bernhard Kainz"],"pdf_url":"https://arxiv.org/pdf/2411.04956v2.pdf","comment":"8 pages, 5 tables, 6 figures; v2 Acknowledgements added"},{"id":"http://arxiv.org/abs/2411.16171v2","updated":"2024-12-12T10:04:33Z","published":"2024-11-25T08:00:21Z","title":"Image Generation Diversity Issues and How to Tame Them","summary":"  Generative methods now produce outputs nearly indistinguishable from real\ndata but often fail to fully capture the data distribution. Unlike quality\nissues, diversity limitations in generative models are hard to detect visually,\nrequiring specific metrics for assessment. In this paper, we draw attention to\nthe current lack of diversity in generative models and the inability of common\nmetrics to measure this. We achieve this by framing diversity as an image\nretrieval problem, where we measure how many real images can be retrieved using\nsynthetic data as queries. This yields the Image Retrieval Score (IRS), an\ninterpretable, hyperparameter-free metric that quantifies the diversity of a\ngenerative model's output. IRS requires only a subset of synthetic samples and\nprovides a statistical measure of confidence. Our experiments indicate that\ncurrent feature extractors commonly used in generative model assessment are\ninadequate for evaluating diversity effectively. Consequently, we perform an\nextensive search for the best feature extractors to assess diversity.\nEvaluation reveals that current diffusion models converge to limited subsets of\nthe real distribution, with no current state-of-the-art models superpassing 77%\nof the diversity of the training data. To address this limitation, we introduce\nDiversity-Aware Diffusion Models (DiADM), a novel approach that improves\ndiversity of unconditional diffusion models without loss of image quality. We\ndo this by disentangling diversity from image quality by using a diversity\naware module that uses pseudo-unconditional features as input. We provide a\nPython package offering unified feature extraction and metric computation to\nfurther facilitate the evaluation of generative models\nhttps://github.com/MischaD/beyondfid.\n","authors":["Mischa Dombrowski","Weitong Zhang","Sarah Cechnicka","Hadrien Reynaud","Bernhard Kainz"],"pdf_url":"https://arxiv.org/pdf/2411.16171v2.pdf","comment":"17 pages, 6 tables, 12 figures; v2 added acknowledgment"},{"id":"http://arxiv.org/abs/2412.09122v1","updated":"2024-12-12T09:57:20Z","published":"2024-12-12T09:57:20Z","title":"LVMark: Robust Watermark for latent video diffusion models","summary":"  Rapid advancements in generative models have made it possible to create\nhyper-realistic videos. As their applicability increases, their unauthorized\nuse has raised significant concerns, leading to the growing demand for\ntechniques to protect the ownership of the generative model itself. While\nexisting watermarking methods effectively embed watermarks into\nimage-generative models, they fail to account for temporal information,\nresulting in poor performance when applied to video-generative models. To\naddress this issue, we introduce a novel watermarking method called LVMark,\nwhich embeds watermarks into video diffusion models. A key component of LVMark\nis a selective weight modulation strategy that efficiently embeds watermark\nmessages into the video diffusion model while preserving the quality of the\ngenerated videos. To accurately decode messages in the presence of malicious\nattacks, we design a watermark decoder that leverages spatio-temporal\ninformation in the 3D wavelet domain through a cross-attention module. To the\nbest of our knowledge, our approach is the first to highlight the potential of\nvideo-generative model watermarking as a valuable tool for enhancing the\neffectiveness of ownership protection in video-generative models.\n","authors":["MinHyuk Jang","Youngdong Jang","JaeHyeok Lee","Kodai Kawamura","Feng Yang","Sangpil Kim"],"pdf_url":"https://arxiv.org/pdf/2412.09122v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08344v2","updated":"2024-12-12T09:52:55Z","published":"2024-12-11T12:34:37Z","title":"CoDTS: Enhancing Sparsely Supervised Collaborative Perception with a\n  Dual Teacher-Student Framework","summary":"  Current collaborative perception methods often rely on fully annotated\ndatasets, which can be expensive to obtain in practical situations. To reduce\nannotation costs, some works adopt sparsely supervised learning techniques and\ngenerate pseudo labels for the missing instances. However, these methods fail\nto achieve an optimal confidence threshold that harmonizes the quality and\nquantity of pseudo labels. To address this issue, we propose an end-to-end\nCollaborative perception Dual Teacher-Student framework (CoDTS), which employs\nadaptive complementary learning to produce both high-quality and high-quantity\npseudo labels. Specifically, the Main Foreground Mining (MFM) module generates\nhigh-quality pseudo labels based on the prediction of the static teacher.\nSubsequently, the Supplement Foreground Mining (SFM) module ensures a balance\nbetween the quality and quantity of pseudo labels by adaptively identifying\nmissing instances based on the prediction of the dynamic teacher. Additionally,\nthe Neighbor Anchor Sampling (NAS) module is incorporated to enhance the\nrepresentation of pseudo labels. To promote the adaptive complementary\nlearning, we implement a staged training strategy that trains the student and\ndynamic teacher in a mutually beneficial manner. Extensive experiments\ndemonstrate that the CoDTS effectively ensures an optimal balance of pseudo\nlabels in both quality and quantity, establishing a new state-of-the-art in\nsparsely supervised collaborative perception.\n","authors":["Yushan Han","Hui Zhang","Honglei Zhang","Jing Wang","Yidong Li"],"pdf_url":"https://arxiv.org/pdf/2412.08344v2.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2408.13499v2","updated":"2024-12-12T09:50:24Z","published":"2024-08-24T06:52:14Z","title":"R2G: Reasoning to Ground in 3D Scenes","summary":"  We propose Reasoning to Ground (R2G), a neural symbolic model that grounds\nthe target objects within 3D scenes in a reasoning manner. In contrast to prior\nworks, R2G explicitly models the 3D scene with a semantic concept-based scene\ngraph; recurrently simulates the attention transferring across object entities;\nthus makes the process of grounding the target objects with the highest\nprobability interpretable. Specifically, we respectively embed multiple object\nproperties within the graph nodes and spatial relations among entities within\nthe edges, utilizing a predefined semantic vocabulary. To guide attention\ntransferring, we employ learning or prompting-based methods to analyze the\nreferential utterance and convert it into reasoning instructions within the\nsame semantic space. In each reasoning round, R2G either (1) merges current\nattention distribution with the similarity between the instruction and embedded\nentity properties or (2) shifts the attention across the scene graph based on\nthe similarity between the instruction and embedded spatial relations. The\nexperiments on Sr3D/Nr3D benchmarks show that R2G achieves a comparable result\nwith the prior works while maintaining improved interpretability, breaking a\nnew path for 3D language grounding.\n","authors":["Yixuan Li","Zan Wang","Wei Liang"],"pdf_url":"https://arxiv.org/pdf/2408.13499v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09115v1","updated":"2024-12-12T09:49:16Z","published":"2024-12-12T09:49:16Z","title":"Vision CNNs trained to estimate spatial latents learned similar\n  ventral-stream-aligned representations","summary":"  Studies of the functional role of the primate ventral visual stream have\ntraditionally focused on object categorization, often ignoring -- despite much\nprior evidence -- its role in estimating \"spatial\" latents such as object\nposition and pose. Most leading ventral stream models are derived by optimizing\nnetworks for object categorization, which seems to imply that the ventral\nstream is also derived under such an objective. Here, we explore an alternative\nhypothesis: Might the ventral stream be optimized for estimating spatial\nlatents? And a closely related question: How different -- if at all -- are\nrepresentations learned from spatial latent estimation compared to\ncategorization? To ask these questions, we leveraged synthetic image datasets\ngenerated by a 3D graphic engine and trained convolutional neural networks\n(CNNs) to estimate different combinations of spatial and category latents. We\nfound that models trained to estimate just a few spatial latents achieve neural\nalignment scores comparable to those trained on hundreds of categories, and the\nspatial latent performance of models strongly correlates with their neural\nalignment. Spatial latent and category-trained models have very similar -- but\nnot identical -- internal representations, especially in their early and middle\nlayers. We provide evidence that this convergence is partly driven by\nnon-target latent variability in the training data, which facilitates the\nimplicit learning of representations of those non-target latents. Taken\ntogether, these results suggest that many training objectives, such as spatial\nlatents, can lead to similar models aligned neurally with the ventral stream.\nThus, one should not assume that the ventral stream is optimized for object\ncategorization only. As a field, we need to continue to sharpen our measures of\ncomparing models to brains to better understand the functional roles of the\nventral stream.\n","authors":["Yudi Xie","Weichen Huang","Esther Alter","Jeremy Schwartz","Joshua B. Tenenbaum","James J. DiCarlo"],"pdf_url":"https://arxiv.org/pdf/2412.09115v1.pdf","comment":"29 pages, 20 figures, ICLR 2025"},{"id":"http://arxiv.org/abs/2412.06257v2","updated":"2024-12-12T09:38:22Z","published":"2024-12-09T07:14:58Z","title":"Advancing Extended Reality with 3D Gaussian Splatting: Innovations and\n  Prospects","summary":"  3D Gaussian Splatting (3DGS) has attracted significant attention for its\npotential to revolutionize 3D representation, rendering, and interaction.\nDespite the rapid growth of 3DGS research, its direct application to Extended\nReality (XR) remains underexplored. Although many studies recognize the\npotential of 3DGS for XR, few have explicitly focused on or demonstrated its\neffectiveness within XR environments. In this paper, we aim to synthesize\ninnovations in 3DGS that show specific potential for advancing XR research and\ndevelopment. We conduct a comprehensive review of publicly available 3DGS\npapers, with a focus on those referencing XR-related concepts. Additionally, we\nperform an in-depth analysis of innovations explicitly relevant to XR and\npropose a taxonomy to highlight their significance. Building on these insights,\nwe propose several prospective XR research areas where 3DGS can make promising\ncontributions, yet remain rarely touched. By investigating the intersection of\n3DGS and XR, this paper provides a roadmap to push the boundaries of XR using\ncutting-edge 3DGS techniques.\n","authors":["Shi Qiu","Binzhu Xie","Qixuan Liu","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2412.06257v2.pdf","comment":"IEEE AIxVR 2025"},{"id":"http://arxiv.org/abs/2412.09105v1","updated":"2024-12-12T09:35:47Z","published":"2024-12-12T09:35:47Z","title":"ResFlow: Fine-tuning Residual Optical Flow for Event-based High Temporal\n  Resolution Motion Estimation","summary":"  Event cameras hold significant promise for high-temporal-resolution (HTR)\nmotion estimation. However, estimating event-based HTR optical flow faces two\nkey challenges: the absence of HTR ground-truth data and the intrinsic sparsity\nof event data. Most existing approaches rely on the flow accumulation paradigms\nto indirectly supervise intermediate flows, often resulting in accumulation\nerrors and optimization difficulties. To address these challenges, we propose a\nresidual-based paradigm for estimating HTR optical flow with event data. Our\napproach separates HTR flow estimation into two stages: global linear motion\nestimation and HTR residual flow refinement. The residual paradigm effectively\nmitigates the impacts of event sparsity on optimization and is compatible with\nany LTR algorithm. Next, to address the challenge posed by the absence of HTR\nground truth, we incorporate novel learning strategies. Specifically, we\ninitially employ a shared refiner to estimate the residual flows, enabling both\nLTR supervision and HTR inference. Subsequently, we introduce regional noise to\nsimulate the residual patterns of intermediate flows, facilitating the\nadaptation from LTR supervision to HTR inference. Additionally, we show that\nthe noise-based strategy supports in-domain self-supervised training.\nComprehensive experimental results demonstrate that our approach achieves\nstate-of-the-art accuracy in both LTR and HTR metrics, highlighting its\neffectiveness and superiority.\n","authors":["Qianang Zhou","Zhiyu Zhu","Junhui Hou","Yongjian Deng","Youfu Li","Junlin Xiong"],"pdf_url":"https://arxiv.org/pdf/2412.09105v1.pdf","comment":"10 pages, 8 figures"},{"id":"http://arxiv.org/abs/2412.09082v1","updated":"2024-12-12T09:08:13Z","published":"2024-12-12T09:08:13Z","title":"Towards Long-Horizon Vision-Language Navigation: Platform, Benchmark and\n  Method","summary":"  Existing Vision-Language Navigation (VLN) methods primarily focus on\nsingle-stage navigation, limiting their effectiveness in multi-stage and\nlong-horizon tasks within complex and dynamic environments. To address these\nlimitations, we propose a novel VLN task, named Long-Horizon Vision-Language\nNavigation (LH-VLN), which emphasizes long-term planning and decision\nconsistency across consecutive subtasks. Furthermore, to support LH-VLN, we\ndevelop an automated data generation platform NavGen, which constructs datasets\nwith complex task structures and improves data utility through a bidirectional,\nmulti-granularity generation approach. To accurately evaluate complex tasks, we\nconstruct the Long-Horizon Planning and Reasoning in VLN (LHPR-VLN) benchmark\nconsisting of 3,260 tasks with an average of 150 task steps, serving as the\nfirst dataset specifically designed for the long-horizon vision-language\nnavigation task. Furthermore, we propose Independent Success Rate (ISR),\nConditional Success Rate (CSR), and CSR weight by Ground Truth (CGT) metrics,\nto provide fine-grained assessments of task completion. To improve model\nadaptability in complex tasks, we propose a novel Multi-Granularity Dynamic\nMemory (MGDM) module that integrates short-term memory blurring with long-term\nmemory retrieval to enable flexible navigation in dynamic environments. Our\nplatform, benchmark and method supply LH-VLN with a robust data generation\npipeline, comprehensive model evaluation dataset, reasonable metrics, and a\nnovel VLN model, establishing a foundational framework for advancing LH-VLN.\n","authors":["Xinshuai Song","Weixing Chen","Yang Liu","Weikai Chen","Guanbin Li","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2412.09082v1.pdf","comment":"A novel Vision-Language Navigation task: Long-Horizon Vision-Language\n  Navigation"},{"id":"http://arxiv.org/abs/2303.15361v2","updated":"2024-12-12T09:06:56Z","published":"2023-03-27T16:32:21Z","title":"A Comprehensive Survey on Test-Time Adaptation under Distribution Shifts","summary":"  Machine learning methods strive to acquire a robust model during the training\nprocess that can effectively generalize to test samples, even in the presence\nof distribution shifts. However, these methods often suffer from performance\ndegradation due to unknown test distributions. Test-time adaptation (TTA), an\nemerging paradigm, has the potential to adapt a pre-trained model to unlabeled\ndata during testing, before making predictions. Recent progress in this\nparadigm has highlighted the significant benefits of using unlabeled data to\ntrain self-adapted models prior to inference. In this survey, we categorize TTA\ninto several distinct groups based on the form of test data, namely, test-time\ndomain adaptation, test-time batch adaptation, and online test-time adaptation.\nFor each category, we provide a comprehensive taxonomy of advanced algorithms\nand discuss various learning scenarios. Furthermore, we analyze relevant\napplications of TTA and discuss open challenges and promising areas for future\nresearch. For a comprehensive list of TTA methods, kindly refer to\n\\url{https://github.com/tim-learn/awesome-test-time-adaptation}.\n","authors":["Jian Liang","Ran He","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2303.15361v2.pdf","comment":"Discussions, comments, and questions are all welcomed in\n  \\url{https://github.com/tim-learn/awesome-test-time-adaptation}"},{"id":"http://arxiv.org/abs/2404.18924v2","updated":"2024-12-12T09:06:01Z","published":"2024-04-29T17:59:02Z","title":"Swin2-MoSE: A New Single Image Super-Resolution Model for Remote Sensing","summary":"  Due to the limitations of current optical and sensor technologies and the\nhigh cost of updating them, the spectral and spatial resolution of satellites\nmay not always meet desired requirements. For these reasons, Remote-Sensing\nSingle-Image Super-Resolution (RS-SISR) techniques have gained significant\ninterest. In this paper, we propose Swin2-MoSE model, an enhanced version of\nSwin2SR. Our model introduces MoE-SM, an enhanced Mixture-of-Experts (MoE) to\nreplace the Feed-Forward inside all Transformer block. MoE-SM is designed with\nSmart-Merger, and new layer for merging the output of individual experts, and\nwith a new way to split the work between experts, defining a new per-example\nstrategy instead of the commonly used per-token one. Furthermore, we analyze\nhow positional encodings interact with each other, demonstrating that\nper-channel bias and per-head bias can positively cooperate. Finally, we\npropose to use a combination of Normalized-Cross-Correlation (NCC) and\nStructural Similarity Index Measure (SSIM) losses, to avoid typical MSE loss\nlimitations. Experimental results demonstrate that Swin2-MoSE outperforms any\nSwin derived models by up to 0.377 - 0.958 dB (PSNR) on task of 2x, 3x and 4x\nresolution-upscaling (Sen2Venus and OLI2MSI datasets). It also outperforms SOTA\nmodels by a good margin, proving to be competitive and with excellent\npotential, especially for complex tasks. Additionally, an analysis of\ncomputational costs is also performed. Finally, we show the efficacy of\nSwin2-MoSE, applying it to a semantic segmentation task (SeasoNet dataset).\nCode and pretrained are available on\nhttps://github.com/IMPLabUniPr/swin2-mose/tree/official_code\n","authors":["Leonardo Rossi","Vittorio Bernuzzi","Tomaso Fontanini","Massimo Bertozzi","Andrea Prati"],"pdf_url":"https://arxiv.org/pdf/2404.18924v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11512v3","updated":"2024-12-12T08:59:33Z","published":"2024-09-17T19:26:21Z","title":"Good Grasps Only: A data engine for self-supervised fine-tuning of pose\n  estimation using grasp poses for verification","summary":"  In this paper, we present a novel method for self-supervised fine-tuning of\npose estimation. Leveraging zero-shot pose estimation, our approach enables the\nrobot to automatically obtain training data without manual labeling. After pose\nestimation the object is grasped, and in-hand pose estimation is used for data\nvalidation. Our pipeline allows the system to fine-tune while the process is\nrunning, removing the need for a learning phase. The motivation behind our work\nlies in the need for rapid setup of pose estimation solutions. Specifically, we\naddress the challenging task of bin picking, which plays a pivotal role in\nflexible robotic setups. Our method is implemented on a robotics work-cell, and\ntested with four different objects. For all objects, our method increases the\nperformance and outperforms a state-of-the-art method trained on the CAD model\nof the objects. Project page available at gogoengine.github.io\n","authors":["Frederik Hagelskjær"],"pdf_url":"https://arxiv.org/pdf/2409.11512v3.pdf","comment":"8 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2404.11824v4","updated":"2024-12-12T08:59:22Z","published":"2024-04-18T01:10:24Z","title":"TextCenGen: Attention-Guided Text-Centric Background Adaptation for\n  Text-to-Image Generation","summary":"  Recent advancements in Text-to-image (T2I) generation have witnessed a shift\nfrom adapting text to fixed backgrounds to creating images around text.\nTraditional approaches are often limited to generate layouts within static\nimages for effective text placement. Our proposed approach, TextCenGen,\nintroduces a dynamic adaptation of the blank region for text-friendly image\ngeneration, emphasizing text-centric design and visual harmony generation. Our\nmethod employs force-directed attention guidance in T2I models to generate\nimages that strategically reserve whitespace for pre-defined text areas, even\nfor text or icons at the golden ratio. Observing how cross-attention maps\naffect object placement, we detect and repel conflicting objects using a\nforce-directed graph approach, combined with a Spatial Excluding\nCross-Attention Constraint for smooth attention in whitespace areas. As a novel\ntask in graphic design, experiments indicate that TextCenGen outperforms\nexisting methods with more harmonious compositions. Furthermore, our method\nsignificantly enhances T2I model outcomes on our specially collected prompt\ndatasets, catering to varied text positions. These results demonstrate the\nefficacy of TextCenGen in creating more harmonious and integrated text-image\ncompositions.\n","authors":["Tianyi Liang","Jiangqi Liu","Yifei Huang","Shiqi Jiang","Sicheng Song","Jianshen Shi","Changbo Wang","Chenhui Li"],"pdf_url":"https://arxiv.org/pdf/2404.11824v4.pdf","comment":"7 pages, 7 figures"},{"id":"http://arxiv.org/abs/2412.09074v1","updated":"2024-12-12T08:59:08Z","published":"2024-12-12T08:59:08Z","title":"DomCLP: Domain-wise Contrastive Learning with Prototype Mixup for\n  Unsupervised Domain Generalization","summary":"  Self-supervised learning (SSL) methods based on the instance discrimination\ntasks with InfoNCE have achieved remarkable success. Despite their success, SSL\nmodels often struggle to generate effective representations for unseen-domain\ndata. To address this issue, research on unsupervised domain generalization\n(UDG), which aims to develop SSL models that can generate domain-irrelevant\nfeatures, has been conducted. Most UDG approaches utilize contrastive learning\nwith InfoNCE to generate representations, and perform feature alignment based\non strong assumptions to generalize domain-irrelevant common features from\nmulti-source domains. However, existing methods that rely on instance\ndiscrimination tasks are not effective at extracting domain-irrelevant common\nfeatures. This leads to the suppression of domain-irrelevant common features\nand the amplification of domain-relevant features, thereby hindering domain\ngeneralization. Furthermore, strong assumptions underlying feature alignment\ncan lead to biased feature learning, reducing the diversity of common features.\nIn this paper, we propose a novel approach, DomCLP, Domain-wise Contrastive\nLearning with Prototype Mixup. We explore how InfoNCE suppresses\ndomain-irrelevant common features and amplifies domain-relevant features. Based\non this analysis, we propose Domain-wise Contrastive Learning (DCon) to enhance\ndomain-irrelevant common features. We also propose Prototype Mixup Learning\n(PMix) to generalize domain-irrelevant common features across multiple domains\nwithout relying on strong assumptions. The proposed method consistently\noutperforms state-of-the-art methods on the PACS and DomainNet datasets across\nvarious label fractions, showing significant improvements. Our code will be\nreleased. Our project page is available at https://github.com/jinsuby/DomCLP.\n","authors":["Jin-Seop Lee","Noo-ri Kim","Jee-Hyong Lee"],"pdf_url":"https://arxiv.org/pdf/2412.09074v1.pdf","comment":"Code page: https://github.com/jinsuby/DomCLP"},{"id":"http://arxiv.org/abs/2412.09073v1","updated":"2024-12-12T08:58:42Z","published":"2024-12-12T08:58:42Z","title":"SVasP: Self-Versatility Adversarial Style Perturbation for Cross-Domain\n  Few-Shot Learning","summary":"  Cross-Domain Few-Shot Learning (CD-FSL) aims to transfer knowledge from seen\nsource domains to unseen target domains, which is crucial for evaluating the\ngeneralization and robustness of models. Recent studies focus on utilizing\nvisual styles to bridge the domain gap between different domains. However, the\nserious dilemma of gradient instability and local optimization problem occurs\nin those style-based CD-FSL methods. This paper addresses these issues and\nproposes a novel crop-global style perturbation method, called\n\\underline{\\textbf{S}}elf-\\underline{\\textbf{V}}ersatility\n\\underline{\\textbf{A}}dversarial \\underline{\\textbf{S}}tyle\n\\underline{\\textbf{P}}erturbation (\\textbf{SVasP}), which enhances the gradient\nstability and escapes from poor sharp minima jointly. Specifically, SVasP\nsimulates more diverse potential target domain adversarial styles via\ndiversifying input patterns and aggregating localized crop style gradients, to\nserve as global style perturbation stabilizers within one image, a concept we\nrefer to as self-versatility. Then a novel objective function is proposed to\nmaximize visual discrepancy while maintaining semantic consistency between\nglobal, crop, and adversarial features. Having the stabilized global style\nperturbation in the training phase, one can obtain a flattened minima in the\nloss landscape, boosting the transferability of the model to the target\ndomains. Extensive experiments on multiple benchmark datasets demonstrate that\nour method significantly outperforms existing state-of-the-art methods. Our\ncodes are available at https://github.com/liwenqianSEU/SVasP.\n","authors":["Wenqian Li","Pengfei Fang","Hui Xue"],"pdf_url":"https://arxiv.org/pdf/2412.09073v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09072v1","updated":"2024-12-12T08:58:20Z","published":"2024-12-12T08:58:20Z","title":"Cross-View Completion Models are Zero-shot Correspondence Estimators","summary":"  In this work, we explore new perspectives on cross-view completion learning\nby drawing an analogy to self-supervised correspondence learning. Through our\nanalysis, we demonstrate that the cross-attention map within cross-view\ncompletion models captures correspondence more effectively than other\ncorrelations derived from encoder or decoder features. We verify the\neffectiveness of the cross-attention map by evaluating on both zero-shot\nmatching and learning-based geometric matching and multi-frame depth\nestimation. Project page is available at https://cvlab-kaist.github.io/ZeroCo/.\n","authors":["Honggyu An","Jinhyeon Kim","Seonghoon Park","Jaewoo Jung","Jisang Han","Sunghwan Hong","Seungryong Kim"],"pdf_url":"https://arxiv.org/pdf/2412.09072v1.pdf","comment":"Project Page: https://cvlab-kaist.github.io/ZeroCo/"},{"id":"http://arxiv.org/abs/2404.06442v2","updated":"2024-12-12T08:48:02Z","published":"2024-04-09T16:42:54Z","title":"QueSTMaps: Queryable Semantic Topological Maps for 3D Scene\n  Understanding","summary":"  Robotic tasks such as planning and navigation require a hierarchical semantic\nunderstanding of a scene, which could include multiple floors and rooms.\nCurrent methods primarily focus on object segmentation for 3D scene\nunderstanding. However, such methods struggle to segment out topological\nregions like \"kitchen\" in the scene. In this work, we introduce a two-step\npipeline to solve this problem. First, we extract a topological map, i.e.,\nfloorplan of the indoor scene using a novel multi-channel occupancy\nrepresentation. Then, we generate CLIP-aligned features and semantic labels for\nevery room instance based on the objects it contains using a self-attention\ntransformer. Our language-topology alignment supports natural language\nquerying, e.g., a \"place to cook\" locates the \"kitchen\". We outperform the\ncurrent state-of-the-art on room segmentation by ~20% and room classification\nby ~12%. Our detailed qualitative analysis and ablation studies provide\ninsights into the problem of joint structural and semantic 3D scene\nunderstanding. Project Page: quest-maps.github.io\n","authors":["Yash Mehan","Kumaraditya Gupta","Rohit Jayanti","Anirudh Govil","Sourav Garg","Madhava Krishna"],"pdf_url":"https://arxiv.org/pdf/2404.06442v2.pdf","comment":"Accepted at 2024 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS) as Oral Presentation. Also presented at the 2nd\n  Workshop on Open-Vocabulary 3D Scene Understanding (OpenSUN3D) at CVPR 2024"},{"id":"http://arxiv.org/abs/2412.09063v1","updated":"2024-12-12T08:46:22Z","published":"2024-12-12T08:46:22Z","title":"An Efficient Framework for Enhancing Discriminative Models via Diffusion\n  Techniques","summary":"  Image classification serves as the cornerstone of computer vision,\ntraditionally achieved through discriminative models based on deep neural\nnetworks. Recent advancements have introduced classification methods derived\nfrom generative models, which offer the advantage of zero-shot classification.\nHowever, these methods suffer from two main drawbacks: high computational\noverhead and inferior performance compared to discriminative models. Inspired\nby the coordinated cognitive processes of rapid-slow pathway interactions in\nthe human brain during visual signal recognition, we propose the\nDiffusion-Based Discriminative Model Enhancement Framework (DBMEF). This\nframework seamlessly integrates discriminative and generative models in a\ntraining-free manner, leveraging discriminative models for initial predictions\nand endowing deep neural networks with rethinking capabilities via diffusion\nmodels. Consequently, DBMEF can effectively enhance the classification accuracy\nand generalization capability of discriminative models in a plug-and-play\nmanner. We have conducted extensive experiments across 17 prevalent deep model\narchitectures with different training methods, including both CNN-based models\nsuch as ResNet and Transformer-based models like ViT, to demonstrate the\neffectiveness of the proposed DBMEF. Specifically, the framework yields a\n1.51\\% performance improvement for ResNet-50 on the ImageNet dataset and 3.02\\%\non the ImageNet-A dataset. In conclusion, our research introduces a novel\nparadigm for image classification, demonstrating stable improvements across\ndifferent datasets and neural networks.\n","authors":["Chunxiao Li","Xiaoxiao Wang","Boming Miao","Chuanlong Xie","Zizhe Wang","Yao Zhu"],"pdf_url":"https://arxiv.org/pdf/2412.09063v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2310.00919v2","updated":"2024-12-12T08:42:35Z","published":"2023-10-02T06:15:50Z","title":"A simple thinking about the application of the attention mechanism in\n  medical ultrasound image segmentation task","summary":"  The AI-based assisted diagnosis programs have been widely investigated on\nmedical ultrasound images. Complex scenario of ultrasound image, in which the\ncoupled interference of internal and external factors is severe, brings a\nunique challenge for localize the object region automatically and precisely in\nultrasound images. In this study, we seek to propose a more general and robust\nBenchmark Attention Adaptive Framework (BAAF) to assist doctors segment or\ndiagnose lesions and tissues in ultrasound images more quickly and accurately.\nDifferent from existing attention schemes, the BAAF consists of a parallel\nhybrid attention module (PHAM) and an adaptive calibration mechanism (ACM).\nSpecifically, BAAF first coarsely calibrates the input features from the\nchannel and spatial dimensions, and then adaptively selects more robust lesion\nor tissue characterizations from the coarse-calibrated feature maps. The design\nof BAAF further optimizes the \"what\" and \"where\" focus and selection problems\nin CNNs and seeks to improve the segmentation accuracy of lesions or tissues in\nmedical ultrasound images. The method is evaluated on four medical ultrasound\nsegmentation tasks, and the adequate experimental results demonstrate the\nremarkable performance improvement over existing state-of-the-art methods. In\naddition, the comparison with existing attention mechanisms also demonstrates\nthe superiority of BAAF. This work provides the possibility for automated\nmedical ultrasound assisted diagnosis and reduces reliance on human accuracy\nand precision.\n","authors":["Gongping Chen","Rui Wang","Xiaotao Yin","Liang Cui","Yu Dai"],"pdf_url":"https://arxiv.org/pdf/2310.00919v2.pdf","comment":"10 pages, 11 figures"},{"id":"http://arxiv.org/abs/2412.05203v2","updated":"2024-12-12T08:37:20Z","published":"2024-12-06T17:32:53Z","title":"Archaeoscape: Bringing Aerial Laser Scanning Archaeology to the Deep\n  Learning Era","summary":"  Airborne Laser Scanning (ALS) technology has transformed modern archaeology\nby unveiling hidden landscapes beneath dense vegetation. However, the lack of\nexpert-annotated, open-access resources has hindered the analysis of ALS data\nusing advanced deep learning techniques. We address this limitation with\nArchaeoscape (available at https://archaeoscape.ai/data/2024/), a novel\nlarge-scale archaeological ALS dataset spanning 888 km$^2$ in Cambodia with\n31,141 annotated archaeological features from the Angkorian period.\nArchaeoscape is over four times larger than comparable datasets, and the first\nALS archaeology resource with open-access data, annotations, and models.\n  We benchmark several recent segmentation models to demonstrate the benefits\nof modern vision techniques for this problem and highlight the unique\nchallenges of discovering subtle human-made structures under dense jungle\ncanopies. By making Archaeoscape available in open access, we hope to bridge\nthe gap between traditional archaeology and modern computer vision methods.\n","authors":["Yohann Perron","Vladyslav Sydorov","Adam P. Wijker","Damian Evans","Christophe Pottier","Loic Landrieu"],"pdf_url":"https://arxiv.org/pdf/2412.05203v2.pdf","comment":"NeurIPS 2024 - Datasets & Benchmarks Track (spotlight)"},{"id":"http://arxiv.org/abs/2407.17847v2","updated":"2024-12-12T08:28:20Z","published":"2024-07-25T08:00:49Z","title":"Move and Act: Enhanced Object Manipulation and Background Integrity for\n  Image Editing","summary":"  Current methods commonly utilize three-branch structures of inversion,\nreconstruction, and editing, to tackle consistent image editing task. However,\nthese methods lack control over the generation position of the edited object\nand have issues with background preservation. To overcome these limitations, we\npropose a tuning-free method with only two branches: inversion and editing.\nThis approach allows users to simultaneously edit the object's action and\ncontrol the generation position of the edited object. Additionally, it achieves\nimproved background preservation. Specifically, we transfer the edited object\ninformation to the target area and repair or preserve the background of other\nareas during the inversion process at a specific time step. In the editing\nstage, we use the image features in self-attention to query the key and value\nof the corresponding time step in the inversion to achieve consistent image\nediting. Impressive image editing results and quantitative evaluation\ndemonstrate the effectiveness of our method. The code is available at\nhttps://github.com/mobiushy/move-act.\n","authors":["Pengfei Jiang","Mingbao Lin","Fei Chao"],"pdf_url":"https://arxiv.org/pdf/2407.17847v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.09055v1","updated":"2024-12-12T08:27:39Z","published":"2024-12-12T08:27:39Z","title":"Hyperbolic-constraint Point Cloud Reconstruction from Single RGB-D\n  Images","summary":"  Reconstructing desired objects and scenes has long been a primary goal in 3D\ncomputer vision. Single-view point cloud reconstruction has become a popular\ntechnique due to its low cost and accurate results. However, single-view\nreconstruction methods often rely on expensive CAD models and complex geometric\npriors. Effectively utilizing prior knowledge about the data remains a\nchallenge. In this paper, we introduce hyperbolic space to 3D point cloud\nreconstruction, enabling the model to represent and understand complex\nhierarchical structures in point clouds with low distortion. We build upon\nprevious methods by proposing a hyperbolic Chamfer distance and a regularized\ntriplet loss to enhance the relationship between partial and complete point\nclouds. Additionally, we design adaptive boundary conditions to improve the\nmodel's understanding and reconstruction of 3D structures. Our model\noutperforms most existing models, and ablation studies demonstrate the\nsignificance of our model and its components. Experimental results show that\nour method significantly improves feature extraction capabilities. Our model\nachieves outstanding performance in 3D reconstruction tasks.\n","authors":["Wenrui Li","Zhe Yang","Wei Han","Hengyu Man","Xingtao Wang","Xiaopeng Fan"],"pdf_url":"https://arxiv.org/pdf/2412.09055v1.pdf","comment":"Accepted by AAAI25"},{"id":"http://arxiv.org/abs/2412.09050v1","updated":"2024-12-12T08:21:19Z","published":"2024-12-12T08:21:19Z","title":"ContextHOI: Spatial Context Learning for Human-Object Interaction\n  Detection","summary":"  Spatial contexts, such as the backgrounds and surroundings, are considered\ncritical in Human-Object Interaction (HOI) recognition, especially when the\ninstance-centric foreground is blurred or occluded. Recent advancements in HOI\ndetectors are usually built upon detection transformer pipelines. While such an\nobject-detection-oriented paradigm shows promise in localizing objects, its\nexploration of spatial context is often insufficient for accurately recognizing\nhuman actions. To enhance the capabilities of object detectors for HOI\ndetection, we present a dual-branch framework named ContextHOI, which\nefficiently captures both object detection features and spatial contexts. In\nthe context branch, we train the model to extract informative spatial context\nwithout requiring additional hand-craft background labels. Furthermore, we\nintroduce context-aware spatial and semantic supervision to the context branch\nto filter out irrelevant noise and capture informative contexts. ContextHOI\nachieves state-of-the-art performance on the HICO-DET and v-coco benchmarks.\nFor further validation, we construct a novel benchmark, HICO-ambiguous, which\nis a subset of HICO-DET that contains images with occluded or impaired instance\ncues. Extensive experiments across all benchmarks, complemented by\nvisualizations, underscore the enhancements provided by ContextHOI, especially\nin recognizing interactions involving occluded or blurred instances.\n","authors":["Mingda Jia","Liming Zhao","Ge Li","Yun Zheng"],"pdf_url":"https://arxiv.org/pdf/2412.09050v1.pdf","comment":"in proceedings of the 39th AAAI Conference on Artificial Intelligence\n  (AAAI-25)"},{"id":"http://arxiv.org/abs/2412.09044v1","updated":"2024-12-12T08:13:29Z","published":"2024-12-12T08:13:29Z","title":"Motif Guided Graph Transformer with Combinatorial Skeleton Prototype\n  Learning for Skeleton-Based Person Re-Identification","summary":"  Person re-identification (re-ID) via 3D skeleton data is a challenging task\nwith significant value in many scenarios. Existing skeleton-based methods\ntypically assume virtual motion relations between all joints, and adopt average\njoint or sequence representations for learning. However, they rarely explore\nkey body structure and motion such as gait to focus on more important body\njoints or limbs, while lacking the ability to fully mine valuable\nspatial-temporal sub-patterns of skeletons to enhance model learning. This\npaper presents a generic Motif guided graph transformer with Combinatorial\nskeleton prototype learning (MoCos) that exploits structure-specific and\ngait-related body relations as well as combinatorial features of skeleton\ngraphs to learn effective skeleton representations for person re-ID. In\nparticular, motivated by the locality within joints' structure and the\nbody-component collaboration in gait, we first propose the motif guided graph\ntransformer (MGT) that incorporates hierarchical structural motifs and gait\ncollaborative motifs, which simultaneously focuses on multi-order local joint\ncorrelations and key cooperative body parts to enhance skeleton relation\nlearning. Then, we devise the combinatorial skeleton prototype learning (CSP)\nthat leverages random spatial-temporal combinations of joint nodes and skeleton\ngraphs to generate diverse sub-skeleton and sub-tracklet representations, which\nare contrasted with the most representative features (prototypes) of each\nidentity to learn class-related semantics and discriminative skeleton\nrepresentations. Extensive experiments validate the superior performance of\nMoCos over existing state-of-the-art models. We further show its generality\nunder RGB-estimated skeletons, different graph modeling, and unsupervised\nscenarios.\n","authors":["Haocong Rao","Chunyan Miao"],"pdf_url":"https://arxiv.org/pdf/2412.09044v1.pdf","comment":"Accepted by AAAI 2025. Codes are available at\n  https://github.com/Kali-Hac/MoCos"},{"id":"http://arxiv.org/abs/2412.09043v1","updated":"2024-12-12T08:10:31Z","published":"2024-12-12T08:10:31Z","title":"DrivingRecon: Large 4D Gaussian Reconstruction Model For Autonomous\n  Driving","summary":"  Photorealistic 4D reconstruction of street scenes is essential for developing\nreal-world simulators in autonomous driving. However, most existing methods\nperform this task offline and rely on time-consuming iterative processes,\nlimiting their practical applications. To this end, we introduce the Large 4D\nGaussian Reconstruction Model (DrivingRecon), a generalizable driving scene\nreconstruction model, which directly predicts 4D Gaussian from surround view\nvideos. To better integrate the surround-view images, the Prune and Dilate\nBlock (PD-Block) is proposed to eliminate overlapping Gaussian points between\nadjacent views and remove redundant background points. To enhance\ncross-temporal information, dynamic and static decoupling is tailored to better\nlearn geometry and motion features. Experimental results demonstrate that\nDrivingRecon significantly improves scene reconstruction quality and novel view\nsynthesis compared to existing methods. Furthermore, we explore applications of\nDrivingRecon in model pre-training, vehicle adaptation, and scene editing. Our\ncode is available at https://github.com/EnVision-Research/DriveRecon.\n","authors":["Hao Lu","Tianshuo Xu","Wenzhao Zheng","Yunpeng Zhang","Wei Zhan","Dalong Du","Masayoshi Tomizuka","Kurt Keutzer","Yingcong Chen"],"pdf_url":"https://arxiv.org/pdf/2412.09043v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07040v2","updated":"2024-12-12T07:52:56Z","published":"2024-09-11T06:12:03Z","title":"Retinex-RAWMamba: Bridging Demosaicing and Denoising for Low-Light RAW\n  Image Enhancement","summary":"  Low-light image enhancement, particularly in cross-domain tasks such as\nmapping from the raw domain to the sRGB domain, remains a significant\nchallenge. Many deep learning-based methods have been developed to address this\nissue and have shown promising results in recent years. However, single-stage\nmethods, which attempt to unify the complex mapping across both domains,\nleading to limited denoising performance. In contrast, two-stage approaches\ntypically decompose a raw image with color filter arrays (CFA) into a\nfour-channel RGGB format before feeding it into a neural network. However, this\nstrategy overlooks the critical role of demosaicing within the Image Signal\nProcessing (ISP) pipeline, leading to color distortions under varying lighting\nconditions, especially in low-light scenarios. To address these issues, we\ndesign a novel Mamba scanning mechanism, called RAWMamba, to effectively handle\nraw images with different CFAs. Furthermore, we present a Retinex Decomposition\nModule (RDM) grounded in Retinex prior, which decouples illumination from\nreflectance to facilitate more effective denoising and automatic non-linear\nexposure correction. By bridging demosaicing and denoising, better raw image\nenhancement is achieved. Experimental evaluations conducted on public datasets\nSID and MCR demonstrate that our proposed RAWMamba achieves state-of-the-art\nperformance on cross-domain mapping.\n","authors":["Xianmin Chen","Peiliang Huang","Xiaoxu Feng","Dingwen Zhang","Longfei Han","Junwei Han"],"pdf_url":"https://arxiv.org/pdf/2409.07040v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09026v1","updated":"2024-12-12T07:42:50Z","published":"2024-12-12T07:42:50Z","title":"Video Anomaly Detection with Motion and Appearance Guided Patch\n  Diffusion Model","summary":"  A recent endeavor in one class of video anomaly detection is to leverage\ndiffusion models and posit the task as a generation problem, where the\ndiffusion model is trained to recover normal patterns exclusively, thus\nreporting abnormal patterns as outliers. Yet, existing attempts neglect the\nvarious formations of anomaly and predict normal samples at the feature level\nregardless that abnormal objects in surveillance videos are often relatively\nsmall. To address this, a novel patch-based diffusion model is proposed,\nspecifically engineered to capture fine-grained local information. We further\nobserve that anomalies in videos manifest themselves as deviations in both\nappearance and motion. Therefore, we argue that a comprehensive solution must\nconsider both of these aspects simultaneously to achieve accurate frame\nprediction. To address this, we introduce innovative motion and appearance\nconditions that are seamlessly integrated into our patch diffusion model. These\nconditions are designed to guide the model in generating coherent and\ncontextually appropriate predictions for both semantic content and motion\nrelations. Experimental results in four challenging video anomaly detection\ndatasets empirically substantiate the efficacy of our proposed approach,\ndemonstrating that it consistently outperforms most existing methods in\ndetecting abnormal behaviors.\n","authors":["Hang Zhou","Jiale Cai","Yuteng Ye","Yonghui Feng","Chenxing Gao","Junqing Yu","Zikai Song","Wei Yang"],"pdf_url":"https://arxiv.org/pdf/2412.09026v1.pdf","comment":"Accept by AAAI2025"},{"id":"http://arxiv.org/abs/2412.09023v1","updated":"2024-12-12T07:38:10Z","published":"2024-12-12T07:38:10Z","title":"STEAM: Squeeze and Transform Enhanced Attention Module","summary":"  Channel and spatial attention mechanisms introduced by earlier works enhance\nthe representation abilities of deep convolutional neural networks (CNNs) but\noften lead to increased parameter and computation costs. While recent\napproaches focus solely on efficient feature context modeling for channel\nattention, we aim to model both channel and spatial attention comprehensively\nwith minimal parameters and reduced computation. Leveraging the principles of\nrelational modeling in graphs, we introduce a constant-parameter module, STEAM:\nSqueeze and Transform Enhanced Attention Module, which integrates channel and\nspatial attention to enhance the representation power of CNNs. To our\nknowledge, we are the first to propose a graph-based approach for modeling both\nchannel and spatial attention, utilizing concepts from multi-head graph\ntransformers. Additionally, we introduce Output Guided Pooling (OGP), which\nefficiently captures spatial context to further enhance spatial attention. We\nextensively evaluate STEAM for large-scale image classification, object\ndetection and instance segmentation on standard benchmark datasets. STEAM\nachieves a 2% increase in accuracy over the standard ResNet-50 model with only\na meager increase in GFLOPs. Furthermore, STEAM outperforms leading modules ECA\nand GCT in terms of accuracy while achieving a three-fold reduction in GFLOPs.\n","authors":["Rishabh Sabharwal","Ram Samarth B B","Parikshit Singh Rathore","Punit Rathore"],"pdf_url":"https://arxiv.org/pdf/2412.09023v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09013v1","updated":"2024-12-12T07:24:13Z","published":"2024-12-12T07:24:13Z","title":"Arbitrary-steps Image Super-resolution via Diffusion Inversion","summary":"  This study presents a new image super-resolution (SR) technique based on\ndiffusion inversion, aiming at harnessing the rich image priors encapsulated in\nlarge pre-trained diffusion models to improve SR performance. We design a\nPartial noise Prediction strategy to construct an intermediate state of the\ndiffusion model, which serves as the starting sampling point. Central to our\napproach is a deep noise predictor to estimate the optimal noise maps for the\nforward diffusion process. Once trained, this noise predictor can be used to\ninitialize the sampling process partially along the diffusion trajectory,\ngenerating the desirable high-resolution result. Compared to existing\napproaches, our method offers a flexible and efficient sampling mechanism that\nsupports an arbitrary number of sampling steps, ranging from one to five. Even\nwith a single sampling step, our method demonstrates superior or comparable\nperformance to recent state-of-the-art approaches. The code and model are\npublicly available at https://github.com/zsyOAOA/InvSR.\n","authors":["Zongsheng Yue","Kang Liao","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2412.09013v1.pdf","comment":"16 pages, 9 figures. Project: https://github.com/zsyOAOA/InvSR"},{"id":"http://arxiv.org/abs/2412.09008v1","updated":"2024-12-12T07:20:32Z","published":"2024-12-12T07:20:32Z","title":"MS2Mesh-XR: Multi-modal Sketch-to-Mesh Generation in XR Environments","summary":"  We present MS2Mesh-XR, a novel multi-modal sketch-to-mesh generation pipeline\nthat enables users to create realistic 3D objects in extended reality (XR)\nenvironments using hand-drawn sketches assisted by voice inputs. In specific,\nusers can intuitively sketch objects using natural hand movements in mid-air\nwithin a virtual environment. By integrating voice inputs, we devise ControlNet\nto infer realistic images based on the drawn sketches and interpreted text\nprompts. Users can then review and select their preferred image, which is\nsubsequently reconstructed into a detailed 3D mesh using the Convolutional\nReconstruction Model. In particular, our proposed pipeline can generate a\nhigh-quality 3D mesh in less than 20 seconds, allowing for immersive\nvisualization and manipulation in run-time XR scenes. We demonstrate the\npracticability of our pipeline through two use cases in XR settings. By\nleveraging natural user inputs and cutting-edge generative AI capabilities, our\napproach can significantly facilitate XR-based creative production and enhance\nuser experiences. Our code and demo will be available at:\nhttps://yueqiu0911.github.io/MS2Mesh-XR/\n","authors":["Yuqi Tong","Yue Qiu","Ruiyang Li","Shi Qiu","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2412.09008v1.pdf","comment":"IEEE AIxVR 2025"},{"id":"http://arxiv.org/abs/2412.08614v2","updated":"2024-12-12T06:33:36Z","published":"2024-12-11T18:37:42Z","title":"Benchmarking Large Vision-Language Models via Directed Scene Graph for\n  Comprehensive Image Captioning","summary":"  Generating detailed captions comprehending text-rich visual content in images\nhas received growing attention for Large Vision-Language Models (LVLMs).\nHowever, few studies have developed benchmarks specifically tailored for\ndetailed captions to measure their accuracy and comprehensiveness. In this\npaper, we introduce a detailed caption benchmark, termed as CompreCap, to\nevaluate the visual context from a directed scene graph view. Concretely, we\nfirst manually segment the image into semantically meaningful regions (i.e.,\nsemantic segmentation mask) according to common-object vocabulary, while also\ndistinguishing attributes of objects within all those regions. Then directional\nrelation labels of these objects are annotated to compose a directed scene\ngraph that can well encode rich compositional information of the image. Based\non our directed scene graph, we develop a pipeline to assess the generated\ndetailed captions from LVLMs on multiple levels, including the object-level\ncoverage, the accuracy of attribute descriptions, the score of key\nrelationships, etc. Experimental results on the CompreCap dataset confirm that\nour evaluation method aligns closely with human evaluation scores across LVLMs.\n","authors":["Fan Lu","Wei Wu","Kecheng Zheng","Shuailei Ma","Biao Gong","Jiawei Liu","Wei Zhai","Yang Cao","Yujun Shen","Zheng-Jun Zha"],"pdf_url":"https://arxiv.org/pdf/2412.08614v2.pdf","comment":"21 pages, 17 figures. Code and Dataset:\n  https://github.com/LuFan31/CompreCap"},{"id":"http://arxiv.org/abs/2412.04729v2","updated":"2024-12-12T06:31:47Z","published":"2024-12-06T02:39:50Z","title":"Espresso: High Compression For Rich Extraction From Videos for Your\n  Vision-Language Model","summary":"  Most of the current vision-language models (VLMs) for videos struggle to\nunderstand videos longer than a few seconds. This is primarily due to the fact\nthat they do not scale to utilizing a large number of frames. In order to\naddress this limitation, we propose Espresso, a novel method that extracts and\ncompresses spatial and temporal information separately. Through extensive\nevaluations, we show that spatial and temporal compression in Espresso each\nhave a positive impact on the long-form video understanding capabilities; when\ncombined, their positive impact increases. Furthermore, we show that Espresso's\nperformance scales well with more training data, and that Espresso is far more\neffective than the existing projectors for VLMs in long-form video\nunderstanding. Moreover, we devise a more difficult evaluation setting for\nEgoSchema called \"needle-in-a-haystack\" that multiplies the lengths of the\ninput videos. Espresso achieves SOTA performance on this task, outperforming\nthe SOTA VLMs that have been trained on much more training data.\n","authors":["Keunwoo Peter Yu","Achal Dave","Rares Ambrus","Jean Mercat"],"pdf_url":"https://arxiv.org/pdf/2412.04729v2.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2409.09269v3","updated":"2024-12-12T06:26:09Z","published":"2024-09-14T02:29:36Z","title":"Guiding Vision-Language Model Selection for Visual Question-Answering\n  Across Tasks, Domains, and Knowledge Types","summary":"  Visual Question-Answering (VQA) has become key to user experience,\nparticularly after improved generalization capabilities of Vision-Language\nModels (VLMs). But evaluating VLMs for an application requirement using a\nstandardized framework in practical settings is still challenging. This paper\naims to solve that using an end-to-end framework. We present VQA360 - a novel\ndataset derived from established VQA benchmarks, annotated with task types,\napplication domains, and knowledge types, for a comprehensive evaluation. We\nalso introduce GoEval, a multimodal evaluation metric developed using GPT-4o,\nachieving a correlation factor of 56.71% with human judgments. Our experiments\nwith state-of-the-art VLMs reveal that no single model excels universally,\nthus, making a right choice a key design decision. Proprietary models such as\nGemini-1.5-Pro and GPT-4o-mini generally outperform others, but open-source\nmodels like InternVL-2-8B and CogVLM-2-Llama-3-19B also demonstrate competitive\nstrengths, while providing additional advantages. Our framework can also be\nextended to other tasks.\n","authors":["Neelabh Sinha","Vinija Jain","Aman Chadha"],"pdf_url":"https://arxiv.org/pdf/2409.09269v3.pdf","comment":"Accepted at The First Workshop of Evaluation of Multi-Modal\n  Generation (EvalMG) in 31st International Conference on Computational\n  Linguistics (COLING), 2025. 8 pages + references + 6 pages of Appendix"},{"id":"http://arxiv.org/abs/2412.08979v1","updated":"2024-12-12T06:26:02Z","published":"2024-12-12T06:26:02Z","title":"A Wander Through the Multimodal Landscape: Efficient Transfer Learning\n  via Low-rank Sequence Multimodal Adapter","summary":"  Efficient transfer learning methods such as adapter-based methods have shown\ngreat success in unimodal models and vision-language models. However, existing\nmethods have two main challenges in fine-tuning multimodal models. Firstly,\nthey are designed for vision-language tasks and fail to extend to situations\nwhere there are more than two modalities. Secondly, they exhibit limited\nexploitation of interactions between modalities and lack efficiency. To address\nthese issues, in this paper, we propose the loW-rank sequence multimodal\nadapter (Wander). We first use the outer product to fuse the information from\ndifferent modalities in an element-wise way effectively. For efficiency, we use\nCP decomposition to factorize tensors into rank-one components and achieve\nsubstantial parameter reduction. Furthermore, we implement a token-level\nlow-rank decomposition to extract more fine-grained features and sequence\nrelationships between modalities. With these designs, Wander enables\ntoken-level interactions between sequences of different modalities in a\nparameter-efficient way. We conduct extensive experiments on datasets with\ndifferent numbers of modalities, where Wander outperforms state-of-the-art\nefficient transfer learning methods consistently. The results fully demonstrate\nthe effectiveness, efficiency and universality of Wander.\n","authors":["Zirun Guo","Xize Cheng","Yangyang Wu","Tao Jin"],"pdf_url":"https://arxiv.org/pdf/2412.08979v1.pdf","comment":"Accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2412.06234v2","updated":"2024-12-12T06:17:36Z","published":"2024-12-09T06:20:51Z","title":"Generative Densification: Learning to Densify Gaussians for\n  High-Fidelity Generalizable 3D Reconstruction","summary":"  Generalized feed-forward Gaussian models have achieved significant progress\nin sparse-view 3D reconstruction by leveraging prior knowledge from large\nmulti-view datasets. However, these models often struggle to represent\nhigh-frequency details due to the limited number of Gaussians. While the\ndensification strategy used in per-scene 3D Gaussian splatting (3D-GS)\noptimization can be adapted to the feed-forward models, it may not be ideally\nsuited for generalized scenarios. In this paper, we propose Generative\nDensification, an efficient and generalizable method to densify Gaussians\ngenerated by feed-forward models. Unlike the 3D-GS densification strategy,\nwhich iteratively splits and clones raw Gaussian parameters, our method\nup-samples feature representations from the feed-forward models and generates\ntheir corresponding fine Gaussians in a single forward pass, leveraging the\nembedded prior knowledge for enhanced generalization. Experimental results on\nboth object-level and scene-level reconstruction tasks demonstrate that our\nmethod outperforms state-of-the-art approaches with comparable or smaller model\nsizes, achieving notable improvements in representing fine details.\n","authors":["Seungtae Nam","Xiangyu Sun","Gyeongjin Kang","Younggeun Lee","Seungjun Oh","Eunbyung Park"],"pdf_url":"https://arxiv.org/pdf/2412.06234v2.pdf","comment":"Project page: https://stnamjef.github.io/GenerativeDensification/"},{"id":"http://arxiv.org/abs/2412.08976v1","updated":"2024-12-12T06:13:32Z","published":"2024-12-12T06:13:32Z","title":"Enhancing Facial Consistency in Conditional Video Generation via Facial\n  Landmark Transformation","summary":"  Landmark-guided character animation generation is an important field.\nGenerating character animations with facial features consistent with a\nreference image remains a significant challenge in conditional video\ngeneration, especially involving complex motions like dancing. Existing methods\noften fail to maintain facial feature consistency due to mismatches between the\nfacial landmarks extracted from source videos and the target facial features in\nthe reference image. To address this problem, we propose a facial landmark\ntransformation method based on the 3D Morphable Model (3DMM). We obtain\ntransformed landmarks that align with the target facial features by\nreconstructing 3D faces from the source landmarks and adjusting the 3DMM\nparameters to match the reference image. Our method improves the facial\nconsistency between the generated videos and the reference images, effectively\nimproving the facial feature mismatch problem.\n","authors":["Lianrui Mu","Xingze Zhou","Wenjie Zheng","Jiangnan Ye","Xiaoyu Liang","Yuchen Yang","Jianhong Bai","Jiedong Zhuang","Haoji Hu"],"pdf_url":"https://arxiv.org/pdf/2412.08976v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08975v1","updated":"2024-12-12T06:13:00Z","published":"2024-12-12T06:13:00Z","title":"Elevating Flow-Guided Video Inpainting with Reference Generation","summary":"  Video inpainting (VI) is a challenging task that requires effective\npropagation of observable content across frames while simultaneously generating\nnew content not present in the original video. In this study, we propose a\nrobust and practical VI framework that leverages a large generative model for\nreference generation in combination with an advanced pixel propagation\nalgorithm. Powered by a strong generative model, our method not only\nsignificantly enhances frame-level quality for object removal but also\nsynthesizes new content in the missing areas based on user-provided text\nprompts. For pixel propagation, we introduce a one-shot pixel pulling method\nthat effectively avoids error accumulation from repeated sampling while\nmaintaining sub-pixel precision. To evaluate various VI methods in realistic\nscenarios, we also propose a high-quality VI benchmark, HQVI, comprising\ncarefully generated videos using alpha matte composition. On public benchmarks\nand the HQVI dataset, our method demonstrates significantly higher visual\nquality and metric scores compared to existing solutions. Furthermore, it can\nprocess high-resolution videos exceeding 2K resolution with ease, underscoring\nits superiority for real-world applications.\n","authors":["Suhwan Cho","Seoung Wug Oh","Sangyoun Lee","Joon-Young Lee"],"pdf_url":"https://arxiv.org/pdf/2412.08975v1.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2412.08973v1","updated":"2024-12-12T06:09:49Z","published":"2024-12-12T06:09:49Z","title":"Is Contrastive Distillation Enough for Learning Comprehensive 3D\n  Representations?","summary":"  Cross-modal contrastive distillation has recently been explored for learning\neffective 3D representations. However, existing methods focus primarily on\nmodality-shared features, neglecting the modality-specific features during the\npre-training process, which leads to suboptimal representations. In this paper,\nwe theoretically analyze the limitations of current contrastive methods for 3D\nrepresentation learning and propose a new framework, namely CMCR, to address\nthese shortcomings. Our approach improves upon traditional methods by better\nintegrating both modality-shared and modality-specific features. Specifically,\nwe introduce masked image modeling and occupancy estimation tasks to guide the\nnetwork in learning more comprehensive modality-specific features. Furthermore,\nwe propose a novel multi-modal unified codebook that learns an embedding space\nshared across different modalities. Besides, we introduce geometry-enhanced\nmasked image modeling to further boost 3D representation learning. Extensive\nexperiments demonstrate that our method mitigates the challenges faced by\ntraditional approaches and consistently outperforms existing image-to-LiDAR\ncontrastive distillation methods in downstream tasks. Code will be available at\nhttps://github.com/Eaphan/CMCR.\n","authors":["Yifan Zhang","Junhui Hou"],"pdf_url":"https://arxiv.org/pdf/2412.08973v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2308.07926v2","updated":"2024-12-12T06:08:32Z","published":"2023-08-15T17:59:56Z","title":"CoDeF: Content Deformation Fields for Temporally Consistent Video\n  Processing","summary":"  We present the content deformation field CoDeF as a new type of video\nrepresentation, which consists of a canonical content field aggregating the\nstatic contents in the entire video and a temporal deformation field recording\nthe transformations from the canonical image (i.e., rendered from the canonical\ncontent field) to each individual frame along the time axis. Given a target\nvideo, these two fields are jointly optimized to reconstruct it through a\ncarefully tailored rendering pipeline. We advisedly introduce some\nregularizations into the optimization process, urging the canonical content\nfield to inherit semantics (e.g., the object shape) from the video. With such a\ndesign, CoDeF naturally supports lifting image algorithms for video processing,\nin the sense that one can apply an image algorithm to the canonical image and\neffortlessly propagate the outcomes to the entire video with the aid of the\ntemporal deformation field. We experimentally show that CoDeF is able to lift\nimage-to-image translation to video-to-video translation and lift keypoint\ndetection to keypoint tracking without any training. More importantly, thanks\nto our lifting strategy that deploys the algorithms on only one image, we\nachieve superior cross-frame consistency in processed videos compared to\nexisting video-to-video translation approaches, and even manage to track\nnon-rigid objects like water and smog. Project page can be found at\nhttps://qiuyu96.github.io/CoDeF/.\n","authors":["Hao Ouyang","Qiuyu Wang","Yuxi Xiao","Qingyan Bai","Juntao Zhang","Kecheng Zheng","Xiaowei Zhou","Qifeng Chen","Yujun Shen"],"pdf_url":"https://arxiv.org/pdf/2308.07926v2.pdf","comment":"Project Webpage: https://qiuyu96.github.io/CoDeF/, Code:\n  https://github.com/qiuyu96/CoDeF"},{"id":"http://arxiv.org/abs/2412.05969v2","updated":"2024-12-12T06:04:06Z","published":"2024-12-08T15:28:30Z","title":"Efficient Semantic Splatting for Remote Sensing Multi-view Segmentation","summary":"  In this paper, we propose a novel semantic splatting approach based on\nGaussian Splatting to achieve efficient and low-latency. Our method projects\nthe RGB attributes and semantic features of point clouds onto the image plane,\nsimultaneously rendering RGB images and semantic segmentation results.\nLeveraging the explicit structure of point clouds and a one-time rendering\nstrategy, our approach significantly enhances efficiency during optimization\nand rendering. Additionally, we employ SAM2 to generate pseudo-labels for\nboundary regions, which often lack sufficient supervision, and introduce\ntwo-level aggregation losses at the 2D feature map and 3D spatial levels to\nimprove the view-consistent and spatial continuity.\n","authors":["Zipeng Qi","Hao Chen","Haotian Zhang","Zhengxia Zou","Zhenwei Shi"],"pdf_url":"https://arxiv.org/pdf/2412.05969v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08965v1","updated":"2024-12-12T05:57:59Z","published":"2024-12-12T05:57:59Z","title":"AFFAKT: A Hierarchical Optimal Transport based Method for Affective\n  Facial Knowledge Transfer in Video Deception Detection","summary":"  The scarcity of high-quality large-scale labeled datasets poses a huge\nchallenge for employing deep learning models in video deception detection. To\naddress this issue, inspired by the psychological theory on the relation\nbetween deception and expressions, we propose a novel method called AFFAKT in\nthis paper, which enhances the classification performance by transferring\nuseful and correlated knowledge from a large facial expression dataset. Two key\nchallenges in knowledge transfer arise: 1) \\textit{how much} knowledge of\nfacial expression data should be transferred and 2) \\textit{how to} effectively\nleverage transferred knowledge for the deception classification model during\ninference. Specifically, the optimal relation mapping between facial expression\nclasses and deception samples is firstly quantified using proposed H-OTKT\nmodule and then transfers knowledge from the facial expression dataset to\ndeception samples. Moreover, a correlation prototype within another proposed\nmodule SRKB is well designed to retain the invariant correlations between\nfacial expression classes and deception classes through momentum updating.\nDuring inference, the transferred knowledge is fine-tuned with the correlation\nprototype using a sample-specific re-weighting strategy. Experimental results\non two deception detection datasets demonstrate the superior performance of our\nproposed method. The interpretability study reveals high associations between\ndeception and negative affections, which coincides with the theory in\npsychology.\n","authors":["Zihan Ji","Xuetao Tian","Ye Liu"],"pdf_url":"https://arxiv.org/pdf/2412.08965v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2402.03348v2","updated":"2024-12-12T05:56:34Z","published":"2024-01-25T07:20:23Z","title":"Respect the model: Fine-grained and Robust Explanation with Sharing\n  Ratio Decomposition","summary":"  The truthfulness of existing explanation methods in authentically elucidating\nthe underlying model's decision-making process has been questioned. Existing\nmethods have deviated from faithfully representing the model, thus susceptible\nto adversarial attacks. To address this, we propose a novel eXplainable AI\n(XAI) method called SRD (Sharing Ratio Decomposition), which sincerely reflects\nthe model's inference process, resulting in significantly enhanced robustness\nin our explanations. Different from the conventional emphasis on the neuronal\nlevel, we adopt a vector perspective to consider the intricate nonlinear\ninteractions between filters. We also introduce an interesting observation\ntermed Activation-Pattern-Only Prediction (APOP), letting us emphasize the\nimportance of inactive neurons and redefine relevance encapsulating all\nrelevant information including both active and inactive neurons. Our method,\nSRD, allows for the recursive decomposition of a Pointwise Feature Vector\n(PFV), providing a high-resolution Effective Receptive Field (ERF) at any\nlayer.\n","authors":["Sangyu Han","Yearim Kim","Nojun Kwak"],"pdf_url":"https://arxiv.org/pdf/2402.03348v2.pdf","comment":"To be published in ICLR 2024"},{"id":"http://arxiv.org/abs/2412.08477v2","updated":"2024-12-12T05:45:10Z","published":"2024-12-11T15:44:08Z","title":"Accurate Water Level Monitoring in AWD Rice Cultivation Using\n  Convolutional Neural Networks","summary":"  The Alternate Wetting and Drying (AWD) method is a rice-growing water\nmanagement technique promoted as a sustainable alternative to Continuous\nFlooding (CF). Climate change has placed the agricultural sector in a\nchallenging position, particularly as global water resources become\nincreasingly scarce, affecting rice production on irrigated lowlands. Rice, a\nstaple food for over half of the world's population, demands significantly more\nwater than other major crops. In Bangladesh, Boro rice, in particular, requires\nconsiderable water inputs during its cultivation. Traditionally, farmers\nmanually measure water levels, a process that is both time-consuming and prone\nto errors. While ultrasonic sensors offer improvements in water height\nmeasurement, they still face limitations, such as susceptibility to weather\nconditions and environmental factors. To address these issues, we propose a\nnovel approach that automates water height measurement using computer vision,\nspecifically through a convolutional neural network (CNN). Our attention-based\narchitecture achieved an $R^2$ score of 0.9885 and a Mean Squared Error (MSE)\nof 0.2766, providing a more accurate and efficient solution for managing AWD\nsystems.\n","authors":["Ahmed Rafi Hasan","Niloy Kumar Kundu","Saad Hasan","Mohammad Rashedul Hoque","Swakkhar Shatabda"],"pdf_url":"https://arxiv.org/pdf/2412.08477v2.pdf","comment":"15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.01916v4","updated":"2024-12-12T05:40:08Z","published":"2024-06-04T02:57:09Z","title":"FastLGS: Speeding up Language Embedded Gaussians with Feature Grid\n  Mapping","summary":"  The semantically interactive radiance field has always been an appealing task\nfor its potential to facilitate user-friendly and automated real-world 3D scene\nunderstanding applications. However, it is a challenging task to achieve high\nquality, efficiency and zero-shot ability at the same time with semantics in\nradiance fields. In this work, we present FastLGS, an approach that supports\nreal-time open-vocabulary query within 3D Gaussian Splatting (3DGS) under high\nresolution. We propose the semantic feature grid to save multi-view CLIP\nfeatures which are extracted based on Segment Anything Model (SAM) masks, and\nmap the grids to low dimensional features for semantic field training through\n3DGS. Once trained, we can restore pixel-aligned CLIP embeddings through\nfeature grids from rendered features for open-vocabulary queries. Comparisons\nwith other state-of-the-art methods prove that FastLGS can achieve the first\nplace performance concerning both speed and accuracy, where FastLGS is 98x\nfaster than LERF and 4x faster than LangSplat. Meanwhile, experiments show that\nFastLGS is adaptive and compatible with many downstream tasks, such as 3D\nsegmentation and 3D object inpainting, which can be easily applied to other 3D\nmanipulation systems.\n","authors":["Yuzhou Ji","He Zhu","Junshu Tang","Wuyi Liu","Zhizhong Zhang","Xin Tan","Yuan Xie"],"pdf_url":"https://arxiv.org/pdf/2406.01916v4.pdf","comment":"This paper is accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2412.08412v2","updated":"2024-12-12T05:36:51Z","published":"2024-12-11T14:30:24Z","title":"Pragmatist: Multiview Conditional Diffusion Models for High-Fidelity 3D\n  Reconstruction from Unposed Sparse Views","summary":"  Inferring 3D structures from sparse, unposed observations is challenging due\nto its unconstrained nature. Recent methods propose to predict implicit\nrepresentations directly from unposed inputs in a data-driven manner, achieving\npromising results. However, these methods do not utilize geometric priors and\ncannot hallucinate the appearance of unseen regions, thus making it challenging\nto reconstruct fine geometric and textural details. To tackle this challenge,\nour key idea is to reformulate this ill-posed problem as conditional novel view\nsynthesis, aiming to generate complete observations from limited input views to\nfacilitate reconstruction. With complete observations, the poses of the input\nviews can be easily recovered and further used to optimize the reconstructed\nobject. To this end, we propose a novel pipeline Pragmatist. First, we generate\na complete observation of the object via a multiview conditional diffusion\nmodel. Then, we use a feed-forward large reconstruction model to obtain the\nreconstructed mesh. To further improve the reconstruction quality, we recover\nthe poses of input views by inverting the obtained 3D representations and\nfurther optimize the texture using detailed input views. Unlike previous\napproaches, our pipeline improves reconstruction by efficiently leveraging\nunposed inputs and generative priors, circumventing the direct resolution of\nhighly ill-posed problems. Extensive experiments show that our approach\nachieves promising performance in several benchmarks.\n","authors":["Songchun Zhang","Chunhui Zhao"],"pdf_url":"https://arxiv.org/pdf/2412.08412v2.pdf","comment":"Accepted by AAAI 2025. 13 pages, 8 figures"},{"id":"http://arxiv.org/abs/2108.02160v2","updated":"2024-12-12T05:30:36Z","published":"2021-08-04T16:38:33Z","title":"MRI to PET Cross-Modality Translation using Globally and Locally Aware\n  GAN (GLA-GAN) for Multi-Modal Diagnosis of Alzheimer's Disease","summary":"  Medical imaging datasets are inherently high dimensional with large\nvariability and low sample sizes that limit the effectiveness of deep learning\nalgorithms. Recently, generative adversarial networks (GANs) with the ability\nto synthesize realist images have shown great potential as an alternative to\nstandard data augmentation techniques. Our work focuses on cross-modality\nsynthesis of fluorodeoxyglucose~(FDG) Positron Emission Tomography~(PET) scans\nfrom structural Magnetic Resonance~(MR) images using generative models to\nfacilitate multi-modal diagnosis of Alzheimer's disease (AD). Specifically, we\npropose a novel end-to-end, globally and locally aware image-to-image\ntranslation GAN (GLA-GAN) with a multi-path architecture that enforces both\nglobal structural integrity and fidelity to local details. We further\nsupplement the standard adversarial loss with voxel-level intensity,\nmulti-scale structural similarity (MS-SSIM) and region-of-interest (ROI) based\nloss components that reduce reconstruction error, enforce structural\nconsistency at different scales and perceive variation in regional sensitivity\nto AD respectively. Experimental results demonstrate that our GLA-GAN not only\ngenerates synthesized FDG-PET scans with enhanced image quality but also\nsuperior clinical utility in improving AD diagnosis compared to\nstate-of-the-art models. Finally, we attempt to interpret some of the internal\nunits of the GAN that are closely related to this specific cross-modality\ngeneration task.\n","authors":["Apoorva Sikka","Skand Peri","Jitender Singh Virk","Usma Niyaz","Deepti R. Bathula"],"pdf_url":"https://arxiv.org/pdf/2108.02160v2.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2412.08949v1","updated":"2024-12-12T05:26:50Z","published":"2024-12-12T05:26:50Z","title":"Multimodal Industrial Anomaly Detection by Crossmodal Reverse\n  Distillation","summary":"  Knowledge distillation (KD) has been widely studied in unsupervised\nIndustrial Image Anomaly Detection (AD), but its application to unsupervised\nmultimodal AD remains underexplored. Existing KD-based methods for multimodal\nAD that use fused multimodal features to obtain teacher representations face\nchallenges. Anomalies in one modality may not be effectively captured in the\nfused teacher features, leading to detection failures. Besides, these methods\ndo not fully leverage the rich intra- and inter-modality information. In this\npaper, we propose Crossmodal Reverse Distillation (CRD) based on Multi-branch\ndesign to realize Multimodal Industrial AD. By assigning independent branches\nto each modality, our method enables finer detection of anomalies within each\nmodality. Furthermore, we enhance the interaction between modalities during the\ndistillation process by designing Crossmodal Filter and Amplifier. With the\nidea of crossmodal mapping, the student network is allowed to better learn\nnormal features while anomalies in all modalities are ensured to be effectively\ndetected. Experimental verifications on the MVTec 3D-AD dataset demonstrate\nthat our method achieves state-of-the-art performance in multimodal anomaly\ndetection and localization.\n","authors":["Xinyue Liu","Jianyuan Wang","Biao Leng","Shuo Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.08949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08948v1","updated":"2024-12-12T05:26:43Z","published":"2024-12-12T05:26:43Z","title":"Mojito: Motion Trajectory and Intensity Control for Video Generation","summary":"  Recent advancements in diffusion models have shown great promise in producing\nhigh-quality video content. However, efficiently training diffusion models\ncapable of integrating directional guidance and controllable motion intensity\nremains a challenging and under-explored area. This paper introduces Mojito, a\ndiffusion model that incorporates both \\textbf{Mo}tion tra\\textbf{j}ectory and\n\\textbf{i}ntensi\\textbf{t}y contr\\textbf{o}l for text to video generation.\nSpecifically, Mojito features a Directional Motion Control module that\nleverages cross-attention to efficiently direct the generated object's motion\nwithout additional training, alongside a Motion Intensity Modulator that uses\noptical flow maps generated from videos to guide varying levels of motion\nintensity. Extensive experiments demonstrate Mojito's effectiveness in\nachieving precise trajectory and intensity control with high computational\nefficiency, generating motion patterns that closely match specified directions\nand intensities, providing realistic dynamics that align well with natural\nmotion in real-world scenarios.\n","authors":["Xuehai He","Shuohang Wang","Jianwei Yang","Xiaoxia Wu","Yiping Wang","Kuan Wang","Zheng Zhan","Olatunji Ruwase","Yelong Shen","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2412.08948v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08947v1","updated":"2024-12-12T05:24:06Z","published":"2024-12-12T05:24:06Z","title":"Selective Visual Prompting in Vision Mamba","summary":"  Pre-trained Vision Mamba (Vim) models have demonstrated exceptional\nperformance across various computer vision tasks in a computationally efficient\nmanner, attributed to their unique design of selective state space models. To\nfurther extend their applicability to diverse downstream vision tasks, Vim\nmodels can be adapted using the efficient fine-tuning technique known as visual\nprompting. However, existing visual prompting methods are predominantly\ntailored for Vision Transformer (ViT)-based models that leverage global\nattention, neglecting the distinctive sequential token-wise compression and\npropagation characteristics of Vim. Specifically, existing prompt tokens\nprefixed to the sequence are insufficient to effectively activate the input and\nforget gates across the entire sequence, hindering the extraction and\npropagation of discriminative information. To address this limitation, we\nintroduce a novel Selective Visual Prompting (SVP) method specifically for the\nefficient fine-tuning of Vim. To prevent the loss of discriminative information\nduring state space propagation, SVP employs lightweight selective prompters for\ntoken-wise prompt generation, ensuring adaptive activation of the update and\nforget gates within Mamba blocks to promote discriminative information\npropagation. Moreover, considering that Vim propagates both shared cross-layer\ninformation and specific inner-layer information, we further refine SVP with a\ndual-path structure: Cross-Prompting and Inner-Prompting. Cross-Prompting\nutilizes shared parameters across layers, while Inner-Prompting employs\ndistinct parameters, promoting the propagation of both shared and specific\ninformation, respectively. Extensive experimental results on various\nlarge-scale benchmarks demonstrate that our proposed SVP significantly\noutperforms state-of-the-art methods. Our code is available at\nhttps://github.com/zhoujiahuan1991/AAAI2025-SVP.\n","authors":["Yifeng Yao","Zichen Liu","Zhenyu Cui","Yuxin Peng","Jiahuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.08947v1.pdf","comment":"in Proceedings of the 39th AAAI Conference on Artificial Intelligence\n  (AAAI-25)"},{"id":"http://arxiv.org/abs/2402.12683v2","updated":"2024-12-12T05:19:43Z","published":"2024-02-20T03:14:47Z","title":"TorchCP: A Python Library for Conformal Prediction","summary":"  Conformal Prediction (CP) has attracted great attention from the research\ncommunity due to its strict theoretical guarantees. However, researchers and\ndevelopers still face challenges of applicability and efficiency when applying\nCP algorithms to deep learning models. In this paper, we introduce \\torchcp, a\ncomprehensive PyTorch-based toolkit to strengthen the usability of CP for deep\nlearning models. \\torchcp implements a wide range of post-hoc and training\nmethods of conformal prediction for various machine learning tasks, including\nclassification, regression, GNN, and LLM. Moreover, we provide user-friendly\ninterfaces and extensive evaluations to easily integrate CP algorithms into\nspecific tasks. Our \\torchcp toolkit, built entirely with PyTorch, enables\nhigh-performance GPU acceleration for deep learning models and mini-batch\ncomputation on large-scale datasets. With the LGPL license, the code is\nopen-sourced at \\url{https://github.com/ml-stat-Sustech/TorchCP} and will be\ncontinuously updated.\n","authors":["Jianguo Huang","Jianqing Song","Xuanning Zhou","Bingyi Jing","Hongxin Wei"],"pdf_url":"https://arxiv.org/pdf/2402.12683v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10882v6","updated":"2024-12-12T05:18:18Z","published":"2024-02-16T18:36:36Z","title":"Universal Prompt Optimizer for Safe Text-to-Image Generation","summary":"  Text-to-Image (T2I) models have shown great performance in generating images\nbased on textual prompts. However, these models are vulnerable to unsafe input\nto generate unsafe content like sexual, harassment and illegal-activity images.\nExisting studies based on image checker, model fine-tuning and embedding\nblocking are impractical in real-world applications. Hence, we propose the\nfirst universal prompt optimizer for safe T2I (POSI) generation in black-box\nscenario. We first construct a dataset consisting of toxic-clean prompt pairs\nby GPT-3.5 Turbo. To guide the optimizer to have the ability of converting\ntoxic prompt to clean prompt while preserving semantic information, we design a\nnovel reward function measuring toxicity and text alignment of generated images\nand train the optimizer through Proximal Policy Optimization. Experiments show\nthat our approach can effectively reduce the likelihood of various T2I models\nin generating inappropriate images, with no significant impact on text\nalignment. It is also flexible to be combined with methods to achieve better\nperformance. Our code is available at https://github.com/wu-zongyu/POSI.\n","authors":["Zongyu Wu","Hongcheng Gao","Yueze Wang","Xiang Zhang","Suhang Wang"],"pdf_url":"https://arxiv.org/pdf/2402.10882v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08941v1","updated":"2024-12-12T05:08:05Z","published":"2024-12-12T05:08:05Z","title":"Optimized Gradient Clipping for Noisy Label Learning","summary":"  Previous research has shown that constraining the gradient of loss function\nwith respect to model-predicted probabilities can enhance the model robustness\nagainst noisy labels. These methods typically specify a fixed optimal threshold\nfor gradient clipping through validation data to obtain the desired robustness\nagainst noise. However, this common practice overlooks the dynamic distribution\nof gradients from both clean and noisy-labeled samples at different stages of\ntraining, significantly limiting the model capability to adapt to the variable\nnature of gradients throughout the training process. To address this issue, we\npropose a simple yet effective approach called Optimized Gradient Clipping\n(OGC), which dynamically adjusts the clipping threshold based on the ratio of\nnoise gradients to clean gradients after clipping, estimated by modeling the\ndistributions of clean and noisy samples. This approach allows us to modify the\nclipping threshold at each training step, effectively controlling the influence\nof noise gradients. Additionally, we provide statistical analysis to certify\nthe noise-tolerance ability of OGC. Our extensive experiments across various\ntypes of label noise, including symmetric, asymmetric, instance-dependent, and\nreal-world noise, demonstrate the effectiveness of our approach. The code and a\ntechnical appendix for better digital viewing are included as supplementary\nmaterials and scheduled to be open-sourced upon publication.\n","authors":["Xichen Ye","Yifan Wu","Weizhong Zhang","Xiaoqiang Li","Yifan Chen","Cheng Jin"],"pdf_url":"https://arxiv.org/pdf/2412.08941v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.08603v2","updated":"2024-12-12T05:04:09Z","published":"2024-12-11T18:26:45Z","title":"Design2GarmentCode: Turning Design Concepts to Tangible Garments Through\n  Program Synthesis","summary":"  Sewing patterns, the essential blueprints for fabric cutting and tailoring,\nact as a crucial bridge between design concepts and producible garments.\nHowever, existing uni-modal sewing pattern generation models struggle to\neffectively encode complex design concepts with a multi-modal nature and\ncorrelate them with vectorized sewing patterns that possess precise geometric\nstructures and intricate sewing relations. In this work, we propose a novel\nsewing pattern generation approach Design2GarmentCode based on Large Multimodal\nModels (LMMs), to generate parametric pattern-making programs from multi-modal\ndesign concepts. LMM offers an intuitive interface for interpreting diverse\ndesign inputs, while pattern-making programs could serve as well-structured and\nsemantically meaningful representations of sewing patterns, and act as a robust\nbridge connecting the cross-domain pattern-making knowledge embedded in LMMs\nwith vectorized sewing patterns. Experimental results demonstrate that our\nmethod can flexibly handle various complex design expressions such as images,\ntextual descriptions, designer sketches, or their combinations, and convert\nthem into size-precise sewing patterns with correct stitches. Compared to\nprevious methods, our approach significantly enhances training efficiency,\ngeneration quality, and authoring flexibility. Our code and data will be\npublicly available.\n","authors":["Feng Zhou","Ruiyang Liu","Chen Liu","Gaofeng He","Yong-Lu Li","Xiaogang Jin","Huamin Wang"],"pdf_url":"https://arxiv.org/pdf/2412.08603v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12004v2","updated":"2024-12-12T05:02:42Z","published":"2024-02-19T09:52:41Z","title":"Direct Consistency Optimization for Robust Customization of\n  Text-to-Image Diffusion Models","summary":"  Text-to-image (T2I) diffusion models, when fine-tuned on a few personal\nimages, can generate visuals with a high degree of consistency. However, such\nfine-tuned models are not robust; they often fail to compose with concepts of\npretrained model or other fine-tuned models. To address this, we propose a\nnovel fine-tuning objective, dubbed Direct Consistency Optimization, which\ncontrols the deviation between fine-tuning and pretrained models to retain the\npretrained knowledge during fine-tuning. Through extensive experiments on\nsubject and style customization, we demonstrate that our method positions\nitself on a superior Pareto frontier between subject (or style) consistency and\nimage-text alignment over all previous baselines; it not only outperforms\nregular fine-tuning objective in image-text alignment, but also shows higher\nfidelity to the reference images than the method that fine-tunes with\nadditional prior dataset. More importantly, the models fine-tuned with our\nmethod can be merged without interference, allowing us to generate custom\nsubjects in a custom style by composing separately customized subject and style\nmodels. Notably, we show that our approach achieves better prompt fidelity and\nsubject fidelity than those post-optimized for merging regular fine-tuned\nmodels.\n","authors":["Kyungmin Lee","Sangkyung Kwak","Kihyuk Sohn","Jinwoo Shin"],"pdf_url":"https://arxiv.org/pdf/2402.12004v2.pdf","comment":"NeurIPS 2024. Project page: https://dco-t2i.github.io/"},{"id":"http://arxiv.org/abs/2412.08940v1","updated":"2024-12-12T05:02:41Z","published":"2024-12-12T05:02:41Z","title":"Deep Clustering using Dirichlet Process Gaussian Mixture and Alpha\n  Jensen-Shannon Divergence Clustering Loss","summary":"  Deep clustering is an emerging topic in deep learning where traditional\nclustering is performed in deep learning feature space. However, clustering and\ndeep learning are often mutually exclusive. In the autoencoder based deep\nclustering, the challenge is how to jointly optimize both clustering and\ndimension reduction together, so that the weights in the hidden layers are not\nonly guided by reconstruction loss, but also by a loss function associated with\nclustering. The current state-of-the-art has two fundamental flaws. First, they\nrely on the mathematical convenience of Kullback-Leibler divergence for the\nclustering loss function but the former is asymmetric. Secondly, they assume\nthe prior knowledge on the number of clusters is always available for their\ndataset of interest. This paper tries to improve on these problems. In the\nfirst problem, we use a Jensen-Shannon divergence to overcome the asymmetric\nissue, specifically using a closed form variant. Next, we introduce an infinite\ncluster representation using Dirichlet process Gaussian mixture model for joint\nclustering and model selection in the latent space which we called deep model\nselection. The number of clusters in the latent space are not fixed but instead\nvary accordingly as they gradually approach the optimal number during training.\nThus, prior knowledge is not required. We evaluate our proposed deep model\nselection method with traditional model selection on large class number\ndatasets such as MIT67 and CIFAR100 and also compare with both traditional\nvariational Bayes model and deep clustering method with convincing results.\n","authors":["Kart-Leong Lim"],"pdf_url":"https://arxiv.org/pdf/2412.08940v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08939v1","updated":"2024-12-12T05:01:17Z","published":"2024-12-12T05:01:17Z","title":"Dynamic Contrastive Knowledge Distillation for Efficient Image\n  Restoration","summary":"  Knowledge distillation (KD) is a valuable yet challenging approach that\nenhances a compact student network by learning from a high-performance but\ncumbersome teacher model. However, previous KD methods for image restoration\noverlook the state of the student during the distillation, adopting a fixed\nsolution space that limits the capability of KD. Additionally, relying solely\non L1-type loss struggles to leverage the distribution information of images.\nIn this work, we propose a novel dynamic contrastive knowledge distillation\n(DCKD) framework for image restoration. Specifically, we introduce dynamic\ncontrastive regularization to perceive the student's learning state and\ndynamically adjust the distilled solution space using contrastive learning.\nAdditionally, we also propose a distribution mapping module to extract and\nalign the pixel-level category distribution of the teacher and student models.\nNote that the proposed DCKD is a structure-agnostic distillation framework,\nwhich can adapt to different backbones and can be combined with methods that\noptimize upper-bound constraints to further enhance model performance.\nExtensive experiments demonstrate that DCKD significantly outperforms the\nstate-of-the-art KD methods across various image restoration tasks and\nbackbones.\n","authors":["Yunshuai Zhou","Junbo Qiao","Jincheng Liao","Wei Li","Simiao Li","Jiao Xie","Yunhang Shen","Jie Hu","Shaohui Lin"],"pdf_url":"https://arxiv.org/pdf/2412.08939v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10476v3","updated":"2024-12-12T04:55:41Z","published":"2024-04-16T11:38:44Z","title":"Enhanced Facial Feature Extraction and Recignation Using Optimal Fully\n  Dispersed Haar-like Filters","summary":"  Haar-like filters are renowned for their simplicity, speed, and accuracy in\nvarious computer vision tasks. This paper proposes a novel algorithm to\nidentify optimal fully dispersed Haar-like filters for enhanced facial feature\nextraction and recognation. Unlike traditional Haar-like filters, these novel\nfilters allow pixels to move freely within images, enabling more effictive\ncapture of intricate local features...\n","authors":["Zeinab Sedaghatjoo","Hossein Hosseinzadeh","Ahmad shirzadi"],"pdf_url":"https://arxiv.org/pdf/2404.10476v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08933v1","updated":"2024-12-12T04:51:05Z","published":"2024-12-12T04:51:05Z","title":"Deep clustering using adversarial net based clustering loss","summary":"  Deep clustering is a recent deep learning technique which combines deep\nlearning with traditional unsupervised clustering. At the heart of deep\nclustering is a loss function which penalizes samples for being an outlier from\ntheir ground truth cluster centers in the latent space. The probabilistic\nvariant of deep clustering reformulates the loss using KL divergence. Often,\nthe main constraint of deep clustering is the necessity of a closed form loss\nfunction to make backpropagation tractable. Inspired by deep clustering and\nadversarial net, we reformulate deep clustering as an adversarial net over\ntraditional closed form KL divergence. Training deep clustering becomes a task\nof minimizing the encoder and maximizing the discriminator. At optimality, this\nmethod theoretically approaches the JS divergence between the distribution\nassumption of the encoder and the discriminator. We demonstrated the\nperformance of our proposed method on several well cited datasets such as SVHN,\nUSPS, MNIST and CIFAR10, achieving on-par or better performance with some of\nthe state-of-the-art deep clustering methods.\n","authors":["Kart-Leong Lim"],"pdf_url":"https://arxiv.org/pdf/2412.08933v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13575v3","updated":"2024-12-12T04:43:56Z","published":"2024-02-21T07:15:16Z","title":"Flexible Physical Camouflage Generation Based on a Differential Approach","summary":"  This study introduces a novel approach to neural rendering, specifically\ntailored for adversarial camouflage, within an extensive 3D rendering\nframework. Our method, named FPA, goes beyond traditional techniques by\nfaithfully simulating lighting conditions and material variations, ensuring a\nnuanced and realistic representation of textures on a 3D target. To achieve\nthis, we employ a generative approach that learns adversarial patterns from a\ndiffusion model. This involves incorporating a specially designed adversarial\nloss and covert constraint loss to guarantee the adversarial and covert nature\nof the camouflage in the physical world. Furthermore, we showcase the\neffectiveness of the proposed camouflage in sticker mode, demonstrating its\nability to cover the target without compromising adversarial information.\nThrough empirical and physical experiments, FPA exhibits strong performance in\nterms of attack success rate and transferability. Additionally, the designed\nsticker-mode camouflage, coupled with a concealment constraint, adapts to the\nenvironment, yielding diverse styles of texture. Our findings highlight the\nversatility and efficacy of the FPA approach in adversarial camouflage\napplications.\n","authors":["Yang Li","Wenyi Tan","Tingrui Wang","Xinkai Liang","Quan Pan"],"pdf_url":"https://arxiv.org/pdf/2402.13575v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08929v1","updated":"2024-12-12T04:34:28Z","published":"2024-12-12T04:34:28Z","title":"CAPrompt: Cyclic Prompt Aggregation for Pre-Trained Model Based Class\n  Incremental Learning","summary":"  Recently, prompt tuning methods for pre-trained models have demonstrated\npromising performance in Class Incremental Learning (CIL). These methods\ntypically involve learning task-specific prompts and predicting the task ID to\nselect the appropriate prompts for inference. However, inaccurate task ID\npredictions can cause severe inconsistencies between the prompts used during\ntraining and inference, leading to knowledge forgetting and performance\ndegradation. Additionally, existing prompt tuning methods rely solely on the\npre-trained model to predict task IDs, without fully leveraging the knowledge\nembedded in the learned prompt parameters, resulting in inferior prediction\nperformance. To address these issues, we propose a novel Cyclic Prompt\nAggregation (CAPrompt) method that eliminates the dependency on task ID\nprediction by cyclically aggregating the knowledge from different prompts.\nSpecifically, rather than predicting task IDs, we introduce an innovative\nprompt aggregation strategy during both training and inference to overcome\nprompt inconsistency by utilizing a weighted sum of different prompts. Thorough\ntheoretical analysis demonstrates that under concave conditions, the aggregated\nprompt achieves lower error compared to selecting a single task-specific\nprompt. Consequently, we incorporate a concave constraint and a linear\nconstraint to guide prompt learning, ensuring compliance with the concave\ncondition requirement. Furthermore, to fully exploit the prompts and achieve\nmore accurate prompt weights, we develop a cyclic weight prediction strategy.\nThis strategy begins with equal weights for each task and automatically adjusts\nthem to more appropriate values in a cyclical manner. Experiments on various\ndatasets demonstrate that our proposed CAPrompt outperforms state-of-the-art\nmethods by 2%-3%. Our code is available at\nhttps://github.com/zhoujiahuan1991/AAAI2025-CAPrompt.\n","authors":["Qiwei Li","Jiahuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.08929v1.pdf","comment":"in Proceedings of the 39th AAAI Conference on Artificial Intelligence\n  (AAAI-25)"},{"id":"http://arxiv.org/abs/2412.04715v3","updated":"2024-12-12T04:32:38Z","published":"2024-12-06T02:10:07Z","title":"Addressing Attribute Leakages in Diffusion-based Image Editing without\n  Training","summary":"  Diffusion models have become a cornerstone in image editing, offering\nflexibility with language prompts and source images. However, a key challenge\nis attribute leakage, where unintended modifications occur in non-target\nregions or within target regions due to attribute interference. Existing\nmethods often suffer from leakage due to naive text embeddings and inadequate\nhandling of End-of-Sequence (EOS) token embeddings. To address this, we propose\nALE-Edit (Attribute-leakage-free editing), a novel framework to minimize\nattribute leakage with three components: (1) Object-Restricted Embeddings (ORE)\nto localize object-specific attributes in text embeddings, (2) Region-Guided\nBlending for Cross-Attention Masking (RGB-CAM) to align attention with target\nregions, and (3) Background Blending (BB) to preserve non-edited regions.\nAdditionally, we introduce ALE-Bench, a benchmark for evaluating attribute\nleakage with new metrics for target-external and target-internal leakage.\nExperiments demonstrate that our framework significantly reduces attribute\nleakage while maintaining high editing quality, providing an efficient and\ntuning-free solution for multi-object image editing.\n","authors":["Sunung Mun","Jinhwan Nam","Sunghyun Cho","Jungseul Ok"],"pdf_url":"https://arxiv.org/pdf/2412.04715v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08922v1","updated":"2024-12-12T04:13:09Z","published":"2024-12-12T04:13:09Z","title":"A Flexible Plug-and-Play Module for Generating Variable-Length","summary":"  Deep supervised hashing has become a pivotal technique in large-scale image\nretrieval, offering significant benefits in terms of storage and search\nefficiency. However, existing deep supervised hashing models predominantly\nfocus on generating fixed-length hash codes. This approach fails to address the\ninherent trade-off between efficiency and effectiveness when using hash codes\nof varying lengths. To determine the optimal hash code length for a specific\ntask, multiple models must be trained for different lengths, leading to\nincreased training time and computational overhead. Furthermore, the current\nparadigm overlooks the potential relationships between hash codes of different\nlengths, limiting the overall effectiveness of the models. To address these\nchallenges, we propose the Nested Hash Layer (NHL), a plug-and-play module\ndesigned for existing deep supervised hashing models. The NHL framework\nintroduces a novel mechanism to simultaneously generate hash codes of varying\nlengths in a nested manner. To tackle the optimization conflicts arising from\nthe multiple learning objectives associated with different code lengths, we\nfurther propose an adaptive weights strategy that dynamically monitors and\nadjusts gradients during training. Additionally, recognizing that the\nstructural information in longer hash codes can provide valuable guidance for\nshorter hash codes, we develop a long-short cascade self-distillation method\nwithin the NHL to enhance the overall quality of the generated hash codes.\nExtensive experiments demonstrate that NHL not only accelerates the training\nprocess but also achieves superior retrieval performance across various deep\nhashing models. Our code is publicly available at\nhttps://github.com/hly1998/NHL.\n","authors":["Liyang He","Yuren Zhang","Rui Li","Zhenya Huang","Runze Wu","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2412.08922v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.06465v3","updated":"2024-12-12T03:56:01Z","published":"2024-12-09T13:10:28Z","title":"Agent Journey Beyond RGB: Unveiling Hybrid Semantic-Spatial\n  Environmental Representations for Vision-and-Language Navigation","summary":"  Navigating unseen environments based on natural language instructions remains\ndifficult for egocentric agents in Vision-and-Language Navigation (VLN). While\nrecent advancements have yielded promising outcomes, they primarily rely on RGB\nimages for environmental representation, often overlooking the underlying\nsemantic knowledge and spatial cues. Intuitively, humans inherently ground\ntextual semantics within the spatial layout during indoor navigation. Inspired\nby this, we propose a versatile Semantic Understanding and Spatial Awareness\n(SUSA) architecture to facilitate navigation. SUSA includes a Textual Semantic\nUnderstanding (TSU) module, which narrows the modality gap between instructions\nand environments by generating and associating the descriptions of\nenvironmental landmarks in the agent's immediate surroundings. Additionally, a\nDepth-based Spatial Perception (DSP) module incrementally constructs a depth\nexploration map, enabling a more nuanced comprehension of environmental\nlayouts. Experimental results demonstrate that SUSA hybrid semantic-spatial\nrepresentations effectively enhance navigation performance, setting new\nstate-of-the-art performance across three VLN benchmarks (REVERIE, R2R, and\nSOON). The source code will be publicly available.\n","authors":["Xuesong Zhang","Yunbo Xu","Jia Li","Zhenzhen Hu","Richnag Hong"],"pdf_url":"https://arxiv.org/pdf/2412.06465v3.pdf","comment":"A technical report consisting of 16 pages, 12 figures, 10 tables"},{"id":"http://arxiv.org/abs/2412.08913v1","updated":"2024-12-12T03:51:50Z","published":"2024-12-12T03:51:50Z","title":"Sensing for Space Safety and Sustainability: A Deep Learning Approach\n  with Vision Transformers","summary":"  The rapid increase of space assets represented by small satellites in low\nEarth orbit can enable ubiquitous digital services for everyone. However, due\nto the dynamic space environment, numerous space objects, complex atmospheric\nconditions, and unexpected events can easily introduce adverse conditions\naffecting space safety, operations, and sustainability of the outer space\nenvironment. This challenge calls for responsive, effective satellite object\ndetection (SOD) solutions that allow a small satellite to assess and respond to\ncollision risks, with the consideration of constrained resources on a small\nsatellite platform. This paper discusses the SOD tasks and onboard deep\nlearning (DL) approach to the tasks. Two new DL models are proposed, called\nGELAN-ViT and GELAN-RepViT, which incorporate vision transformer (ViT) into the\nGeneralized Efficient Layer Aggregation Network (GELAN) architecture and\naddress limitations by separating the convolutional neural network and ViT\npaths. These models outperform the state-of-the-art YOLOv9-t in terms of mean\naverage precision (mAP) and computational costs. On the SOD dataset, our\nproposed models can achieve around 95% mAP50 with giga-floating point\noperations (GFLOPs) reduced by over 5.0. On the VOC 2012 dataset, they can\nachieve $\\geq$ 60.7% mAP50 with GFLOPs reduced by over 5.2.\n","authors":["Wenxuan Zhang","Peng Hu"],"pdf_url":"https://arxiv.org/pdf/2412.08913v1.pdf","comment":"To be published in the 12th Annual IEEE International Conference on\n  Wireless for Space and Extreme Environments (WiSEE 2024)"},{"id":"http://arxiv.org/abs/2412.08912v1","updated":"2024-12-12T03:49:22Z","published":"2024-12-12T03:49:22Z","title":"Reversing the Damage: A QP-Aware Transformer-Diffusion Approach for 8K\n  Video Restoration under Codec Compression","summary":"  In this paper, we introduce DiQP; a novel Transformer-Diffusion model for\nrestoring 8K video quality degraded by codec compression. To the best of our\nknowledge, our model is the first to consider restoring the artifacts\nintroduced by various codecs (AV1, HEVC) by Denoising Diffusion without\nconsidering additional noise. This approach allows us to model the complex,\nnon-Gaussian nature of compression artifacts, effectively learning to reverse\nthe degradation. Our architecture combines the power of Transformers to capture\nlong-range dependencies with an enhanced windowed mechanism that preserves\nspatiotemporal context within groups of pixels across frames. To further\nenhance restoration, the model incorporates auxiliary \"Look Ahead\" and \"Look\nAround\" modules, providing both future and surrounding frame information to aid\nin reconstructing fine details and enhancing overall visual quality. Extensive\nexperiments on different datasets demonstrate that our model outperforms\nstate-of-the-art methods, particularly for high-resolution videos such as 4K\nand 8K, showcasing its effectiveness in restoring perceptually pleasing videos\nfrom highly compressed sources.\n","authors":["Ali Mollaahmadi Dehaghi","Reza Razavi","Mohammad Moshirpour"],"pdf_url":"https://arxiv.org/pdf/2412.08912v1.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2412.08907v1","updated":"2024-12-12T03:39:44Z","published":"2024-12-12T03:39:44Z","title":"GaGA: Towards Interactive Global Geolocation Assistant","summary":"  Global geolocation, which seeks to predict the geographical location of\nimages captured anywhere in the world, is one of the most challenging tasks in\nthe field of computer vision. In this paper, we introduce an innovative\ninteractive global geolocation assistant named GaGA, built upon the flourishing\nlarge vision-language models (LVLMs). GaGA uncovers geographical clues within\nimages and combines them with the extensive world knowledge embedded in LVLMs\nto determine the geolocations while also providing justifications and\nexplanations for the prediction results. We further designed a novel\ninteractive geolocation method that surpasses traditional static inference\napproaches. It allows users to intervene, correct, or provide clues for the\npredictions, making the model more flexible and practical. The development of\nGaGA relies on the newly proposed Multi-modal Global Geolocation (MG-Geo)\ndataset, a comprehensive collection of 5 million high-quality image-text pairs.\nGaGA achieves state-of-the-art performance on the GWS15k dataset, improving\naccuracy by 4.57% at the country level and 2.92% at the city level, setting a\nnew benchmark. These advancements represent a significant leap forward in\ndeveloping highly accurate, interactive geolocation systems with global\napplicability.\n","authors":["Zhiyang Dou","Zipeng Wang","Xumeng Han","Chenhui Qiang","Kuiran Wang","Guorong Li","Zhibei Huang","Zhenjun Han"],"pdf_url":"https://arxiv.org/pdf/2412.08907v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08896v1","updated":"2024-12-12T03:19:44Z","published":"2024-12-12T03:19:44Z","title":"LV-CadeNet: Long View Feature Convolution-Attention Fusion\n  Encoder-Decoder Network for Clinical MEG Spike Detection","summary":"  It is widely acknowledged that the epileptic foci can be pinpointed by source\nlocalizing interictal epileptic discharges (IEDs) via Magnetoencephalography\n(MEG). However, manual detection of IEDs, which appear as spikes in MEG data,\nis extremely labor intensive and requires considerable professional expertise,\nlimiting the broader adoption of MEG technology. Numerous studies have focused\non automatic detection of MEG spikes to overcome this challenge, but these\nefforts often validate their models on synthetic datasets with balanced\npositive and negative samples. In contrast, clinical MEG data is highly\nimbalanced, raising doubts on the real-world efficacy of these models. To\naddress this issue, we introduce LV-CadeNet, a Long View feature\nConvolution-Attention fusion Encoder-Decoder Network, designed for automatic\nMEG spike detection in real-world clinical scenarios. Beyond addressing the\ndisparity between training data distribution and clinical test data through\nsemi-supervised learning, our approach also mimics human specialists by\nconstructing long view morphological input data. Moreover, we propose an\nadvanced convolution-attention module to extract temporal and spatial features\nfrom the input data. LV-CadeNet significantly improves the accuracy of MEG\nspike detection, boosting it from 42.31\\% to 54.88\\% on a novel clinical\ndataset sourced from Sanbo Brain Hospital Capital Medical University. This\ndataset, characterized by a highly imbalanced distribution of positive and\nnegative samples, accurately represents real-world clinical scenarios.\n","authors":["Kuntao Xiao","Xiongfei Wang","Pengfei Teng","Yi Sun","Wanli Yang","Liang Zhang","Hanyang Dong","Guoming Luan","Shurong Sheng"],"pdf_url":"https://arxiv.org/pdf/2412.08896v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07689v2","updated":"2024-12-12T02:47:24Z","published":"2024-12-10T17:27:32Z","title":"DriveMM: All-in-One Large Multimodal Model for Autonomous Driving","summary":"  Large Multimodal Models (LMMs) have demonstrated exceptional comprehension\nand interpretation capabilities in Autonomous Driving (AD) by incorporating\nlarge language models. Despite the advancements, current data-driven AD\napproaches tend to concentrate on a single dataset and specific tasks,\nneglecting their overall capabilities and ability to generalize. To bridge\nthese gaps, we propose DriveMM, a general large multimodal model designed to\nprocess diverse data inputs, such as images and multi-view videos, while\nperforming a broad spectrum of AD tasks, including perception, prediction, and\nplanning. Initially, the model undergoes curriculum pre-training to process\nvaried visual signals and perform basic visual comprehension and perception\ntasks. Subsequently, we augment and standardize various AD-related datasets to\nfine-tune the model, resulting in an all-in-one LMM for autonomous driving. To\nassess the general capabilities and generalization ability, we conduct\nevaluations on six public benchmarks and undertake zero-shot transfer on an\nunseen dataset, where DriveMM achieves state-of-the-art performance across all\ntasks. We hope DriveMM as a promising solution for future end-to-end autonomous\ndriving applications in the real world. Project page with code:\nhttps://github.com/zhijian11/DriveMM.\n","authors":["Zhijian Huang","Chengjian Feng","Feng Yan","Baihui Xiao","Zequn Jie","Yujie Zhong","Xiaodan Liang","Lin Ma"],"pdf_url":"https://arxiv.org/pdf/2412.07689v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08879v1","updated":"2024-12-12T02:27:46Z","published":"2024-12-12T02:27:46Z","title":"Video Repurposing from User Generated Content: A Large-scale Dataset and\n  Benchmark","summary":"  The demand for producing short-form videos for sharing on social media\nplatforms has experienced significant growth in recent times. Despite notable\nadvancements in the fields of video summarization and highlight detection,\nwhich can create partially usable short films from raw videos, these approaches\nare often domain-specific and require an in-depth understanding of real-world\nvideo content. To tackle this predicament, we propose Repurpose-10K, an\nextensive dataset comprising over 10,000 videos with more than 120,000\nannotated clips aimed at resolving the video long-to-short task. Recognizing\nthe inherent constraints posed by untrained human annotators, which can result\nin inaccurate annotations for repurposed videos, we propose a two-stage\nsolution to obtain annotations from real-world user-generated content.\nFurthermore, we offer a baseline model to address this challenging task by\nintegrating audio, visual, and caption aspects through a cross-modal fusion and\nalignment framework. We aspire for our work to ignite groundbreaking research\nin the lesser-explored realms of video repurposing. The code and data will be\navailable at https://github.com/yongliang-wu/Repurpose.\n","authors":["Yongliang Wu","Wenbo Zhu","Jiawang Cao","Yi Lu","Bozheng Li","Weiheng Chi","Zihan Qiu","Lirian Su","Haolin Zheng","Jay Wu","Xu Yang"],"pdf_url":"https://arxiv.org/pdf/2412.08879v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.08871v1","updated":"2024-12-12T02:07:17Z","published":"2024-12-12T02:07:17Z","title":"Inference-Time Diffusion Model Distillation","summary":"  Diffusion distillation models effectively accelerate reverse sampling by\ncompressing the process into fewer steps. However, these models still exhibit a\nperformance gap compared to their pre-trained diffusion model counterparts,\nexacerbated by distribution shifts and accumulated errors during multi-step\nsampling. To address this, we introduce Distillation++, a novel inference-time\ndistillation framework that reduces this gap by incorporating teacher-guided\nrefinement during sampling. Inspired by recent advances in conditional\nsampling, our approach recasts student model sampling as a proximal\noptimization problem with a score distillation sampling loss (SDS). To this\nend, we integrate distillation optimization during reverse sampling, which can\nbe viewed as teacher guidance that drives student sampling trajectory towards\nthe clean manifold using pre-trained diffusion models. Thus, Distillation++\nimproves the denoising process in real-time without additional source data or\nfine-tuning. Distillation++ demonstrates substantial improvements over\nstate-of-the-art distillation baselines, particularly in early sampling stages,\npositioning itself as a robust guided sampling process crafted for diffusion\ndistillation models. Code:\nhttps://github.com/geonyeong-park/inference_distillation.\n","authors":["Geon Yeong Park","Sang Wan Lee","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2412.08871v1.pdf","comment":"Code: https://github.com/geonyeong-park/inference_distillation"},{"id":"http://arxiv.org/abs/2412.06324v2","updated":"2024-12-12T01:48:58Z","published":"2024-12-09T09:18:58Z","title":"World knowledge-enhanced Reasoning Using Instruction-guided Interactor\n  in Autonomous Driving","summary":"  The Multi-modal Large Language Models (MLLMs) with extensive world knowledge\nhave revitalized autonomous driving, particularly in reasoning tasks within\nperceivable regions. However, when faced with perception-limited areas (dynamic\nor static occlusion regions), MLLMs struggle to effectively integrate\nperception ability with world knowledge for reasoning. These perception-limited\nregions can conceal crucial safety information, especially for vulnerable road\nusers. In this paper, we propose a framework, which aims to improve autonomous\ndriving performance under perceptionlimited conditions by enhancing the\nintegration of perception capabilities and world knowledge. Specifically, we\npropose a plug-and-play instruction-guided interaction module that bridges\nmodality gaps and significantly reduces the input sequence length, allowing it\nto adapt effectively to multi-view video inputs. Furthermore, to better\nintegrate world knowledge with driving-related tasks, we have collected and\nrefined a large-scale multi-modal dataset that includes 2 million natural\nlanguage QA pairs, 1.7 million grounding task data. To evaluate the model's\nutilization of world knowledge, we introduce an object-level risk assessment\ndataset comprising 200K QA pairs, where the questions necessitate multi-step\nreasoning leveraging world knowledge for resolution. Extensive experiments\nvalidate the effectiveness of our proposed method.\n","authors":["Mingliang Zhai","Cheng Li","Zengyuan Guo","Ningrui Yang","Xiameng Qin","Yuwei Wu","Sanyuan Zhao","Junyu Han","Ji Tao","Yunde Jia"],"pdf_url":"https://arxiv.org/pdf/2412.06324v2.pdf","comment":"AAAI 2025. 14 pages. Supplementary Material"},{"id":"http://arxiv.org/abs/2306.16869v3","updated":"2024-12-12T01:37:29Z","published":"2023-06-29T11:38:22Z","title":"NeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural\n  Network Inference in Low-Voltage Regimes","summary":"  Deep neural networks (DNNs) have become ubiquitous in machine learning, but\ntheir energy consumption remains problematically high. An effective strategy\nfor reducing such consumption is supply-voltage reduction, but if done too\naggressively, it can lead to accuracy degradation. This is due to random\nbit-flips in static random access memory (SRAM), where model parameters are\nstored. To address this challenge, we have developed NeuralFuse, a novel add-on\nmodule that handles the energy-accuracy tradeoff in low-voltage regimes by\nlearning input transformations and using them to generate error-resistant data\nrepresentations, thereby protecting DNN accuracy in both nominal and\nlow-voltage scenarios. As well as being easy to implement, NeuralFuse can be\nreadily applied to DNNs with limited access, such cloud-based APIs that are\naccessed remotely or non-configurable hardware. Our experimental results\ndemonstrate that, at a 1% bit-error rate, NeuralFuse can reduce SRAM access\nenergy by up to 24% while recovering accuracy by up to 57%. To the best of our\nknowledge, this is the first approach to addressing low-voltage-induced bit\nerrors that requires no model retraining.\n","authors":["Hao-Lun Sun","Lei Hsiung","Nandhini Chandramoorthy","Pin-Yu Chen","Tsung-Yi Ho"],"pdf_url":"https://arxiv.org/pdf/2306.16869v3.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2412.08859v1","updated":"2024-12-12T01:36:18Z","published":"2024-12-12T01:36:18Z","title":"ViUniT: Visual Unit Tests for More Robust Visual Programming","summary":"  Programming based approaches to reasoning tasks have substantially expanded\nthe types of questions models can answer about visual scenes. Yet on benchmark\nvisual reasoning data, when models answer correctly, they produce incorrect\nprograms 33% of the time. These models are often right for the wrong reasons\nand risk unexpected failures on new data. Unit tests play a foundational role\nin ensuring code correctness and could be used to repair such failures. We\npropose Visual Unit Testing (ViUniT), a framework to improve the reliability of\nvisual programs by automatically generating unit tests. In our framework, a\nunit test is represented as a novel image and answer pair meant to verify the\nlogical correctness of a program produced for a given query. Our method\nleverages a language model to create unit tests in the form of image\ndescriptions and expected answers and image synthesis to produce corresponding\nimages. We conduct a comprehensive analysis of what constitutes an effective\nvisual unit test suite, exploring unit test generation, sampling strategies,\nimage generation methods, and varying the number of programs and unit tests.\nAdditionally, we introduce four applications of visual unit tests: best program\nselection, answer refusal, re-prompting, and unsupervised reward formulations\nfor reinforcement learning. Experiments with two models across three datasets\nin visual question answering and image-text matching demonstrate that ViUniT\nimproves model performance by 11.4%. Notably, it enables 7B open-source models\nto outperform gpt-4o-mini by an average of 7.7% and reduces the occurrence of\nprograms that are correct for the wrong reasons by 40%.\n","authors":["Artemis Panagopoulou","Honglu Zhou","Silvio Savarese","Caiming Xiong","Chris Callison-Burch","Mark Yatskar","Juan Carlos Niebles"],"pdf_url":"https://arxiv.org/pdf/2412.08859v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08849v1","updated":"2024-12-12T01:11:50Z","published":"2024-12-12T01:11:50Z","title":"Labits: Layered Bidirectional Time Surfaces Representation for Event\n  Camera-based Continuous Dense Trajectory Estimation","summary":"  Event cameras provide a compelling alternative to traditional frame-based\nsensors, capturing dynamic scenes with high temporal resolution and low\nlatency. Moving objects trigger events with precise timestamps along their\ntrajectory, enabling smooth continuous-time estimation. However, few works have\nattempted to optimize the information loss during event representation\nconstruction, imposing a ceiling on this task. Fully exploiting event cameras\nrequires representations that simultaneously preserve fine-grained temporal\ninformation, stable and characteristic 2D visual features, and temporally\nconsistent information density, an unmet challenge in existing representations.\nWe introduce Labits: Layered Bidirectional Time Surfaces, a simple yet elegant\nrepresentation designed to retain all these features. Additionally, we propose\na dedicated module for extracting active pixel local optical flow (APLOF),\nsignificantly boosting the performance. Our approach achieves an impressive 49%\nreduction in trajectory end-point error (TEPE) compared to the previous\nstate-of-the-art on the MultiFlow dataset. The code will be released upon\nacceptance.\n","authors":["Zhongyang Zhang","Jiacheng Qiu","Shuyang Cui","Yijun Luo","Tauhidur Rahman"],"pdf_url":"https://arxiv.org/pdf/2412.08849v1.pdf","comment":"24 pages, 12 figures, 9 tables"},{"id":"http://arxiv.org/abs/2412.09765v1","updated":"2024-12-12T23:57:01Z","published":"2024-12-12T23:57:01Z","title":"L-WISE: Boosting Human Image Category Learning Through Model-Based Image\n  Selection And Enhancement","summary":"  The currently leading artificial neural network (ANN) models of the visual\nventral stream -- which are derived from a combination of performance\noptimization and robustification methods -- have demonstrated a remarkable\ndegree of behavioral alignment with humans on visual categorization tasks.\nExtending upon previous work, we show that not only can these models guide\nimage perturbations that change the induced human category percepts, but they\nalso can enhance human ability to accurately report the original ground truth.\nFurthermore, we find that the same models can also be used out-of-the-box to\npredict the proportion of correct human responses to individual images,\nproviding a simple, human-aligned estimator of the relative difficulty of each\nimage. Motivated by these observations, we propose to augment visual learning\nin humans in a way that improves human categorization accuracy at test time.\nOur learning augmentation approach consists of (i) selecting images based on\ntheir model-estimated recognition difficulty, and (ii) using image\nperturbations that aid recognition for novice learners. We find that combining\nthese model-based strategies gives rise to test-time categorization accuracy\ngains of 33-72% relative to control subjects without these interventions,\ndespite using the same number of training feedback trials. Surprisingly, beyond\nthe accuracy gain, the training time for the augmented learning group was also\nshorter by 20-23%. We demonstrate the efficacy of our approach in a\nfine-grained categorization task with natural images, as well as tasks in two\nclinically relevant image domains -- histology and dermoscopy -- where visual\nlearning is notoriously challenging. To the best of our knowledge, this is the\nfirst application of ANNs to increase visual learning performance in humans by\nenhancing category-specific features.\n","authors":["Morgan B. Talbot","Gabriel Kreiman","James J. DiCarlo","Guy Gaziv"],"pdf_url":"https://arxiv.org/pdf/2412.09765v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10945v2","updated":"2024-12-12T23:51:26Z","published":"2024-08-20T15:34:27Z","title":"HiRED: Attention-Guided Token Dropping for Efficient Inference of\n  High-Resolution Vision-Language Models in Resource-Constrained Environments","summary":"  High-resolution Vision-Language Models (VLMs) have been widely used in\nmultimodal tasks to enhance accuracy by preserving detailed image information.\nHowever, these models often generate excessive visual tokens due to encoding\nmultiple partitions of the input image. Processing these excessive visual\ntokens is computationally challenging, especially in resource-constrained\nenvironments with commodity GPUs. To support high-resolution images while\nmeeting resource constraints, we propose High-Resolution Early Dropping\n(HiRED), a token-dropping scheme that operates within a fixed token budget\nbefore the Large Language Model (LLM) stage. HiRED can be integrated with\nexisting high-resolution VLMs in a plug-and-play manner, as it requires no\nadditional training while still maintaining superior accuracy. We strategically\nuse the vision encoder's attention in the initial layers to assess the visual\ncontent of each image partition and allocate the token budget accordingly.\nThen, using the attention in the final layer, we select the most important\nvisual tokens from each partition within the allocated budget, dropping the\nrest. Empirically, when applied to LLaVA-Next-7B on NVIDIA TESLA P40 GPU, HiRED\nwith a 20% token budget increases token generation throughput by 4.7, reduces\nfirst-token generation latency by 15 seconds, and saves 2.3 GB of GPU memory\nfor a single inference. The code is available at\nhttps://github.com/hasanar1f/HiRED.\n","authors":["Kazi Hasan Ibn Arif","JinYi Yoon","Dimitrios S. Nikolopoulos","Hans Vandierendonck","Deepu John","Bo Ji"],"pdf_url":"https://arxiv.org/pdf/2408.10945v2.pdf","comment":"Accepted in AAAI 2025"},{"id":"http://arxiv.org/abs/2408.10283v3","updated":"2024-12-12T23:33:03Z","published":"2024-08-19T00:31:05Z","title":"Perception-based multiplicative noise removal using SDEs","summary":"  Multiplicative noise, also known as speckle or pepper noise, commonly affects\nimages produced by synthetic aperture radar (SAR), lasers, or optical lenses.\nUnlike additive noise, which typically arises from thermal processes or\nexternal factors, multiplicative noise is inherent to the system, originating\nfrom the fluctuation in diffuse reflections. These fluctuations result in\nmultiple copies of the same signal with varying magnitudes being combined.\nConsequently, despeckling, or removing multiplicative noise, necessitates\ndifferent techniques compared to those used for additive noise removal.\n  In this paper, we propose a novel approach using Stochastic Differential\nEquations based diffusion models to address multiplicative noise. We\ndemonstrate that multiplicative noise can be effectively modeled as a Geometric\nBrownian Motion process in the logarithmic domain. Utilizing the Fokker-Planck\nequation, we derive the corresponding reverse process for image denoising. To\nvalidate our method, we conduct extensive experiments on two different\ndatasets, comparing our approach to both classical signal processing techniques\nand contemporary CNN-based noise removal models. Our results indicate that the\nproposed method significantly outperforms existing methods on perception-based\nmetrics such as FID and LPIPS, while maintaining competitive performance on\ntraditional metrics like PSNR and SSIM.\n","authors":["An Vuong","Thinh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2408.10283v3.pdf","comment":"15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.09754v1","updated":"2024-12-12T23:10:54Z","published":"2024-12-12T23:10:54Z","title":"ViCaS: A Dataset for Combining Holistic and Pixel-level Video\n  Understanding using Captions with Grounded Segmentation","summary":"  Recent advances in multimodal large language models (MLLMs) have expanded\nresearch in video understanding, primarily focusing on high-level tasks such as\nvideo captioning and question-answering. Meanwhile, a smaller body of work\naddresses dense, pixel-precise segmentation tasks, which typically involve\ncategory-guided or referral-based object segmentation. Although both research\ndirections are essential for developing models with human-level video\ncomprehension, they have largely evolved separately, with distinct benchmarks\nand architectures. This paper aims to unify these efforts by introducing ViCaS,\na new dataset containing thousands of challenging videos, each annotated with\ndetailed, human-written captions and temporally consistent, pixel-accurate\nmasks for multiple objects with phrase grounding. Our benchmark evaluates\nmodels on both holistic/high-level understanding and language-guided,\npixel-precise segmentation. We also present carefully validated evaluation\nmeasures and propose an effective model architecture that can tackle our\nbenchmark. Project page: https://ali2500.github.io/vicas-project/\n","authors":["Ali Athar","Xueqing Deng","Liang-Chieh Chen"],"pdf_url":"https://arxiv.org/pdf/2412.09754v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04094v2","updated":"2024-12-12T22:13:21Z","published":"2024-12-05T12:00:00Z","title":"Magnetic Resonance Imaging Feature-Based Subtyping and Model Ensemble\n  for Enhanced Brain Tumor Segmentation","summary":"  Accurate and automatic segmentation of brain tumors in multi-parametric\nmagnetic resonance imaging (mpMRI) is essential for quantitative measurements,\nwhich play an increasingly important role in clinical diagnosis and prognosis.\nThe International Brain Tumor Segmentation (BraTS) Challenge 2024 offers a\nunique benchmarking opportunity, including various types of brain tumors in\nboth adult and pediatric populations, such as pediatric brain tumors (PED),\nmeningiomas (MEN-RT) and brain metastases (MET), among others. Compared to\nprevious editions, BraTS 2024 has implemented changes to substantially increase\nclinical relevance, such as refined tumor regions for evaluation. We propose a\ndeep learning-based ensemble approach that integrates state-of-the-art\nsegmentation models. Additionally, we introduce innovative, adaptive pre- and\npost-processing techniques that employ MRI-based radiomic analyses to\ndifferentiate tumor subtypes. Given the heterogeneous nature of the tumors\npresent in the BraTS datasets, this approach enhances the precision and\ngeneralizability of segmentation models. On the final testing sets, our method\nachieved mean lesion-wise Dice similarity coefficients of 0.926, 0.801, and\n0.688 for the whole tumor in PED, MEN-RT, and MET, respectively. These results\ndemonstrate the effectiveness of our approach in improving segmentation\nperformance and generalizability for various brain tumor types.\n  The source code of our implementation is available at\nhttps://github.com/Precision-MedicalImaging-Group/HOPE-Segmenter-Kids.\nAdditionally, an open-source web-application is accessible at\nhttps://segmenter.hope4kids.io/ which uses the docker container\naparida12/brats-peds-2024:v20240913 .\n","authors":["Zhifan Jiang","Daniel Capellán-Martín","Abhijeet Parida","Austin Tapp","Xinyang Liu","María J. Ledesma-Carbayo","Syed Muhammad Anwar","Marius George Linguraru"],"pdf_url":"https://arxiv.org/pdf/2412.04094v2.pdf","comment":"11 pages, 4 figures, 3 tables. This paper was accepted at\n  MICCAI-BraTS 2024"},{"id":"http://arxiv.org/abs/2412.09741v1","updated":"2024-12-12T22:08:53Z","published":"2024-12-12T22:08:53Z","title":"On Round-Off Errors and Gaussian Blur in Superresolution and in Image\n  Registration","summary":"  Superresolution theory and techniques seek to recover signals from samples in\nthe presence of blur and noise. Discrete image registration can be an approach\nto fuse information from different sets of samples of the same signal.\nQuantization errors in the spatial domain are inherent to digital images. We\nconsider superresolution and discrete image registration for one-dimensional\nspatially-limited piecewise constant functions which are subject to blur which\nis Gaussian or a mixture of Gaussians as well as to round-off errors. We\ndescribe a signal-dependent measurement matrix which captures both types of\neffects. For this setting we show that the difficulties in determining the\ndiscontinuity points from two sets of samples even in the absence of other\ntypes of noise. If the samples are also subject to statistical noise, then it\nis necessary to align and segment the data sequences to make the most effective\ninferences about the amplitudes and discontinuity points. Under some conditions\non the blur, the noise, and the distance between discontinuity points, we prove\nthat we can correctly align and determine the first samples following each\ndiscontinuity point in two data sequences with an approach based on dynamic\nprogramming.\n","authors":["Serap A. Savari"],"pdf_url":"https://arxiv.org/pdf/2412.09741v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09739v1","updated":"2024-12-12T22:03:33Z","published":"2024-12-12T22:03:33Z","title":"Agtech Framework for Cranberry-Ripening Analysis Using Vision Foundation\n  Models","summary":"  Agricultural domains are being transformed by recent advances in AI and\ncomputer vision that support quantitative visual evaluation. Using aerial and\nground imaging over a time series, we develop a framework for characterizing\nthe ripening process of cranberry crops, a crucial component for precision\nagriculture tasks such as comparing crop breeds (high-throughput phenotyping)\nand detecting disease. Using drone imaging, we capture images from 20 waypoints\nacross multiple bogs, and using ground-based imaging (hand-held camera), we\nimage same bog patch using fixed fiducial markers. Both imaging methods are\nrepeated to gather a multi-week time series spanning the entire growing season.\nAerial imaging provides multiple samples to compute a distribution of albedo\nvalues. Ground imaging enables tracking of individual berries for a detailed\nview of berry appearance changes. Using vision transformers (ViT) for feature\ndetection after segmentation, we extract a high dimensional feature descriptor\nof berry appearance. Interpretability of appearance is critical for plant\nbiologists and cranberry growers to support crop breeding decisions (e.g.\\\ncomparison of berry varieties from breeding programs). For interpretability, we\ncreate a 2D manifold of cranberry appearance by using a UMAP dimensionality\nreduction on ViT features. This projection enables quantification of ripening\npaths and a useful metric of ripening rate. We demonstrate the comparison of\nfour cranberry varieties based on our ripening assessments. This work is the\nfirst of its kind and has future impact for cranberries and for other crops\nincluding wine grapes, olives, blueberries, and maize. Aerial and ground\ndatasets are made publicly available.\n","authors":["Faith Johnson","Ryan Meegan","Jack Lowry","Peter Oudemans","Kristin Dana"],"pdf_url":"https://arxiv.org/pdf/2412.09739v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2309.00028"},{"id":"http://arxiv.org/abs/2411.09893v2","updated":"2024-12-12T21:52:20Z","published":"2024-11-15T02:37:14Z","title":"Memory Proxy Maps for Visual Navigation","summary":"  Visual navigation takes inspiration from humans, who navigate in previously\nunseen environments using vision without detailed environment maps. Inspired by\nthis, we introduce a novel no-RL, no-graph, no-odometry approach to visual\nnavigation using feudal learning to build a three tiered agent. Key to our\napproach is a memory proxy map (MPM), an intermediate representation of the\nenvironment learned in a self-supervised manner by the high-level manager agent\nthat serves as a simplified memory, approximating what the agent has seen. We\ndemonstrate that recording observations in this learned latent space is an\neffective and efficient memory proxy that can remove the need for graphs and\nodometry in visual navigation tasks. For the mid-level manager agent, we\ndevelop a waypoint network (WayNet) that outputs intermediate subgoals, or\nwaypoints, imitating human waypoint selection during local navigation. For the\nlow-level worker agent, we learn a classifier over a discrete action space that\navoids local obstacles and moves the agent towards the WayNet waypoint. The\nresulting feudal navigation network offers a novel approach with no RL, no\ngraph, no odometry, and no metric map; all while achieving SOTA results on the\nimage goal navigation task.\n","authors":["Faith Johnson","Bryan Bo Cao","Ashwin Ashok","Shubham Jain","Kristin Dana"],"pdf_url":"https://arxiv.org/pdf/2411.09893v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2402.12498"},{"id":"http://arxiv.org/abs/2402.12498v3","updated":"2024-12-12T21:46:40Z","published":"2024-02-19T20:05:41Z","title":"Feudal Networks for Visual Navigation","summary":"  Visual navigation follows the intuition that humans can navigate without\ndetailed maps. A common approach is interactive exploration while building a\ntopological graph with images at nodes that can be used for planning. Recent\nvariations learn from passive videos and can navigate using complex social and\nsemantic cues. However, a significant number of training videos are needed,\nlarge graphs are utilized, and scenes are not unseen since odometry is\nutilized. We introduce a new approach to visual navigation using feudal\nlearning, which employs a hierarchical structure consisting of a worker agent,\na mid-level manager, and a high-level manager. Key to the feudal learning\nparadigm, agents at each level see a different aspect of the task and operate\nat different spatial and temporal scales. Two unique modules are developed in\nthis framework. For the high-level manager, we learn a memory proxy map in a\nself supervised manner to record prior observations in a learned latent space\nand avoid the use of graphs and odometry. For the mid-level manager, we develop\na waypoint network that outputs intermediate subgoals imitating human waypoint\nselection during local navigation. This waypoint network is pre-trained using a\nnew, small set of teleoperation videos that we make publicly available, with\ntraining environments different from testing environments. The resulting feudal\nnavigation network achieves near SOTA performance, while providing a novel\nno-RL, no-graph, no-odometry, no-metric map approach to the image goal\nnavigation task.\n","authors":["Faith Johnson","Bryan Bo Cao","Ashwin Ashok","Shubham Jain","Kristin Dana"],"pdf_url":"https://arxiv.org/pdf/2402.12498v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09731v1","updated":"2024-12-12T21:44:08Z","published":"2024-12-12T21:44:08Z","title":"Double-Exponential Increases in Inference Energy: The Cost of the Race\n  for Accuracy","summary":"  Deep learning models in computer vision have achieved significant success but\npose increasing concerns about energy consumption and sustainability. Despite\nthese concerns, there is a lack of comprehensive understanding of their energy\nefficiency during inference. In this study, we conduct a comprehensive analysis\nof the inference energy consumption of 1,200 ImageNet classification models -\nthe largest evaluation of its kind to date. Our findings reveal a steep\ndiminishing return in accuracy gains relative to the increase in energy usage,\nhighlighting sustainability concerns in the pursuit of marginal improvements.\nWe identify key factors contributing to energy consumption and demonstrate\nmethods to improve energy efficiency. To promote more sustainable AI practices,\nwe introduce an energy efficiency scoring system and develop an interactive web\napplication that allows users to compare models based on accuracy and energy\nconsumption. By providing extensive empirical data and practical tools, we aim\nto facilitate informed decision-making and encourage collaborative efforts in\ndeveloping energy-efficient AI technologies.\n","authors":["Zeyu Yang","Karel Adamek","Wesley Armour"],"pdf_url":"https://arxiv.org/pdf/2412.09731v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09726v1","updated":"2024-12-12T21:31:27Z","published":"2024-12-12T21:31:27Z","title":"The Unreasonable Effectiveness of Gaussian Score Approximation for\n  Diffusion Models and its Applications","summary":"  By learning the gradient of smoothed data distributions, diffusion models can\niteratively generate samples from complex distributions. The learned score\nfunction enables their generalization capabilities, but how the learned score\nrelates to the score of the underlying data manifold remains largely unclear.\nHere, we aim to elucidate this relationship by comparing learned neural scores\nto the scores of two kinds of analytically tractable distributions: Gaussians\nand Gaussian mixtures. The simplicity of the Gaussian model makes it\ntheoretically attractive, and we show that it admits a closed-form solution and\npredicts many qualitative aspects of sample generation dynamics. We claim that\nthe learned neural score is dominated by its linear (Gaussian) approximation\nfor moderate to high noise scales, and supply both theoretical and empirical\narguments to support this claim. Moreover, the Gaussian approximation\nempirically works for a larger range of noise scales than naive theory suggests\nit should, and is preferentially learned early in training. At smaller noise\nscales, we observe that learned scores are better described by a coarse-grained\n(Gaussian mixture) approximation of training data than by the score of the\ntraining distribution, a finding consistent with generalization. Our findings\nenable us to precisely predict the initial phase of trained models' sampling\ntrajectories through their Gaussian approximations. We show that this allows\nthe skipping of the first 15-30% of sampling steps while maintaining high\nsample quality (with a near state-of-the-art FID score of 1.93 on CIFAR-10\nunconditional generation). This forms the foundation of a novel hybrid sampling\nmethod, termed analytical teleportation, which can seamlessly integrate with\nand accelerate existing samplers, including DPM-Solver-v3 and UniPC. Our\nfindings suggest ways to improve the design and training of diffusion models.\n","authors":["Binxu Wang","John J. Vastola"],"pdf_url":"https://arxiv.org/pdf/2412.09726v1.pdf","comment":"69 pages, 34 figures. Published in TMLR. Previous shorter versions at\n  arxiv.org/abs/2303.02490 and arxiv.org/abs/2311.10892"},{"id":"http://arxiv.org/abs/2411.08027v2","updated":"2024-12-12T21:29:57Z","published":"2024-11-12T18:56:58Z","title":"LLMPhy: Complex Physical Reasoning Using Large Language Models and World\n  Models","summary":"  Physical reasoning is an important skill needed for robotic agents when\noperating in the real world. However, solving such reasoning problems often\ninvolves hypothesizing and reflecting over complex multi-body interactions\nunder the effect of a multitude of physical forces and thus learning all such\ninteractions poses a significant hurdle for state-of-the-art machine learning\nframeworks, including large language models (LLMs). To study this problem, we\npropose a new physical reasoning task and a dataset, dubbed TraySim. Our task\ninvolves predicting the dynamics of several objects on a tray that is given an\nexternal impact -- the domino effect of the ensued object interactions and\ntheir dynamics thus offering a challenging yet controlled setup, with the goal\nof reasoning being to infer the stability of the objects after the impact. To\nsolve this complex physical reasoning task, we present LLMPhy, a zero-shot\nblack-box optimization framework that leverages the physics knowledge and\nprogram synthesis abilities of LLMs, and synergizes these abilities with the\nworld models built into modern physics engines. Specifically, LLMPhy uses an\nLLM to generate code to iteratively estimate the physical hyperparameters of\nthe system (friction, damping, layout, etc.) via an implicit\nanalysis-by-synthesis approach using a (non-differentiable) simulator in the\nloop and uses the inferred parameters to imagine the dynamics of the scene\ntowards solving the reasoning task. To show the effectiveness of LLMPhy, we\npresent experiments on our TraySim dataset to predict the steady-state poses of\nthe objects. Our results show that the combination of the LLM and the physics\nengine leads to state-of-the-art zero-shot physical reasoning performance,\nwhile demonstrating superior convergence against standard black-box\noptimization methods and better estimation of the physical parameters.\n","authors":["Anoop Cherian","Radu Corcodel","Siddarth Jain","Diego Romeres"],"pdf_url":"https://arxiv.org/pdf/2411.08027v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09723v1","updated":"2024-12-12T21:04:53Z","published":"2024-12-12T21:04:53Z","title":"MAC-Ego3D: Multi-Agent Gaussian Consensus for Real-Time Collaborative\n  Ego-Motion and Photorealistic 3D Reconstruction","summary":"  Real-time multi-agent collaboration for ego-motion estimation and\nhigh-fidelity 3D reconstruction is vital for scalable spatial intelligence.\nHowever, traditional methods produce sparse, low-detail maps, while recent\ndense mapping approaches struggle with high latency. To overcome these\nchallenges, we present MAC-Ego3D, a novel framework for real-time collaborative\nphotorealistic 3D reconstruction via Multi-Agent Gaussian Consensus. MAC-Ego3D\nenables agents to independently construct, align, and iteratively refine local\nmaps using a unified Gaussian splat representation. Through Intra-Agent\nGaussian Consensus, it enforces spatial coherence among neighboring Gaussian\nsplats within an agent. For global alignment, parallelized Inter-Agent Gaussian\nConsensus, which asynchronously aligns and optimizes local maps by regularizing\nmulti-agent Gaussian splats, seamlessly integrates them into a high-fidelity 3D\nmodel. Leveraging Gaussian primitives, MAC-Ego3D supports efficient RGB-D\nrendering, enabling rapid inter-agent Gaussian association and alignment.\nMAC-Ego3D bridges local precision and global coherence, delivering higher\nefficiency, largely reducing localization error, and improving mapping\nfidelity. It establishes a new SOTA on synthetic and real-world benchmarks,\nachieving a 15x increase in inference speed, order-of-magnitude reductions in\nego-motion estimation error for partial cases, and RGB PSNR gains of 4 to 10\ndB. Our code will be made publicly available at\nhttps://github.com/Xiaohao-Xu/MAC-Ego3D .\n","authors":["Xiaohao Xu","Feng Xue","Shibo Zhao","Yike Pan","Sebastian Scherer","Xiaonan Huang"],"pdf_url":"https://arxiv.org/pdf/2412.09723v1.pdf","comment":"27 pages, 25 figures"},{"id":"http://arxiv.org/abs/2412.09718v1","updated":"2024-12-12T20:48:06Z","published":"2024-12-12T20:48:06Z","title":"BayesAdapter: enhanced uncertainty estimation in CLIP few-shot\n  adaptation","summary":"  The emergence of large pre-trained vision-language models (VLMs) represents a\nparadigm shift in machine learning, with unprecedented results in a broad span\nof visual recognition tasks. CLIP, one of the most popular VLMs, has exhibited\nremarkable zero-shot and transfer learning capabilities in classification. To\ntransfer CLIP to downstream tasks, adapters constitute a parameter-efficient\napproach that avoids backpropagation through the large model (unlike related\nprompt learning methods). However, CLIP adapters have been developed to target\ndiscriminative performance, and the quality of their uncertainty estimates has\nbeen overlooked. In this work we show that the discriminative performance of\nstate-of-the-art CLIP adapters does not always correlate with their uncertainty\nestimation capabilities, which are essential for a safe deployment in\nreal-world scenarios. We also demonstrate that one of such adapters is obtained\nthrough MAP inference from a more general probabilistic framework. Based on\nthis observation we introduce BayesAdapter, which leverages Bayesian inference\nto estimate a full probability distribution instead of a single point, better\ncapturing the variability inherent in the parameter space. In a comprehensive\nempirical evaluation we show that our approach obtains high quality uncertainty\nestimates in the predictions, standing out in calibration and selective\nclassification. Our code is publicly available at:\nhttps://github.com/pablomorales92/BayesAdapter.\n","authors":["Pablo Morales-Álvarez","Stergios Christodoulidis","Maria Vakalopoulou","Pablo Piantanida","Jose Dolz"],"pdf_url":"https://arxiv.org/pdf/2412.09718v1.pdf","comment":"30 pages, 5 figures, 23 tables"},{"id":"http://arxiv.org/abs/2412.09715v1","updated":"2024-12-12T20:37:52Z","published":"2024-12-12T20:37:52Z","title":"Human vs. AI: A Novel Benchmark and a Comparative Study on the Detection\n  of Generated Images and the Impact of Prompts","summary":"  With the advent of publicly available AI-based text-to-image systems, the\nprocess of creating photorealistic but fully synthetic images has been largely\ndemocratized. This can pose a threat to the public through a simplified spread\nof disinformation. Machine detectors and human media expertise can help to\ndifferentiate between AI-generated (fake) and real images and counteract this\ndanger. Although AI generation models are highly prompt-dependent, the impact\nof the prompt on the fake detection performance has rarely been investigated\nyet. This work therefore examines the influence of the prompt's level of detail\non the detectability of fake images, both with an AI detector and in a user\nstudy. For this purpose, we create a novel dataset, COCOXGEN, which consists of\nreal photos from the COCO dataset as well as images generated with SDXL and\nFooocus using prompts of two standardized lengths. Our user study with 200\nparticipants shows that images generated with longer, more detailed prompts are\ndetected significantly more easily than those generated with short prompts.\nSimilarly, an AI-based detection model achieves better performance on images\ngenerated with longer prompts. However, humans and AI models seem to pay\nattention to different details, as we show in a heat map analysis.\n","authors":["Philipp Moeßner","Heike Adel"],"pdf_url":"https://arxiv.org/pdf/2412.09715v1.pdf","comment":"Accepted at Workshop on Detecting AI Generated Content (at COLING\n  2025)"},{"id":"http://arxiv.org/abs/2412.09706v1","updated":"2024-12-12T20:01:24Z","published":"2024-12-12T20:01:24Z","title":"Diffusion-Enhanced Test-time Adaptation with Text and Image Augmentation","summary":"  Existing test-time prompt tuning (TPT) methods focus on single-modality data,\nprimarily enhancing images and using confidence ratings to filter out\ninaccurate images. However, while image generation models can produce visually\ndiverse images, single-modality data enhancement techniques still fail to\ncapture the comprehensive knowledge provided by different modalities.\nAdditionally, we note that the performance of TPT-based methods drops\nsignificantly when the number of augmented images is limited, which is not\nunusual given the computational expense of generative augmentation. To address\nthese issues, we introduce IT3A, a novel test-time adaptation method that\nutilizes a pre-trained generative model for multi-modal augmentation of each\ntest sample from unknown new domains. By combining augmented data from\npre-trained vision and language models, we enhance the ability of the model to\nadapt to unknown new test data. Additionally, to ensure that key semantics are\naccurately retained when generating various visual and text enhancements, we\nemploy cosine similarity filtering between the logits of the enhanced images\nand text with the original test data. This process allows us to filter out some\nspurious augmentation and inadequate combinations. To leverage the diverse\nenhancements provided by the generation model across different modals, we have\nreplaced prompt tuning with an adapter for greater flexibility in utilizing\ntext templates. Our experiments on the test datasets with distribution shifts\nand domain gaps show that in a zero-shot setting, IT3A outperforms\nstate-of-the-art test-time prompt tuning methods with a 5.50% increase in\naccuracy.\n","authors":["Chun-Mei Feng","Yuanyang He","Jian Zou","Salman Khan","Huan Xiong","Zhen Li","Wangmeng Zuo","Rick Siow Mong Goh","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2412.09706v1.pdf","comment":"Accepted by International Journal of Computer Vision"},{"id":"http://arxiv.org/abs/2412.09696v1","updated":"2024-12-12T19:23:50Z","published":"2024-12-12T19:23:50Z","title":"Soybean Maturity Prediction using 2D Contour Plots from Drone based Time\n  Series Imagery","summary":"  Plant breeding programs require assessments of days to maturity for accurate\nselection and placement of entries in appropriate tests. In the early stages of\nthe breeding pipeline, soybean breeding programs assign relative maturity\nratings to experimental varieties that indicate their suitable maturity zones.\nTraditionally, the estimation of maturity value for breeding varieties has\ninvolved breeders manually inspecting fields and assessing maturity value\nvisually. This approach relies heavily on rater judgment, making it subjective\nand time-consuming. This study aimed to develop a machine-learning model for\nevaluating soybean maturity using UAV-based time-series imagery. Images were\ncaptured at three-day intervals, beginning as the earliest varieties started\nmaturing and continuing until the last varieties fully matured. The data\ncollected for this experiment consisted of 22,043 plots collected across three\nyears (2021 to 2023) and represent relative maturity groups 1.6 - 3.9. We\nutilized contour plot images extracted from the time-series UAV RGB imagery as\ninput for a neural network model. This contour plot approach encoded the\ntemporal and spatial variation within each plot into a single image. A deep\nlearning model was trained to utilize this contour plot to predict maturity\nratings. This model significantly improves accuracy and robustness, achieving\nup to 85% accuracy. We also evaluate the model's accuracy as we reduce the\nnumber of time points, quantifying the trade-off between temporal resolution\nand maturity prediction. The predictive model offers a scalable, objective, and\nefficient means of assessing crop maturity, enabling phenomics and ML\napproaches to reduce the reliance on manual inspection and subjective\nassessment. This approach enables the automatic prediction of relative maturity\nratings in a breeding program, saving time and resources.\n","authors":["Bitgoeul Kim","Samuel W. Blair","Talukder Z. Jubery","Soumik Sarkar","Arti Singh","Asheesh K. Singh","Baskar Ganapathysubramanian"],"pdf_url":"https://arxiv.org/pdf/2412.09696v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09694v1","updated":"2024-12-12T19:21:20Z","published":"2024-12-12T19:21:20Z","title":"Omni-ID: Holistic Identity Representation Designed for Generative Tasks","summary":"  We introduce Omni-ID, a novel facial representation designed specifically for\ngenerative tasks. Omni-ID encodes holistic information about an individual's\nappearance across diverse expressions and poses within a fixed-size\nrepresentation. It consolidates information from a varied number of\nunstructured input images into a structured representation, where each entry\nrepresents certain global or local identity features. Our approach uses a\nfew-to-many identity reconstruction training paradigm, where a limited set of\ninput images is used to reconstruct multiple target images of the same\nindividual in various poses and expressions. A multi-decoder framework is\nfurther employed to leverage the complementary strengths of diverse decoders\nduring training. Unlike conventional representations, such as CLIP and ArcFace,\nwhich are typically learned through discriminative or contrastive objectives,\nOmni-ID is optimized with a generative objective, resulting in a more\ncomprehensive and nuanced identity capture for generative tasks. Trained on our\nMFHQ dataset -- a multi-view facial image collection, Omni-ID demonstrates\nsubstantial improvements over conventional representations across various\ngenerative tasks.\n","authors":["Guocheng Qian","Kuan-Chieh Wang","Or Patashnik","Negin Heravi","Daniil Ostashev","Sergey Tulyakov","Daniel Cohen-Or","Kfir Aberman"],"pdf_url":"https://arxiv.org/pdf/2412.09694v1.pdf","comment":"Webpage: https://snap-research.github.io/Omni-ID"},{"id":"http://arxiv.org/abs/2412.09692v1","updated":"2024-12-12T19:14:52Z","published":"2024-12-12T19:14:52Z","title":"TOAP: Towards Better Robustness in Universal Transferable Anti-Facial\n  Retrieval","summary":"  Deep hash-based retrieval techniques are widely used in facial retrieval\nsystems to improve the efficiency of facial matching. However, it also brings\nthe risk of privacy leakage. Deep hash models are easily influenced by\nadversarial examples, which can be leveraged to prevent the malicious retrieval\nof private images. The existing adversarial example methods against deep hash\nmodels focus on universality and transferability, lacking the research on its\nrobustness in online social networks (OSNs), which leads to their failure in\nanti-retrieval after post-processing. Therefore, we provide the first in-depth\ndiscussion on robustness adversarial perturbation in universal transferable\nanti-facial retrieval and propose Three-in-One Adversarial Perturbation (TOAP).\nSpecifically, we firstly analyze the performance of deep hash models after\npost-processing and construct a local and global Compression Generator (CG) to\nsimulate complex post-processing scenarios. Then, we explore the variation\npatterns of the model's objective under image post-processing and propose\nrobust optimization objectives, cluster centers and data space centers,\noptimizing them using meta-learning. Finally, we iteratively optimize\nperturbation by alternately generating adversarial examples and fine-tuning the\nCG, balancing the performance of perturbation while enhancing CG's ability to\nmitigate them. Numerous experiments demonstrate that, in addition to its\nadvantages in universality and transferability, TOAP significantly outperforms\ncurrent state-of-the-art methods in multiple robustness metrics. It further\nimproves universality and transferability by 5% to 28%, and achieves up to\nabout 33% significant improvement in several simulated post-processing\nscenarios as well as mainstream OSNs, demonstrating that TOAP can effectively\nprotect private images from malicious retrieval in real-world scenarios.\n","authors":["Yunna Lv","Long Tang","Dengpan Ye","Caiyun Xie","Jiacheng Deng","Yiheng He"],"pdf_url":"https://arxiv.org/pdf/2412.09692v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09687v1","updated":"2024-12-12T19:03:53Z","published":"2024-12-12T19:03:53Z","title":"DQA: An Efficient Method for Deep Quantization of Deep Neural Network\n  Activations","summary":"  Quantization of Deep Neural Network (DNN) activations is a commonly used\ntechnique to reduce compute and memory demands during DNN inference, which can\nbe particularly beneficial on resource-constrained devices. To achieve high\naccuracy, existing methods for quantizing activations rely on complex\nmathematical computations or perform extensive searches for the best\nhyper-parameters. However, these expensive operations are impractical on\ndevices with limited computation capabilities, memory capacities, and energy\nbudgets. Furthermore, many existing methods do not focus on sub-6-bit (or deep)\nquantization.\n  To fill these gaps, in this paper we propose DQA (Deep Quantization of DNN\nActivations), a new method that focuses on sub-6-bit quantization of\nactivations and leverages simple shifting-based operations and Huffman coding\nto be efficient and achieve high accuracy. We evaluate DQA with 3, 4, and 5-bit\nquantization levels and three different DNN models for two different tasks,\nimage classification and image segmentation, on two different datasets. DQA\nshows significantly better accuracy (up to 29.28%) compared to the direct\nquantization method and the state-of-the-art NoisyQuant for sub-6-bit\nquantization.\n","authors":["Wenhao Hu","Paul Henderson","José Cano"],"pdf_url":"https://arxiv.org/pdf/2412.09687v1.pdf","comment":"Accepted to Second Workshop on Machine Learning with New Compute\n  Paradigms at NeurIPS 2024 (MLNCP 2024)"},{"id":"http://arxiv.org/abs/2412.09680v1","updated":"2024-12-12T19:00:21Z","published":"2024-12-12T19:00:21Z","title":"PBR-NeRF: Inverse Rendering with Physics-Based Neural Fields","summary":"  We tackle the ill-posed inverse rendering problem in 3D reconstruction with a\nNeural Radiance Field (NeRF) approach informed by Physics-Based Rendering (PBR)\ntheory, named PBR-NeRF. Our method addresses a key limitation in most NeRF and\n3D Gaussian Splatting approaches: they estimate view-dependent appearance\nwithout modeling scene materials and illumination. To address this limitation,\nwe present an inverse rendering (IR) model capable of jointly estimating scene\ngeometry, materials, and illumination. Our model builds upon recent NeRF-based\nIR approaches, but crucially introduces two novel physics-based priors that\nbetter constrain the IR estimation. Our priors are rigorously formulated as\nintuitive loss terms and achieve state-of-the-art material estimation without\ncompromising novel view synthesis quality. Our method is easily adaptable to\nother inverse rendering and 3D reconstruction frameworks that require material\nestimation. We demonstrate the importance of extending current neural rendering\napproaches to fully model scene properties beyond geometry and view-dependent\nappearance. Code is publicly available at https://github.com/s3anwu/pbrnerf\n","authors":["Sean Wu","Shamik Basu","Tim Broedermann","Luc Van Gool","Christos Sakaridis"],"pdf_url":"https://arxiv.org/pdf/2412.09680v1.pdf","comment":"16 pages, 7 figures. Code is publicly available at\n  https://github.com/s3anwu/pbrnerf"},{"id":"http://arxiv.org/abs/2412.09668v1","updated":"2024-12-12T18:53:49Z","published":"2024-12-12T18:53:49Z","title":"Vision-Language Models Represent Darker-Skinned Black Individuals as\n  More Homogeneous than Lighter-Skinned Black Individuals","summary":"  Vision-Language Models (VLMs) combine Large Language Model (LLM) capabilities\nwith image processing, enabling tasks like image captioning and text-to-image\ngeneration. Yet concerns persist about their potential to amplify human-like\nbiases, including skin tone bias. Skin tone bias, where darker-skinned\nindividuals face more negative stereotyping than lighter-skinned individuals,\nis well-documented in the social sciences but remains under-explored in\nArtificial Intelligence (AI), particularly in VLMs. While well-documented in\nthe social sciences, this bias remains under-explored in AI, particularly in\nVLMs. Using the GAN Face Database, we sampled computer-generated images of\nBlack American men and women, controlling for skin tone variations while\nkeeping other features constant. We then asked VLMs to write stories about\nthese faces and compared the homogeneity of the generated stories. Stories\ngenerated by VLMs about darker-skinned Black individuals were more homogeneous\nthan those about lighter-skinned individuals in three of four models, and Black\nwomen were consistently represented more homogeneously than Black men across\nall models. Interaction effects revealed a greater impact of skin tone on women\nin two VLMs, while the other two showed nonsignificant results, reflecting\nknown stereotyping patterns. These findings underscore the propagation of\nbiases from single-modality AI systems to multimodal models and highlight the\nneed for further research to address intersectional biases in AI.\n","authors":["Messi H. J. Lee","Soyeon Jeon"],"pdf_url":"https://arxiv.org/pdf/2412.09668v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09658v1","updated":"2024-12-12T08:23:07Z","published":"2024-12-12T08:23:07Z","title":"SEGT: A General Spatial Expansion Group Transformer for nuScenes\n  Lidar-based Object Detection Task","summary":"  In the technical report, we present a novel transformer-based framework for\nnuScenes lidar-based object detection task, termed Spatial Expansion Group\nTransformer (SEGT). To efficiently handle the irregular and sparse nature of\npoint cloud, we propose migrating the voxels into distinct specialized ordered\nfields with the general spatial expansion strategies, and employ group\nattention mechanisms to extract the exclusive feature maps within each field.\nSubsequently, we integrate the feature representations across different ordered\nfields by alternately applying diverse expansion strategies, thereby enhancing\nthe model's ability to capture comprehensive spatial information. The method\nwas evaluated on the nuScenes lidar-based object detection test dataset,\nachieving an NDS score of 73.5 without Test-Time Augmentation (TTA) and 74.2\nwith TTA, demonstrating the effectiveness of the proposed method.\n","authors":["Cheng Mei","Hao He","Yahui Liu","Zhenhua Guo"],"pdf_url":"https://arxiv.org/pdf/2412.09658v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09656v1","updated":"2024-12-12T02:09:04Z","published":"2024-12-12T02:09:04Z","title":"From Noise to Nuance: Advances in Deep Generative Image Models","summary":"  Deep learning-based image generation has undergone a paradigm shift since\n2021, marked by fundamental architectural breakthroughs and computational\ninnovations. Through reviewing architectural innovations and empirical results,\nthis paper analyzes the transition from traditional generative methods to\nadvanced architectures, with focus on compute-efficient diffusion models and\nvision transformer architectures. We examine how recent developments in Stable\nDiffusion, DALL-E, and consistency models have redefined the capabilities and\nperformance boundaries of image synthesis, while addressing persistent\nchallenges in efficiency and quality. Our analysis focuses on the evolution of\nlatent space representations, cross-attention mechanisms, and\nparameter-efficient training methodologies that enable accelerated inference\nunder resource constraints. While more efficient training methods enable faster\ninference, advanced control mechanisms like ControlNet and regional attention\nsystems have simultaneously improved generation precision and content\ncustomization. We investigate how enhanced multi-modal understanding and\nzero-shot generation capabilities are reshaping practical applications across\nindustries. Our analysis demonstrates that despite remarkable advances in\ngeneration quality and computational efficiency, critical challenges remain in\ndeveloping resource-conscious architectures and interpretable generation\nsystems for industrial applications. The paper concludes by mapping promising\nresearch directions, including neural architecture optimization and explainable\ngeneration frameworks.\n","authors":["Benji Peng","Chia Xin Liang","Ziqian Bi","Ming Liu","Yichao Zhang","Tianyang Wang","Keyu Chen","Xinyuan Song","Pohsun Feng"],"pdf_url":"https://arxiv.org/pdf/2412.09656v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10471v1","updated":"2024-12-12T23:39:54Z","published":"2024-12-12T23:39:54Z","title":"VCA: Video Curious Agent for Long Video Understanding","summary":"  Long video understanding poses unique challenges due to their temporal\ncomplexity and low information density. Recent works address this task by\nsampling numerous frames or incorporating auxiliary tools using LLMs, both of\nwhich result in high computational costs. In this work, we introduce a\ncuriosity-driven video agent with self-exploration capability, dubbed as VCA.\nBuilt upon VLMs, VCA autonomously navigates video segments and efficiently\nbuilds a comprehensive understanding of complex video sequences. Instead of\ndirectly sampling frames, VCA employs a tree-search structure to explore video\nsegments and collect frames. Rather than relying on external feedback or\nreward, VCA leverages VLM's self-generated intrinsic reward to guide its\nexploration, enabling it to capture the most crucial information for reasoning.\nExperimental results on multiple long video benchmarks demonstrate our\napproach's superior effectiveness and efficiency.\n","authors":["Zeyuan Yang","Delin Chen","Xueyang Yu","Maohao Shen","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2412.10471v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10464v1","updated":"2024-12-12T15:52:40Z","published":"2024-12-12T15:52:40Z","title":"Automatic Detection, Positioning and Counting of Grape Bunches Using\n  Robots","summary":"  In order to promote agricultural automatic picking and yield estimation\ntechnology, this project designs a set of automatic detection, positioning and\ncounting algorithms for grape bunches, and applies it to agricultural robots.\nThe Yolov3 detection network is used to realize the accurate detection of grape\nbunches, and the local tracking algorithm is added to eliminate relocation.\nThen it obtains the accurate 3D spatial position of the central points of grape\nbunches using the depth distance and the spatial restriction method. Finally,\nthe counting of grape bunches is completed. It is verified using the\nagricultural robot in the simulated vineyard environment. The project code is\nreleased at:\nhttps://github.com/XuminGaoGithub/Grape_bunches_count_using_robots.\n","authors":["Xumin Gao"],"pdf_url":"https://arxiv.org/pdf/2412.10464v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10460v1","updated":"2024-12-12T11:30:41Z","published":"2024-12-12T11:30:41Z","title":"Enriching Multimodal Sentiment Analysis through Textual Emotional\n  Descriptions of Visual-Audio Content","summary":"  Multimodal Sentiment Analysis (MSA) stands as a critical research frontier,\nseeking to comprehensively unravel human emotions by amalgamating text, audio,\nand visual data. Yet, discerning subtle emotional nuances within audio and\nvideo expressions poses a formidable challenge, particularly when emotional\npolarities across various segments appear similar. In this paper, our objective\nis to spotlight emotion-relevant attributes of audio and visual modalities to\nfacilitate multimodal fusion in the context of nuanced emotional shifts in\nvisual-audio scenarios. To this end, we introduce DEVA, a progressive fusion\nframework founded on textual sentiment descriptions aimed at accentuating\nemotional features of visual-audio content. DEVA employs an Emotional\nDescription Generator (EDG) to transmute raw audio and visual data into\ntextualized sentiment descriptions, thereby amplifying their emotional\ncharacteristics. These descriptions are then integrated with the source data to\nyield richer, enhanced features. Furthermore, DEVA incorporates the Text-guided\nProgressive Fusion Module (TPF), leveraging varying levels of text as a core\nmodality guide. This module progressively fuses visual-audio minor modalities\nto alleviate disparities between text and visual-audio modalities. Experimental\nresults on widely used sentiment analysis benchmark datasets, including MOSI,\nMOSEI, and CH-SIMS, underscore significant enhancements compared to\nstate-of-the-art models. Moreover, fine-grained emotion experiments corroborate\nthe robust sensitivity of DEVA to subtle emotional variations.\n","authors":["Sheng Wu","Xiaobao Wang","Longbiao Wang","Dongxiao He","Jianwu Dang"],"pdf_url":"https://arxiv.org/pdf/2412.10460v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10458v1","updated":"2024-12-12T08:27:15Z","published":"2024-12-12T08:27:15Z","title":"Motion Generation Review: Exploring Deep Learning for Lifelike Animation\n  with Manifold","summary":"  Human motion generation involves creating natural sequences of human body\nposes, widely used in gaming, virtual reality, and human-computer interaction.\nIt aims to produce lifelike virtual characters with realistic movements,\nenhancing virtual agents and immersive experiences. While previous work has\nfocused on motion generation based on signals like movement, music, text, or\nscene background, the complexity of human motion and its relationships with\nthese signals often results in unsatisfactory outputs. Manifold learning offers\na solution by reducing data dimensionality and capturing subspaces of effective\nmotion. In this review, we present a comprehensive overview of manifold\napplications in human motion generation, one of the first in this domain. We\nexplore methods for extracting manifolds from unstructured data, their\napplication in motion generation, and discuss their advantages and future\ndirections. This survey aims to provide a broad perspective on the field and\nstimulate new approaches to ongoing challenges.\n","authors":["Jiayi Zhao","Dongdong Weng","Qiuxin Du","Zeyu Tian"],"pdf_url":"https://arxiv.org/pdf/2412.10458v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10457v1","updated":"2024-12-12T08:13:18Z","published":"2024-12-12T08:13:18Z","title":"Explaining Model Overfitting in CNNs via GMM Clustering","summary":"  Convolutional Neural Networks (CNNs) have demonstrated remarkable prowess in\nthe field of computer vision. However, their opaque decision-making processes\npose significant challenges for practical applications. In this study, we\nprovide quantitative metrics for assessing CNN filters by clustering the\nfeature maps corresponding to individual filters in the model via Gaussian\nMixture Model (GMM). By analyzing the clustering results, we screen out some\nanomaly filters associated with outlier samples. We further analyze the\nrelationship between the anomaly filters and model overfitting, proposing three\nhypotheses. This method is universally applicable across diverse CNN\narchitectures without modifications, as evidenced by its successful application\nto models like AlexNet and LeNet-5. We present three meticulously designed\nexperiments demonstrating our hypotheses from the perspectives of model\nbehavior, dataset characteristics, and filter impacts. Through this work, we\noffer a novel perspective for evaluating the CNN performance and gain new\ninsights into the operational behavior of model overfitting.\n","authors":["Hui Dou","Xinyu Mu","Mengjun Yi","Feng Han","Jian Zhao","Furao Shen"],"pdf_url":"https://arxiv.org/pdf/2412.10457v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10456v1","updated":"2024-12-12T08:03:54Z","published":"2024-12-12T08:03:54Z","title":"FovealNet: Advancing AI-Driven Gaze Tracking Solutions for Optimized\n  Foveated Rendering System Performance in Virtual Reality","summary":"  Leveraging real-time eye-tracking, foveated rendering optimizes hardware\nefficiency and enhances visual quality virtual reality (VR). This approach\nleverages eye-tracking techniques to determine where the user is looking,\nallowing the system to render high-resolution graphics only in the foveal\nregion-the small area of the retina where visual acuity is highest, while the\nperipheral view is rendered at lower resolution. However, modern deep\nlearning-based gaze-tracking solutions often exhibit a long-tail distribution\nof tracking errors, which can degrade user experience and reduce the benefits\nof foveated rendering by causing misalignment and decreased visual quality.\n  This paper introduces \\textit{FovealNet}, an advanced AI-driven gaze tracking\nframework designed to optimize system performance by strategically enhancing\ngaze tracking accuracy. To further reduce the implementation cost of the gaze\ntracking algorithm, FovealNet employs an event-based cropping method that\neliminates over $64.8\\%$ of irrelevant pixels from the input image.\nAdditionally, it incorporates a simple yet effective token-pruning strategy\nthat dynamically removes tokens on the fly without compromising tracking\naccuracy. Finally, to support different runtime rendering configurations, we\npropose a system performance-aware multi-resolution training strategy, allowing\nthe gaze tracking DNN to adapt and optimize overall system performance more\neffectively. Evaluation results demonstrate that FovealNet achieves at least\n$1.42\\times$ speed up compared to previous methods and 13\\% increase in\nperceptual quality for foveated output.\n","authors":["Wenxuan Liu","Monde Duinkharjav","Qi Sun","Sai Qian Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.10456v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10455v1","updated":"2024-12-12T07:34:09Z","published":"2024-12-12T07:34:09Z","title":"Geo-LLaVA: A Large Multi-Modal Model for Solving Geometry Math Problems\n  with Meta In-Context Learning","summary":"  Geometry mathematics problems pose significant challenges for large language\nmodels (LLMs) because they involve visual elements and spatial reasoning.\nCurrent methods primarily rely on symbolic character awareness to address these\nproblems. Considering geometry problem solving is a relatively nascent field\nwith limited suitable datasets and currently almost no work on solid geometry\nproblem solving, we collect a geometry question-answer dataset by sourcing\ngeometric data from Chinese high school education websites, referred to as\nGeoMath. It contains solid geometry questions and answers with accurate\nreasoning steps as compensation for existing plane geometry datasets.\nAdditionally, we propose a Large Multi-modal Model (LMM) framework named\nGeo-LLaVA, which incorporates retrieval augmentation with supervised\nfine-tuning (SFT) in the training stage, called meta-training, and employs\nin-context learning (ICL) during inference to improve performance. Our\nfine-tuned model with ICL attains the state-of-the-art performance of 65.25%\nand 42.36% on selected questions of the GeoQA dataset and GeoMath dataset\nrespectively with proper inference steps. Notably, our model initially endows\nthe ability to solve solid geometry problems and supports the generation of\nreasonable solid geometry picture descriptions and problem-solving steps. Our\nresearch sets the stage for further exploration of LLMs in multi-modal math\nproblem-solving, particularly in geometry math problems.\n","authors":["Shihao Xu","Yiyang Luo","Wei Shi"],"pdf_url":"https://arxiv.org/pdf/2412.10455v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10453v1","updated":"2024-12-12T07:06:22Z","published":"2024-12-12T07:06:22Z","title":"Analysis of Object Detection Models for Tiny Object in Satellite\n  Imagery: A Dataset-Centric Approach","summary":"  In recent years, significant advancements have been made in deep\nlearning-based object detection algorithms, revolutionizing basic computer\nvision tasks, notably in object detection, tracking, and segmentation. This\npaper delves into the intricate domain of Small-Object-Detection (SOD) within\nsatellite imagery, highlighting the unique challenges stemming from wide\nimaging ranges, object distribution, and their varying appearances in\nbird's-eye-view satellite images. Traditional object detection models face\ndifficulties in detecting small objects due to limited contextual information\nand class imbalances. To address this, our research presents a meticulously\ncurated dataset comprising 3000 images showcasing cars, ships, and airplanes in\nsatellite imagery. Our study aims to provide valuable insights into small\nobject detection in satellite imagery by empirically evaluating\nstate-of-the-art models. Furthermore, we tackle the challenges of satellite\nvideo-based object tracking, employing the Byte Track algorithm on the SAT-MTB\ndataset. Through rigorous experimentation, we aim to offer a comprehensive\nunderstanding of the efficacy of state-of-the-art models in\nSmall-Object-Detection for satellite applications. Our findings shed light on\nthe effectiveness of these models and pave the way for future advancements in\nsatellite imagery analysis.\n","authors":["Kailas PS","Selvakumaran R","Palani Murugan","Ramesh Kumar V","Malaya Kumar Biswal M"],"pdf_url":"https://arxiv.org/pdf/2412.10453v1.pdf","comment":"Conference Proceesings of AIAA SciTech Forum 2025 and Exposition"},{"id":"http://arxiv.org/abs/2412.10452v1","updated":"2024-12-12T06:40:14Z","published":"2024-12-12T06:40:14Z","title":"Structurally Consistent MRI Colorization using Cross-modal Fusion\n  Learning","summary":"  Medical image colorization can greatly enhance the interpretability of the\nunderlying imaging modality and provide insights into human anatomy. The\nobjective of medical image colorization is to transfer a diverse spectrum of\ncolors distributed across human anatomy from Cryosection data to source MRI\ndata while retaining the structures of the MRI. To achieve this, we propose a\nnovel architecture for structurally consistent color transfer to the source MRI\ndata. Our architecture fuses segmentation semantics of Cryosection images for\nstable contextual colorization of various organs in MRI images. For\ncolorization, we neither require precise registration between MRI and\nCryosection images, nor segmentation of MRI images. Additionally, our\narchitecture incorporates a feature compression-and-activation mechanism to\ncapture organ-level global information and suppress noise, enabling the\ndistinction of organ-specific data in MRI scans for more accurate and realistic\norgan-specific colorization. Our experiments demonstrate that our architecture\nsurpasses the existing methods and yields better quantitative and qualitative\nresults.\n","authors":["Mayuri Mathur","Anav Chaudhary","Saurabh Kumar Gupta","Ojaswa Sharma"],"pdf_url":"https://arxiv.org/pdf/2412.10452v1.pdf","comment":"9 pages, 6 figures, 2 Tables"}],"Robotics":[{"id":"http://arxiv.org/abs/2412.09624v1","updated":"2024-12-12T18:59:57Z","published":"2024-12-12T18:59:57Z","title":"GenEx: Generating an Explorable World","summary":"  Understanding, navigating, and exploring the 3D physical real world has long\nbeen a central challenge in the development of artificial intelligence. In this\nwork, we take a step toward this goal by introducing GenEx, a system capable of\nplanning complex embodied world exploration, guided by its generative\nimagination that forms priors (expectations) about the surrounding\nenvironments. GenEx generates an entire 3D-consistent imaginative environment\nfrom as little as a single RGB image, bringing it to life through panoramic\nvideo streams. Leveraging scalable 3D world data curated from Unreal Engine,\nour generative model is rounded in the physical world. It captures a continuous\n360-degree environment with little effort, offering a boundless landscape for\nAI agents to explore and interact with. GenEx achieves high-quality world\ngeneration, robust loop consistency over long trajectories, and demonstrates\nstrong 3D capabilities such as consistency and active 3D mapping. Powered by\ngenerative imagination of the world, GPT-assisted agents are equipped to\nperform complex embodied tasks, including both goal-agnostic exploration and\ngoal-driven navigation. These agents utilize predictive expectation regarding\nunseen parts of the physical world to refine their beliefs, simulate different\noutcomes based on potential decisions, and make more informed choices. In\nsummary, we demonstrate that GenEx provides a transformative platform for\nadvancing embodied AI in imaginative spaces and brings potential for extending\nthese capabilities to real-world exploration.\n","authors":["Taiming Lu","Tianmin Shu","Junfei Xiao","Luoxin Ye","Jiahao Wang","Cheng Peng","Chen Wei","Daniel Khashabi","Rama Chellappa","Alan Yuille","Jieneng Chen"],"pdf_url":"https://arxiv.org/pdf/2412.09624v1.pdf","comment":"Website: GenEx.world"},{"id":"http://arxiv.org/abs/2412.09620v1","updated":"2024-12-12T18:59:54Z","published":"2024-12-12T18:59:54Z","title":"Learning Camera Movement Control from Real-World Drone Videos","summary":"  This study seeks to automate camera movement control for filming existing\nsubjects into attractive videos, contrasting with the creation of non-existent\ncontent by directly generating the pixels. We select drone videos as our test\ncase due to their rich and challenging motion patterns, distinctive viewing\nangles, and precise controls. Existing AI videography methods struggle with\nlimited appearance diversity in simulation training, high costs of recording\nexpert operations, and difficulties in designing heuristic-based goals to cover\nall scenarios. To avoid these issues, we propose a scalable method that\ninvolves collecting real-world training data to improve diversity, extracting\ncamera trajectories automatically to minimize annotation costs, and training an\neffective architecture that does not rely on heuristics. Specifically, we\ncollect 99k high-quality trajectories by running 3D reconstruction on online\nvideos, connecting camera poses from consecutive frames to formulate 3D camera\npaths, and using Kalman filter to identify and remove low-quality data.\nMoreover, we introduce DVGFormer, an auto-regressive transformer that leverages\nthe camera path and images from all past frames to predict camera movement in\nthe next frame. We evaluate our system across 38 synthetic natural scenes and 7\nreal city 3D scans. We show that our system effectively learns to perform\nchallenging camera movements such as navigating through obstacles, maintaining\nlow altitude to increase perceived speed, and orbiting towers and buildings,\nwhich are very useful for recording high-quality videos. Data and code are\navailable at dvgformer.github.io.\n","authors":["Yunzhong Hou","Liang Zheng","Philip Torr"],"pdf_url":"https://arxiv.org/pdf/2412.09620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09617v1","updated":"2024-12-12T18:59:46Z","published":"2024-12-12T18:59:46Z","title":"NormalFlow: Fast, Robust, and Accurate Contact-based Object 6DoF Pose\n  Tracking with Vision-based Tactile Sensors","summary":"  Tactile sensing is crucial for robots aiming to achieve human-level\ndexterity. Among tactile-dependent skills, tactile-based object tracking serves\nas the cornerstone for many tasks, including manipulation, in-hand\nmanipulation, and 3D reconstruction. In this work, we introduce NormalFlow, a\nfast, robust, and real-time tactile-based 6DoF tracking algorithm. Leveraging\nthe precise surface normal estimation of vision-based tactile sensors,\nNormalFlow determines object movements by minimizing discrepancies between the\ntactile-derived surface normals. Our results show that NormalFlow consistently\noutperforms competitive baselines and can track low-texture objects like table\nsurfaces. For long-horizon tracking, we demonstrate when rolling the sensor\naround a bead for 360 degrees, NormalFlow maintains a rotational tracking error\nof 2.5 degrees. Additionally, we present state-of-the-art tactile-based 3D\nreconstruction results, showcasing the high accuracy of NormalFlow. We believe\nNormalFlow unlocks new possibilities for high-precision perception and\nmanipulation tasks that involve interacting with objects using hands. The video\ndemo, code, and dataset are available on our website:\nhttps://joehjhuang.github.io/normalflow.\n","authors":["Hung-Jui Huang","Michael Kaess","Wenzhen Yuan"],"pdf_url":"https://arxiv.org/pdf/2412.09617v1.pdf","comment":"8 pages, published in 2024 RA-L, website link:\n  https://joehjhuang.github.io/normalflow"},{"id":"http://arxiv.org/abs/2412.09602v1","updated":"2024-12-12T18:59:13Z","published":"2024-12-12T18:59:13Z","title":"Hidden Biases of End-to-End Driving Datasets","summary":"  End-to-end driving systems have made rapid progress, but have so far not been\napplied to the challenging new CARLA Leaderboard 2.0. Further, while there is a\nlarge body of literature on end-to-end architectures and training strategies,\nthe impact of the training dataset is often overlooked. In this work, we make a\nfirst attempt at end-to-end driving for Leaderboard 2.0. Instead of\ninvestigating architectures, we systematically analyze the training dataset,\nleading to new insights: (1) Expert style significantly affects downstream\npolicy performance. (2) In complex data sets, the frames should not be weighted\non the basis of simplistic criteria such as class frequencies. (3) Instead,\nestimating whether a frame changes the target labels compared to previous\nframes can reduce the size of the dataset without removing important\ninformation. By incorporating these findings, our model ranks first and second\nrespectively on the map and sensors tracks of the 2024 CARLA Challenge, and\nsets a new state-of-the-art on the Bench2Drive test routes. Finally, we uncover\na design flaw in the current evaluation metrics and propose a modification for\nfuture challenges. Our dataset, code, and pre-trained models are publicly\navailable at https://github.com/autonomousvision/carla_garage.\n","authors":["Julian Zimmerlin","Jens Beißwenger","Bernhard Jaeger","Andreas Geiger","Kashyap Chitta"],"pdf_url":"https://arxiv.org/pdf/2412.09602v1.pdf","comment":"Technical report for the CVPR 2024 Workshop on Foundation Models for\n  Autonomous Systems. Runner-up of the track 'CARLA Autonomous Driving\n  Challenge' in the 2024 Autonomous Grand Challenge\n  (https://opendrivelab.com/challenge2024/)"},{"id":"http://arxiv.org/abs/2412.09584v1","updated":"2024-12-12T18:55:14Z","published":"2024-12-12T18:55:14Z","title":"BaB-ND: Long-Horizon Motion Planning with Branch-and-Bound and Neural\n  Dynamics","summary":"  Neural-network-based dynamics models learned from observational data have\nshown strong predictive capabilities for scene dynamics in robotic manipulation\ntasks. However, their inherent non-linearity presents significant challenges\nfor effective planning. Current planning methods, often dependent on extensive\nsampling or local gradient descent, struggle with long-horizon motion planning\ntasks involving complex contact events. In this paper, we present a\nGPU-accelerated branch-and-bound (BaB) framework for motion planning in\nmanipulation tasks that require trajectory optimization over neural dynamics\nmodels. Our approach employs a specialized branching heuristics to divide the\nsearch space into subdomains, and applies a modified bound propagation method,\ninspired by the state-of-the-art neural network verifier alpha-beta-CROWN, to\nefficiently estimate objective bounds within these subdomains. The branching\nprocess guides planning effectively, while the bounding process strategically\nreduces the search space. Our framework achieves superior planning performance,\ngenerating high-quality state-action trajectories and surpassing existing\nmethods in challenging, contact-rich manipulation tasks such as non-prehensile\nplanar pushing with obstacles, object sorting, and rope routing in both\nsimulated and real-world settings. Furthermore, our framework supports various\nneural network architectures, ranging from simple multilayer perceptrons to\nadvanced graph neural dynamics models, and scales efficiently with different\nmodel sizes.\n","authors":["Keyi Shen","Jiangwei Yu","Huan Zhang","Yunzhu Li"],"pdf_url":"https://arxiv.org/pdf/2412.09584v1.pdf","comment":"The first two authors contributed equally. Project Page:\n  https://robopil.github.io/bab-nd/"},{"id":"http://arxiv.org/abs/2407.16677v4","updated":"2024-12-12T18:40:16Z","published":"2024-07-23T17:44:54Z","title":"From Imitation to Refinement -- Residual RL for Precise Assembly","summary":"  Recent advances in Behavior Cloning (BC) have made it easy to teach robots\nnew tasks. However, we find that the ease of teaching comes at the cost of\nunreliable performance that saturates with increasing data for tasks requiring\nprecision. The performance saturation can be attributed to two critical\nfactors: (a) distribution shift resulting from the use of offline data and (b)\nthe lack of closed-loop corrective control caused by action chucking\n(predicting a set of future actions executed open-loop) critical for BC\nperformance. Our key insight is that by predicting action chunks, BC policies\nfunction more like trajectory \"planners\" than closed-loop controllers necessary\nfor reliable execution. To address these challenges, we devise a simple yet\neffective method, ResiP (Residual for Precise Manipulation), that overcomes the\nreliability problem while retaining BC's ease of teaching and long-horizon\ncapabilities. ResiP augments a frozen, chunked BC model with a fully\nclosed-loop residual policy trained with reinforcement learning (RL) that\naddresses distribution shifts and introduces closed-loop corrections over\nopen-loop execution of action chunks predicted by the BC trajectory planner.\nVideos, code, and data: https://residual-assembly.github.io.\n","authors":["Lars Ankile","Anthony Simeonov","Idan Shenfeld","Marcel Torne","Pulkit Agrawal"],"pdf_url":"https://arxiv.org/pdf/2407.16677v4.pdf","comment":"Project website: https://residual-assembly.github.io"},{"id":"http://arxiv.org/abs/2412.09520v1","updated":"2024-12-12T18:06:22Z","published":"2024-12-12T18:06:22Z","title":"GainAdaptor: Learning Quadrupedal Locomotion with Dual Actors for\n  Adaptable and Energy-Efficient Walking on Various Terrains","summary":"  Deep reinforcement learning (DRL) has emerged as an innovative solution for\ncontrolling legged robots in challenging environments using minimalist\narchitectures. Traditional control methods for legged robots, such as inverse\ndynamics, either directly manage joint torques or use proportional-derivative\n(PD) controllers to regulate joint positions at a higher level. In case of DRL,\ndirect torque control presents significant challenges, leading to a preference\nfor joint position control. However, this approach necessitates careful\nadjustment of joint PD gains, which can limit both adaptability and efficiency.\nIn this paper, we propose GainAdaptor, an adaptive gain control framework that\nautonomously tunes joint PD gains to enhance terrain adaptability and energy\nefficiency. The framework employs a dual-actor algorithm to dynamically adjust\nthe PD gains based on varying ground conditions. By utilizing a divided action\nspace, GainAdaptor efficiently learns stable and energy-efficient locomotion.\nWe validate the effectiveness of the proposed method through experiments\nconducted on a Unitree Go1 robot, demonstrating improved locomotion performance\nacross diverse terrains.\n","authors":["Mincheol Kim","Nahyun Kwon","Jung-Yup Kim"],"pdf_url":"https://arxiv.org/pdf/2412.09520v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.00272v2","updated":"2024-12-12T17:58:46Z","published":"2024-09-30T22:48:32Z","title":"Decentralized Input and State Estimation for Multi-agent System with\n  Dynamic Topology and Heterogeneous Sensor Network","summary":"  A crucial challenge in decentralized systems is state estimation in the\npresence of unknown inputs, particularly within heterogeneous sensor networks\nwith dynamic topologies. While numerous consensus algorithms have been\nintroduced, they often require extensive information exchange or multiple\ncommunication iterations to ensure estimation accuracy. This paper proposes an\nefficient algorithm that achieves an unbiased and optimal solution comparable\nto filters with full information about other agents. This is accomplished\nthrough the use of information filter decomposition and the fusion of inputs\nvia covariance intersection. Our method requires only a single communication\niteration for exchanging individual estimates between agents, instead of\nmultiple rounds of information exchange, thus preserving agents' privacy by\navoiding the sharing of explicit observations and system equations.\nFurthermore, to address the challenges posed by dynamic communication\ntopologies, we propose two practical strategies to handle issues arising from\nintermittent observations and incomplete state estimation, thereby enhancing\nthe robustness and accuracy of the estimation process. Experiments and ablation\nstudies conducted in both stationary and dynamic environments demonstrate the\nsuperiority of our algorithm over other baselines. Notably, it performs as well\nas, or even better than, algorithms that have a global view of all neighbors.\n","authors":["Zida Wu","Ankur Mehta"],"pdf_url":"https://arxiv.org/pdf/2410.00272v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09505v1","updated":"2024-12-12T17:54:55Z","published":"2024-12-12T17:54:55Z","title":"Integrating Vision Systems and STPA for Robust Landing and Take-Off in\n  VTOL Aircraft","summary":"  Vertical take-off and landing (VTOL) unmanned aerial vehicles (UAVs) are\nversatile platforms widely used in applications such as surveillance, search\nand rescue, and urban air mobility. Despite their potential, the critical\nphases of take-off and landing in uncertain and dynamic environments pose\nsignificant safety challenges due to environmental uncertainties, sensor noise,\nand system-level interactions. This paper presents an integrated approach\ncombining vision-based sensor fusion with System-Theoretic Process Analysis\n(STPA) to enhance the safety and robustness of VTOL UAV operations during\ntake-off and landing. By incorporating fiducial markers, such as AprilTags,\ninto the control architecture, and performing comprehensive hazard analysis, we\nidentify unsafe control actions and propose mitigation strategies. Key\ncontributions include developing the control structure with vision system\ncapable of identifying a fiducial marker, multirotor controller and\ncorresponding unsafe control actions and mitigation strategies. The proposed\nsolution is expected to improve the reliability and safety of VTOL UAV\noperations, paving the way for resilient autonomous systems.\n","authors":["Sandeep Banik","Jinrae Kim","Naira Hovakimyan","Luca Carlone","John P. Thomas","Nancy G. Leveson"],"pdf_url":"https://arxiv.org/pdf/2412.09505v1.pdf","comment":"12 pages, 5 figures and 5 tables. Submitted to SciTech 2025"},{"id":"http://arxiv.org/abs/2412.09496v1","updated":"2024-12-12T17:43:58Z","published":"2024-12-12T17:43:58Z","title":"iKap: Kinematics-aware Planning with Imperative Learning","summary":"  Trajectory planning in robotics aims to generate collision-free pose\nsequences that can be reliably executed. Recently, vision-to-planning systems\nhave garnered increasing attention for their efficiency and ability to\ninterpret and adapt to surrounding environments. However, traditional modular\nsystems suffer from increased latency and error propagation, while purely\ndata-driven approaches often overlook the robot's kinematic constraints. This\noversight leads to discrepancies between planned trajectories and those that\nare executable. To address these challenges, we propose iKap, a novel\nvision-to-planning system that integrates the robot's kinematic model directly\ninto the learning pipeline. iKap employs a self-supervised learning approach\nand incorporates the state transition model within a differentiable bi-level\noptimization framework. This integration ensures the network learns\ncollision-free waypoints while satisfying kinematic constraints, enabling\ngradient back-propagation for end-to-end training. Our experimental results\ndemonstrate that iKap achieves higher success rates and reduced latency\ncompared to the state-of-the-art methods. Besides the complete system, iKap\noffers a visual-to-planning network that seamlessly integrates kinematics into\nvarious controllers, providing a robust solution for robots navigating complex\nand dynamic environments.\n","authors":["Qihang Li","Zhuoqun Chen","Haoze Zheng","Haonan He","Shaoshu Su","Junyi Geng","Chen Wang"],"pdf_url":"https://arxiv.org/pdf/2412.09496v1.pdf","comment":"6 pages, 6 figures"},{"id":"http://arxiv.org/abs/2404.17070v2","updated":"2024-12-12T17:34:17Z","published":"2024-04-25T22:41:59Z","title":"Deep Reinforcement Learning for Bipedal Locomotion: A Brief Survey","summary":"  Bipedal robots are gaining global recognition due to their potential\napplications and advancements in artificial intelligence, particularly through\nDeep Reinforcement Learning (DRL). While DRL has significantly advanced bipedal\nlocomotion, the development of a unified framework capable of handling a wide\nrange of tasks remains an ongoing challenge. This survey systematically\ncategorises, compares, and analyses existing DRL frameworks for bipedal\nlocomotion, organising them into end-to-end and hierarchical control schemes.\nEnd-to-end frameworks are evaluated based on their learning approaches, while\nhierarchical frameworks are examined in terms of layered structures that\nintegrate learning-based or traditional model-based methods. We provide a\ndetailed evaluation of the composition, strengths, limitations, and\ncapabilities of each framework. Additionally, this survey identifies key\nresearch gaps and proposes future directions aimed at creating a more\nintegrated and efficient framework for bipedal locomotion, with wide-ranging\napplications in real-world environments.\n","authors":["Lingfan Bao","Joseph Humphreys","Tianhu Peng","Chengxu Zhou"],"pdf_url":"https://arxiv.org/pdf/2404.17070v2.pdf","comment":"17 pages, 8 figures"},{"id":"http://arxiv.org/abs/2412.09466v1","updated":"2024-12-12T17:15:22Z","published":"2024-12-12T17:15:22Z","title":"Distributional Reinforcement Learning based Integrated Decision Making\n  and Control for Autonomous Surface Vehicles","summary":"  With the growing demands for Autonomous Surface Vehicles (ASVs) in recent\nyears, the number of ASVs being deployed for various maritime missions is\nexpected to increase rapidly in the near future. However, it is still\nchallenging for ASVs to perform sensor-based autonomous navigation in\nobstacle-filled and congested waterways, where perception errors, closely\ngathered vehicles and limited maneuvering space near buoys may cause\ndifficulties in following the Convention on the International Regulations for\nPreventing Collisions at Sea (COLREGs). To address these issues, we propose a\nnovel Distributional Reinforcement Learning based navigation system that can\nwork with onboard LiDAR and odometry sensors to generate arbitrary thrust\ncommands in continuous action space. Comprehensive evaluations of the proposed\nsystem in high-fidelity Gazebo simulations show its ability to decide whether\nto follow COLREGs or take other beneficial actions based on the scenarios\nencountered, offering superior performance in navigation safety and efficiency\ncompared to systems using state-of-the-art Distributional RL,\nnon-Distributional RL and classical methods.\n","authors":["Xi Lin","Paul Szenher","Yewei Huang","Brendan Englot"],"pdf_url":"https://arxiv.org/pdf/2412.09466v1.pdf","comment":"IEEE Robotics and Automation Letters (RA-L)"},{"id":"http://arxiv.org/abs/2403.13208v2","updated":"2024-12-12T16:56:05Z","published":"2024-03-19T23:57:55Z","title":"CaDRE: Controllable and Diverse Generation of Safety-Critical Driving\n  Scenarios using Real-World Trajectories","summary":"  Simulation is an indispensable tool in the development and testing of\nautonomous vehicles (AVs), offering an efficient and safe alternative to road\ntesting. An outstanding challenge with simulation-based testing is the\ngeneration of safety-critical scenarios, which are essential to ensure that AVs\ncan handle rare but potentially fatal situations. This paper addresses this\nchallenge by introducing a novel framework, CaDRE, to generate realistic,\ndiverse, and controllable safety-critical scenarios. Our approach optimizes for\nboth the quality and diversity of scenarios by employing a unique formulation\nand algorithm that integrates real-world scenarios, domain knowledge, and\nblack-box optimization. We validate the effectiveness of our framework through\nextensive testing in three representative types of traffic scenarios. The\nresults demonstrate superior performance in generating diverse and high-quality\nscenarios with greater sample efficiency than existing reinforcement learning\n(RL) and sampling-based methods.\n","authors":["Peide Huang","Wenhao Ding","Benjamin Stoler","Jonathan Francis","Bingqing Chen","Ding Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.13208v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09440v1","updated":"2024-12-12T16:56:01Z","published":"2024-12-12T16:56:01Z","title":"Learning to Adapt: Bio-Inspired Gait Strategies for Versatile Quadruped\n  Locomotion","summary":"  Deep reinforcement learning (DRL) has revolutionised quadruped robot\nlocomotion, but existing control frameworks struggle to generalise beyond their\ntraining-induced observational scope, resulting in limited adaptability. In\ncontrast, animals achieve exceptional adaptability through gait transition\nstrategies, diverse gait utilisation, and seamless adjustment to immediate\nenvironmental demands. Inspired by these capabilities, we present a novel DRL\nframework that incorporates key attributes of animal locomotion: gait\ntransition strategies, pseudo gait procedural memory, and adaptive motion\nadjustments. This approach enables our framework to achieve unparalleled\nadaptability, demonstrated through blind zero-shot deployment on complex\nterrains and recovery from critically unstable states. Our findings offer\nvaluable insights into the biomechanics of animal locomotion, paving the way\nfor robust, adaptable robotic systems.\n","authors":["Joseph Humphreys","Chengxu Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.09440v1.pdf","comment":"15 pages, 8 figures, journal paper"},{"id":"http://arxiv.org/abs/2412.09424v1","updated":"2024-12-12T16:30:50Z","published":"2024-12-12T16:30:50Z","title":"Slope Considered Online Nonlinear Trajectory Planning with Differential\n  Energy Model for Autonomous Driving","summary":"  Achieving energy-efficient trajectory planning for autonomous driving remains\na challenge due to the limitations of model-agnostic approaches. This study\naddresses this gap by introducing an online nonlinear programming trajectory\noptimization framework that integrates a differentiable energy model into\nautonomous systems. By leveraging traffic and slope profile predictions within\na safety-critical framework, the proposed method enhances fuel efficiency for\nboth sedans and diesel trucks by 3.71\\% and 7.15\\%, respectively, when compared\nto traditional model-agnostic quadratic programming techniques. These\nimprovements translate to a potential \\$6.14 billion economic benefit for the\nU.S. trucking industry. This work bridges the gap between model-agnostic\nautonomous driving and model-aware ECO-driving, highlighting a practical\npathway for integrating energy efficiency into real-time trajectory planning.\n","authors":["Zhaofeng Tian","Lichen Xia","Weisong Shi"],"pdf_url":"https://arxiv.org/pdf/2412.09424v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09417v1","updated":"2024-12-12T16:25:10Z","published":"2024-12-12T16:25:10Z","title":"Reinforcement Learning Within the Classical Robotics Stack: A Case Study\n  in Robot Soccer","summary":"  Robot decision-making in partially observable, real-time, dynamic, and\nmulti-agent environments remains a difficult and unsolved challenge. Model-free\nreinforcement learning (RL) is a promising approach to learning decision-making\nin such domains, however, end-to-end RL in complex environments is often\nintractable. To address this challenge in the RoboCup Standard Platform League\n(SPL) domain, we developed a novel architecture integrating RL within a\nclassical robotics stack, while employing a multi-fidelity sim2real approach\nand decomposing behavior into learned sub-behaviors with heuristic selection.\nOur architecture led to victory in the 2024 RoboCup SPL Challenge Shield\nDivision. In this work, we fully describe our system's architecture and\nempirically analyze key design decisions that contributed to its success. Our\napproach demonstrates how RL-based behaviors can be integrated into complete\nrobot behavior architectures.\n","authors":["Adam Labiosa","Zhihan Wang","Siddhant Agarwal","William Cong","Geethika Hemkumar","Abhinav Narayan Harish","Benjamin Hong","Josh Kelle","Chen Li","Yuhao Li","Zisen Shao","Peter Stone","Josiah P. Hanna"],"pdf_url":"https://arxiv.org/pdf/2412.09417v1.pdf","comment":"Submitted to ICRA 2025"},{"id":"http://arxiv.org/abs/2412.09387v1","updated":"2024-12-12T15:53:58Z","published":"2024-12-12T15:53:58Z","title":"Distributed Intelligent System Architecture for UAV-Assisted Monitoring\n  of Wind Energy Infrastructure","summary":"  With the rapid development of green energy, the efficiency and reliability of\nwind turbines are key to sustainable renewable energy production. For that\nreason, this paper presents a novel intelligent system architecture designed\nfor the dynamic collection and real-time processing of visual data to detect\ndefects in wind turbines. The system employs advanced algorithms within a\ndistributed framework to enhance inspection accuracy and efficiency using\nunmanned aerial vehicles (UAVs) with integrated visual and thermal sensors. An\nexperimental study conducted at the \"Staryi Sambir-1\" wind power plant in\nUkraine demonstrates the system's effectiveness, showing a significant\nimprovement in defect detection accuracy (up to 94%) and a reduction in\ninspection time per turbine (down to 1.5 hours) compared to traditional\nmethods. The results show that the proposed intelligent system architecture\nprovides a scalable and reliable solution for wind turbine maintenance,\ncontributing to the durability and performance of renewable energy\ninfrastructure.\n","authors":["Serhii Svystun","Oleksandr Melnychenko","Pavlo Radiuk","Oleg Savenko","Andrii Lysyi"],"pdf_url":"https://arxiv.org/pdf/2412.09387v1.pdf","comment":"Wind turbine inspection, UAV, intelligent systems, distributed\n  architecture, defect detection, renewable energy maintenance, automated\n  monitoring"},{"id":"http://arxiv.org/abs/2412.09342v1","updated":"2024-12-12T15:10:22Z","published":"2024-12-12T15:10:22Z","title":"Diffusion Predictive Control with Constraints","summary":"  Diffusion models have recently gained popularity for policy learning in\nrobotics due to their ability to capture high-dimensional and multimodal\ndistributions. However, diffusion policies are inherently stochastic and\ntypically trained offline, limiting their ability to handle unseen and dynamic\nconditions where novel constraints not represented in the training data must be\nsatisfied. To overcome this limitation, we propose diffusion predictive control\nwith constraints (DPCC), an algorithm for diffusion-based control with explicit\nstate and action constraints that can deviate from those in the training data.\nDPCC uses constraint tightening and incorporates model-based projections into\nthe denoising process of a trained trajectory diffusion model. This allows us\nto generate constraint-satisfying, dynamically feasible, and goal-reaching\ntrajectories for predictive control. We show through simulations of a robot\nmanipulator that DPCC outperforms existing methods in satisfying novel\ntest-time constraints while maintaining performance on the learned control\ntask.\n","authors":["Ralf Römer","Alexander von Rohr","Angela P. Schoellig"],"pdf_url":"https://arxiv.org/pdf/2412.09342v1.pdf","comment":"Code: https://github.com/ralfroemer99/dpcc. 14 pages, 3 figures, 3\n  tables"},{"id":"http://arxiv.org/abs/2412.09286v1","updated":"2024-12-12T13:56:36Z","published":"2024-12-12T13:56:36Z","title":"Learning Novel Skills from Language-Generated Demonstrations","summary":"  Current robot learning algorithms for acquiring novel skills often rely on\ndemonstration datasets or environment interactions, resulting in high labor\ncosts and potential safety risks. To address these challenges, this study\nproposes a skill-learning framework that enables robots to acquire novel skills\nfrom natural language instructions. The proposed pipeline leverages\nvision-language models to generate demonstration videos of novel skills, which\nare processed by an inverse dynamics model to extract actions from the\nunlabeled demonstrations. These actions are subsequently mapped to\nenvironmental contexts via imitation learning, enabling robots to learn new\nskills effectively. Experimental evaluations in the MetaWorld simulation\nenvironments demonstrate the pipeline's capability to generate high-fidelity\nand reliable demonstrations. Using the generated demonstrations, various skill\nlearning algorithms achieve an accomplishment rate three times the original on\nnovel tasks. These results highlight a novel approach to robot learning,\noffering a foundation for the intuitive and intelligent acquisition of novel\nrobotic skills.\n","authors":["Ao-Qun Jin","Tian-Yu Xiang","Xiao-Hu Zhou","Mei-Jiang Gui","Xiao-Liang Xie","Shi-Qi Liu","Shuang-Yi Wang","Yue Cao","Sheng-Bin Duan","Fu-Chao Xie","Zeng-Guang Hou"],"pdf_url":"https://arxiv.org/pdf/2412.09286v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09265v1","updated":"2024-12-12T13:22:02Z","published":"2024-12-12T13:22:02Z","title":"Score and Distribution Matching Policy: Advanced Accelerated Visuomotor\n  Policies via Matched Distillation","summary":"  Visual-motor policy learning has advanced with architectures like\ndiffusion-based policies, known for modeling complex robotic trajectories.\nHowever, their prolonged inference times hinder high-frequency control tasks\nrequiring real-time feedback. While consistency distillation (CD) accelerates\ninference, it introduces errors that compromise action quality. To address\nthese limitations, we propose the Score and Distribution Matching Policy (SDM\nPolicy), which transforms diffusion-based policies into single-step generators\nthrough a two-stage optimization process: score matching ensures alignment with\ntrue action distributions, and distribution matching minimizes KL divergence\nfor consistency. A dual-teacher mechanism integrates a frozen teacher for\nstability and an unfrozen teacher for adversarial training, enhancing\nrobustness and alignment with target distributions. Evaluated on a 57-task\nsimulation benchmark, SDM Policy achieves a 6x inference speedup while having\nstate-of-the-art action quality, providing an efficient and reliable framework\nfor high-frequency robotic tasks.\n","authors":["Bofang Jia","Pengxiang Ding","Can Cui","Mingyang Sun","Pengfang Qian","Zhaoxin Fan","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2412.09265v1.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2405.07017v3","updated":"2024-12-12T12:13:29Z","published":"2024-05-11T13:51:33Z","title":"Robot Agnostic Visual Servoing considering kinematic constraints enabled\n  by a decoupled network trajectory planner structure","summary":"  We propose a visual servoing method consisting of a detection network and a\nvelocity trajectory planner. First, the detection network estimates the objects\nposition and orientation in the image space. Furthermore, these are normalized\nand filtered. The direction and orientation is then the input to the trajectory\nplanner, which considers the kinematic constrains of the used robotic system.\nThis allows safe and stable control, since the kinematic boundary values are\ntaken into account in planning. Also, by having direction estimation and\nvelocity planner separated, the learning part of the method does not directly\ninfluence the control value. This also enables the transfer of the method to\ndifferent robotic systems without retraining, therefore being robot agnostic.\nWe evaluate our method on different visual servoing tasks with and without\nclutter on two different robotic systems. Our method achieved mean absolute\nposition errors of <0.5 mm and orientation errors of <1{\\deg}. Additionally, we\ntransferred the method to a new system which differs in robot and camera,\nemphasizing robot agnostic capability of our method.\n","authors":["Constantin Schempp","Christian Friedrich"],"pdf_url":"https://arxiv.org/pdf/2405.07017v3.pdf","comment":"\\copyright 2024 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2404.14285v2","updated":"2024-12-12T12:12:51Z","published":"2024-04-22T15:35:33Z","title":"LLM-Personalize: Aligning LLM Planners with Human Preferences via\n  Reinforced Self-Training for Housekeeping Robots","summary":"  Large language models (LLMs) have shown significant potential for robotics\napplications, particularly task planning, by harnessing their language\ncomprehension and text generation capabilities. However, in applications such\nas household robotics, a critical gap remains in the personalization of these\nmodels to individual user preferences. We introduce LLM-Personalize, a novel\nframework with an optimization pipeline designed to personalize LLM planners\nfor household robotics. Our LLM-Personalize framework features an LLM planner\nthat performs iterative planning in multi-room, partially-observable household\nscenarios, making use of a scene graph constructed with local observations. The\ngenerated plan consists of a sequence of high-level actions which are\nsubsequently executed by a controller. Central to our approach is the\noptimization pipeline, which combines imitation learning and iterative\nself-training to personalize the LLM planner. In particular, the imitation\nlearning phase performs initial LLM alignment from demonstrations, and\nbootstraps the model to facilitate effective iterative self-training, which\nfurther explores and aligns the model to user preferences. We evaluate\nLLM-Personalize on Housekeep, a challenging simulated real-world 3D benchmark\nfor household rearrangements, and show that LLM-Personalize achieves more than\na 30 percent increase in success rate over existing LLM planners, showcasing\nsignificantly improved alignment with human preferences. Project page:\nhttps://gdg94.github.io/projectllmpersonalize/.\n","authors":["Dongge Han","Trevor McInroe","Adam Jelley","Stefano V. Albrecht","Peter Bell","Amos Storkey"],"pdf_url":"https://arxiv.org/pdf/2404.14285v2.pdf","comment":"COLING 2025"},{"id":"http://arxiv.org/abs/2407.09649v3","updated":"2024-12-12T10:51:53Z","published":"2024-07-12T19:09:26Z","title":"VDB-GPDF: Online Gaussian Process Distance Field with VDB Structure","summary":"  Robots reason about the environment through dedicated representations.\nPopular choices for dense representations exploit Truncated Signed Distance\nFunctions (TSDF) and Octree data structures. However, TSDF provides a\nprojective or non-projective signed distance obtained directly from depth\nmeasurements that overestimate the Euclidean distance. Octrees, despite being\nmemory efficient, require tree traversal and can lead to increased runtime in\nlarge scenarios. Other representations based on the Gaussian Process (GP)\ndistance fields are appealing due to their probabilistic and continuous nature,\nbut the computational complexity is a concern. In this paper, we present an\nonline efficient mapping framework that seamlessly couples GP distance fields\nand the fast-access OpenVDB data structure. The key aspect is a latent Local GP\nSigned Distance Field (L-GPDF) contained in a local VDB structure that allows\nfast queries of the Euclidean distance, surface properties and their\nuncertainties for arbitrary points in the field of view. Probabilistic fusion\nis then performed by merging the inferred values of these points into a global\nVDB structure that is efficiently maintained over time. After fusion, the\nsurface mesh is recovered, and a global GP Signed Distance Field (G-GPDF) is\ngenerated and made available for downstream applications to query accurate\ndistance and gradients. A comparison with the state-of-the-art frameworks shows\nsuperior efficiency and accuracy of the inferred distance field and comparable\nreconstruction performance. https://github.com/UTS-RI/VDB_GPDF\n","authors":["Lan Wu","Cedric Le Gentil","Teresa Vidal-Calleja"],"pdf_url":"https://arxiv.org/pdf/2407.09649v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18917v2","updated":"2024-12-12T10:39:06Z","published":"2024-05-29T09:19:50Z","title":"Causal Action Influence Aware Counterfactual Data Augmentation","summary":"  Offline data are both valuable and practical resources for teaching robots\ncomplex behaviors. Ideally, learning agents should not be constrained by the\nscarcity of available demonstrations, but rather generalize beyond the training\ndistribution. However, the complexity of real-world scenarios typically\nrequires huge amounts of data to prevent neural network policies from picking\nup on spurious correlations and learning non-causal relationships. We propose\nCAIAC, a data augmentation method that can create feasible synthetic\ntransitions from a fixed dataset without having access to online environment\ninteractions. By utilizing principled methods for quantifying causal influence,\nwe are able to perform counterfactual reasoning by swapping\n$\\it{action}$-unaffected parts of the state-space between independent\ntrajectories in the dataset. We empirically show that this leads to a\nsubstantial increase in robustness of offline learning algorithms against\ndistributional shift.\n","authors":["Núria Armengol Urpí","Marco Bagatella","Marin Vlastelica","Georg Martius"],"pdf_url":"https://arxiv.org/pdf/2405.18917v2.pdf","comment":"Accepted in 41st International Conference on Machine Learning (ICML\n  2024)"},{"id":"http://arxiv.org/abs/2412.09149v1","updated":"2024-12-12T10:34:26Z","published":"2024-12-12T10:34:26Z","title":"Student-Informed Teacher Training","summary":"  Imitation learning with a privileged teacher has proven effective for\nlearning complex control behaviors from high-dimensional inputs, such as\nimages. In this framework, a teacher is trained with privileged task\ninformation, while a student tries to predict the actions of the teacher with\nmore limited observations, e.g., in a robot navigation task, the teacher might\nhave access to distances to nearby obstacles, while the student only receives\nvisual observations of the scene. However, privileged imitation learning faces\na key challenge: the student might be unable to imitate the teacher's behavior\ndue to partial observability. This problem arises because the teacher is\ntrained without considering if the student is capable of imitating the learned\nbehavior. To address this teacher-student asymmetry, we propose a framework for\njoint training of the teacher and student policies, encouraging the teacher to\nlearn behaviors that can be imitated by the student despite the latters'\nlimited access to information and its partial observability. Based on the\nperformance bound in imitation learning, we add (i) the approximated action\ndifference between teacher and student as a penalty term to the reward function\nof the teacher, and (ii) a supervised teacher-student alignment step. We\nmotivate our method with a maze navigation task and demonstrate its\neffectiveness on complex vision-based quadrotor flight and manipulation tasks.\n","authors":["Nico Messikommer","Jiaxu Xing","Elie Aljalbout","Davide Scaramuzza"],"pdf_url":"https://arxiv.org/pdf/2412.09149v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08496v2","updated":"2024-12-12T10:07:12Z","published":"2024-12-11T15:59:46Z","title":"Drift-free Visual SLAM using Digital Twins","summary":"  Globally-consistent localization in urban environments is crucial for\nautonomous systems such as self-driving vehicles and drones, as well as\nassistive technologies for visually impaired people. Traditional\nVisual-Inertial Odometry (VIO) and Visual Simultaneous Localization and Mapping\n(VSLAM) methods, though adequate for local pose estimation, suffer from drift\nin the long term due to reliance on local sensor data. While GPS counteracts\nthis drift, it is unavailable indoors and often unreliable in urban areas. An\nalternative is to localize the camera to an existing 3D map using\nvisual-feature matching. This can provide centimeter-level accurate\nlocalization but is limited by the visual similarities between the current view\nand the map. This paper introduces a novel approach that achieves accurate and\nglobally-consistent localization by aligning the sparse 3D point cloud\ngenerated by the VIO/VSLAM system to a digital twin using point-to-plane\nmatching; no visual data association is needed. The proposed method provides a\n6-DoF global measurement tightly integrated into the VIO/VSLAM system.\nExperiments run on a high-fidelity GPS simulator and real-world data collected\nfrom a drone demonstrate that our approach outperforms state-of-the-art VIO-GPS\nsystems and offers superior robustness against viewpoint changes compared to\nthe state-of-the-art Visual SLAM systems.\n","authors":["Roxane Merat","Giovanni Cioffi","Leonard Bauersfeld","Davide Scaramuzza"],"pdf_url":"https://arxiv.org/pdf/2412.08496v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09121v1","updated":"2024-12-12T09:57:10Z","published":"2024-12-12T09:57:10Z","title":"MMD-OPT : Maximum Mean Discrepancy Based Sample Efficient Collision Risk\n  Minimization for Autonomous Driving","summary":"  We propose MMD-OPT: a sample-efficient approach for minimizing the risk of\ncollision under arbitrary prediction distribution of the dynamic obstacles.\nMMD-OPT is based on embedding distribution in Reproducing Kernel Hilbert Space\n(RKHS) and the associated Maximum Mean Discrepancy (MMD). We show how these two\nconcepts can be used to define a sample efficient surrogate for collision risk\nestimate. We perform extensive simulations to validate the effectiveness of\nMMD-OPT on both synthetic and real-world datasets. Importantly, we show that\ntrajectory optimization with our MMD-based collision risk surrogate leads to\nsafer trajectories at low sample regimes than popular alternatives based on\nConditional Value at Risk (CVaR).\n","authors":["Basant Sharma","Arun Kumar Singh"],"pdf_url":"https://arxiv.org/pdf/2412.09121v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09117v1","updated":"2024-12-12T09:51:55Z","published":"2024-12-12T09:51:55Z","title":"Reconfigurable Intelligent Surface for Internet of Robotic Things","summary":"  With the rapid development of artificial intelligence, robotics, and Internet\nof Things, multi-robot systems are progressively acquiring human-like\nenvironmental perception and understanding capabilities, empowering them to\ncomplete complex tasks through autonomous decision-making and interaction.\nHowever, the Internet of Robotic Things (IoRT) faces significant challenges in\nterms of spectrum resources, sensing accuracy, communication latency, and\nenergy supply. To address these issues, a reconfigurable intelligent surface\n(RIS)-aided IoRT network is proposed to enhance the overall performance of\nrobotic communication, sensing, computation, and energy harvesting. In the case\nstudies, by jointly optimizing parameters such as transceiver beamforming,\nrobot trajectories, and RIS coefficients, solutions based on multi-agent deep\nreinforcement learning and multi-objective optimization are proposed to solve\nproblems such as beamforming design, path planning, target sensing, and data\naggregation. Numerical results are provided to demonstrate the effectiveness of\nproposed solutions in improve communication quality, sensing accuracy,\ncomputation error, and energy efficiency of RIS-aided IoRT networks.\n","authors":["Wanli Ni","Ruyu Luo","Xinran Zhang","Peng Wang","Wen Wang","Hui Tian"],"pdf_url":"https://arxiv.org/pdf/2412.09117v1.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2412.09114v1","updated":"2024-12-12T09:47:10Z","published":"2024-12-12T09:47:10Z","title":"Hybrid Model-Data Fault Diagnosis for Wafer Handler Robots: Tilt and\n  Broken Belt Cases","summary":"  This work proposes a hybrid model- and data-based scheme for fault detection,\nisolation, and estimation (FDIE) for a class of wafer handler (WH) robots. The\nproposed hybrid scheme consists of: 1) a linear filter that simultaneously\nestimates system states and fault-induced signals from sensing and actuation\ndata; and 2) a data-driven classifier, in the form of a support vector machine\n(SVM), that detects and isolates the fault type using estimates generated by\nthe filter. We demonstrate the effectiveness of the scheme for two critical\nfault types for WH robots used in the semiconductor industry: broken-belt in\nthe lower arm of the WH robot (an abrupt fault) and tilt in the robot arms (an\nincipient fault). We derive explicit models of the robot motion dynamics\ninduced by these faults and test the diagnostics scheme in a realistic\nsimulation-based case study. These case study results demonstrate that the\nproposed hybrid FDIE scheme achieves superior performance compared to purely\ndata-driven methods.\n","authors":["Tim van Esch","Farhad Ghanipoor","Carlos Murguia","Nathan van de Wouw"],"pdf_url":"https://arxiv.org/pdf/2412.09114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13321v3","updated":"2024-12-12T09:43:40Z","published":"2024-03-20T05:57:20Z","title":"Robotics meets Fluid Dynamics: A Characterization of the Induced Airflow\n  below a Quadrotor as a Turbulent Jet","summary":"  The widespread adoption of quadrotors for diverse applications, from\nagriculture to public safety, necessitates an understanding of the aerodynamic\ndisturbances they create. This paper introduces a computationally lightweight\nmodel for estimating the time-averaged magnitude of the induced flow below\nquadrotors in hover. Unlike related approaches that rely on expensive\ncomputational fluid dynamics (CFD) simulations or drone specific time-consuming\nempirical measurements, our method leverages classical theory from turbulent\nflows. By analyzing over 16 hours of flight data from drones of varying sizes\nwithin a large motion capture system, we show for the first time that the\ncombined flow from all drone propellers is well-approximated by a turbulent jet\nafter 2.5 drone-diameters below the vehicle. Using a novel normalization and\nscaling, we experimentally identify model parameters that describe a unified\nmean velocity field below differently sized quadrotors. The model, which\nrequires only the drone's mass, propeller size, and drone size for\ncalculations, accurately describes the far-field airflow over a long-range in a\nvery large volume which is impractical to simulate using CFD. Our model offers\na practical tool for ensuring safer operations near humans, optimizing sensor\nplacements and drone control in multi-agent scenarios. We demonstrate the\nlatter by designing a controller that compensates for the downwash of another\ndrone, leading to a four times lower altitude deviation when passing below.\n","authors":["Leonard Bauersfeld","Koen Muller","Dominic Ziegler","Filippo Coletti","Davide Scaramuzza"],"pdf_url":"https://arxiv.org/pdf/2403.13321v3.pdf","comment":"7+1 pages"},{"id":"http://arxiv.org/abs/2409.11512v3","updated":"2024-12-12T08:59:33Z","published":"2024-09-17T19:26:21Z","title":"Good Grasps Only: A data engine for self-supervised fine-tuning of pose\n  estimation using grasp poses for verification","summary":"  In this paper, we present a novel method for self-supervised fine-tuning of\npose estimation. Leveraging zero-shot pose estimation, our approach enables the\nrobot to automatically obtain training data without manual labeling. After pose\nestimation the object is grasped, and in-hand pose estimation is used for data\nvalidation. Our pipeline allows the system to fine-tune while the process is\nrunning, removing the need for a learning phase. The motivation behind our work\nlies in the need for rapid setup of pose estimation solutions. Specifically, we\naddress the challenging task of bin picking, which plays a pivotal role in\nflexible robotic setups. Our method is implemented on a robotics work-cell, and\ntested with four different objects. For all objects, our method increases the\nperformance and outperforms a state-of-the-art method trained on the CAD model\nof the objects. Project page available at gogoengine.github.io\n","authors":["Frederik Hagelskjær"],"pdf_url":"https://arxiv.org/pdf/2409.11512v3.pdf","comment":"8 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2404.06442v2","updated":"2024-12-12T08:48:02Z","published":"2024-04-09T16:42:54Z","title":"QueSTMaps: Queryable Semantic Topological Maps for 3D Scene\n  Understanding","summary":"  Robotic tasks such as planning and navigation require a hierarchical semantic\nunderstanding of a scene, which could include multiple floors and rooms.\nCurrent methods primarily focus on object segmentation for 3D scene\nunderstanding. However, such methods struggle to segment out topological\nregions like \"kitchen\" in the scene. In this work, we introduce a two-step\npipeline to solve this problem. First, we extract a topological map, i.e.,\nfloorplan of the indoor scene using a novel multi-channel occupancy\nrepresentation. Then, we generate CLIP-aligned features and semantic labels for\nevery room instance based on the objects it contains using a self-attention\ntransformer. Our language-topology alignment supports natural language\nquerying, e.g., a \"place to cook\" locates the \"kitchen\". We outperform the\ncurrent state-of-the-art on room segmentation by ~20% and room classification\nby ~12%. Our detailed qualitative analysis and ablation studies provide\ninsights into the problem of joint structural and semantic 3D scene\nunderstanding. Project Page: quest-maps.github.io\n","authors":["Yash Mehan","Kumaraditya Gupta","Rohit Jayanti","Anirudh Govil","Sourav Garg","Madhava Krishna"],"pdf_url":"https://arxiv.org/pdf/2404.06442v2.pdf","comment":"Accepted at 2024 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS) as Oral Presentation. Also presented at the 2nd\n  Workshop on Open-Vocabulary 3D Scene Understanding (OpenSUN3D) at CVPR 2024"},{"id":"http://arxiv.org/abs/2402.17363v5","updated":"2024-12-12T06:39:46Z","published":"2024-02-27T09:55:34Z","title":"CGGM: A conditional graph generation model with adaptive sparsity for\n  node anomaly detection in IoT networks","summary":"  Dynamic graphs are extensively employed for detecting anomalous behavior in\nnodes within the Internet of Things (IoT). Graph generative models are often\nused to address the issue of imbalanced node categories in dynamic graphs.\nNevertheless, the constraints it faces include the monotonicity of adjacency\nrelationships, the difficulty in constructing multi-dimensional features for\nnodes, and the lack of a method for end-to-end generation of multiple\ncategories of nodes. In this paper, we propose a novel graph generation model,\ncalled CGGM, specifically for generating samples belonging to the minority\nclass. The framework consists two core module: a conditional graph generation\nmodule and a graph-based anomaly detection module. The generative module adapts\nto the sparsity of the matrix by downsampling a noise adjacency matrix, and\nincorporates a multi-dimensional feature encoder based on multi-head\nself-attention to capture latent dependencies among features. Additionally, a\nlatent space constraint is combined with the distribution distance to\napproximate the latent distribution of real data. The graph-based anomaly\ndetection module utilizes the generated balanced dataset to predict the node\nbehaviors. Extensive experiments have shown that CGGM outperforms the\nstate-of-the-art methods in terms of accuracy and divergence. The results also\ndemonstrate CGGM can generated diverse data categories, that enhancing the\nperformance of multi-category classification task.\n","authors":["Munan Li","Xianshi Su","Runze Ma","Tongbang Jiang","Zijian Li","Tony Q. S. Quek"],"pdf_url":"https://arxiv.org/pdf/2402.17363v5.pdf","comment":"10 pages, 19 figures"},{"id":"http://arxiv.org/abs/2412.08983v1","updated":"2024-12-12T06:33:31Z","published":"2024-12-12T06:33:31Z","title":"An Event-Triggered Framework for Trust-Mediated Human-Autonomy\n  Interaction","summary":"  Inspired by the increased cooperation between humans and autonomous systems,\nwe present a new hybrid systems framework capturing the interconnected dynamics\nunderlying these interactions. The framework accommodates models arising from\nboth the autonomous systems and cognitive psychology literature in order to\nrepresent key elements such as human trust in the autonomous system. The\nintermittent nature of human interactions are incorporated by asynchronous\nevent-triggered sampling at the framework's human-autonomous system interfaces.\nWe illustrate important considerations for tuning framework parameters by\ninvestigating a practical application to an autonomous robotic swarm search and\nrescue scenario. In this way, we demonstrate how the proposed framework may\nassist in designing more efficient and effective interactions between humans\nand autonomous systems.\n","authors":["Daniel A. Williams","Airlie Chapman","Chris Manzie"],"pdf_url":"https://arxiv.org/pdf/2412.08983v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08971v1","updated":"2024-12-12T06:07:28Z","published":"2024-12-12T06:07:28Z","title":"Motor Imagery Teleoperation of a Mobile Robot Using a Low-Cost\n  Brain-Computer Interface for Multi-Day Validation","summary":"  Brain-computer interfaces (BCI) have the potential to provide transformative\ncontrol in prosthetics, assistive technologies (wheelchairs), robotics, and\nhuman-computer interfaces. While Motor Imagery (MI) offers an intuitive\napproach to BCI control, its practical implementation is often limited by the\nrequirement for expensive devices, extensive training data, and complex\nalgorithms, leading to user fatigue and reduced accessibility. In this paper,\nwe demonstrate that effective MI-BCI control of a mobile robot in real-world\nsettings can be achieved using a fine-tuned Deep Neural Network (DNN) with a\nsliding window, eliminating the need for complex feature extractions for\nreal-time robot control. The fine-tuning process optimizes the convolutional\nand attention layers of the DNN to adapt to each user's daily MI data streams,\nreducing training data by 70% and minimizing user fatigue from extended data\ncollection. Using a low-cost (~$3k), 16-channel, non-invasive, open-source\nelectroencephalogram (EEG) device, four users teleoperated a quadruped robot\nover three days. The system achieved 78% accuracy on a single-day validation\ndataset and maintained a 75% validation accuracy over three days without\nextensive retraining from day-to-day. For real-world robot command\nclassification, we achieved an average of 62% accuracy. By providing empirical\nevidence that MI-BCI systems can maintain performance over multiple days with\nreduced training data to DNN and a low-cost EEG device, our work enhances the\npracticality and accessibility of BCI technology. This advancement makes BCI\napplications more feasible for real-world scenarios, particularly in\ncontrolling robotic systems.\n","authors":["Yujin An","Daniel Mitchell","John Lathrop","David Flynn","Soon-Jo Chung"],"pdf_url":"https://arxiv.org/pdf/2412.08971v1.pdf","comment":"IEEE Telepresence 2024"},{"id":"http://arxiv.org/abs/2412.08934v1","updated":"2024-12-12T04:53:39Z","published":"2024-12-12T04:53:39Z","title":"A cheat sheet for probability distributions of orientational data","summary":"  The need for statistical models of orientations arises in many applications\nin engineering and computer science. Orientational data appear as sets of\nangles, unit vectors, rotation matrices or quaternions. In the field of\ndirectional statistics, a lot of advances have been made in modelling such\ntypes of data. However, only a few of these tools are used in engineering and\ncomputer science applications. Hence, this paper aims to serve as a cheat sheet\nfor those probability distributions of orientations. Models for 1-DOF, 2-DOF\nand 3-DOF orientations are discussed. For each of them, expressions for the\ndensity function, fitting to data, and sampling are presented. The paper is\nwritten with a compromise between engineering and statistics in terms of\nnotation and terminology. A Python library with functions for some of these\nmodels is provided. Using this library, two examples of applications to real\ndata are presented.\n","authors":["P. C. Lopez-Custodio"],"pdf_url":"https://arxiv.org/pdf/2412.08934v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08909v1","updated":"2024-12-12T03:45:07Z","published":"2024-12-12T03:45:07Z","title":"Continuous Gaussian Process Pre-Optimization for Asynchronous\n  Event-Inertial Odometry","summary":"  Event cameras, as bio-inspired sensors, are asynchronously triggered with\nhigh-temporal resolution compared to intensity cameras. Recent work has focused\non fusing the event measurements with inertial measurements to enable\nego-motion estimation in high-speed and HDR environments. However, existing\nmethods predominantly rely on IMU preintegration designed mainly for\nsynchronous sensors and discrete-time frameworks. In this paper, we propose a\ncontinuous-time preintegration method based on the Temporal Gaussian Process\n(TGP) called GPO. Concretely, we model the preintegration as a time-indexed\nmotion trajectory and leverage an efficient two-step optimization to initialize\nthe precision preintegration pseudo-measurements. Our method realizes a linear\nand constant time cost for initialization and query, respectively. To further\nvalidate the proposal, we leverage the GPO to design an asynchronous\nevent-inertial odometry and compare with other asynchronous fusion schemes\nwithin the same odometry system. Experiments conducted on both public and\nown-collected datasets demonstrate that the proposed GPO offers significant\nadvantages in terms of precision and efficiency, outperforming existing\napproaches in handling asynchronous sensor fusion.\n","authors":["Zhixiang Wang","Xudong Li","Yizhai Zhang","Fan Zhang","Panfeng Huang"],"pdf_url":"https://arxiv.org/pdf/2412.08909v1.pdf","comment":"8pages"},{"id":"http://arxiv.org/abs/2412.07689v2","updated":"2024-12-12T02:47:24Z","published":"2024-12-10T17:27:32Z","title":"DriveMM: All-in-One Large Multimodal Model for Autonomous Driving","summary":"  Large Multimodal Models (LMMs) have demonstrated exceptional comprehension\nand interpretation capabilities in Autonomous Driving (AD) by incorporating\nlarge language models. Despite the advancements, current data-driven AD\napproaches tend to concentrate on a single dataset and specific tasks,\nneglecting their overall capabilities and ability to generalize. To bridge\nthese gaps, we propose DriveMM, a general large multimodal model designed to\nprocess diverse data inputs, such as images and multi-view videos, while\nperforming a broad spectrum of AD tasks, including perception, prediction, and\nplanning. Initially, the model undergoes curriculum pre-training to process\nvaried visual signals and perform basic visual comprehension and perception\ntasks. Subsequently, we augment and standardize various AD-related datasets to\nfine-tune the model, resulting in an all-in-one LMM for autonomous driving. To\nassess the general capabilities and generalization ability, we conduct\nevaluations on six public benchmarks and undertake zero-shot transfer on an\nunseen dataset, where DriveMM achieves state-of-the-art performance across all\ntasks. We hope DriveMM as a promising solution for future end-to-end autonomous\ndriving applications in the real world. Project page with code:\nhttps://github.com/zhijian11/DriveMM.\n","authors":["Zhijian Huang","Chengjian Feng","Feng Yan","Baihui Xiao","Zequn Jie","Yujie Zhong","Xiaodan Liang","Lin Ma"],"pdf_url":"https://arxiv.org/pdf/2412.07689v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08855v1","updated":"2024-12-12T01:29:01Z","published":"2024-12-12T01:29:01Z","title":"Real-Time Algorithms for Game-Theoretic Motion Planning and Control in\n  Autonomous Racing using Near-Potential Function","summary":"  Autonomous racing extends beyond the challenge of controlling a racecar at\nits physical limits. Professional racers employ strategic maneuvers to outwit\nother competing opponents to secure victory. While modern control algorithms\ncan achieve human-level performance by computing offline racing lines for\nsingle-car scenarios, research on real-time algorithms for multi-car autonomous\nracing is limited. To bridge this gap, we develop game-theoretic modeling\nframework that incorporates the competitive aspect of autonomous racing like\novertaking and blocking through a novel policy parametrization, while operating\nthe car at its limit. Furthermore, we propose an algorithmic approach to\ncompute the (approximate) Nash equilibrium strategy, which represents the\noptimal approach in the presence of competing agents. Specifically, we\nintroduce an algorithm inspired by recently introduced framework of dynamic\nnear-potential function, enabling real-time computation of the Nash\nequilibrium. Our approach comprises two phases: offline and online. During the\noffline phase, we use simulated racing data to learn a near-potential function\nthat approximates utility changes for agents. This function facilitates the\nonline computation of approximate Nash equilibria by maximizing its value. We\nevaluate our method in a head-to-head 3-car racing scenario, demonstrating\nsuperior performance compared to several existing baselines.\n","authors":["Dvij Kalaria","Chinmay Maheshwari","Shankar Sastry"],"pdf_url":"https://arxiv.org/pdf/2412.08855v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08830v1","updated":"2024-12-12T00:04:07Z","published":"2024-12-12T00:04:07Z","title":"EMATO: Energy-Model-Aware Trajectory Optimization for Autonomous Driving","summary":"  Autonomous driving lacks strong proof of energy efficiency with the\nenergy-model-agnostic trajectory planning. To achieve an energy consumption\nmodel-aware trajectory planning for autonomous driving, this study proposes an\nonline nonlinear programming method that optimizes the polynomial trajectories\ngenerated by the Frenet polynomial method while considering both traffic\ntrajectories and road slope prediction. This study further investigates how the\nenergy model can be leveraged in different driving conditions to achieve higher\nenergy efficiency. Case studies, quantitative studies, and ablation studies are\nconducted in a sedan and truck model to prove the effectiveness of the method.\n","authors":["Zhaofeng Tian","Lichen Xia","Weisong Shi"],"pdf_url":"https://arxiv.org/pdf/2412.08830v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09743v1","updated":"2024-12-12T22:19:27Z","published":"2024-12-12T22:19:27Z","title":"Should We Learn Contact-Rich Manipulation Policies from Sampling-Based\n  Planners?","summary":"  The tremendous success of behavior cloning (BC) in robotic manipulation has\nbeen largely confined to tasks where demonstrations can be effectively\ncollected through human teleoperation. However, demonstrations for contact-rich\nmanipulation tasks that require complex coordination of multiple contacts are\ndifficult to collect due to the limitations of current teleoperation\ninterfaces. We investigate how to leverage model-based planning and\noptimization to generate training data for contact-rich dexterous manipulation\ntasks. Our analysis reveals that popular sampling-based planners like rapidly\nexploring random tree (RRT), while efficient for motion planning, produce\ndemonstrations with unfavorably high entropy. This motivates modifications to\nour data generation pipeline that prioritizes demonstration consistency while\nmaintaining solution diversity. Combined with a diffusion-based\ngoal-conditioned BC approach, our method enables effective policy learning and\nzero-shot transfer to hardware for two challenging contact-rich manipulation\ntasks.\n","authors":["Huaijiang Zhu","Tong Zhao","Xinpei Ni","Jiuguang Wang","Kuan Fang","Ludovic Righetti","Tao Pang"],"pdf_url":"https://arxiv.org/pdf/2412.09743v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12498v3","updated":"2024-12-12T21:46:40Z","published":"2024-02-19T20:05:41Z","title":"Feudal Networks for Visual Navigation","summary":"  Visual navigation follows the intuition that humans can navigate without\ndetailed maps. A common approach is interactive exploration while building a\ntopological graph with images at nodes that can be used for planning. Recent\nvariations learn from passive videos and can navigate using complex social and\nsemantic cues. However, a significant number of training videos are needed,\nlarge graphs are utilized, and scenes are not unseen since odometry is\nutilized. We introduce a new approach to visual navigation using feudal\nlearning, which employs a hierarchical structure consisting of a worker agent,\na mid-level manager, and a high-level manager. Key to the feudal learning\nparadigm, agents at each level see a different aspect of the task and operate\nat different spatial and temporal scales. Two unique modules are developed in\nthis framework. For the high-level manager, we learn a memory proxy map in a\nself supervised manner to record prior observations in a learned latent space\nand avoid the use of graphs and odometry. For the mid-level manager, we develop\na waypoint network that outputs intermediate subgoals imitating human waypoint\nselection during local navigation. This waypoint network is pre-trained using a\nnew, small set of teleoperation videos that we make publicly available, with\ntraining environments different from testing environments. The resulting feudal\nnavigation network achieves near SOTA performance, while providing a novel\nno-RL, no-graph, no-odometry, no-metric map approach to the image goal\nnavigation task.\n","authors":["Faith Johnson","Bryan Bo Cao","Ashwin Ashok","Shubham Jain","Kristin Dana"],"pdf_url":"https://arxiv.org/pdf/2402.12498v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.08027v2","updated":"2024-12-12T21:29:57Z","published":"2024-11-12T18:56:58Z","title":"LLMPhy: Complex Physical Reasoning Using Large Language Models and World\n  Models","summary":"  Physical reasoning is an important skill needed for robotic agents when\noperating in the real world. However, solving such reasoning problems often\ninvolves hypothesizing and reflecting over complex multi-body interactions\nunder the effect of a multitude of physical forces and thus learning all such\ninteractions poses a significant hurdle for state-of-the-art machine learning\nframeworks, including large language models (LLMs). To study this problem, we\npropose a new physical reasoning task and a dataset, dubbed TraySim. Our task\ninvolves predicting the dynamics of several objects on a tray that is given an\nexternal impact -- the domino effect of the ensued object interactions and\ntheir dynamics thus offering a challenging yet controlled setup, with the goal\nof reasoning being to infer the stability of the objects after the impact. To\nsolve this complex physical reasoning task, we present LLMPhy, a zero-shot\nblack-box optimization framework that leverages the physics knowledge and\nprogram synthesis abilities of LLMs, and synergizes these abilities with the\nworld models built into modern physics engines. Specifically, LLMPhy uses an\nLLM to generate code to iteratively estimate the physical hyperparameters of\nthe system (friction, damping, layout, etc.) via an implicit\nanalysis-by-synthesis approach using a (non-differentiable) simulator in the\nloop and uses the inferred parameters to imagine the dynamics of the scene\ntowards solving the reasoning task. To show the effectiveness of LLMPhy, we\npresent experiments on our TraySim dataset to predict the steady-state poses of\nthe objects. Our results show that the combination of the LLM and the physics\nengine leads to state-of-the-art zero-shot physical reasoning performance,\nwhile demonstrating superior convergence against standard black-box\noptimization methods and better estimation of the physical parameters.\n","authors":["Anoop Cherian","Radu Corcodel","Siddarth Jain","Diego Romeres"],"pdf_url":"https://arxiv.org/pdf/2411.08027v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.18507v2","updated":"2024-12-12T19:59:24Z","published":"2024-11-27T16:50:42Z","title":"At First Contact: Stiffness Estimation Using Vibrational Information for\n  Prosthetic Grasp Modulation","summary":"  Stiffness estimation is crucial for delicate object manipulation in robotic\nand prosthetic hands but remains challenging due to dependence on force and\ndisplacement measurement and real-time sensory integration. This study presents\na piezoelectric sensing framework for stiffness estimation at first contact\nduring pinch grasps, addressing the limitations of traditional force-based\nmethods. Inspired by human skin, a multimodal tactile sensor that captures\nvibrational and force data is developed and integrated into a prosthetic hand's\nfingertip. Machine learning models, including support vector machines and\nconvolutional neural networks, demonstrate that vibrational signals within the\ncritical 15 ms after first contact reliably encode stiffness, achieving\nclassification accuracies up to 98.6% and regression errors as low as 2.39\nShore A on real-world objects of varying stiffness. Inference times of less\nthan 1.5 ms are significantly faster than the average grasp closure time (16.65\nms in our dataset), enabling real-time stiffness estimation before the object\nis fully grasped. By leveraging the transient asymmetry in grasp dynamics,\nwhere one finger contacts the object before the others, this method enables\nearly grasp modulation, enhancing safety and intuitiveness in prosthetic hands\nwhile offering broad applications in robotics.\n","authors":["Anway S. Pimpalkar","Ariel Slepyan","Nitish V. Thakor"],"pdf_url":"https://arxiv.org/pdf/2411.18507v2.pdf","comment":"5 pages, 7 figures, for IEEE Sensors Letters"},{"id":"http://arxiv.org/abs/2412.09690v1","updated":"2024-12-12T19:09:06Z","published":"2024-12-12T19:09:06Z","title":"Full Magnetometer and Gyroscope Bias Estimation using Angular Rates:\n  Theory and Experimental Evaluation of a Factor Graph-Based Approach","summary":"  Despite their widespread use in determining system attitude,\nMicro-Electro-Mechanical Systems (MEMS) Attitude and Heading Reference Systems\n(AHRS) are limited by sensor measurement biases. This paper introduces a method\ncalled MAgnetometer and GYroscope Calibration (MAGYC), leveraging three-axis\nangular rate measurements from an angular rate gyroscope to estimate both the\nhard- and soft-iron biases of magnetometers as well as the bias of gyroscopes.\n  We present two implementation methods of this approach based on batch and\nonline incremental factor graphs. Our method imposes fewer restrictions on\ninstrument movements required for calibration, eliminates the need for\nknowledge of the local magnetic field magnitude or instrument's attitude, and\nfacilitates integration into factor graph algorithms for Smoothing and Mapping\nframeworks.\n  We validate the proposed methods through numerical simulations and in-field\nexperimental evaluations with a sensor onboard an underwater vehicle. By\nimplementing the proposed method in field data of a seafloor mapping dive, the\ndead reckoning-based position estimation error of the underwater vehicle was\nreduced from 10% to 0.5% of the distance traveled.\n","authors":["Sebastián Rodríguez-Martínez","Giancarlo Troni"],"pdf_url":"https://arxiv.org/pdf/2412.09690v1.pdf","comment":"10 pages, 9 figures, submitted to IEEE Journal of Ocean Engineering.\n  arXiv admin note: substantial text overlap with arXiv:2410.13827"},{"id":"http://arxiv.org/abs/2412.10464v1","updated":"2024-12-12T15:52:40Z","published":"2024-12-12T15:52:40Z","title":"Automatic Detection, Positioning and Counting of Grape Bunches Using\n  Robots","summary":"  In order to promote agricultural automatic picking and yield estimation\ntechnology, this project designs a set of automatic detection, positioning and\ncounting algorithms for grape bunches, and applies it to agricultural robots.\nThe Yolov3 detection network is used to realize the accurate detection of grape\nbunches, and the local tracking algorithm is added to eliminate relocation.\nThen it obtains the accurate 3D spatial position of the central points of grape\nbunches using the depth distance and the spatial restriction method. Finally,\nthe counting of grape bunches is completed. It is verified using the\nagricultural robot in the simulated vineyard environment. The project code is\nreleased at:\nhttps://github.com/XuminGaoGithub/Grape_bunches_count_using_robots.\n","authors":["Xumin Gao"],"pdf_url":"https://arxiv.org/pdf/2412.10464v1.pdf","comment":null}]},"2024-12-13T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2412.10373v1","updated":"2024-12-13T18:59:54Z","published":"2024-12-13T18:59:54Z","title":"GaussianWorld: Gaussian World Model for Streaming 3D Occupancy\n  Prediction","summary":"  3D occupancy prediction is important for autonomous driving due to its\ncomprehensive perception of the surroundings. To incorporate sequential inputs,\nmost existing methods fuse representations from previous frames to infer the\ncurrent 3D occupancy. However, they fail to consider the continuity of driving\nscenarios and ignore the strong prior provided by the evolution of 3D scenes\n(e.g., only dynamic objects move). In this paper, we propose a\nworld-model-based framework to exploit the scene evolution for perception. We\nreformulate 3D occupancy prediction as a 4D occupancy forecasting problem\nconditioned on the current sensor input. We decompose the scene evolution into\nthree factors: 1) ego motion alignment of static scenes; 2) local movements of\ndynamic objects; and 3) completion of newly-observed scenes. We then employ a\nGaussian world model (GaussianWorld) to explicitly exploit these priors and\ninfer the scene evolution in the 3D Gaussian space considering the current RGB\nobservation. We evaluate the effectiveness of our framework on the widely used\nnuScenes dataset. Our GaussianWorld improves the performance of the\nsingle-frame counterpart by over 2% in mIoU without introducing additional\ncomputations. Code: https://github.com/zuosc19/GaussianWorld.\n","authors":["Sicheng Zuo","Wenzhao Zheng","Yuanhui Huang","Jie Zhou","Jiwen Lu"],"pdf_url":"https://arxiv.org/pdf/2412.10373v1.pdf","comment":"Code is available at: https://github.com/zuosc19/GaussianWorld"},{"id":"http://arxiv.org/abs/2412.10372v1","updated":"2024-12-13T18:59:40Z","published":"2024-12-13T18:59:40Z","title":"UniMed-CLIP: Towards a Unified Image-Text Pretraining Paradigm for\n  Diverse Medical Imaging Modalities","summary":"  Vision-Language Models (VLMs) trained via contrastive learning have achieved\nnotable success in natural image tasks. However, their application in the\nmedical domain remains limited due to the scarcity of openly accessible,\nlarge-scale medical image-text datasets. Existing medical VLMs either train on\nclosed-source proprietary or relatively small open-source datasets that do not\ngeneralize well. Similarly, most models remain specific to a single or limited\nnumber of medical imaging domains, again restricting their applicability to\nother modalities. To address this gap, we introduce UniMed, a large-scale,\nopen-source multi-modal medical dataset comprising over 5.3 million image-text\npairs across six diverse imaging modalities: X-ray, CT, MRI, Ultrasound,\nPathology, and Fundus. UniMed is developed using a data-collection framework\nthat leverages Large Language Models (LLMs) to transform modality-specific\nclassification datasets into image-text formats while incorporating existing\nimage-text data from the medical domain, facilitating scalable VLM pretraining.\nUsing UniMed, we trained UniMed-CLIP, a unified VLM for six modalities that\nsignificantly outperforms existing generalist VLMs and matches\nmodality-specific medical VLMs, achieving notable gains in zero-shot\nevaluations. For instance, UniMed-CLIP improves over BiomedCLIP (trained on\nproprietary data) by an absolute gain of +12.61, averaged over 21 datasets,\nwhile using 3x less training data. To facilitate future research, we release\nUniMed dataset, training codes, and models at\nhttps://github.com/mbzuai-oryx/UniMed-CLIP.\n","authors":["Muhammad Uzair Khattak","Shahina Kunhimon","Muzammal Naseer","Salman Khan","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2412.10372v1.pdf","comment":"Code, models and demo available at\n  https://github.com/mbzuai-oryx/UniMed-CLIP"},{"id":"http://arxiv.org/abs/2412.10371v1","updated":"2024-12-13T18:59:30Z","published":"2024-12-13T18:59:30Z","title":"GaussianAD: Gaussian-Centric End-to-End Autonomous Driving","summary":"  Vision-based autonomous driving shows great potential due to its satisfactory\nperformance and low costs. Most existing methods adopt dense representations\n(e.g., bird's eye view) or sparse representations (e.g., instance boxes) for\ndecision-making, which suffer from the trade-off between comprehensiveness and\nefficiency. This paper explores a Gaussian-centric end-to-end autonomous\ndriving (GaussianAD) framework and exploits 3D semantic Gaussians to\nextensively yet sparsely describe the scene. We initialize the scene with\nuniform 3D Gaussians and use surrounding-view images to progressively refine\nthem to obtain the 3D Gaussian scene representation. We then use sparse\nconvolutions to efficiently perform 3D perception (e.g., 3D detection, semantic\nmap construction). We predict 3D flows for the Gaussians with dynamic semantics\nand plan the ego trajectory accordingly with an objective of future scene\nforecasting. Our GaussianAD can be trained in an end-to-end manner with\noptional perception labels when available. Extensive experiments on the widely\nused nuScenes dataset verify the effectiveness of our end-to-end GaussianAD on\nvarious tasks including motion planning, 3D occupancy prediction, and 4D\noccupancy forecasting. Code: https://github.com/wzzheng/GaussianAD.\n","authors":["Wenzhao Zheng","Junjie Wu","Yao Zheng","Sicheng Zuo","Zixun Xie","Longchao Yang","Yong Pan","Zhihui Hao","Peng Jia","Xianpeng Lang","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.10371v1.pdf","comment":"Code is available at: https://github.com/wzzheng/GaussianAD"},{"id":"http://arxiv.org/abs/2412.10369v1","updated":"2024-12-13T18:58:48Z","published":"2024-12-13T18:58:48Z","title":"A Grounded Typology of Word Classes","summary":"  We propose a grounded approach to meaning in language typology. We treat data\nfrom perceptual modalities, such as images, as a language-agnostic\nrepresentation of meaning. Hence, we can quantify the function--form\nrelationship between images and captions across languages. Inspired by\ninformation theory, we define \"groundedness\", an empirical measure of\ncontextual semantic contentfulness (formulated as a difference in surprisal)\nwhich can be computed with multilingual multimodal language models. As a proof\nof concept, we apply this measure to the typology of word classes. Our measure\ncaptures the contentfulness asymmetry between functional (grammatical) and\nlexical (content) classes across languages, but contradicts the view that\nfunctional classes do not convey content. Moreover, we find universal trends in\nthe hierarchy of groundedness (e.g., nouns > adjectives > verbs), and show that\nour measure partly correlates with psycholinguistic concreteness norms in\nEnglish. We release a dataset of groundedness scores for 30 languages. Our\nresults suggest that the grounded typology approach can provide quantitative\nevidence about semantic function in language.\n","authors":["Coleman Haley","Sharon Goldwater","Edoardo Ponti"],"pdf_url":"https://arxiv.org/pdf/2412.10369v1.pdf","comment":"19 pages, 5 figures"},{"id":"http://arxiv.org/abs/2412.10362v1","updated":"2024-12-13T18:55:19Z","published":"2024-12-13T18:55:19Z","title":"OP-LoRA: The Blessing of Dimensionality","summary":"  Low-rank adapters enable fine-tuning of large models with only a small number\nof parameters, thus reducing storage costs and minimizing the risk of\ncatastrophic forgetting. However, they often pose optimization challenges, with\npoor convergence. To overcome these challenges, we introduce an\nover-parameterized approach that accelerates training without increasing\ninference costs. This method reparameterizes low-rank adaptation by employing a\nseparate MLP and learned embedding for each layer. The learned embedding is\ninput to the MLP, which generates the adapter parameters. Such\noverparamaterization has been shown to implicitly function as an adaptive\nlearning rate and momentum, accelerating optimization. At inference time, the\nMLP can be discarded, leaving behind a standard low-rank adapter. To study the\neffect of MLP overparameterization on a small yet difficult proxy task, we\nimplement it for matrix factorization, and find it achieves faster convergence\nand lower final loss. Extending this approach to larger-scale tasks, we observe\nconsistent performance gains across domains. We achieve improvements in\nvision-language tasks and especially notable increases in image generation,\nwith CMMD scores improving by up to 15 points.\n","authors":["Piotr Teterwak","Kate Saenko","Bryan A. Plummer","Ser-Nam Lim"],"pdf_url":"https://arxiv.org/pdf/2412.10362v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10360v1","updated":"2024-12-13T18:53:24Z","published":"2024-12-13T18:53:24Z","title":"Apollo: An Exploration of Video Understanding in Large Multimodal Models","summary":"  Despite the rapid integration of video perception capabilities into Large\nMultimodal Models (LMMs), the underlying mechanisms driving their video\nunderstanding remain poorly understood. Consequently, many design decisions in\nthis domain are made without proper justification or analysis. The high\ncomputational cost of training and evaluating such models, coupled with limited\nopen research, hinders the development of video-LMMs. To address this, we\npresent a comprehensive study that helps uncover what effectively drives video\nunderstanding in LMMs.\n  We begin by critically examining the primary contributors to the high\ncomputational requirements associated with video-LMM research and discover\nScaling Consistency, wherein design and training decisions made on smaller\nmodels and datasets (up to a critical size) effectively transfer to larger\nmodels. Leveraging these insights, we explored many video-specific aspects of\nvideo-LMMs, including video sampling, architectures, data composition, training\nschedules, and more. For example, we demonstrated that fps sampling during\ntraining is vastly preferable to uniform frame sampling and which vision\nencoders are the best for video representation.\n  Guided by these findings, we introduce Apollo, a state-of-the-art family of\nLMMs that achieve superior performance across different model sizes. Our models\ncan perceive hour-long videos efficiently, with Apollo-3B outperforming most\nexisting $7$B models with an impressive 55.1 on LongVideoBench. Apollo-7B is\nstate-of-the-art compared to 7B LMMs with a 70.9 on MLVU, and 63.3 on\nVideo-MME.\n","authors":["Orr Zohar","Xiaohan Wang","Yann Dubois","Nikhil Mehta","Tong Xiao","Philippe Hansen-Estruch","Licheng Yu","Xiaofang Wang","Felix Juefei-Xu","Ning Zhang","Serena Yeung-Levy","Xide Xia"],"pdf_url":"https://arxiv.org/pdf/2412.10360v1.pdf","comment":"https://apollo-lmms.github.io"},{"id":"http://arxiv.org/abs/2412.10353v1","updated":"2024-12-13T18:49:25Z","published":"2024-12-13T18:49:25Z","title":"Robust image classification with multi-modal large language models","summary":"  Deep Neural Networks are vulnerable to adversarial examples, i.e., carefully\ncrafted input samples that can cause models to make incorrect predictions with\nhigh confidence. To mitigate these vulnerabilities, adversarial training and\ndetection-based defenses have been proposed to strengthen models in advance.\nHowever, most of these approaches focus on a single data modality, overlooking\nthe relationships between visual patterns and textual descriptions of the\ninput. In this paper, we propose a novel defense, Multi-Shield, designed to\ncombine and complement these defenses with multi-modal information to further\nenhance their robustness. Multi-Shield leverages multi-modal large language\nmodels to detect adversarial examples and abstain from uncertain\nclassifications when there is no alignment between textual and visual\nrepresentations of the input. Extensive evaluations on CIFAR-10 and ImageNet\ndatasets, using robust and non-robust image classification models, demonstrate\nthat Multi-Shield can be easily integrated to detect and reject adversarial\nexamples, outperforming the original defenses.\n","authors":["Francesco Villani","Igor Maljkovic","Dario Lazzaro","Angelo Sotgiu","Antonio Emanuele Cinà","Fabio Roli"],"pdf_url":"https://arxiv.org/pdf/2412.10353v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10351v1","updated":"2024-12-13T18:47:11Z","published":"2024-12-13T18:47:11Z","title":"VibrantVS: A high-resolution multi-task transformer for forest canopy\n  height estimation","summary":"  This paper explores the application of a novel multi-task vision transformer\n(ViT) model for the estimation of canopy height models (CHMs) using 4-band\nNational Agriculture Imagery Program (NAIP) imagery across the western United\nStates. We compare the effectiveness of this model in terms of accuracy and\nprecision aggregated across ecoregions and class heights versus three other\nbenchmark peer-reviewed models. Key findings suggest that, while other\nbenchmark models can provide high precision in localized areas, the VibrantVS\nmodel has substantial advantages across a broad reach of ecoregions in the\nwestern United States with higher accuracy, higher precision, the ability to\ngenerate updated inference at a cadence of three years or less, and high\nspatial resolution. The VibrantVS model provides significant value for\necological monitoring and land management decisions for wildfire mitigation.\n","authors":["Tony Chang","Kiarie Ndegwa","Andreas Gros","Vincent A. Landau","Luke J. Zachmann","Bogdan State","Mitchell A. Gritts","Colton W. Miller","Nathan E. Rutenbeck","Scott Conway","Guy Bayes"],"pdf_url":"https://arxiv.org/pdf/2412.10351v1.pdf","comment":"15 pages, 12 figures"},{"id":"http://arxiv.org/abs/2412.10349v1","updated":"2024-12-13T18:45:26Z","published":"2024-12-13T18:45:26Z","title":"Ensuring Force Safety in Vision-Guided Robotic Manipulation via Implicit\n  Tactile Calibration","summary":"  In dynamic environments, robots often encounter constrained movement\ntrajectories when manipulating objects with specific properties, such as doors.\nTherefore, applying the appropriate force is crucial to prevent damage to both\nthe robots and the objects. However, current vision-guided robot state\ngeneration methods often falter in this regard, as they lack the integration of\ntactile perception. To tackle this issue, this paper introduces a novel state\ndiffusion framework termed SafeDiff. It generates a prospective state sequence\nfrom the current robot state and visual context observation while incorporating\nreal-time tactile feedback to refine the sequence. As far as we know, this is\nthe first study specifically focused on ensuring force safety in robotic\nmanipulation. It significantly enhances the rationality of state planning, and\nthe safe action trajectory is derived from inverse dynamics based on this\nrefined planning. In practice, unlike previous approaches that concatenate\nvisual and tactile data to generate future robot state sequences, our method\nemploys tactile data as a calibration signal to adjust the robot's state within\nthe state space implicitly. Additionally, we've developed a large-scale\nsimulation dataset called SafeDoorManip50k, offering extensive multimodal data\nto train and evaluate the proposed method. Extensive experiments show that our\nvisual-tactile model substantially mitigates the risk of harmful forces in the\ndoor opening, across both simulated and real-world settings.\n","authors":["Lai Wei","Jiahua Ma","Yibo Hu","Ruimao Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.10349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10348v1","updated":"2024-12-13T18:45:18Z","published":"2024-12-13T18:45:18Z","title":"A dual contrastive framework","summary":"  In current multimodal tasks, models typically freeze the encoder and decoder\nwhile adapting intermediate layers to task-specific goals, such as region\ncaptioning. Region-level visual understanding presents significant challenges\nfor large-scale vision-language models. While limited spatial awareness is a\nknown issue, coarse-grained pretraining, in particular, exacerbates the\ndifficulty of optimizing latent representations for effective encoder-decoder\nalignment. We propose AlignCap, a framework designed to enhance region-level\nunderstanding through fine-grained alignment of latent spaces. Our approach\nintroduces a novel latent feature refinement module that enhances conditioned\nlatent space representations to improve region-level captioning performance. We\nalso propose an innovative alignment strategy, the semantic space alignment\nmodule, which boosts the quality of multimodal representations. Additionally,\nwe incorporate contrastive learning in a novel manner within both modules to\nfurther enhance region-level captioning performance. To address spatial\nlimitations, we employ a General Object Detection (GOD) method as a data\npreprocessing pipeline that enhances spatial reasoning at the regional level.\nExtensive experiments demonstrate that our approach significantly improves\nregion-level captioning performance across various tasks\n","authors":["Yuan Sun","Zhao Zhang","Jorge Ortiz"],"pdf_url":"https://arxiv.org/pdf/2412.10348v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10342v1","updated":"2024-12-13T18:40:10Z","published":"2024-12-13T18:40:10Z","title":"Iris: Breaking GUI Complexity with Adaptive Focus and Self-Refining","summary":"  Digital agents are increasingly employed to automate tasks in interactive\ndigital environments such as web pages, software applications, and operating\nsystems. While text-based agents built on Large Language Models (LLMs) often\nrequire frequent updates due to platform-specific APIs, visual agents\nleveraging Multimodal Large Language Models (MLLMs) offer enhanced adaptability\nby interacting directly with Graphical User Interfaces (GUIs). However, these\nagents face significant challenges in visual perception, particularly when\nhandling high-resolution, visually complex digital environments. This paper\nintroduces Iris, a foundational visual agent that addresses these challenges\nthrough two key innovations: Information-Sensitive Cropping (ISC) and\nSelf-Refining Dual Learning (SRDL). ISC dynamically identifies and prioritizes\nvisually dense regions using a edge detection algorithm, enabling efficient\nprocessing by allocating more computational resources to areas with higher\ninformation density. SRDL enhances the agent's ability to handle complex tasks\nby leveraging a dual-learning loop, where improvements in referring (describing\nUI elements) reinforce grounding (locating elements) and vice versa, all\nwithout requiring additional annotated data. Empirical evaluations demonstrate\nthat Iris achieves state-of-the-art performance across multiple benchmarks with\nonly 850K GUI annotations, outperforming methods using 10x more training data.\nThese improvements further translate to significant gains in both web and OS\nagent downstream tasks.\n","authors":["Zhiqi Ge","Juncheng Li","Xinglei Pang","Minghe Gao","Kaihang Pan","Wang Lin","Hao Fei","Wenqiao Zhang","Siliang Tang","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2412.10342v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10339v1","updated":"2024-12-13T18:35:27Z","published":"2024-12-13T18:35:27Z","title":"A Universal Degradation-based Bridging Technique for Domain Adaptive\n  Semantic Segmentation","summary":"  Semantic segmentation often suffers from significant performance degradation\nwhen the trained network is applied to a different domain. To address this\nissue, unsupervised domain adaptation (UDA) has been extensively studied.\nExisting methods introduce the domain bridging techniques to mitigate\nsubstantial domain gap, which construct intermediate domains to facilitate the\ngradual transfer of knowledge across different domains. However, these\nstrategies often require dataset-specific designs and may generate unnatural\nintermediate distributions that lead to semantic shift. In this paper, we\npropose DiDA, a universal degradation-based bridging technique formalized as a\ndiffusion forward process. DiDA consists of two key modules: (1)\nDegradation-based Intermediate Domain Construction, which creates continuous\nintermediate domains through simple image degradation operations to encourage\nlearning domain-invariant features as domain differences gradually diminish;\n(2) Semantic Shift Compensation, which leverages a diffusion encoder to encode\nand compensate for semantic shift information with degraded time-steps,\npreserving discriminative representations in the intermediate domains. As a\nplug-and-play solution, DiDA supports various degradation operations and\nseamlessly integrates with existing UDA methods. Extensive experiments on\nprevalent synthetic-to-real semantic segmentation benchmarks demonstrate that\nDiDA consistently improves performance across different settings and achieves\nnew state-of-the-art results when combined with existing methods.\n","authors":["Wangkai Li","Rui Sun","Tianzhu Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.10339v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10338v1","updated":"2024-12-13T18:33:18Z","published":"2024-12-13T18:33:18Z","title":"XYScanNet: An Interpretable State Space Model for Perceptual Image\n  Deblurring","summary":"  Deep state-space models (SSMs), like recent Mamba architectures, are emerging\nas a promising alternative to CNN and Transformer networks. Existing\nMamba-based restoration methods process the visual data by leveraging a\nflatten-and-scan strategy that converts image patches into a 1D sequence before\nscanning. However, this scanning paradigm ignores local pixel dependencies and\nintroduces spatial misalignment by positioning distant pixels incorrectly\nadjacent, which reduces local noise-awareness and degrades image sharpness in\nlow-level vision tasks. To overcome these issues, we propose a novel\nslice-and-scan strategy that alternates scanning along intra- and inter-slices.\nWe further design a new Vision State Space Module (VSSM) for image deblurring,\nand tackle the inefficiency challenges of the current Mamba-based vision\nmodule. Building upon this, we develop XYScanNet, an SSM architecture\nintegrated with a lightweight feature fusion module for enhanced image\ndeblurring. XYScanNet, maintains competitive distortion metrics and\nsignificantly improves perceptual performance. Experimental results show that\nXYScanNet enhances KID by $17\\%$ compared to the nearest competitor. Our code\nwill be released soon.\n","authors":["Hanzhou Liu","Chengkai Liu","Jiacong Xu","Peng Jiang","Mi Lu"],"pdf_url":"https://arxiv.org/pdf/2412.10338v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10316v1","updated":"2024-12-13T17:58:06Z","published":"2024-12-13T17:58:06Z","title":"BrushEdit: All-In-One Image Inpainting and Editing","summary":"  Image editing has advanced significantly with the development of diffusion\nmodels using both inversion-based and instruction-based methods. However,\ncurrent inversion-based approaches struggle with big modifications (e.g.,\nadding or removing objects) due to the structured nature of inversion noise,\nwhich hinders substantial changes. Meanwhile, instruction-based methods often\nconstrain users to black-box operations, limiting direct interaction for\nspecifying editing regions and intensity. To address these limitations, we\npropose BrushEdit, a novel inpainting-based instruction-guided image editing\nparadigm, which leverages multimodal large language models (MLLMs) and image\ninpainting models to enable autonomous, user-friendly, and interactive\nfree-form instruction editing. Specifically, we devise a system enabling\nfree-form instruction editing by integrating MLLMs and a dual-branch image\ninpainting model in an agent-cooperative framework to perform editing category\nclassification, main object identification, mask acquisition, and editing area\ninpainting. Extensive experiments show that our framework effectively combines\nMLLMs and inpainting models, achieving superior performance across seven\nmetrics including mask region preservation and editing effect coherence.\n","authors":["Yaowei Li","Yuxuan Bian","Xuan Ju","Zhaoyang Zhang","Ying Shan","Qiang Xu"],"pdf_url":"https://arxiv.org/pdf/2412.10316v1.pdf","comment":"WebPage available at\n  https://liyaowei-stu.github.io/project/BrushEdit/"},{"id":"http://arxiv.org/abs/2412.10308v1","updated":"2024-12-13T17:42:53Z","published":"2024-12-13T17:42:53Z","title":"TrafficLoc: Localizing Traffic Surveillance Cameras in 3D Scenes","summary":"  We tackle the problem of localizing the traffic surveillance cameras in\ncooperative perception. To overcome the lack of large-scale real-world\nintersection datasets, we introduce Carla Intersection, a new simulated dataset\nwith 75 urban and rural intersections in Carla. Moreover, we introduce a novel\nneural network, TrafficLoc, localizing traffic cameras within a 3D reference\nmap. TrafficLoc employs a coarse-to-fine matching pipeline. For image-point\ncloud feature fusion, we propose a novel Geometry-guided Attention Loss to\naddress cross-modal viewpoint inconsistencies. During coarse matching, we\npropose an Inter-Intra Contrastive Learning to achieve precise alignment while\npreserving distinctiveness among local intra-features within image patch-point\ngroup pairs. Besides, we introduce Dense Training Alignment with a soft-argmax\noperator to consider additional features when regressing the final position.\nExtensive experiments show that our TrafficLoc improves the localization\naccuracy over the state-of-the-art Image-to-point cloud registration methods by\na large margin (up to 86%) on Carla Intersection and generalizes well to\nreal-world data. TrafficLoc also achieves new SOTA performance on KITTI and\nNuScenes datasets, demonstrating strong localization ability across both\nin-vehicle and traffic cameras. Our project page is publicly available at\nhttps://tum-luk.github.io/projects/trafficloc/.\n","authors":["Yan Xia","Yunxiang Lu","Rui Song","Oussema Dhaouadi","João F. Henriques","Daniel Cremers"],"pdf_url":"https://arxiv.org/pdf/2412.10308v1.pdf","comment":"17 pages, 12 figures"},{"id":"http://arxiv.org/abs/2412.10302v1","updated":"2024-12-13T17:37:48Z","published":"2024-12-13T17:37:48Z","title":"DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced\n  Multimodal Understanding","summary":"  We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2.\n","authors":["Zhiyu Wu","Xiaokang Chen","Zizheng Pan","Xingchao Liu","Wen Liu","Damai Dai","Huazuo Gao","Yiyang Ma","Chengyue Wu","Bingxuan Wang","Zhenda Xie","Yu Wu","Kai Hu","Jiawei Wang","Yaofeng Sun","Yukun Li","Yishi Piao","Kang Guan","Aixin Liu","Xin Xie","Yuxiang You","Kai Dong","Xingkai Yu","Haowei Zhang","Liang Zhao","Yisong Wang","Chong Ruan"],"pdf_url":"https://arxiv.org/pdf/2412.10302v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10300v1","updated":"2024-12-13T17:35:42Z","published":"2024-12-13T17:35:42Z","title":"Iterating the Transient Light Transport Matrix for Non-Line-of-Sight\n  Imaging","summary":"  Active imaging systems sample the Transient Light Transport Matrix (TLTM) for\na scene by sequentially illuminating various positions in this scene using a\ncontrollable light source, and then measuring the resulting spatiotemporal\nlight transport with time of flight (ToF) sensors. Time-resolved\nNon-line-of-sight (NLOS) imaging employs an active imaging system that measures\npart of the TLTM of an intermediary relay surface, and uses the indirect\nreflections of light encoded within this TLTM to \"see around corners\". Such\nimaging systems have applications in diverse areas such as disaster response,\nremote surveillance, and autonomous navigation. While existing NLOS imaging\nsystems usually measure a subset of the full TLTM, development of customized\ngated Single Photon Avalanche Diode (SPAD) arrays\n\\cite{riccardo_fast-gated_2022} has made it feasible to probe the full\nmeasurement space. In this work, we demonstrate that the full TLTM on the relay\nsurface can be processed with efficient algorithms to computationally focus and\ndetect our illumination in different parts of the hidden scene, turning the\nrelay surface into a second-order active imaging system. These algorithms allow\nus to iterate on the measured, first-order TLTM, and extract a \\textbf{second\norder TLTM for surfaces in the hidden scene}. We showcase three applications of\nTLTMs in NLOS imaging: (1) Scene Relighting with novel illumination, (2)\nSeparation of direct and indirect components of light transport in the hidden\nscene, and (3) Dual Photography. Additionally, we empirically demonstrate that\nSPAD arrays enable parallel acquisition of photons, effectively mitigating long\nacquisition times.\n","authors":["Talha Sultan","Eric Brandt","Khadijeh Masumnia-Bisheh","Simone Riccardo","Pavel Polynkin","Alberto Tosi","Andreas Velten"],"pdf_url":"https://arxiv.org/pdf/2412.10300v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10294v1","updated":"2024-12-13T17:26:45Z","published":"2024-12-13T17:26:45Z","title":"Coherent 3D Scene Diffusion From a Single RGB Image","summary":"  We present a novel diffusion-based approach for coherent 3D scene\nreconstruction from a single RGB image. Our method utilizes an\nimage-conditioned 3D scene diffusion model to simultaneously denoise the 3D\nposes and geometries of all objects within the scene. Motivated by the\nill-posed nature of the task and to obtain consistent scene reconstruction\nresults, we learn a generative scene prior by conditioning on all scene objects\nsimultaneously to capture the scene context and by allowing the model to learn\ninter-object relationships throughout the diffusion process. We further propose\nan efficient surface alignment loss to facilitate training even in the absence\nof full ground-truth annotation, which is common in publicly available\ndatasets. This loss leverages an expressive shape representation, which enables\ndirect point sampling from intermediate shape predictions. By framing the task\nof single RGB image 3D scene reconstruction as a conditional diffusion process,\nour approach surpasses current state-of-the-art methods, achieving a 12.04%\nimprovement in AP3D on SUN RGB-D and a 13.43% increase in F-Score on Pix3D.\n","authors":["Manuel Dahnert","Angela Dai","Norman Müller","Matthias Nießner"],"pdf_url":"https://arxiv.org/pdf/2412.10294v1.pdf","comment":"Project Page: https://www.manuel-dahnert.com/research/scene-diffusion\n  - Accepted at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2412.10292v1","updated":"2024-12-13T17:22:50Z","published":"2024-12-13T17:22:50Z","title":"Prompt-Guided Mask Proposal for Two-Stage Open-Vocabulary Segmentation","summary":"  We tackle the challenge of open-vocabulary segmentation, where we need to\nidentify objects from a wide range of categories in different environments,\nusing text prompts as our input. To overcome this challenge, existing methods\noften use multi-modal models like CLIP, which combine image and text features\nin a shared embedding space to bridge the gap between limited and extensive\nvocabulary recognition, resulting in a two-stage approach: In the first stage,\na mask generator takes an input image to generate mask proposals, and the in\nthe second stage the target mask is picked based on the query. However, the\nexpected target mask may not exist in the generated mask proposals, which leads\nto an unexpected output mask. In our work, we propose a novel approach named\nPrompt-guided Mask Proposal (PMP) where the mask generator takes the input text\nprompts and generates masks guided by these prompts. Compared with mask\nproposals generated without input prompts, masks generated by PMP are better\naligned with the input prompts. To realize PMP, we designed a cross-attention\nmechanism between text tokens and query tokens which is capable of generating\nprompt-guided mask proposals after each decoding. We combined our PMP with\nseveral existing works employing a query-based segmentation backbone and the\nexperiments on five benchmark datasets demonstrate the effectiveness of this\napproach, showcasing significant improvements over the current two-stage models\n(1% ~ 3% absolute performance gain in terms of mIOU). The steady improvement in\nperformance across these benchmarks indicates the effective generalization of\nour proposed lightweight prompt-aware method.\n","authors":["Yu-Jhe Li","Xinyang Zhang","Kun Wan","Lantao Yu","Ajinkya Kale","Xin Lu"],"pdf_url":"https://arxiv.org/pdf/2412.10292v1.pdf","comment":"17 pages. Work done during 2023 summer and has been released"},{"id":"http://arxiv.org/abs/2412.08582v2","updated":"2024-12-13T17:11:38Z","published":"2024-12-11T17:57:25Z","title":"Utilizing Multi-step Loss for Single Image Reflection Removal","summary":"  Image reflection removal is crucial for restoring image quality. Distorted\nimages can negatively impact tasks like object detection and image\nsegmentation. In this paper, we present a novel approach for image reflection\nremoval using a single image. Instead of focusing on model architecture, we\nintroduce a new training technique that can be generalized to image-to-image\nproblems, with input and output being similar in nature. This technique is\nembodied in our multi-step loss mechanism, which has proven effective in the\nreflection removal task. Additionally, we address the scarcity of reflection\nremoval training data by synthesizing a high-quality, non-linear synthetic\ndataset called RefGAN using Pix2Pix GAN. This dataset significantly enhances\nthe model's ability to learn better patterns for reflection removal. We also\nutilize a ranged depth map, extracted from the depth estimation of the ambient\nimage, as an auxiliary feature, leveraging its property of lacking depth\nestimations for reflections. Our approach demonstrates superior performance on\nthe SIR^2 benchmark and other real-world datasets, proving its effectiveness by\noutperforming other state-of-the-art models.\n","authors":["Abdelrahman Elnenaey","Marwan Torki"],"pdf_url":"https://arxiv.org/pdf/2412.08582v2.pdf","comment":"6 pages, 6 figures, IEEE AICCSA 2024"},{"id":"http://arxiv.org/abs/2412.04279v2","updated":"2024-12-13T16:59:08Z","published":"2024-12-05T16:00:55Z","title":"Targeted Hard Sample Synthesis Based on Estimated Pose and Occlusion\n  Error for Improved Object Pose Estimation","summary":"  6D Object pose estimation is a fundamental component in robotics enabling\nefficient interaction with the environment. It is particularly challenging in\nbin-picking applications, where objects may be textureless and in difficult\nposes, and occlusion between objects of the same type may cause confusion even\nin well-trained models. We propose a novel method of hard example synthesis\nthat is model-agnostic, using existing simulators and the modeling of pose\nerror in both the camera-to-object viewsphere and occlusion space. Through\nevaluation of the model performance with respect to the distribution of object\nposes and occlusions, we discover regions of high error and generate realistic\ntraining samples to specifically target these regions. With our training\napproach, we demonstrate an improvement in correct detection rate of up to 20%\nacross several ROBI-dataset objects using state-of-the-art pose estimation\nmodels.\n","authors":["Alan Li","Angela P. Schoellig"],"pdf_url":"https://arxiv.org/pdf/2412.04279v2.pdf","comment":"To be published in IEEE Robotics and Automation Letters (RA-L)"},{"id":"http://arxiv.org/abs/2410.01697v3","updated":"2024-12-13T16:55:33Z","published":"2024-10-02T16:05:03Z","title":"MOREL: Enhancing Adversarial Robustness through Multi-Objective\n  Representation Learning","summary":"  Extensive research has shown that deep neural networks (DNNs) are vulnerable\nto slight adversarial perturbations$-$small changes to the input data that\nappear insignificant but cause the model to produce drastically different\noutputs. In addition to augmenting training data with adversarial examples\ngenerated from a specific attack method, most of the current defense strategies\nnecessitate modifying the original model architecture components to improve\nrobustness or performing test-time data purification to handle adversarial\nattacks. In this work, we demonstrate that strong feature representation\nlearning during training can significantly enhance the original model's\nrobustness. We propose MOREL, a multi-objective feature representation learning\napproach, encouraging classification models to produce similar features for\ninputs within the same class, despite perturbations. Our training method\ninvolves an embedding space where cosine similarity loss and multi-positive\ncontrastive loss are used to align natural and adversarial features from the\nmodel encoder and ensure tight clustering. Concurrently, the classifier is\nmotivated to achieve accurate predictions. Through extensive experiments, we\ndemonstrate that our approach significantly enhances the robustness of DNNs\nagainst white-box and black-box adversarial attacks, outperforming other\nmethods that similarly require no architectural changes or test-time data\npurification. Our code is available at https://github.com/salomonhotegni/MOREL\n","authors":["Sedjro Salomon Hotegni","Sebastian Peitz"],"pdf_url":"https://arxiv.org/pdf/2410.01697v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10275v1","updated":"2024-12-13T16:52:13Z","published":"2024-12-13T16:52:13Z","title":"TIV-Diffusion: Towards Object-Centric Movement for Text-driven Image to\n  Video Generation","summary":"  Text-driven Image to Video Generation (TI2V) aims to generate controllable\nvideo given the first frame and corresponding textual description. The primary\nchallenges of this task lie in two parts: (i) how to identify the target\nobjects and ensure the consistency between the movement trajectory and the\ntextual description. (ii) how to improve the subjective quality of generated\nvideos. To tackle the above challenges, we propose a new diffusion-based TI2V\nframework, termed TIV-Diffusion, via object-centric textual-visual alignment,\nintending to achieve precise control and high-quality video generation based on\ntextual-described motion for different objects. Concretely, we enable our\nTIV-Diffuion model to perceive the textual-described objects and their motion\ntrajectory by incorporating the fused textual and visual knowledge through\nscale-offset modulation. Moreover, to mitigate the problems of object\ndisappearance and misaligned objects and motion, we introduce an object-centric\ntextual-visual alignment module, which reduces the risk of misaligned\nobjects/motion by decoupling the objects in the reference image and aligning\ntextual features with each object individually. Based on the above innovations,\nour TIV-Diffusion achieves state-of-the-art high-quality video generation\ncompared with existing TI2V methods.\n","authors":["Xingrui Wang","Xin Li","Yaosi Hu","Hanxin Zhu","Chen Hou","Cuiling Lan","Zhibo Chen"],"pdf_url":"https://arxiv.org/pdf/2412.10275v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10273v1","updated":"2024-12-13T16:46:46Z","published":"2024-12-13T16:46:46Z","title":"Probabilistic Inverse Cameras: Image to 3D via Multiview Geometry","summary":"  We introduce a hierarchical probabilistic approach to go from a 2D image to\nmultiview 3D: a diffusion \"prior\" models the unseen 3D geometry, which then\nconditions a diffusion \"decoder\" to generate novel views of the subject. We use\na pointmap-based geometric representation in a multiview image format to\ncoordinate the generation of multiple target views simultaneously. We\nfacilitate correspondence between views by assuming fixed target camera poses\nrelative to the source camera, and constructing a predictable distribution of\ngeometric features per target. Our modular, geometry-driven approach to\nnovel-view synthesis (called \"unPIC\") beats SoTA baselines such as CAT3D and\nOne-2-3-45 on held-out objects from ObjaverseXL, as well as real-world objects\nranging from Google Scanned Objects, Amazon Berkeley Objects, to the Digital\nTwin Catalog.\n","authors":["Rishabh Kabra","Drew A. Hudson","Sjoerd van Steenkiste","Joao Carreira","Niloy J. Mitra"],"pdf_url":"https://arxiv.org/pdf/2412.10273v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09199v2","updated":"2024-12-13T16:44:42Z","published":"2024-12-12T11:49:18Z","title":"MVC-VPR: Mutual Learning of Viewpoint Classification and Visual Place\n  Recognition","summary":"  Visual Place Recognition (VPR) aims to robustly identify locations by\nleveraging image retrieval based on descriptors encoded from environmental\nimages. However, drastic appearance changes of images captured from different\nviewpoints at the same location pose incoherent supervision signals for\ndescriptor learning, which severely hinder the performance of VPR. Previous\nwork proposes classifying images based on manually defined rules or ground\ntruth labels for viewpoints, followed by descriptor training based on the\nclassification results. However, not all datasets have ground truth labels of\nviewpoints and manually defined rules may be suboptimal, leading to degraded\ndescriptor performance.To address these challenges, we introduce the mutual\nlearning of viewpoint self-classification and VPR. Starting from coarse\nclassification based on geographical coordinates, we progress to finer\nclassification of viewpoints using simple clustering techniques. The dataset is\npartitioned in an unsupervised manner while simultaneously training a\ndescriptor extractor for place recognition. Experimental results show that this\napproach almost perfectly partitions the dataset based on viewpoints, thus\nachieving mutually reinforcing effects. Our method even excels state-of-the-art\n(SOTA) methods that partition datasets using ground truth labels.\n","authors":["Qiwen Gu","Xufei Wang","Fenglin Zhang","Junqiao Zhao","Siyue Tao","Chen Ye","Tiantian Feng","Changjun Jiang"],"pdf_url":"https://arxiv.org/pdf/2412.09199v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2412.10261v1","updated":"2024-12-13T16:30:35Z","published":"2024-12-13T16:30:35Z","title":"MVQ:Towards Efficient DNN Compression and Acceleration with Masked\n  Vector Quantization","summary":"  Vector quantization(VQ) is a hardware-friendly DNN compression method that\ncan reduce the storage cost and weight-loading datawidth of hardware\naccelerators. However, conventional VQ techniques lead to significant accuracy\nloss because the important weights are not well preserved. To tackle this\nproblem, a novel approach called MVQ is proposed, which aims at better\napproximating important weights with a limited number of codewords. At the\nalgorithm level, our approach removes the less important weights through N:M\npruning and then minimizes the vector clustering error between the remaining\nweights and codewords by the masked k-means algorithm. Only distances between\nthe unpruned weights and the codewords are computed, which are then used to\nupdate the codewords. At the architecture level, our accelerator implements\nvector quantization on an EWS (Enhanced weight stationary) CNN accelerator and\nproposes a sparse systolic array design to maximize the benefits brought by\nmasked vector quantization.\\\\ Our algorithm is validated on various models for\nimage classification, object detection, and segmentation tasks. Experimental\nresults demonstrate that MVQ not only outperforms conventional vector\nquantization methods at comparable compression ratios but also reduces FLOPs.\nUnder ASIC evaluation, our MVQ accelerator boosts energy efficiency by\n2.3$\\times$ and reduces the size of the systolic array by 55\\% when compared\nwith the base EWS accelerator. Compared to the previous sparse accelerators,\nMVQ achieves 1.73$\\times$ higher energy efficiency.\n","authors":["Shuaiting Li","Chengxuan Wang","Juncan Deng","Zeyu Wang","Zewen Ye","Zongsheng Wang","Haibin Shen","Kejie Huang"],"pdf_url":"https://arxiv.org/pdf/2412.10261v1.pdf","comment":"Accepted by ASPLOS '25"},{"id":"http://arxiv.org/abs/2412.10258v1","updated":"2024-12-13T16:29:00Z","published":"2024-12-13T16:29:00Z","title":"Copy-Move Detection in Optical Microscopy: A Segmentation Network and A\n  Dataset","summary":"  With increasing revelations of academic fraud, detecting forged experimental\nimages in the biomedical field has become a public concern. The challenge lies\nin the fact that copy-move targets can include background tissue, small\nforeground objects, or both, which may be out of the training domain and\nsubject to unseen attacks, rendering standard object-detection-based approaches\nless effective. To address this, we reformulate the problem of detecting\nbiomedical copy-move forgery regions as an intra-image co-saliency detection\ntask and propose CMSeg-Net, a copy-move forgery segmentation network capable of\nidentifying unseen duplicated areas. Built on a multi-resolution\nencoder-decoder architecture, CMSeg-Net incorporates self-correlation and\ncorrelation-assisted spatial-attention modules to detect intra-image regional\nsimilarities within feature tensors at each observation scale. This design\nhelps distinguish even small copy-move targets in complex microscopic images\nfrom other similar objects. Furthermore, we created a copy-move forgery dataset\nof optical microscopic images, named FakeParaEgg, using open data from the ICIP\n2022 Challenge to support CMSeg-Net's development and verify its performance.\nExtensive experiments demonstrate that our approach outperforms previous\nstate-of-the-art methods on the FakeParaEgg dataset and other open copy-move\ndetection datasets, including CASIA-CMFD, CoMoFoD, and CMF. The FakeParaEgg\ndataset, our source code, and the CMF dataset with our manually defined\nsegmentation ground truths available at\n``https://github.com/YoursEver/FakeParaEgg''.\n","authors":["Hao-Chiang Shao","Yuan-Rong Liao","Tse-Yu Tseng","Yen-Liang Chuo","Fong-Yi Lin"],"pdf_url":"https://arxiv.org/pdf/2412.10258v1.pdf","comment":"submitted to IEEE SPL"},{"id":"http://arxiv.org/abs/2412.10235v1","updated":"2024-12-13T16:06:46Z","published":"2024-12-13T16:06:46Z","title":"EnvPoser: Environment-aware Realistic Human Motion Estimation from\n  Sparse Observations with Uncertainty Modeling","summary":"  Estimating full-body motion using the tracking signals of head and hands from\nVR devices holds great potential for various applications. However, the\nsparsity and unique distribution of observations present a significant\nchallenge, resulting in an ill-posed problem with multiple feasible solutions\n(i.e., hypotheses). This amplifies uncertainty and ambiguity in full-body\nmotion estimation, especially for the lower-body joints. Therefore, we propose\na new method, EnvPoser, that employs a two-stage framework to perform full-body\nmotion estimation using sparse tracking signals and pre-scanned environment\nfrom VR devices. EnvPoser models the multi-hypothesis nature of human motion\nthrough an uncertainty-aware estimation module in the first stage. In the\nsecond stage, we refine these multi-hypothesis estimates by integrating\nsemantic and geometric environmental constraints, ensuring that the final\nmotion estimation aligns realistically with both the environmental context and\nphysical interactions. Qualitative and quantitative experiments on two public\ndatasets demonstrate that our method achieves state-of-the-art performance,\nhighlighting significant improvements in human motion estimation within\nmotion-environment interaction scenarios.\n","authors":["Songpengcheng Xia","Yu Zhang","Zhuo Su","Xiaozheng Zheng","Zheng Lv","Guidong Wang","Yongjie Zhang","Qi Wu","Lei Chu","Ling Pei"],"pdf_url":"https://arxiv.org/pdf/2412.10235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10231v1","updated":"2024-12-13T16:01:19Z","published":"2024-12-13T16:01:19Z","title":"SuperGSeg: Open-Vocabulary 3D Segmentation with Structured\n  Super-Gaussians","summary":"  3D Gaussian Splatting has recently gained traction for its efficient training\nand real-time rendering. While the vanilla Gaussian Splatting representation is\nmainly designed for view synthesis, more recent works investigated how to\nextend it with scene understanding and language features. However, existing\nmethods lack a detailed comprehension of scenes, limiting their ability to\nsegment and interpret complex structures. To this end, We introduce SuperGSeg,\na novel approach that fosters cohesive, context-aware scene representation by\ndisentangling segmentation and language field distillation. SuperGSeg first\nemploys neural Gaussians to learn instance and hierarchical segmentation\nfeatures from multi-view images with the aid of off-the-shelf 2D masks. These\nfeatures are then leveraged to create a sparse set of what we call\nSuper-Gaussians. Super-Gaussians facilitate the distillation of 2D language\nfeatures into 3D space. Through Super-Gaussians, our method enables\nhigh-dimensional language feature rendering without extreme increases in GPU\nmemory. Extensive experiments demonstrate that SuperGSeg outperforms prior\nworks on both open-vocabulary object localization and semantic segmentation\ntasks.\n","authors":["Siyun Liang","Sen Wang","Kunyi Li","Michael Niemeyer","Stefano Gasperini","Nassir Navab","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2412.10231v1.pdf","comment":"13 pages, 8 figures"},{"id":"http://arxiv.org/abs/2412.10224v1","updated":"2024-12-13T15:49:18Z","published":"2024-12-13T15:49:18Z","title":"SPT: Sequence Prompt Transformer for Interactive Image Segmentation","summary":"  Interactive segmentation aims to extract objects of interest from an image\nbased on user-provided clicks. In real-world applications, there is often a\nneed to segment a series of images featuring the same target object. However,\nexisting methods typically process one image at a time, failing to consider the\nsequential nature of the images. To overcome this limitation, we propose a\nnovel method called Sequence Prompt Transformer (SPT), the first to utilize\nsequential image information for interactive segmentation. Our model comprises\ntwo key components: (1) Sequence Prompt Transformer (SPT) for acquiring\ninformation from sequence of images, clicks and masks to improve accurate. (2)\nTop-k Prompt Selection (TPS) selects precise prompts for SPT to further enhance\nthe segmentation effect. Additionally, we create the ADE20K-Seq benchmark to\nbetter evaluate model performance. We evaluate our approach on multiple\nbenchmark datasets and show that our model surpasses state-of-the-art methods\nacross all datasets.\n","authors":["Senlin Cheng","Haopeng Sun"],"pdf_url":"https://arxiv.org/pdf/2412.10224v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08490v3","updated":"2024-12-13T15:49:03Z","published":"2024-10-11T03:31:40Z","title":"CAS-GAN for Contrast-free Angiography Synthesis","summary":"  Iodinated contrast agents are widely utilized in numerous interventional\nprocedures, yet posing substantial health risks to patients. This paper\npresents CAS-GAN, a novel GAN framework that serves as a \"virtual contrast\nagent\" to synthesize X-ray angiographies via disentanglement representation\nlearning and vessel semantic guidance, thereby reducing the reliance on\niodinated contrast agents during interventional procedures. Specifically, our\napproach disentangles X-ray angiographies into background and vessel\ncomponents, leveraging medical prior knowledge. A specialized predictor then\nlearns to map the interrelationships between these components. Additionally, a\nvessel semantic-guided generator and a corresponding loss function are\nintroduced to enhance the visual fidelity of generated images. Experimental\nresults on the XCAD dataset demonstrate the state-of-the-art performance of our\nCAS-GAN, achieving a FID of 5.87 and a MMD of 0.016. These promising results\nhighlight CAS-GAN's potential for clinical applications.\n","authors":["De-Xing Huang","Xiao-Hu Zhou","Mei-Jiang Gui","Xiao-Liang Xie","Shi-Qi Liu","Shuang-Yi Wang","Hao Li","Tian-Yu Xiang","Zeng-Guang Hou"],"pdf_url":"https://arxiv.org/pdf/2410.08490v3.pdf","comment":"IEEE Symposium Series on Computational Intelligence (SSCI 2025)"},{"id":"http://arxiv.org/abs/2412.10219v1","updated":"2024-12-13T15:41:08Z","published":"2024-12-13T15:41:08Z","title":"Learning Complex Non-Rigid Image Edits from Multimodal Conditioning","summary":"  In this paper we focus on inserting a given human (specifically, a single\nimage of a person) into a novel scene. Our method, which builds on top of\nStable Diffusion, yields natural looking images while being highly controllable\nwith text and pose. To accomplish this we need to train on pairs of images, the\nfirst a reference image with the person, the second a \"target image\" showing\nthe same person (with a different pose and possibly in a different background).\nAdditionally we require a text caption describing the new pose relative to that\nin the reference image. In this paper we present a novel dataset following this\ncriteria, which we create using pairs of frames from human-centric and\naction-rich videos and employing a multimodal LLM to automatically summarize\nthe difference in human pose for the text captions. We demonstrate that\nidentity preservation is a more challenging task in scenes \"in-the-wild\", and\nespecially scenes where there is an interaction between persons and objects.\nCombining the weak supervision from noisy captions, with robust 2D pose\nimproves the quality of person-object interactions.\n","authors":["Nikolai Warner","Jack Kolb","Meera Hahn","Vighnesh Birodkar","Jonathan Huang","Irfan Essa"],"pdf_url":"https://arxiv.org/pdf/2412.10219v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07889v2","updated":"2024-12-13T15:39:27Z","published":"2024-12-10T19:48:57Z","title":"Low-Latency Scalable Streaming for Event-Based Vision","summary":"  Recently, we have witnessed the rise of novel ``event-based'' camera sensors\nfor high-speed, low-power video capture. Rather than recording discrete image\nframes, these sensors output asynchronous ``event'' tuples with microsecond\nprecision, only when the brightness change of a given pixel exceeds a certain\nthreshold. Although these sensors have enabled compelling new computer vision\napplications, these applications often require expensive, power-hungry GPU\nsystems, rendering them incompatible for deployment on the low-power devices\nfor which event cameras are optimized. Whereas receiver-driven rate adaptation\nis a crucial feature of modern video streaming solutions, this topic is\nunderexplored in the realm of event-based vision systems. On a real-world event\ncamera dataset, we first demonstrate that a state-of-the-art object detection\napplication is resilient to dramatic data loss, and that this loss may be\nweighted towards the end of each temporal window. We then propose a scalable\nstreaming method for event-based data based on Media Over QUIC, prioritizing\nobject detection performance and low latency. The application server can\nreceive complementary event data across several streams simultaneously, and\ndrop streams as needed to maintain a certain latency. With a latency target of\n5 ms for end-to-end transmission across a small network, we observe an average\nreduction in detection mAP as low as 0.36. With a more relaxed latency target\nof 50 ms, we observe an average mAP reduction as low as 0.19.\n","authors":["Andrew Hamara","Benjamin Kilpatrick","Alex Baratta","Brendon Kofink","Andrew C. Freeman"],"pdf_url":"https://arxiv.org/pdf/2412.07889v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10211v1","updated":"2024-12-13T15:34:34Z","published":"2024-12-13T15:34:34Z","title":"RAID-Database: human Responses to Affine Image Distortions","summary":"  Image quality databases are used to train models for predicting subjective\nhuman perception. However, most existing databases focus on distortions\ncommonly found in digital media and not in natural conditions. Affine\ntransformations are particularly relevant to study, as they are among the most\ncommonly encountered by human observers in everyday life. This Data Descriptor\npresents a set of human responses to suprathreshold affine image transforms\n(rotation, translation, scaling) and Gaussian noise as convenient reference to\ncompare with previously existing image quality databases. The responses were\nmeasured using well established psychophysics: the Maximum Likelihood\nDifference Scaling method. The set contains responses to 864 distorted images.\nThe experiments involved 105 observers and more than 20000 comparisons of\nquadruples of images. The quality of the dataset is ensured because (a) it\nreproduces the classical Pi\\'eron's law, (b) it reproduces classical absolute\ndetection thresholds, and (c) it is consistent with conventional image quality\ndatabases but improves them according to Group-MAD experiments.\n","authors":["Paula Daudén-Oliver","David Agost-Beltran","Emilio Sansano-Sansano","Valero Laparra","Jesús Malo","Marina Martínez-Garcia"],"pdf_url":"https://arxiv.org/pdf/2412.10211v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10209v1","updated":"2024-12-13T15:31:22Z","published":"2024-12-13T15:31:22Z","title":"GAF: Gaussian Avatar Reconstruction from Monocular Videos via Multi-view\n  Diffusion","summary":"  We propose a novel approach for reconstructing animatable 3D Gaussian avatars\nfrom monocular videos captured by commodity devices like smartphones.\nPhotorealistic 3D head avatar reconstruction from such recordings is\nchallenging due to limited observations, which leaves unobserved regions\nunder-constrained and can lead to artifacts in novel views. To address this\nproblem, we introduce a multi-view head diffusion model, leveraging its priors\nto fill in missing regions and ensure view consistency in Gaussian splatting\nrenderings. To enable precise viewpoint control, we use normal maps rendered\nfrom FLAME-based head reconstruction, which provides pixel-aligned inductive\nbiases. We also condition the diffusion model on VAE features extracted from\nthe input image to preserve details of facial identity and appearance. For\nGaussian avatar reconstruction, we distill multi-view diffusion priors by using\niteratively denoised images as pseudo-ground truths, effectively mitigating\nover-saturation issues. To further improve photorealism, we apply latent\nupsampling to refine the denoised latent before decoding it into an image. We\nevaluate our method on the NeRSemble dataset, showing that GAF outperforms the\nprevious state-of-the-art methods in novel view synthesis by a 5.34\\% higher\nSSIM score. Furthermore, we demonstrate higher-fidelity avatar reconstructions\nfrom monocular videos captured on commodity devices.\n","authors":["Jiapeng Tang","Davide Davoli","Tobias Kirschstein","Liam Schoneveld","Matthias Niessner"],"pdf_url":"https://arxiv.org/pdf/2412.10209v1.pdf","comment":"Paper Video: https://youtu.be/QuIYTljvhyg Project Page:\n  https://tangjiapeng.github.io/projects/GAF"},{"id":"http://arxiv.org/abs/2405.08909v2","updated":"2024-12-13T15:22:55Z","published":"2024-05-14T19:02:33Z","title":"ADA-Track++: End-to-End Multi-Camera 3D Multi-Object Tracking with\n  Alternating Detection and Association","summary":"  Many query-based approaches for 3D Multi-Object Tracking (MOT) adopt the\ntracking-by-attention paradigm, utilizing track queries for identity-consistent\ndetection and object queries for identity-agnostic track spawning.\nTracking-by-attention, however, entangles detection and tracking queries in one\nembedding for both the detection and tracking task, which is sub-optimal. Other\napproaches resemble the tracking-by-detection paradigm and detect objects using\ndecoupled track and detection queries followed by a subsequent association.\nThese methods, however, do not leverage synergies between the detection and\nassociation task. Combining the strengths of both paradigms, we introduce\nADA-Track++, a novel end-to-end framework for 3D MOT from multi-view cameras.\nWe introduce a learnable data association module based on edge-augmented\ncross-attention, leveraging appearance and geometric features. We also propose\nan auxiliary token in this attention-based association module, which helps\nmitigate disproportionately high attention to incorrect association targets\ncaused by attention normalization. Furthermore, we integrate this association\nmodule into the decoder layer of a DETR-based 3D detector, enabling\nsimultaneous DETR-like query-to-image cross-attention for detection and\nquery-to-query cross-attention for data association. By stacking these decoder\nlayers, queries are refined for the detection and association task alternately,\neffectively harnessing the task dependencies. We evaluate our method on the\nnuScenes dataset and demonstrate the advantage of our approach compared to the\ntwo previous paradigms.\n","authors":["Shuxiao Ding","Lukas Schneider","Marius Cordts","Juergen Gall"],"pdf_url":"https://arxiv.org/pdf/2405.08909v2.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2305.05349v2","updated":"2024-12-13T15:17:28Z","published":"2023-05-09T11:20:11Z","title":"Towards the Characterization of Representations Learned via\n  Capsule-based Network Architectures","summary":"  Capsule Networks (CapsNets) have been re-introduced as a more compact and\ninterpretable alternative to standard deep neural networks. While recent\nefforts have proved their compression capabilities, to date, their\ninterpretability properties have not been fully assessed. Here, we conduct a\nsystematic and principled study towards assessing the interpretability of these\ntypes of networks. Moreover, we pay special attention towards analyzing the\nlevel to which part-whole relationships are indeed encoded within the learned\nrepresentation. Our analysis in the MNIST, SVHN, PASCAL-part and CelebA\ndatasets suggest that the representations encoded in CapsNets might not be as\ndisentangled nor strictly related to parts-whole relationships as is commonly\nstated in the literature.\n","authors":["Saja Tawalbeh","José Oramas"],"pdf_url":"https://arxiv.org/pdf/2305.05349v2.pdf","comment":"This paper consist of 32 pages including 19 figures. This paper\n  concern about interpretation of capsule networks"},{"id":"http://arxiv.org/abs/2412.10184v1","updated":"2024-12-13T14:55:24Z","published":"2024-12-13T14:55:24Z","title":"Sims: An Interactive Tool for Geospatial Matching and Clustering","summary":"  Acquiring, processing, and visualizing geospatial data requires significant\ncomputing resources, especially for large spatio-temporal domains. This\nchallenge hinders the rapid discovery of predictive features, which is\nessential for advancing geospatial modeling. To address this, we developed\nSimilarity Search (Sims), a no-code web tool that allows users to visualize,\ncompare, cluster, and perform similarity search over defined regions of\ninterest using Google Earth Engine as a backend. Sims is designed to complement\nexisting modeling tools by focusing on feature exploration rather than model\ncreation. We demonstrate the utility of Sims through a case study analyzing\nsimulated maize yield data in Rwanda, where we evaluate how different\ncombinations of soil, weather, and agronomic features affect the clustering of\nyield response zones. Sims is open source and available at\nhttps://github.com/microsoft/Sims\n","authors":["Akram Zaytar","Girmaw Abebe Tadesse","Caleb Robinson","Eduardo G. Bendito","Medha Devare","Meklit Chernet","Gilles Q. Hacheme","Rahul Dodhia","Juan M. Lavista Ferres"],"pdf_url":"https://arxiv.org/pdf/2412.10184v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10182v1","updated":"2024-12-13T14:53:47Z","published":"2024-12-13T14:53:47Z","title":"Multi-Head Encoding for Extreme Label Classification","summary":"  The number of categories of instances in the real world is normally huge, and\neach instance may contain multiple labels. To distinguish these massive labels\nutilizing machine learning, eXtreme Label Classification (XLC) has been\nestablished. However, as the number of categories increases, the number of\nparameters and nonlinear operations in the classifier also rises. This results\nin a Classifier Computational Overload Problem (CCOP). To address this, we\npropose a Multi-Head Encoding (MHE) mechanism, which replaces the vanilla\nclassifier with a multi-head classifier. During the training process, MHE\ndecomposes extreme labels into the product of multiple short local labels, with\neach head trained on these local labels. During testing, the predicted labels\ncan be directly calculated from the local predictions of each head. This\nreduces the computational load geometrically. Then, according to the\ncharacteristics of different XLC tasks, e.g., single-label, multi-label, and\nmodel pretraining tasks, three MHE-based implementations, i.e., Multi-Head\nProduct, Multi-Head Cascade, and Multi-Head Sampling, are proposed to more\neffectively cope with CCOP. Moreover, we theoretically demonstrate that MHE can\nachieve performance approximately equivalent to that of the vanilla classifier\nby generalizing the low-rank approximation problem from Frobenius-norm to\nCross-Entropy. Experimental results show that the proposed methods achieve\nstate-of-the-art performance while significantly streamlining the training and\ninference processes of XLC tasks. The source code has been made public at\nhttps://github.com/Anoise/MHE.\n","authors":["Daojun Liang","Haixia Zhang","Dongfeng Yuan","Minggao Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.10182v1.pdf","comment":"20 pages, 12 figs, Published in TPAMI"},{"id":"http://arxiv.org/abs/2412.10181v1","updated":"2024-12-13T14:53:07Z","published":"2024-12-13T14:53:07Z","title":"Ultra-High Resolution Segmentation via Boundary-Enhanced Patch-Merging\n  Transformer","summary":"  Segmentation of ultra-high resolution (UHR) images is a critical task with\nnumerous applications, yet it poses significant challenges due to high spatial\nresolution and rich fine details. Recent approaches adopt a dual-branch\narchitecture, where a global branch learns long-range contextual information\nand a local branch captures fine details. However, they struggle to handle the\nconflict between global and local information while adding significant extra\ncomputational cost. Inspired by the human visual system's ability to rapidly\norient attention to important areas with fine details and filter out irrelevant\ninformation, we propose a novel UHR segmentation method called\nBoundary-enhanced Patch-merging Transformer (BPT). BPT consists of two key\ncomponents: (1) Patch-Merging Transformer (PMT) for dynamically allocating\ntokens to informative regions to acquire global and local representations, and\n(2) Boundary-Enhanced Module (BEM) that leverages boundary information to\nenrich fine details. Extensive experiments on multiple UHR image segmentation\nbenchmarks demonstrate that our BPT outperforms previous state-of-the-art\nmethods without introducing extra computational overhead. Codes will be\nreleased to facilitate research.\n","authors":["Haopeng Sun"],"pdf_url":"https://arxiv.org/pdf/2412.10181v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10178v1","updated":"2024-12-13T14:50:26Z","published":"2024-12-13T14:50:26Z","title":"SwiftTry: Fast and Consistent Video Virtual Try-On with Diffusion Models","summary":"  Given an input video of a person and a new garment, the objective of this\npaper is to synthesize a new video where the person is wearing the specified\ngarment while maintaining spatiotemporal consistency. While significant\nadvances have been made in image-based virtual try-ons, extending these\nsuccesses to video often results in frame-to-frame inconsistencies. Some\napproaches have attempted to address this by increasing the overlap of frames\nacross multiple video chunks, but this comes at a steep computational cost due\nto the repeated processing of the same frames, especially for long video\nsequence. To address these challenges, we reconceptualize video virtual try-on\nas a conditional video inpainting task, with garments serving as input\nconditions. Specifically, our approach enhances image diffusion models by\nincorporating temporal attention layers to improve temporal coherence. To\nreduce computational overhead, we introduce ShiftCaching, a novel technique\nthat maintains temporal consistency while minimizing redundant computations.\nFurthermore, we introduce the \\dataname~dataset, a new video try-on dataset\nfeaturing more complex backgrounds, challenging movements, and higher\nresolution compared to existing public datasets. Extensive experiments show\nthat our approach outperforms current baselines, particularly in terms of video\nconsistency and inference speed. Data and code are available at\nhttps://github.com/VinAIResearch/swift-try\n","authors":["Hung Nguyen","Quang Qui-Vinh Nguyen","Khoi Nguyen","Rang Nguyen"],"pdf_url":"https://arxiv.org/pdf/2412.10178v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10176v1","updated":"2024-12-13T14:45:11Z","published":"2024-12-13T14:45:11Z","title":"UN-DETR: Promoting Objectness Learning via Joint Supervision for Unknown\n  Object Detection","summary":"  Unknown Object Detection (UOD) aims to identify objects of unseen categories,\ndiffering from the traditional detection paradigm limited by the closed-world\nassumption. A key component of UOD is learning a generalized representation,\ni.e. objectness for both known and unknown categories to distinguish and\nlocalize objects from the background in a class-agnostic manner. However,\nprevious methods obtain supervision signals for learning objectness in\nisolation from either localization or classification information, leading to\npoor performance for UOD. To address this issue, we propose a transformer-based\nUOD framework, UN-DETR. Based on this, we craft Instance Presence Score (IPS)\nto represent the probability of an object's presence. For the purpose of\ninformation complementarity, IPS employs a strategy of joint supervised\nlearning, integrating attributes representing general objectness from the\npositional and the categorical latent space as supervision signals. To enhance\nIPS learning, we introduce a one-to-many assignment strategy to incorporate\nmore supervision. Then, we propose Unbiased Query Selection to provide premium\ninitial query vectors for the decoder. Additionally, we propose an IPS-guided\npost-process strategy to filter redundant boxes and correct classification\npredictions for known and unknown objects. Finally, we pretrain the entire\nUN-DETR in an unsupervised manner, in order to obtain objectness prior. Our\nUN-DETR is comprehensively evaluated on multiple UOD and known detection\nbenchmarks, demonstrating its effectiveness and achieving state-of-the-art\nperformance.\n","authors":["Haomiao Liu","Hao Xu","Chuhuai Yue","Bo Ma"],"pdf_url":"https://arxiv.org/pdf/2412.10176v1.pdf","comment":"Accepted by AAAI-2025;15 pages, 11figures"},{"id":"http://arxiv.org/abs/2412.10159v1","updated":"2024-12-13T14:20:43Z","published":"2024-12-13T14:20:43Z","title":"Arbitrary Reading Order Scene Text Spotter with Local Semantics Guidance","summary":"  Scene text spotting has attracted the enthusiasm of relative researchers in\nrecent years. Most existing scene text spotters follow the\ndetection-then-recognition paradigm, where the vanilla detection module hardly\ndetermines the reading order and leads to failure recognition. After rethinking\nthe auto-regressive scene text recognition method, we find that a well-trained\nrecognizer can implicitly perceive the local semantics of all characters in a\ncomplete word or a sentence without a character-level detection module. Local\nsemantic knowledge not only includes text content but also spatial information\nin the right reading order. Motivated by the above analysis, we propose the\nLocal Semantics Guided scene text Spotter (LSGSpotter), which auto-regressively\ndecodes the position and content of characters guided by the local semantics.\nSpecifically, two effective modules are proposed in LSGSpotter. On the one\nhand, we design a Start Point Localization Module (SPLM) for locating text\nstart points to determine the right reading order. On the other hand, a\nMulti-scale Adaptive Attention Module (MAAM) is proposed to adaptively\naggregate text features in a local area. In conclusion, LSGSpotter achieves the\narbitrary reading order spotting task without the limitation of sophisticated\ndetection, while alleviating the cost of computational resources with the grid\nsampling strategy. Extensive experiment results show LSGSpotter achieves\nstate-of-the-art performance on the InverseText benchmark. Moreover, our\nspotter demonstrates superior performance on English benchmarks for\narbitrary-shaped text, achieving improvements of 0.7\\% and 2.5\\% on Total-Text\nand SCUT-CTW1500, respectively. These results validate our text spotter is\neffective for scene texts in arbitrary reading order and shape.\n","authors":["Jiahao Lyu","Wei Wang","Dongbao Yang","Jinwen Zhong","Yu Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.10159v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.10155v1","updated":"2024-12-13T14:12:55Z","published":"2024-12-13T14:12:55Z","title":"WordVIS: A Color Worth A Thousand Words","summary":"  Document classification is considered a critical element in automated\ndocument processing systems. In recent years multi-modal approaches have become\nincreasingly popular for document classification. Despite their improvements,\nthese approaches are underutilized in the industry due to their requirement for\na tremendous volume of training data and extensive computational power. In this\npaper, we attempt to address these issues by embedding textual features\ndirectly into the visual space, allowing lightweight image-based classifiers to\nachieve state-of-the-art results using small-scale datasets in document\nclassification. To evaluate the efficacy of the visual features generated from\nour approach on limited data, we tested on the standard dataset Tobacco-3482.\nOur experiments show a tremendous improvement in image-based classifiers,\nachieving an improvement of 4.64% using ResNet50 with no document pre-training.\nIt also sets a new record for the best accuracy of the Tobacco-3482 dataset\nwith a score of 91.14% using the image-based DocXClassifier with no document\npre-training. The simplicity of the approach, its resource requirements, and\nsubsequent results provide a good prospect for its use in industrial use cases.\n","authors":["Umar Khan"," Saifullah","Stefan Agne","Andreas Dengel","Sheraz Ahmed"],"pdf_url":"https://arxiv.org/pdf/2412.10155v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10153v1","updated":"2024-12-13T14:11:42Z","published":"2024-12-13T14:11:42Z","title":"EVOS: Efficient Implicit Neural Training via EVOlutionary Selector","summary":"  We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies.\n","authors":["Weixiang Zhang","Shuzhao Xie","Chengwei Ren","Siyi Xie","Chen Tang","Shijia Ge","Mingzi Wang","Zhi Wang"],"pdf_url":"https://arxiv.org/pdf/2412.10153v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03551v3","updated":"2024-12-13T14:11:35Z","published":"2024-03-06T08:51:09Z","title":"Enhanced Low-Dose CT Image Reconstruction by Domain and Task Shifting\n  Gaussian Denoisers","summary":"  Computed tomography from a low radiation dose (LDCT) is challenging due to\nhigh noise in the projection data. Popular approaches for LDCT image\nreconstruction are two-stage methods, typically consisting of the filtered\nbackprojection (FBP) algorithm followed by a neural network for LDCT image\nenhancement. Two-stage methods are attractive for their simplicity and\npotential for computational efficiency, typically requiring only a single FBP\nand a neural network forward pass for inference. However, the best\nreconstruction quality is currently achieved by unrolled iterative methods\n(Learned Primal-Dual and ItNet), which are more complex and thus have a higher\ncomputational cost for training and inference. We propose a method combining\nthe simplicity and efficiency of two-stage methods with state-of-the-art\nreconstruction quality. Our strategy utilizes a neural network pretrained for\nGaussian noise removal from natural grayscale images, fine-tuned for LDCT image\nenhancement. We call this method FBP-DTSGD (Domain and Task Shifted Gaussian\nDenoisers) as the fine-tuning is a task shift from Gaussian denoising to\nenhancing LDCT images and a domain shift from natural grayscale to LDCT images.\nAn ablation study with three different pretrained Gaussian denoisers indicates\nthat the performance of FBP-DTSGD does not depend on a specific denoising\narchitecture, suggesting future advancements in Gaussian denoising could\nbenefit the method. The study also shows that pretraining on natural images\nenhances LDCT reconstruction quality, especially with limited training data.\nNotably, pretraining involves no additional cost, as existing pretrained models\nare used. The proposed method currently holds the top mean position in the\nLoDoPaB-CT challenge.\n","authors":["Tim Selig","Thomas März","Martin Storath","Andreas Weinmann"],"pdf_url":"https://arxiv.org/pdf/2403.03551v3.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2412.10151v1","updated":"2024-12-13T14:11:26Z","published":"2024-12-13T14:11:26Z","title":"VLR-Bench: Multilingual Benchmark Dataset for Vision-Language Retrieval\n  Augmented Generation","summary":"  We propose the VLR-Bench, a visual question answering (VQA) benchmark for\nevaluating vision language models (VLMs) based on retrieval augmented\ngeneration (RAG). Unlike existing evaluation datasets for external\nknowledge-based VQA, the proposed VLR-Bench includes five input passages. This\nallows testing of the ability to determine which passage is useful for\nanswering a given query, a capability lacking in previous research. In this\ncontext, we constructed a dataset of 32,000 automatically generated\ninstruction-following examples, which we denote as VLR-IF. This dataset is\nspecifically designed to enhance the RAG capabilities of VLMs by enabling them\nto learn how to generate appropriate answers based on input passages. We\nevaluated the validity of the proposed benchmark and training data and verified\nits performance using the state-of-the-art Llama3-based VLM, the Llava-Llama-3\nmodel. The proposed VLR-Bench and VLR-IF datasets are publicly available\nonline.\n","authors":["Hyeonseok Lim","Dongjae Shin","Seohyun Song","Inho Won","Minjun Kim","Junghun Yuk","Haneol Jang","KyungTae Lim"],"pdf_url":"https://arxiv.org/pdf/2412.10151v1.pdf","comment":"The 31st International Conference on Computational Linguistics\n  (COLING 2025), 19 pages"},{"id":"http://arxiv.org/abs/2412.10146v1","updated":"2024-12-13T14:02:41Z","published":"2024-12-13T14:02:41Z","title":"Investigating generalization capabilities of neural networks by means of\n  loss landscapes and Hessian analysis","summary":"  This paper studies generalization capabilities of neural networks (NNs) using\nnew and improved PyTorch library Loss Landscape Analysis (LLA). LLA facilitates\nvisualization and analysis of loss landscapes along with the properties of NN\nHessian. Different approaches to NN loss landscape plotting are discussed with\nparticular focus on normalization techniques showing that conventional methods\ncannot always ensure correct visualization when batch normalization layers are\npresent in NN architecture. The use of Hessian axes is shown to be able to\nmitigate this effect, and methods for choosing Hessian axes are proposed. In\naddition, spectra of Hessian eigendecomposition are studied and it is shown\nthat typical spectra exist for a wide range of NNs. This allows to propose\nquantitative criteria for Hessian analysis that can be applied to evaluate NN\nperformance and assess its generalization capabilities. Generalization\nexperiments are conducted using ImageNet-1K pre-trained models along with\nseveral models trained as part of this study. The experiment include training\nmodels on one dataset and testing on another one to maximize experiment\nsimilarity to model performance in the Wild. It is shown that when datasets\nchange, the changes in criteria correlate with the changes in accuracy, making\nthe proposed criteria a computationally efficient estimate of generalization\nability, which is especially useful for extremely large datasets.\n","authors":["Nikita Gabdullin"],"pdf_url":"https://arxiv.org/pdf/2412.10146v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.07009v2","updated":"2024-12-13T13:56:20Z","published":"2024-08-13T16:15:50Z","title":"Imagen 3","summary":"  We introduce Imagen 3, a latent diffusion model that generates high quality\nimages from text prompts. We describe our quality and responsibility\nevaluations. Imagen 3 is preferred over other state-of-the-art (SOTA) models at\nthe time of evaluation. In addition, we discuss issues around safety and\nrepresentation, as well as methods we used to minimize the potential harm of\nour models.\n","authors":[" Imagen-Team-Google"," :","Jason Baldridge","Jakob Bauer","Mukul Bhutani","Nicole Brichtova","Andrew Bunner","Lluis Castrejon","Kelvin Chan","Yichang Chen","Sander Dieleman","Yuqing Du","Zach Eaton-Rosen","Hongliang Fei","Nando de Freitas","Yilin Gao","Evgeny Gladchenko","Sergio Gómez Colmenarejo","Mandy Guo","Alex Haig","Will Hawkins","Hexiang Hu","Huilian Huang","Tobenna Peter Igwe","Christos Kaplanis","Siavash Khodadadeh","Yelin Kim","Ksenia Konyushkova","Karol Langner","Eric Lau","Shixin Luo","Soňa Mokrá","Henna Nandwani","Yasumasa Onoe","Aäron van den Oord","Zarana Parekh","Jordi Pont-Tuset","Hang Qi","Rui Qian","Deepak Ramachandran","Poorva Rane","Abdullah Rashwan","Ali Razavi","Robert Riachi","Hansa Srinivasan","Srivatsan Srinivasan","Robin Strudel","Benigno Uria","Oliver Wang","Su Wang","Austin Waters","Chris Wolff","Auriel Wright","Zhisheng Xiao","Hao Xiong","Keyang Xu","Marc van Zee","Junlin Zhang","Katie Zhang","Wenlei Zhou","Konrad Zolna","Ola Aboubakar","Canfer Akbulut","Oscar Akerlund","Isabela Albuquerque","Nina Anderson","Marco Andreetto","Lora Aroyo","Ben Bariach","David Barker","Sherry Ben","Dana Berman","Courtney Biles","Irina Blok","Pankil Botadra","Jenny Brennan","Karla Brown","John Buckley","Rudy Bunel","Elie Bursztein","Christina Butterfield","Ben Caine","Viral Carpenter","Norman Casagrande","Ming-Wei Chang","Solomon Chang","Shamik Chaudhuri","Tony Chen","John Choi","Dmitry Churbanau","Nathan Clement","Matan Cohen","Forrester Cole","Mikhail Dektiarev","Vincent Du","Praneet Dutta","Tom Eccles","Ndidi Elue","Ashley Feden","Shlomi Fruchter","Frankie Garcia","Roopal Garg","Weina Ge","Ahmed Ghazy","Bryant Gipson","Andrew Goodman","Dawid Górny","Sven Gowal","Khyatti Gupta","Yoni Halpern","Yena Han","Susan Hao","Jamie Hayes","Jonathan Heek","Amir Hertz","Ed Hirst","Emiel Hoogeboom","Tingbo Hou","Heidi Howard","Mohamed Ibrahim","Dirichi Ike-Njoku","Joana Iljazi","Vlad Ionescu","William Isaac","Reena Jana","Gemma Jennings","Donovon Jenson","Xuhui Jia","Kerry Jones","Xiaoen Ju","Ivana Kajic","Christos Kaplanis","Burcu Karagol Ayan","Jacob Kelly","Suraj Kothawade","Christina Kouridi","Ira Ktena","Jolanda Kumakaw","Dana Kurniawan","Dmitry Lagun","Lily Lavitas","Jason Lee","Tao Li","Marco Liang","Maggie Li-Calis","Yuchi Liu","Javier Lopez Alberca","Peggy Lu","Kristian Lum","Yukun Ma","Chase Malik","John Mellor","Thomas Mensink","Inbar Mosseri","Tom Murray","Aida Nematzadeh","Paul Nicholas","João Gabriel Oliveira","Guillermo Ortiz-Jimenez","Michela Paganini","Tom Le Paine","Roni Paiss","Alicia Parrish","Anne Peckham","Vikas Peswani","Igor Petrovski","Tobias Pfaff","Alex Pirozhenko","Ryan Poplin","Utsav Prabhu","Yuan Qi","Matthew Rahtz","Cyrus Rashtchian","Charvi Rastogi","Amit Raul","Ali Razavi","Sylvestre-Alvise Rebuffi","Susanna Ricco","Felix Riedel","Dirk Robinson","Pankaj Rohatgi","Bill Rosgen","Sarah Rumbley","Moonkyung Ryu","Anthony Salgado","Tim Salimans","Sahil Singla","Florian Schroff","Candice Schumann","Tanmay Shah","Brendan Shillingford","Kaushik Shivakumar","Dennis Shtatnov","Zach Singer","Evgeny Sluzhaev","Valerii Sokolov","Thibault Sottiaux","Florian Stimberg","Brad Stone","David Stutz","Yu-Chuan Su","Eric Tabellion","Shuai Tang","David Tao","Kurt Thomas","Gregory Thornton","Andeep Toor","Cristian Udrescu","Aayush Upadhyay","Cristina Vasconcelos","Alex Vasiloff","Andrey Voynov","Amanda Walker","Luyu Wang","Miaosen Wang","Simon Wang","Stanley Wang","Qifei Wang","Yuxiao Wang","Ágoston Weisz","Olivia Wiles","Chenxia Wu","Xingyu Federico Xu","Andrew Xue","Jianbo Yang","Luo Yu","Mete Yurtoglu","Ali Zand","Han Zhang","Jiageng Zhang","Catherine Zhao","Adilet Zhaxybay","Miao Zhou","Shengqi Zhu","Zhenkai Zhu","Dawn Bloxwich","Mahyar Bordbar","Luis C. Cobo","Eli Collins","Shengyang Dai","Tulsee Doshi","Anca Dragan","Douglas Eck","Demis Hassabis","Sissie Hsiao","Tom Hume","Koray Kavukcuoglu","Helen King","Jack Krawczyk","Yeqing Li","Kathy Meier-Hellstern","Andras Orban","Yury Pinsky","Amar Subramanya","Oriol Vinyals","Ting Yu","Yori Zwols"],"pdf_url":"https://arxiv.org/pdf/2408.07009v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10137v1","updated":"2024-12-13T13:38:41Z","published":"2024-12-13T13:38:41Z","title":"Constraint-Aware Zero-Shot Vision-Language Navigation in Continuous\n  Environments","summary":"  We address the task of Vision-Language Navigation in Continuous Environments\n(VLN-CE) under the zero-shot setting. Zero-shot VLN-CE is particularly\nchallenging due to the absence of expert demonstrations for training and\nminimal environment structural prior to guide navigation. To confront these\nchallenges, we propose a Constraint-Aware Navigator (CA-Nav), which reframes\nzero-shot VLN-CE as a sequential, constraint-aware sub-instruction completion\nprocess. CA-Nav continuously translates sub-instructions into navigation plans\nusing two core modules: the Constraint-Aware Sub-instruction Manager (CSM) and\nthe Constraint-Aware Value Mapper (CVM). CSM defines the completion criteria\nfor decomposed sub-instructions as constraints and tracks navigation progress\nby switching sub-instructions in a constraint-aware manner. CVM, guided by\nCSM's constraints, generates a value map on the fly and refines it using\nsuperpixel clustering to improve navigation stability. CA-Nav achieves the\nstate-of-the-art performance on two VLN-CE benchmarks, surpassing the previous\nbest method by 12 percent and 13 percent in Success Rate on the validation\nunseen splits of R2R-CE and RxR-CE, respectively. Moreover, CA-Nav demonstrates\nits effectiveness in real-world robot deployments across various indoor scenes\nand instructions.\n","authors":["Kehan Chen","Dong An","Yan Huang","Rongtao Xu","Yifei Su","Yonggen Ling","Ian Reid","Liang Wang"],"pdf_url":"https://arxiv.org/pdf/2412.10137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10122v1","updated":"2024-12-13T13:07:08Z","published":"2024-12-13T13:07:08Z","title":"The Art of Deception: Color Visual Illusions and Diffusion Models","summary":"  Visual illusions in humans arise when interpreting out-of-distribution\nstimuli: if the observer is adapted to certain statistics, perception of\noutliers deviates from reality. Recent studies have shown that artificial\nneural networks (ANNs) can also be deceived by visual illusions. This\nrevelation raises profound questions about the nature of visual information.\nWhy are two independent systems, both human brains and ANNs, susceptible to the\nsame illusions? Should any ANN be capable of perceiving visual illusions? Are\nthese perceptions a feature or a flaw? In this work, we study how visual\nillusions are encoded in diffusion models. Remarkably, we show that they\npresent human-like brightness/color shifts in their latent space. We use this\nfact to demonstrate that diffusion models can predict visual illusions.\nFurthermore, we also show how to generate new unseen visual illusions in\nrealistic images using text-to-image diffusion models. We validate this ability\nthrough psychophysical experiments that show how our model-generated illusions\nalso fool humans.\n","authors":["Alex Gomez-Villa","Kai Wang","Alejandro C. Parraga","Bartlomiej Twardowski","Jesus Malo","Javier Vazquez-Corral","Joost van de Weijer"],"pdf_url":"https://arxiv.org/pdf/2412.10122v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10116v1","updated":"2024-12-13T12:59:12Z","published":"2024-12-13T12:59:12Z","title":"HS-FPN: High Frequency and Spatial Perception FPN for Tiny Object\n  Detection","summary":"  The introduction of Feature Pyramid Network (FPN) has significantly improved\nobject detection performance. However, substantial challenges remain in\ndetecting tiny objects, as their features occupy only a very small proportion\nof the feature maps. Although FPN integrates multi-scale features, it does not\ndirectly enhance or enrich the features of tiny objects. Furthermore, FPN lacks\nspatial perception ability. To address these issues, we propose a novel High\nFrequency and Spatial Perception Feature Pyramid Network (HS-FPN) with two\ninnovative modules. First, we designed a high frequency perception module (HFP)\nthat generates high frequency responses through high pass filters. These high\nfrequency responses are used as mask weights from both spatial and channel\nperspectives to enrich and highlight the features of tiny objects in the\noriginal feature maps. Second, we developed a spatial dependency perception\nmodule (SDP) to capture the spatial dependencies that FPN lacks. Our\nexperiments demonstrate that detectors based on HS-FPN exhibit competitive\nadvantages over state-of-the-art models on the AI-TOD dataset for tiny object\ndetection.\n","authors":["Zican Shi","Jing Hu","Jie Ren","Hengkang Ye","Xuyang Yuan","Yan Ouyang","Jia He","Bo Ji","Junyu Guo"],"pdf_url":"https://arxiv.org/pdf/2412.10116v1.pdf","comment":"13 pages,12 figures,7 tables"},{"id":"http://arxiv.org/abs/2412.10115v1","updated":"2024-12-13T12:57:47Z","published":"2024-12-13T12:57:47Z","title":"Filter or Compensate: Towards Invariant Representation from Distribution\n  Shift for Anomaly Detection","summary":"  Recent Anomaly Detection (AD) methods have achieved great success with\nIn-Distribution (ID) data. However, real-world data often exhibits distribution\nshift, causing huge performance decay on traditional AD methods. From this\nperspective, few previous work has explored AD with distribution shift, and the\ndistribution-invariant normality learning has been proposed based on the\nReverse Distillation (RD) framework. However, we observe the misalignment issue\nbetween the teacher and the student network that causes detection failure,\nthereby propose FiCo, Filter or Compensate, to address the distribution shift\nissue in AD. FiCo firstly compensates the distribution-specific information to\nreduce the misalignment between the teacher and student network via the\nDistribution-Specific Compensation (DiSCo) module, and secondly filters all\nabnormal information to capture distribution-invariant normality with the\nDistribution-Invariant Filter (DiIFi) module. Extensive experiments on three\ndifferent AD benchmarks demonstrate the effectiveness of FiCo, which\noutperforms all existing state-of-the-art (SOTA) methods, and even achieves\nbetter results on the ID scenario compared with RD-based methods. Our code is\navailable at https://github.com/znchen666/FiCo.\n","authors":["Zining Chen","Xingshuang Luo","Weiqiu Wang","Zhicheng Zhao","Fei Su","Aidong Men"],"pdf_url":"https://arxiv.org/pdf/2412.10115v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.10106v1","updated":"2024-12-13T12:47:30Z","published":"2024-12-13T12:47:30Z","title":"A Cascaded Dilated Convolution Approach for Mpox Lesion Classification","summary":"  The global outbreak of Mpox virus, classified as a Public Health Emergency of\nInternational Concern by WHO, presents significant diagnostic challenges due to\nits visual similarity to other skin lesion diseases. Current clinical detection\ntechniques face limitations in accuracy and efficiency, necessitating improved\nautomated diagnostic solutions. This study introduces a novel Cascaded Atrous\nGroup Attention (CAGA) module, specifically designed to enhance multi-scale\nfeature representation while optimizing computational efficiency. By\nintegrating CAGA with EfficientViT-L1 as the backbone architecture, our\napproach achieves state-of-the-art performance with a score of 0.98% on the\nMCSI dataset, while reducing model parameters by 37.5% compared to the original\nEfficientViT-L1. This reduction in computational complexity maintains\ndiagnostic accuracy while enabling broader deployment across\nresource-constrained healthcare settings. Extensive validation across two other\nbenchmark datasets, including MSID and MSLD, demonstrate the model's\nrobustness, consistently outperforming existing approaches. Our findings\nsuggest that CAGA's efficient feature extraction mechanism could be adapted for\nother medical imaging tasks requiring fine-grained visual discrimination.\n","authors":["Ayush Deshmukh"],"pdf_url":"https://arxiv.org/pdf/2412.10106v1.pdf","comment":"(7 pages, 2 figures, 5 tables)"},{"id":"http://arxiv.org/abs/2412.07205v2","updated":"2024-12-13T12:38:04Z","published":"2024-12-10T05:50:50Z","title":"Crack-EdgeSAM Self-Prompting Crack Segmentation System for Edge Devices","summary":"  Structural health monitoring (SHM) is essential for the early detection of\ninfrastructure defects, such as cracks in concrete bridge pier. but often faces\nchallenges in efficiency and accuracy in complex environments. Although the\nSegment Anything Model (SAM) achieves excellent segmentation performance, its\ncomputational demands limit its suitability for real-time applications on edge\ndevices. To address these challenges, this paper proposes Crack-EdgeSAM, a\nself-prompting crack segmentation system that integrates YOLOv8 for generating\nprompt boxes and a fine-tuned EdgeSAM model for crack segmentation. To ensure\ncomputational efficiency, the method employs ConvLoRA, a Parameter-Efficient\nFine-Tuning (PEFT) technique, along with DiceFocalLoss to fine-tune the EdgeSAM\nmodel. Our experimental results on public datasets and the climbing robot\nautomatic inspections demonstrate that the system achieves high segmentation\naccuracy and significantly enhanced inference speed compared to the most recent\nmethods. Notably, the system processes 1024 x 1024 pixels images at 46 FPS on\nour PC and 8 FPS on Jetson Orin Nano.\n","authors":["Yingchu Wang","Ji He","Shijie Yu"],"pdf_url":"https://arxiv.org/pdf/2412.07205v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09612v2","updated":"2024-12-13T12:27:52Z","published":"2024-12-12T18:59:40Z","title":"Olympus: A Universal Task Router for Computer Vision Tasks","summary":"  We introduce Olympus, a new approach that transforms Multimodal Large\nLanguage Models (MLLMs) into a unified framework capable of handling a wide\narray of computer vision tasks. Utilizing a controller MLLM, Olympus delegates\nover 20 specialized tasks across images, videos, and 3D objects to dedicated\nmodules. This instruction-based routing enables complex workflows through\nchained actions without the need for training heavy generative models. Olympus\neasily integrates with existing MLLMs, expanding their capabilities with\ncomparable performance. Experimental results demonstrate that Olympus achieves\nan average routing accuracy of 94.75% across 20 tasks and precision of 91.82%\nin chained action scenarios, showcasing its effectiveness as a universal task\nrouter that can solve a diverse range of computer vision tasks. Project page:\nhttp://yuanze-lin.me/Olympus_page/\n","authors":["Yuanze Lin","Yunsheng Li","Dongdong Chen","Weijian Xu","Ronald Clark","Philip H. S. Torr"],"pdf_url":"https://arxiv.org/pdf/2412.09612v2.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2412.10091v1","updated":"2024-12-13T12:27:47Z","published":"2024-12-13T12:27:47Z","title":"Data Pruning Can Do More: A Comprehensive Data Pruning Approach for\n  Object Re-identification","summary":"  Previous studies have demonstrated that not each sample in a dataset is of\nequal importance during training. Data pruning aims to remove less important or\ninformative samples while still achieving comparable results as training on the\noriginal (untruncated) dataset, thereby reducing storage and training costs.\nHowever, the majority of data pruning methods are applied to image\nclassification tasks. To our knowledge, this work is the first to explore the\nfeasibility of these pruning methods applied to object re-identification (ReID)\ntasks, while also presenting a more comprehensive data pruning approach. By\nfully leveraging the logit history during training, our approach offers a more\naccurate and comprehensive metric for quantifying sample importance, as well as\ncorrecting mislabeled samples and recognizing outliers. Furthermore, our\napproach is highly efficient, reducing the cost of importance score estimation\nby 10 times compared to existing methods. Our approach is a plug-and-play,\narchitecture-agnostic framework that can eliminate/reduce 35%, 30%, and 5% of\nsamples/training time on the VeRi, MSMT17 and Market1501 datasets,\nrespectively, with negligible loss in accuracy (< 0.1%). The lists of\nimportant, mislabeled, and outlier samples from these ReID datasets are\navailable at https://github.com/Zi-Y/data-pruning-reid.\n","authors":["Zi Yang","Haojin Yang","Soumajit Majumder","Jorge Cardoso","Guillermo Gallego"],"pdf_url":"https://arxiv.org/pdf/2412.10091v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10089v1","updated":"2024-12-13T12:25:16Z","published":"2024-12-13T12:25:16Z","title":"Guidance Not Obstruction: A Conjugate Consistent Enhanced Strategy for\n  Domain Generalization","summary":"  Domain generalization addresses domain shift in real-world applications. Most\napproaches adopt a domain angle, seeking invariant representation across\ndomains by aligning their marginal distributions, irrespective of individual\nclasses, naturally leading to insufficient exploration of discriminative\ninformation. Switching to a class angle, we find that multiple domain-related\npeaks or clusters within the same individual classes must emerge due to\ndistribution shift. In other words, marginal alignment does not guarantee\nconditional alignment, leading to suboptimal generalization. Therefore, we\nargue that acquiring discriminative generalization between classes within\ndomains is crucial. In contrast to seeking distribution alignment, we endeavor\nto safeguard domain-related between-class discrimination. To this end, we\ndevise a novel Conjugate Consistent Enhanced Module, namely Con2EM, based on a\ndistribution over domains, i.e., a meta-distribution. Specifically, we employ a\nnovel distribution-level Universum strategy to generate supplementary diverse\ndomain-related class-conditional distributions, thereby enhancing\ngeneralization. This allows us to resample from these generated distributions\nto provide feedback to the primordial instance-level classifier, further\nimproving its adaptability to the target-agnostic. To ensure generation\naccuracy, we establish an additional distribution-level classifier to\nregularize these conditional distributions. Extensive experiments have been\nconducted to demonstrate its effectiveness and low computational cost compared\nto SOTAs.\n","authors":["Meng Cao","Songcan Chen"],"pdf_url":"https://arxiv.org/pdf/2412.10089v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10084v1","updated":"2024-12-13T12:18:26Z","published":"2024-12-13T12:18:26Z","title":"ProbeSDF: Light Field Probes for Neural Surface Reconstruction","summary":"  SDF-based differential rendering frameworks have achieved state-of-the-art\nmultiview 3D shape reconstruction. In this work, we re-examine this family of\napproaches by minimally reformulating its core appearance model in a way that\nsimultaneously yields faster computation and increased performance. To this\ngoal, we exhibit a physically-inspired minimal radiance parametrization\ndecoupling angular and spatial contributions, by encoding them with a small\nnumber of features stored in two respective volumetric grids of different\nresolutions. Requiring as little as four parameters per voxel, and a tiny MLP\ncall inside a single fully fused kernel, our approach allows to enhance\nperformance with both surface and image (PSNR) metrics, while providing a\nsignificant training speedup and real-time rendering. We show this performance\nto be consistently achieved on real data over two widely different and popular\napplication fields, generic object and human subject shape reconstruction,\nusing four representative and challenging datasets.\n","authors":["Briac Toussaint","Diego Thomas","Jean-Sébastien Franco"],"pdf_url":"https://arxiv.org/pdf/2412.10084v1.pdf","comment":"10 pages, 9 figures"},{"id":"http://arxiv.org/abs/2412.10078v1","updated":"2024-12-13T12:10:53Z","published":"2024-12-13T12:10:53Z","title":"Toy-GS: Assembling Local Gaussians for Precisely Rendering Large-Scale\n  Free Camera Trajectories","summary":"  Currently, 3D rendering for large-scale free camera trajectories, namely,\narbitrary input camera trajectories, poses significant challenges: 1) The\ndistribution and observation angles of the cameras are irregular, and various\ntypes of scenes are included in the free trajectories; 2) Processing the entire\npoint cloud and all images at once for large-scale scenes requires a\nsubstantial amount of GPU memory. This paper presents a Toy-GS method for\naccurately rendering large-scale free camera trajectories. Specifically, we\npropose an adaptive spatial division approach for free trajectories to divide\ncameras and the sparse point cloud of the entire scene into various regions\naccording to camera poses. Training each local Gaussian in parallel for each\narea enables us to concentrate on texture details and minimize GPU memory\nusage. Next, we use the multi-view constraint and position-aware point adaptive\ncontrol (PPAC) to improve the rendering quality of texture details. In\naddition, our regional fusion approach combines local and global Gaussians to\nenhance rendering quality with an increasing number of divided areas. Extensive\nexperiments have been carried out to confirm the effectiveness and efficiency\nof Toy-GS, leading to state-of-the-art results on two public large-scale\ndatasets as well as our SCUTic dataset. Our proposal demonstrates an\nenhancement of 1.19 dB in PSNR and conserves 7 G of GPU memory when compared to\nvarious benchmarks.\n","authors":["Xiaohan Zhang","Zhenyu Sun","Yukui Qiu","Junyan Su","Qi Liu"],"pdf_url":"https://arxiv.org/pdf/2412.10078v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.03231v3","updated":"2024-12-13T12:09:48Z","published":"2023-03-06T15:48:33Z","title":"StyO: Stylize Your Face in Only One-shot","summary":"  This paper focuses on face stylization with a single artistic target.\nExisting works for this task often fail to retain the source content while\nachieving geometry variation. Here, we present a novel StyO model, ie. Stylize\nthe face in only One-shot, to solve the above problem. In particular, StyO\nexploits a disentanglement and recombination strategy. It first disentangles\nthe content and style of source and target images into identifiers, which are\nthen recombined in a cross manner to derive the stylized face image. In this\nway, StyO decomposes complex images into independent and specific attributes,\nand simplifies one-shot face stylization as the combination of different\nattributes from input images, thus producing results better matching face\ngeometry of target image and content of source one. StyO is implemented with\nlatent diffusion models (LDM) and composed of two key modules: 1) Identifier\nDisentanglement Learner (IDL) for disentanglement phase. It represents\nidentifiers as contrastive text prompts, ie. positive and negative\ndescriptions. And it introduces a novel triple reconstruction loss to fine-tune\nthe pre-trained LDM for encoding style and content into corresponding\nidentifiers; 2) Fine-grained Content Controller (FCC) for the recombination\nphase. It recombines disentangled identifiers from IDL to form an augmented\ntext prompt for generating stylized faces. In addition, FCC also constrains the\ncross-attention maps of latent and text features to preserve source face\ndetails in results. The extensive evaluation shows that StyO produces\nhigh-quality images on numerous paintings of various styles and outperforms the\ncurrent state-of-the-art.\n","authors":["Bonan Li","Zicheng Zhang","Xuecheng Nie","Congying Han","Yinhan Hu","Xinmin Qiu","Tiande Guo"],"pdf_url":"https://arxiv.org/pdf/2303.03231v3.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2408.12772v2","updated":"2024-12-13T12:01:44Z","published":"2024-08-23T00:15:43Z","title":"Symmetric masking strategy enhances the performance of Masked Image\n  Modeling","summary":"  Masked Image Modeling (MIM) is a technique in self-supervised learning that\nfocuses on acquiring detailed visual representations from unlabeled images by\nestimating the missing pixels in randomly masked sections. It has proven to be\na powerful tool for the preliminary training of Vision Transformers (ViTs),\nyielding impressive results across various tasks. Nevertheless, most MIM\nmethods heavily depend on the random masking strategy to formulate the pretext\ntask. This strategy necessitates numerous trials to ascertain the optimal\ndropping ratio, which can be resource-intensive, requiring the model to be\npre-trained for anywhere between 800 to 1600 epochs. Furthermore, this approach\nmay not be suitable for all datasets. In this work, we propose a new masking\nstrategy that effectively helps the model capture global and local features.\nBased on this masking strategy, SymMIM, our proposed training pipeline for MIM\nis introduced. SymMIM achieves a new SOTA accuracy of 85.9\\% on ImageNet using\nViT-Large and surpasses previous SOTA across downstream tasks such as image\nclassification, semantic segmentation, object detection, instance segmentation\ntasks, and so on.\n","authors":["Khanh-Binh Nguyen","Chae Jung Park"],"pdf_url":"https://arxiv.org/pdf/2408.12772v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10061v1","updated":"2024-12-13T11:44:56Z","published":"2024-12-13T11:44:56Z","title":"Quaffure: Real-Time Quasi-Static Neural Hair Simulation","summary":"  Realistic hair motion is crucial for high-quality avatars, but it is often\nlimited by the computational resources available for real-time applications. To\naddress this challenge, we propose a novel neural approach to predict\nphysically plausible hair deformations that generalizes to various body poses,\nshapes, and hairstyles. Our model is trained using a self-supervised loss,\neliminating the need for expensive data generation and storage. We demonstrate\nour method's effectiveness through numerous results across a wide range of pose\nand shape variations, showcasing its robust generalization capabilities and\ntemporally smooth results. Our approach is highly suitable for real-time\napplications with an inference time of only a few milliseconds on consumer\nhardware and its ability to scale to predicting the drape of 1000 grooms in 0.3\nseconds.\n","authors":["Tuur Stuyck","Gene Wei-Chin Lin","Egor Larionov","Hsiao-yu Chen","Aljaz Bozic","Nikolaos Sarafianos","Doug Roble"],"pdf_url":"https://arxiv.org/pdf/2412.10061v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17832v2","updated":"2024-12-13T11:40:57Z","published":"2024-11-26T19:13:38Z","title":"SVGDreamer++: Advancing Editability and Diversity in Text-Guided SVG\n  Generation","summary":"  Recently, text-guided scalable vector graphics (SVG) synthesis has\ndemonstrated significant potential in domains such as iconography and\nsketching. However, SVGs generated from existing Text-to-SVG methods often lack\neditability and exhibit deficiencies in visual quality and diversity. In this\npaper, we propose a novel text-guided vector graphics synthesis method to\naddress these limitations. To enhance the editability of output SVGs, we\nintroduce a Hierarchical Image VEctorization (HIVE) framework that operates at\nthe semantic object level and supervises the optimization of components within\nthe vector object. This approach facilitates the decoupling of vector graphics\ninto distinct objects and component levels. Our proposed HIVE algorithm,\ninformed by image segmentation priors, not only ensures a more precise\nrepresentation of vector graphics but also enables fine-grained editing\ncapabilities within vector objects. To improve the diversity of output SVGs, we\npresent a Vectorized Particle-based Score Distillation (VPSD) approach. VPSD\naddresses over-saturation issues in existing methods and enhances sample\ndiversity. A pre-trained reward model is incorporated to re-weight vector\nparticles, improving aesthetic appeal and enabling faster convergence.\nAdditionally, we design a novel adaptive vector primitives control strategy,\nwhich allows for the dynamic adjustment of the number of primitives, thereby\nenhancing the presentation of graphic details. Extensive experiments validate\nthe effectiveness of the proposed method, demonstrating its superiority over\nbaseline methods in terms of editability, visual quality, and diversity. We\nalso show that our new method supports up to six distinct vector styles,\ncapable of generating high-quality vector assets suitable for stylized vector\ndesign and poster design. Code and demo will be released at:\nhttp://ximinng.github.io/SVGDreamerV2Project/\n","authors":["Ximing Xing","Qian Yu","Chuang Wang","Haitao Zhou","Jing Zhang","Dong Xu"],"pdf_url":"https://arxiv.org/pdf/2411.17832v2.pdf","comment":"17 pages, 17 figures. Project Page:\n  http://ximinng.github.io/SVGDreamerV2Project/. arXiv admin note: text overlap\n  with arXiv:2312.16476"},{"id":"http://arxiv.org/abs/2412.10051v1","updated":"2024-12-13T11:26:38Z","published":"2024-12-13T11:26:38Z","title":"TSGaussian: Semantic and Depth-Guided Target-Specific Gaussian Splatting\n  from Sparse Views","summary":"  Recent advances in Gaussian Splatting have significantly advanced the field,\nachieving both panoptic and interactive segmentation of 3D scenes. However,\nexisting methodologies often overlook the critical need for reconstructing\nspecified targets with complex structures from sparse views. To address this\nissue, we introduce TSGaussian, a novel framework that combines semantic\nconstraints with depth priors to avoid geometry degradation in challenging\nnovel view synthesis tasks. Our approach prioritizes computational resources on\ndesignated targets while minimizing background allocation. Bounding boxes from\nYOLOv9 serve as prompts for Segment Anything Model to generate 2D mask\npredictions, ensuring semantic accuracy and cost efficiency. TSGaussian\neffectively clusters 3D gaussians by introducing a compact identity encoding\nfor each Gaussian ellipsoid and incorporating 3D spatial consistency\nregularization. Leveraging these modules, we propose a pruning strategy to\neffectively reduce redundancy in 3D gaussians. Extensive experiments\ndemonstrate that TSGaussian outperforms state-of-the-art methods on three\nstandard datasets and a new challenging dataset we collected, achieving\nsuperior results in novel view synthesis of specific objects. Code is available\nat: https://github.com/leon2000-ai/TSGaussian.\n","authors":["Liang Zhao","Zehan Bao","Yi Xie","Hong Chen","Yaohui Chen","Weifu Li"],"pdf_url":"https://arxiv.org/pdf/2412.10051v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10050v1","updated":"2024-12-13T11:22:01Z","published":"2024-12-13T11:22:01Z","title":"ManipGPT: Is Affordance Segmentation by Large Vision Models Enough for\n  Articulated Object Manipulation?","summary":"  Visual actionable affordance has emerged as a transformative approach in\nrobotics, focusing on perceiving interaction areas prior to manipulation.\nTraditional methods rely on pixel sampling to identify successful interaction\nsamples or processing pointclouds for affordance mapping. However, these\napproaches are computationally intensive and struggle to adapt to diverse and\ndynamic environments. This paper introduces ManipGPT, a framework designed to\npredict optimal interaction areas for articulated objects using a large\npre-trained vision transformer (ViT). We created a dataset of 9.9k simulated\nand real images to bridge the sim-to-real gap and enhance real-world\napplicability. By fine-tuning the vision transformer on this small dataset, we\nsignificantly improved part-level affordance segmentation, adapting the model's\nin-context segmentation capabilities to robot manipulation scenarios. This\nenables effective manipulation across simulated and real-world environments by\ngenerating part-level affordance masks, paired with an impedance adaptation\npolicy, sufficiently eliminating the need for complex datasets or perception\nsystems.\n","authors":["Taewhan Kim","Hojin Bae","Zeming Li","Xiaoqi Li","Iaroslav Ponomarenko","Ruihai Wu","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2412.10050v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.10049v1","updated":"2024-12-13T11:20:59Z","published":"2024-12-13T11:20:59Z","title":"SuperMark: Robust and Training-free Image Watermarking via\n  Diffusion-based Super-Resolution","summary":"  In today's digital landscape, the blending of AI-generated and authentic\ncontent has underscored the need for copyright protection and content\nauthentication. Watermarking has become a vital tool to address these\nchallenges, safeguarding both generated and real content. Effective\nwatermarking methods must withstand various distortions and attacks. Current\ndeep watermarking techniques often use an encoder-noise layer-decoder\narchitecture and include distortions to enhance robustness. However, they\nstruggle to balance robustness and fidelity and remain vulnerable to adaptive\nattacks, despite extensive training. To overcome these limitations, we propose\nSuperMark, a robust, training-free watermarking framework. Inspired by the\nparallels between watermark embedding/extraction in watermarking and the\ndenoising/noising processes in diffusion models, SuperMark embeds the watermark\ninto initial Gaussian noise using existing techniques. It then applies\npre-trained Super-Resolution (SR) models to denoise the watermarked noise,\nproducing the final watermarked image. For extraction, the process is reversed:\nthe watermarked image is inverted back to the initial watermarked noise via\nDDIM Inversion, from which the embedded watermark is extracted. This flexible\nframework supports various noise injection methods and diffusion-based SR\nmodels, enabling enhanced customization. The robustness of the DDIM Inversion\nprocess against perturbations allows SuperMark to achieve strong resilience to\ndistortions while maintaining high fidelity. Experiments demonstrate that\nSuperMark achieves fidelity comparable to existing methods while significantly\nimproving robustness. Under standard distortions, it achieves an average\nwatermark extraction accuracy of 99.46%, and 89.29% under adaptive attacks.\nMoreover, SuperMark shows strong transferability across datasets, SR models,\nembedding methods, and resolutions.\n","authors":["Runyi Hu","Jie Zhang","Yiming Li","Jiwei Li","Qing Guo","Han Qiu","Tianwei Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.10049v1.pdf","comment":"robust image watermarking"},{"id":"http://arxiv.org/abs/2412.10040v1","updated":"2024-12-13T11:00:57Z","published":"2024-12-13T11:00:57Z","title":"RemDet: Rethinking Efficient Model Design for UAV Object Detection","summary":"  Object detection in Unmanned Aerial Vehicle (UAV) images has emerged as a\nfocal area of research, which presents two significant challenges: i) objects\nare typically small and dense within vast images; ii) computational resource\nconstraints render most models unsuitable for real-time deployment. Current\nreal-time object detectors are not optimized for UAV images, and complex\nmethods designed for small object detection often lack real-time capabilities.\nTo address these challenges, we propose a novel detector, RemDet (Reparameter\nefficient multiplication Detector). Our contributions are as follows: 1)\nRethinking the challenges of existing detectors for small and dense UAV images,\nand proposing information loss as a design guideline for efficient models. 2)\nWe introduce the ChannelC2f module to enhance small object detection\nperformance, demonstrating that high-dimensional representations can\neffectively mitigate information loss. 3) We design the GatedFFN module to\nprovide not only strong performance but also low latency, effectively\naddressing the challenges of real-time detection. Our research reveals that\nGatedFFN, through the use of multiplication, is more cost-effective than\nfeed-forward networks for high-dimensional representation. 4) We propose the\nCED module, which combines the advantages of ViT and CNN downsampling to\neffectively reduce information loss. It specifically enhances context\ninformation for small and dense objects. Extensive experiments on large UAV\ndatasets, Visdrone and UAVDT, validate the real-time efficiency and superior\nperformance of our methods. On the challenging UAV dataset VisDrone, our\nmethods not only provided state-of-the-art results, improving detection by more\nthan 3.4%, but also achieve 110 FPS on a single 4090.Codes are available at\n(this URL)(https://github.com/HZAI-ZJNU/RemDet).\n","authors":["Chen Li","Rui Zhao","Zeyu Wang","Huiying Xu","Xinzhong Zhu"],"pdf_url":"https://arxiv.org/pdf/2412.10040v1.pdf","comment":"Accepted to AAAI25"},{"id":"http://arxiv.org/abs/2412.10033v1","updated":"2024-12-13T10:48:38Z","published":"2024-12-13T10:48:38Z","title":"Timealign: A multi-modal object detection method for time misalignment\n  fusing in autonomous driving","summary":"  The multi-modal perception methods are thriving in the autonomous driving\nfield due to their better usage of complementary data from different sensors.\nSuch methods depend on calibration and synchronization between sensors to get\naccurate environmental information. There have already been studies about\nspace-alignment robustness in autonomous driving object detection process,\nhowever, the research for time-alignment is relatively few. As in reality\nexperiments, LiDAR point clouds are more challenging for real-time data\ntransfer, our study used historical frames of LiDAR to better align features\nwhen the LiDAR data lags exist. We designed a Timealign module to predict and\ncombine LiDAR features with observation to tackle such time misalignment based\non SOTA GraphBEV framework.\n","authors":["Zhihang Song","Lihui Peng","Jianming Hu","Danya Yao","Yi Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.10033v1.pdf","comment":"8 pages, 3 figures"},{"id":"http://arxiv.org/abs/2412.10032v1","updated":"2024-12-13T10:47:05Z","published":"2024-12-13T10:47:05Z","title":"Object-Focused Data Selection for Dense Prediction Tasks","summary":"  Dense prediction tasks such as object detection and segmentation require\nhigh-quality labels at pixel level, which are costly to obtain. Recent advances\nin foundation models have enabled the generation of autolabels, which we find\nto be competitive but not yet sufficient to fully replace human annotations,\nespecially for more complex datasets. Thus, we consider the challenge of\nselecting a representative subset of images for labeling from a large pool of\nunlabeled images under a constrained annotation budget. This task is further\ncomplicated by imbalanced class distributions, as rare classes are often\nunderrepresented in selected subsets. We propose object-focused data selection\n(OFDS) which leverages object-level representations to ensure that the selected\nimage subsets semantically cover the target classes, including rare ones. We\nvalidate OFDS on PASCAL VOC and Cityscapes for object detection and semantic\nsegmentation tasks. Our experiments demonstrate that prior methods which employ\nimage-level representations fail to consistently outperform random selection.\nIn contrast, OFDS consistently achieves state-of-the-art performance with\nsubstantial improvements over all baselines in scenarios with imbalanced class\ndistributions. Moreover, we demonstrate that pre-training with autolabels on\nthe full datasets before fine-tuning on human-labeled subsets selected by OFDS\nfurther enhances the final performance.\n","authors":["Niclas Popp","Dan Zhang","Jan Hendrik Metzen","Matthias Hein","Lukas Schott"],"pdf_url":"https://arxiv.org/pdf/2412.10032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10031v1","updated":"2024-12-13T10:45:25Z","published":"2024-12-13T10:45:25Z","title":"FM2S: Self-Supervised Fluorescence Microscopy Denoising With Single\n  Noisy Image","summary":"  Fluorescence microscopy has significantly advanced biological research by\nvisualizing detailed cellular structures and biological processes. However,\nsuch image denoising task often faces challenges due to difficulty in precisely\nmodeling the inherent noise and acquiring clean images for training, which\nconstrains most existing methods. In this paper, we propose an efficient\nself-supervised denoiser Fluorescence Micrograph to Self (FM2S), enabling a\nhigh-quality denoised result with a single noisy image. Our method introduces\nan adaptive global-local Noise Addition module for data augmentation,\naddressing generalization problems caused by discrepancies between synthetic\nand real-world noise. We then train a two-layer neural network to learn the\nmapping from the noise-added image to the filtered image, achieving a balance\nbetween noise removal and computational efficiency. Experimental results\ndemonstrate that FM2S excels in various microscope types and noise levels in\nterms of denoising effects and time consumption, obtaining an average PSNR\nimprovement of around 6 dB over the original noisy image in a few seconds. The\ncode is available at https://github.com/Danielement321/FM2S.\n","authors":["Jizhihui Liu","Qixun Teng","Junjun Jiang"],"pdf_url":"https://arxiv.org/pdf/2412.10031v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10029v1","updated":"2024-12-13T10:39:31Z","published":"2024-12-13T10:39:31Z","title":"Enhancing Fine-Grained Vision-Language Pretraining with Negative\n  Augmented Samples","summary":"  Existing Vision-Language Pretraining (VLP) methods have achieved remarkable\nimprovements across a variety of vision-language tasks, confirming their\neffectiveness in capturing coarse-grained semantic correlations. However, their\ncapability for fine-grained understanding, which is critical for many nuanced\nvision-language applications, remains limited. Prevailing VLP models often\noverlook the intricate distinctions in expressing different modal features and\ntypically depend on the similarity of holistic features for cross-modal\ninteractions. Moreover, these models directly align and integrate features from\ndifferent modalities, focusing more on coarse-grained general representations,\nthus failing to capture the nuanced differences necessary for tasks demanding a\nmore detailed perception. In response to these limitations, we introduce\nNegative Augmented Samples(NAS), a refined vision-language pretraining model\nthat innovatively incorporates NAS to specifically address the challenge of\nfine-grained understanding. NAS utilizes a Visual Dictionary(VD) as a semantic\nbridge between visual and linguistic domains. Additionally, it employs a\nNegative Visual Augmentation(NVA) method based on the VD to generate\nchallenging negative image samples. These samples deviate from positive samples\nexclusively at the token level, thereby necessitating that the model discerns\nthe subtle disparities between positive and negative samples with greater\nprecision. Comprehensive experiments validate the efficacy of NAS components\nand underscore its potential to enhance fine-grained vision-language\ncomprehension.\n","authors":["Yeyuan Wang","Dehong Gao","Lei Yi","Linbo Jin","Jinxia Zhang","Libin Yang","Xiaoyan Cai"],"pdf_url":"https://arxiv.org/pdf/2412.10029v1.pdf","comment":"15pages, Accepted by AAAI2025, full paper"},{"id":"http://arxiv.org/abs/2412.10028v1","updated":"2024-12-13T10:39:27Z","published":"2024-12-13T10:39:27Z","title":"Mr. DETR: Instructive Multi-Route Training for Detection Transformers","summary":"  Existing methods enhance the training of detection transformers by\nincorporating an auxiliary one-to-many assignment. In this work, we treat the\nmodel as a multi-task framework, simultaneously performing one-to-one and\none-to-many predictions. We investigate the roles of each component in the\ntransformer decoder across these two training targets, including\nself-attention, cross-attention, and feed-forward network. Our empirical\nresults demonstrate that any independent component in the decoder can\neffectively learn both targets simultaneously, even when other components are\nshared. This finding leads us to propose a multi-route training mechanism,\nfeaturing a primary route for one-to-one prediction and two auxiliary training\nroutes for one-to-many prediction. We enhance the training mechanism with a\nnovel instructive self-attention that dynamically and flexibly guides object\nqueries for one-to-many prediction. The auxiliary routes are removed during\ninference, ensuring no impact on model architecture or inference cost. We\nconduct extensive experiments on various baselines, achieving consistent\nimprovements as shown in Figure 1.\n","authors":["Chang-Bin Zhang","Yujie Zhong","Kai Han"],"pdf_url":"https://arxiv.org/pdf/2412.10028v1.pdf","comment":"Tech. report"},{"id":"http://arxiv.org/abs/2407.07627v2","updated":"2024-12-13T10:09:55Z","published":"2024-07-10T13:07:39Z","title":"Synthetic to Authentic: Transferring Realism to 3D Face Renderings for\n  Boosting Face Recognition","summary":"  In this paper, we investigate the potential of image-to-image translation\n(I2I) techniques for transferring realism to 3D-rendered facial images in the\ncontext of Face Recognition (FR) systems. The primary motivation for using\n3D-rendered facial images lies in their ability to circumvent the challenges\nassociated with collecting large real face datasets for training FR systems.\nThese images are generated entirely by 3D rendering engines, facilitating the\ngeneration of synthetic identities. However, it has been observed that FR\nsystems trained on such synthetic datasets underperform when compared to those\ntrained on real datasets, on various FR benchmarks. In this work, we\ndemonstrate that by transferring the realism to 3D-rendered images (i.e.,\nmaking the 3D-rendered images look more real), we can boost the performance of\nFR systems trained on these more photorealistic images. This improvement is\nevident when these systems are evaluated against FR benchmarks utilizing\nreal-world data, thereby paving new pathways for employing synthetic data in\nreal-world applications.\n","authors":["Parsa Rahimi","Behrooz Razeghi","Sebastien Marcel"],"pdf_url":"https://arxiv.org/pdf/2407.07627v2.pdf","comment":"ECCV24 Synthetic Data for Computer Vision (Oral)"},{"id":"http://arxiv.org/abs/2412.09602v2","updated":"2024-12-13T09:51:22Z","published":"2024-12-12T18:59:13Z","title":"Hidden Biases of End-to-End Driving Datasets","summary":"  End-to-end driving systems have made rapid progress, but have so far not been\napplied to the challenging new CARLA Leaderboard 2.0. Further, while there is a\nlarge body of literature on end-to-end architectures and training strategies,\nthe impact of the training dataset is often overlooked. In this work, we make a\nfirst attempt at end-to-end driving for Leaderboard 2.0. Instead of\ninvestigating architectures, we systematically analyze the training dataset,\nleading to new insights: (1) Expert style significantly affects downstream\npolicy performance. (2) In complex data sets, the frames should not be weighted\non the basis of simplistic criteria such as class frequencies. (3) Instead,\nestimating whether a frame changes the target labels compared to previous\nframes can reduce the size of the dataset without removing important\ninformation. By incorporating these findings, our model ranks first and second\nrespectively on the map and sensors tracks of the 2024 CARLA Challenge, and\nsets a new state-of-the-art on the Bench2Drive test routes. Finally, we uncover\na design flaw in the current evaluation metrics and propose a modification for\nfuture challenges. Our dataset, code, and pre-trained models are publicly\navailable at https://github.com/autonomousvision/carla_garage.\n","authors":["Julian Zimmerlin","Jens Beißwenger","Bernhard Jaeger","Andreas Geiger","Kashyap Chitta"],"pdf_url":"https://arxiv.org/pdf/2412.09602v2.pdf","comment":"Technical report for the CVPR 2024 Workshop on Foundation Models for\n  Autonomous Systems. Runner-up of the track 'CARLA Autonomous Driving\n  Challenge' in the 2024 Autonomous Grand Challenge\n  (https://opendrivelab.com/challenge2024/)"},{"id":"http://arxiv.org/abs/2412.08378v2","updated":"2024-12-13T09:49:53Z","published":"2024-12-11T13:41:21Z","title":"HyViLM: Enhancing Fine-Grained Recognition with a Hybrid Encoder for\n  Vision-Language Models","summary":"  Recently, there has been growing interest in the capability of multimodal\nlarge language models (MLLMs) to process high-resolution images. A common\napproach currently involves dynamically cropping the original high-resolution\nimage into smaller sub-images, which are then fed into a vision encoder that\nwas pre-trained on lower-resolution images. However, this cropping approach\noften truncates objects and connected areas in the original image, causing\nsemantic breaks. To address this limitation, we introduce HyViLM, designed to\nprocess images of any resolution while retaining the overall context during\nencoding. Specifically, we: (i) Design a new visual encoder called Hybrid\nEncoder that not only encodes individual sub-images but also interacts with\ndetailed global visual features, significantly improving the model's ability to\nencode high-resolution images. (ii) Propose an optimal feature fusion strategy\nfor the dynamic cropping approach, effectively leveraging information from\ndifferent layers of the vision encoder. Compared with the state-of-the-art\nMLLMs under the same setting, our HyViLM outperforms existing MLLMs in nine out\nof ten tasks. Specifically, HyViLM achieves a 9.6% improvement in performance\non the TextVQA task and a 6.9% enhancement on the DocVQA task.\n","authors":["Shiding Zhu","Wenhui Dong","Jun Song","Yingbo Wang","Yanan Guo","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2412.08378v2.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2412.10004v1","updated":"2024-12-13T09:41:48Z","published":"2024-12-13T09:41:48Z","title":"NeRF-Texture: Synthesizing Neural Radiance Field Textures","summary":"  Texture synthesis is a fundamental problem in computer graphics that would\nbenefit various applications. Existing methods are effective in handling 2D\nimage textures. In contrast, many real-world textures contain meso-structure in\nthe 3D geometry space, such as grass, leaves, and fabrics, which cannot be\neffectively modeled using only 2D image textures. We propose a novel texture\nsynthesis method with Neural Radiance Fields (NeRF) to capture and synthesize\ntextures from given multi-view images. In the proposed NeRF texture\nrepresentation, a scene with fine geometric details is disentangled into the\nmeso-structure textures and the underlying base shape. This allows textures\nwith meso-structure to be effectively learned as latent features situated on\nthe base shape, which are fed into a NeRF decoder trained simultaneously to\nrepresent the rich view-dependent appearance. Using this implicit\nrepresentation, we can synthesize NeRF-based textures through patch matching of\nlatent features. However, inconsistencies between the metrics of the\nreconstructed content space and the latent feature space may compromise the\nsynthesis quality. To enhance matching performance, we further regularize the\ndistribution of latent features by incorporating a clustering constraint. In\naddition to generating NeRF textures over a planar domain, our method can also\nsynthesize NeRF textures over curved surfaces, which are practically useful.\nExperimental results and evaluations demonstrate the effectiveness of our\napproach.\n","authors":["Yi-Hua Huang","Yan-Pei Cao","Yu-Kun Lai","Ying Shan","Lin Gao"],"pdf_url":"https://arxiv.org/pdf/2412.10004v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10002v1","updated":"2024-12-13T09:40:37Z","published":"2024-12-13T09:40:37Z","title":"NowYouSee Me: Context-Aware Automatic Audio Description","summary":"  Audio Description (AD) plays a pivotal role as an application system aimed at\nguaranteeing accessibility in multimedia content, which provides additional\nnarrations at suitable intervals to describe visual elements, catering\nspecifically to the needs of visually impaired audiences. In this paper, we\nintroduce $\\mathrm{CA^3D}$, the pioneering unified Context-Aware Automatic\nAudio Description system that provides AD event scripts with precise locations\nin the long cinematic content. Specifically, $\\mathrm{CA^3D}$ system consists\nof: 1) a Temporal Feature Enhancement Module to efficiently capture longer term\ndependencies, 2) an anchor-based AD event detector with feature suppression\nmodule that localizes the AD events and extracts discriminative feature for AD\ngeneration, and 3) a self-refinement module that leverages the generated output\nto tweak AD event boundaries from coarse to fine. Unlike conventional methods\nwhich rely on metadata and ground truth AD timestamp for AD detection and\ngeneration tasks, the proposed $\\mathrm{CA^3D}$ is the first end-to-end\ntrainable system that only uses visual cue. Extensive experiments demonstrate\nthat the proposed $\\mathrm{CA^3D}$ improves existing architectures for both AD\nevent detection and script generation metrics, establishing the new\nstate-of-the-art performances in the AD automation.\n","authors":["Seon-Ho Lee","Jue Wang","David Fan","Zhikang Zhang","Linda Liu","Xiang Hao","Vimal Bhat","Xinyu Li"],"pdf_url":"https://arxiv.org/pdf/2412.10002v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2412.09998v1","updated":"2024-12-13T09:35:34Z","published":"2024-12-13T09:35:34Z","title":"Cycle-Consistent Bridge Diffusion Model for Accelerated MRI\n  Reconstruction","summary":"  Accelerated MRI reconstruction techniques aim to reduce examination time\nwhile maintaining high image fidelity, which is highly desirable in clinical\nsettings for improving patient comfort and hospital efficiency. Existing deep\nlearning methods typically reconstruct images from under-sampled data with\ntraditional reconstruction approaches, but they still struggle to provide\nhigh-fidelity results. Diffusion models show great potential to improve\nfidelity of generated images in recent years. However, their inference process\nstarting with a random Gaussian noise introduces instability into the results\nand usually requires thousands of sampling steps, resulting in sub-optimal\nreconstruction quality and low efficiency. To address these challenges, we\npropose Cycle-Consistent Bridge Diffusion Model (CBDM). CBDM employs two bridge\ndiffusion models to construct a cycle-consistent diffusion process with a\nconsistency loss, enhancing the fine-grained details of reconstructed images\nand reducing the number of diffusion steps. Moreover, CBDM incorporates a\nContourlet Decomposition Embedding Module (CDEM) which captures multi-scale\nstructural texture knowledge in images through frequency domain decomposition\npyramids and directional filter banks to improve structural fidelity. Extensive\nexperiments demonstrate the superiority of our model by higher reconstruction\nquality and fewer training iterations, achieving a new state of the art for\naccelerated MRI reconstruction in both fastMRI and IXI datasets.\n","authors":["Tao Song","Yicheng Wu","Minhao Hu","Xiangde Luo","Guoting Luo","Guotai Wang","Yi Guo","Feng Xu","Shaoting Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.09998v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09997v1","updated":"2024-12-13T09:32:08Z","published":"2024-12-13T09:32:08Z","title":"GT23D-Bench: A Comprehensive General Text-to-3D Generation Benchmark","summary":"  Recent advances in General Text-to-3D (GT23D) have been significant. However,\nthe lack of a benchmark has hindered systematic evaluation and progress due to\nissues in datasets and metrics: 1) The largest 3D dataset Objaverse suffers\nfrom omitted annotations, disorganization, and low-quality. 2) Existing metrics\nonly evaluate textual-image alignment without considering the 3D-level quality.\nTo this end, we are the first to present a comprehensive benchmark for GT23D\ncalled GT23D-Bench consisting of: 1) a 400k high-fidelity and well-organized 3D\ndataset that curated issues in Objaverse through a systematical\nannotation-organize-filter pipeline; and 2) comprehensive 3D-aware evaluation\nmetrics which encompass 10 clearly defined metrics thoroughly accounting for\nmulti-dimension of GT23D. Notably, GT23D-Bench features three properties: 1)\nMultimodal Annotations. Our dataset annotates each 3D object with 64-view depth\nmaps, normal maps, rendered images, and coarse-to-fine captions. 2) Holistic\nEvaluation Dimensions. Our metrics are dissected into a) Textual-3D Alignment\nmeasures textual alignment with multi-granularity visual 3D representations;\nand b) 3D Visual Quality which considers texture fidelity, multi-view\nconsistency, and geometry correctness. 3) Valuable Insights. We delve into the\nperformance of current GT23D baselines across different evaluation dimensions\nand provide insightful analysis. Extensive experiments demonstrate that our\nannotations and metrics are aligned with human preferences.\n","authors":["Sitong Su","Xiao Cai","Lianli Gao","Pengpeng Zeng","Qinhong Du","Mengqi Li","Heng Tao Shen","Jingkuan Song"],"pdf_url":"https://arxiv.org/pdf/2412.09997v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09991v1","updated":"2024-12-13T09:25:18Z","published":"2024-12-13T09:25:18Z","title":"Visual Object Tracking across Diverse Data Modalities: A Review","summary":"  Visual Object Tracking (VOT) is an attractive and significant research area\nin computer vision, which aims to recognize and track specific targets in video\nsequences where the target objects are arbitrary and class-agnostic. The VOT\ntechnology could be applied in various scenarios, processing data of diverse\nmodalities such as RGB, thermal infrared and point cloud. Besides, since no one\nsensor could handle all the dynamic and varying environments, multi-modal VOT\nis also investigated. This paper presents a comprehensive survey of the recent\nprogress of both single-modal and multi-modal VOT, especially the deep learning\nmethods. Specifically, we first review three types of mainstream single-modal\nVOT, including RGB, thermal infrared and point cloud tracking. In particular,\nwe conclude four widely-used single-modal frameworks, abstracting their schemas\nand categorizing the existing inheritors. Then we summarize four kinds of\nmulti-modal VOT, including RGB-Depth, RGB-Thermal, RGB-LiDAR and RGB-Language.\nMoreover, the comparison results in plenty of VOT benchmarks of the discussed\nmodalities are presented. Finally, we provide recommendations and insightful\nobservations, inspiring the future development of this fast-growing literature.\n","authors":["Mengmeng Wang","Teli Ma","Shuo Xin","Xiaojun Hou","Jiazheng Xing","Guang Dai","Jingdong Wang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2412.09991v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09982v1","updated":"2024-12-13T09:09:14Z","published":"2024-12-13T09:09:14Z","title":"SplineGS: Robust Motion-Adaptive Spline for Real-Time Dynamic 3D\n  Gaussians from Monocular Video","summary":"  Synthesizing novel views from in-the-wild monocular videos is challenging due\nto scene dynamics and the lack of multi-view cues. To address this, we propose\nSplineGS, a COLMAP-free dynamic 3D Gaussian Splatting (3DGS) framework for\nhigh-quality reconstruction and fast rendering from monocular videos. At its\ncore is a novel Motion-Adaptive Spline (MAS) method, which represents\ncontinuous dynamic 3D Gaussian trajectories using cubic Hermite splines with a\nsmall number of control points. For MAS, we introduce a Motion-Adaptive Control\npoints Pruning (MACP) method to model the deformation of each dynamic 3D\nGaussian across varying motions, progressively pruning control points while\nmaintaining dynamic modeling integrity. Additionally, we present a joint\noptimization strategy for camera parameter estimation and 3D Gaussian\nattributes, leveraging photometric and geometric consistency. This eliminates\nthe need for Structure-from-Motion preprocessing and enhances SplineGS's\nrobustness in real-world conditions. Experiments show that SplineGS\nsignificantly outperforms state-of-the-art methods in novel view synthesis\nquality for dynamic scenes from monocular videos, achieving thousands times\nfaster rendering speed.\n","authors":["Jongmin Park","Minh-Quan Viet Bui","Juan Luis Gonzalez Bello","Jaeho Moon","Jihyong Oh","Munchurl Kim"],"pdf_url":"https://arxiv.org/pdf/2412.09982v1.pdf","comment":"The first two authors contributed equally to this work (equal\n  contribution). The last two authors advised equally to this work. Please\n  visit our project page at this https://kaist-viclab.github.io/splinegs-site/"},{"id":"http://arxiv.org/abs/2412.09981v1","updated":"2024-12-13T09:08:02Z","published":"2024-12-13T09:08:02Z","title":"SUMI-IFL: An Information-Theoretic Framework for Image Forgery\n  Localization with Sufficiency and Minimality Constraints","summary":"  Image forgery localization (IFL) is a crucial technique for preventing\ntampered image misuse and protecting social safety. However, due to the rapid\ndevelopment of image tampering technologies, extracting more comprehensive and\naccurate forgery clues remains an urgent challenge. To address these\nchallenges, we introduce a novel information-theoretic IFL framework named\nSUMI-IFL that imposes sufficiency-view and minimality-view constraints on\nforgery feature representation. First, grounded in the theoretical analysis of\nmutual information, the sufficiency-view constraint is enforced on the feature\nextraction network to ensure that the latent forgery feature contains\ncomprehensive forgery clues. Considering that forgery clues obtained from a\nsingle aspect alone may be incomplete, we construct the latent forgery feature\nby integrating several individual forgery features from multiple perspectives.\nSecond, based on the information bottleneck, the minimality-view constraint is\nimposed on the feature reasoning network to achieve an accurate and concise\nforgery feature representation that counters the interference of task-unrelated\nfeatures. Extensive experiments show the superior performance of SUMI-IFL to\nexisting state-of-the-art methods, not only on in-dataset comparisons but also\non cross-dataset comparisons.\n","authors":["Ziqi Sheng","Wei Lu","Xiangyang Luo","Jiantao Zhou","Xiaochun Cao"],"pdf_url":"https://arxiv.org/pdf/2412.09981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.06251v3","updated":"2024-12-13T09:02:22Z","published":"2024-02-09T08:59:37Z","title":"Single Channel EEG Based Insomnia Identification Without Sleep Stage\n  Annotations","summary":"  This paper proposes a new approach to identifying patients with insomnia\nusing a single EEG channel, without the need for sleep stage annotation. Data\npreprocessing, feature extraction, feature selection, and classification\ntechniques are used to automatically detect insomnia based on features\nextracted from spectral and temporal domains, including relative power in the\ndelta, sigma, beta and gamma bands, total power, absolute slow wave power,\npower ratios, mean, zero crossing rate, mobility, and complexity. A Pearson\ncorrelation coefficient, t-test, p-value, and two rules are used to select the\noptimal set of features for accurately classifying insomnia patients and\nrejecting negatively affecting features. Classification schemes including a\ngeneral artificial neural network, convolutional neural network, and support\nvector machine are applied to the optimal feature set to distinguish between\ninsomnia patients and healthy subjects. The performance of the model is\nvalidated using 50 insomnia patients and 50 healthy subjects, with the Fp2\nchannel and 1D-CNN classifier achieving the highest accuracy and Cohen's kappa\ncoefficient at 97.85% and 94.15%, respectively. The developed model has the\npotential to simplify current sleep monitoring systems and enable in-home\nambulatory monitoring.\n","authors":["Chan-Yun Yang","Nilantha Premakumara","Hooman Samani","Chinthaka Premachandra"],"pdf_url":"https://arxiv.org/pdf/2402.06251v3.pdf","comment":"30 Pages, 9 figures and 12 tables"},{"id":"http://arxiv.org/abs/2412.09966v1","updated":"2024-12-13T08:49:25Z","published":"2024-12-13T08:49:25Z","title":"EP-CFG: Energy-Preserving Classifier-Free Guidance","summary":"  Classifier-free guidance (CFG) is widely used in diffusion models but often\nintroduces over-contrast and over-saturation artifacts at higher guidance\nstrengths. We present EP-CFG (Energy-Preserving Classifier-Free Guidance),\nwhich addresses these issues by preserving the energy distribution of the\nconditional prediction during the guidance process. Our method simply rescales\nthe energy of the guided output to match that of the conditional prediction at\neach denoising step, with an optional robust variant for improved artifact\nsuppression. Through experiments, we show that EP-CFG maintains natural image\nquality and preserves details across guidance strengths while retaining CFG's\nsemantic alignment benefits, all with minimal computational overhead.\n","authors":["Kai Zhang","Fujun Luan","Sai Bi","Jianming Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.09966v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08050v2","updated":"2024-12-13T08:38:29Z","published":"2024-12-11T02:56:23Z","title":"BSAFusion: A Bidirectional Stepwise Feature Alignment Network for\n  Unaligned Medical Image Fusion","summary":"  If unaligned multimodal medical images can be simultaneously aligned and\nfused using a single-stage approach within a unified processing framework, it\nwill not only achieve mutual promotion of dual tasks but also help reduce the\ncomplexity of the model. However, the design of this model faces the challenge\nof incompatible requirements for feature fusion and alignment; specifically,\nfeature alignment requires consistency among corresponding features, whereas\nfeature fusion requires the features to be complementary to each other. To\naddress this challenge, this paper proposes an unaligned medical image fusion\nmethod called Bidirectional Stepwise Feature Alignment and Fusion (BSFA-F)\nstrategy. To reduce the negative impact of modality differences on cross-modal\nfeature matching, we incorporate the Modal Discrepancy-Free Feature\nRepresentation (MDF-FR) method into BSFA-F. MDF-FR utilizes a Modality Feature\nRepresentation Head (MFRH) to integrate the global information of the input\nimage. By injecting the information contained in MFRH of the current image into\nother modality images, it effectively reduces the impact of modality\ndifferences on feature alignment while preserving the complementary information\ncarried by different images. In terms of feature alignment, BSFA-F employs a\nbidirectional stepwise alignment deformation field prediction strategy based on\nthe path independence of vector displacement between two points. This strategy\nsolves the problem of large spans and inaccurate deformation field prediction\nin single-step alignment. Finally, Multi-Modal Feature Fusion block achieves\nthe fusion of aligned features. The experimental results across multiple\ndatasets demonstrate the effectiveness of our method. The source code is\navailable at https://github.com/slrl123/BSAFusion.\n","authors":["Huafeng Li","Dayong Su","Qing Cai","Yafei Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.08050v2.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.09960v1","updated":"2024-12-13T08:37:30Z","published":"2024-12-13T08:37:30Z","title":"END$^2$: Robust Dual-Decoder Watermarking Framework Against\n  Non-Differentiable Distortions","summary":"  DNN-based watermarking methods have rapidly advanced, with the\n``Encoder-Noise Layer-Decoder'' (END) framework being the most widely used. To\nensure end-to-end training, the noise layer in the framework must be\ndifferentiable. However, real-world distortions are often non-differentiable,\nleading to challenges in end-to-end training. Existing solutions only treat the\ndistortion perturbation as additive noise, which does not fully integrate the\neffect of distortion in training. To better incorporate non-differentiable\ndistortions into training, we propose a novel dual-decoder architecture\n(END$^2$). Unlike conventional END architecture, our method employs two\nstructurally identical decoders: the Teacher Decoder, processing pure\nwatermarked images, and the Student Decoder, handling distortion-perturbed\nimages. The gradient is backpropagated only through the Teacher Decoder branch\nto optimize the encoder thus bypassing the problem of non-differentiability. To\nensure resistance to arbitrary distortions, we enforce alignment of the two\ndecoders' feature representations by maximizing the cosine similarity between\ntheir intermediate vectors on a hypersphere. Extensive experiments demonstrate\nthat our scheme outperforms state-of-the-art algorithms under various\nnon-differentiable distortions. Moreover, even without the differentiability\nconstraint, our method surpasses baselines with a differentiable noise layer.\nOur approach is effective and easily implementable across all END\narchitectures, enhancing practicality and generalizability.\n","authors":["Nan Sun","Han Fang","Yuxing Lu","Chengxin Zhao","Hefei Ling"],"pdf_url":"https://arxiv.org/pdf/2412.09960v1.pdf","comment":"9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2412.09959v1","updated":"2024-12-13T08:34:46Z","published":"2024-12-13T08:34:46Z","title":"Efficient Dataset Distillation via Diffusion-Driven Patch Selection for\n  Improved Generalization","summary":"  Dataset distillation offers an efficient way to reduce memory and\ncomputational costs by optimizing a smaller dataset with performance comparable\nto the full-scale original. However, for large datasets and complex deep\nnetworks (e.g., ImageNet-1K with ResNet-101), the extensive optimization space\nlimits performance, reducing its practicality. Recent approaches employ\npre-trained diffusion models to generate informative images directly, avoiding\npixel-level optimization and achieving notable results. However, these methods\noften face challenges due to distribution shifts between pre-trained models and\ntarget datasets, along with the need for multiple distillation steps across\nvarying settings. To address these issues, we propose a novel framework\northogonal to existing diffusion-based distillation methods, leveraging\ndiffusion models for selection rather than generation. Our method starts by\npredicting noise generated by the diffusion model based on input images and\ntext prompts (with or without label text), then calculates the corresponding\nloss for each pair. With the loss differences, we identify distinctive regions\nof the original images. Additionally, we perform intra-class clustering and\nranking on selected patches to maintain diversity constraints. This streamlined\nframework enables a single-step distillation process, and extensive experiments\ndemonstrate that our approach outperforms state-of-the-art methods across\nvarious metrics.\n","authors":["Xinhao Zhong","Shuoyang Sun","Xulin Gu","Zhaoyang Xu","Yaowei Wang","Jianlong Wu","Bin Chen"],"pdf_url":"https://arxiv.org/pdf/2412.09959v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2408.02752 by other authors"},{"id":"http://arxiv.org/abs/2412.09954v1","updated":"2024-12-13T08:24:12Z","published":"2024-12-13T08:24:12Z","title":"$\\textrm{A}^{\\textrm{2}}$RNet: Adversarial Attack Resilient Network for\n  Robust Infrared and Visible Image Fusion","summary":"  Infrared and visible image fusion (IVIF) is a crucial technique for enhancing\nvisual performance by integrating unique information from different modalities\ninto one fused image. Exiting methods pay more attention to conducting fusion\nwith undisturbed data, while overlooking the impact of deliberate interference\non the effectiveness of fusion results. To investigate the robustness of fusion\nmodels, in this paper, we propose a novel adversarial attack resilient network,\ncalled $\\textrm{A}^{\\textrm{2}}$RNet. Specifically, we develop an adversarial\nparadigm with an anti-attack loss function to implement adversarial attacks and\ntraining. It is constructed based on the intrinsic nature of IVIF and provide a\nrobust foundation for future research advancements. We adopt a Unet as the\npipeline with a transformer-based defensive refinement module (DRM) under this\nparadigm, which guarantees fused image quality in a robust coarse-to-fine\nmanner. Compared to previous works, our method mitigates the adverse effects of\nadversarial perturbations, consistently maintaining high-fidelity fusion\nresults. Furthermore, the performance of downstream tasks can also be well\nmaintained under adversarial attacks. Code is available at\nhttps://github.com/lok-18/A2RNet.\n","authors":["Jiawei Li","Hongwei Yu","Jiansheng Chen","Xinlong Ding","Jinlong Wang","Jinyuan Liu","Bochao Zou","Huimin Ma"],"pdf_url":"https://arxiv.org/pdf/2412.09954v1.pdf","comment":"9 pages, 8 figures, The 39th Annual AAAI Conference on Artificial\n  Intelligence"},{"id":"http://arxiv.org/abs/2412.09951v1","updated":"2024-12-13T08:14:24Z","published":"2024-12-13T08:14:24Z","title":"WiseAD: Knowledge Augmented End-to-End Autonomous Driving with\n  Vision-Language Model","summary":"  The emergence of general human knowledge and impressive logical reasoning\ncapacity in rapidly progressed vision-language models (VLMs) have driven\nincreasing interest in applying VLMs to high-level autonomous driving tasks,\nsuch as scene understanding and decision-making. However, an in-depth study on\nthe relationship between knowledge proficiency, especially essential driving\nexpertise, and closed-loop autonomous driving performance requires further\nexploration. In this paper, we investigate the effects of the depth and breadth\nof fundamental driving knowledge on closed-loop trajectory planning and\nintroduce WiseAD, a specialized VLM tailored for end-to-end autonomous driving\ncapable of driving reasoning, action justification, object recognition, risk\nanalysis, driving suggestions, and trajectory planning across diverse\nscenarios. We employ joint training on driving knowledge and planning datasets,\nenabling the model to perform knowledge-aligned trajectory planning\naccordingly. Extensive experiments indicate that as the diversity of driving\nknowledge extends, critical accidents are notably reduced, contributing 11.9%\nand 12.4% improvements in the driving score and route completion on the Carla\nclosed-loop evaluations, achieving state-of-the-art performance. Moreover,\nWiseAD also demonstrates remarkable performance in knowledge evaluations on\nboth in-domain and out-of-domain datasets.\n","authors":["Songyan Zhang","Wenhui Huang","Zihui Gao","Hao Chen","Chen Lv"],"pdf_url":"https://arxiv.org/pdf/2412.09951v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07689v3","updated":"2024-12-13T08:13:44Z","published":"2024-12-10T17:27:32Z","title":"DriveMM: All-in-One Large Multimodal Model for Autonomous Driving","summary":"  Large Multimodal Models (LMMs) have demonstrated exceptional comprehension\nand interpretation capabilities in Autonomous Driving (AD) by incorporating\nlarge language models. Despite the advancements, current data-driven AD\napproaches tend to concentrate on a single dataset and specific tasks,\nneglecting their overall capabilities and ability to generalize. To bridge\nthese gaps, we propose DriveMM, a general large multimodal model designed to\nprocess diverse data inputs, such as images and multi-view videos, while\nperforming a broad spectrum of AD tasks, including perception, prediction, and\nplanning. Initially, the model undergoes curriculum pre-training to process\nvaried visual signals and perform basic visual comprehension and perception\ntasks. Subsequently, we augment and standardize various AD-related datasets to\nfine-tune the model, resulting in an all-in-one LMM for autonomous driving. To\nassess the general capabilities and generalization ability, we conduct\nevaluations on six public benchmarks and undertake zero-shot transfer on an\nunseen dataset, where DriveMM achieves state-of-the-art performance across all\ntasks. We hope DriveMM as a promising solution for future end-to-end autonomous\ndriving applications in the real world. Project page with code:\nhttps://github.com/zhijian11/DriveMM.\n","authors":["Zhijian Huang","Chengjian Feng","Feng Yan","Baihui Xiao","Zequn Jie","Yujie Zhong","Xiaodan Liang","Lin Ma"],"pdf_url":"https://arxiv.org/pdf/2412.07689v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09296v2","updated":"2024-12-13T08:11:13Z","published":"2024-12-12T14:12:07Z","title":"GoHD: Gaze-oriented and Highly Disentangled Portrait Animation with\n  Rhythmic Poses and Realistic Expression","summary":"  Audio-driven talking head generation necessitates seamless integration of\naudio and visual data amidst the challenges posed by diverse input portraits\nand intricate correlations between audio and facial motions. In response, we\npropose a robust framework GoHD designed to produce highly realistic,\nexpressive, and controllable portrait videos from any reference identity with\nany motion. GoHD innovates with three key modules: Firstly, an animation module\nutilizing latent navigation is introduced to improve the generalization ability\nacross unseen input styles. This module achieves high disentanglement of motion\nand identity, and it also incorporates gaze orientation to rectify unnatural\neye movements that were previously overlooked. Secondly, a conformer-structured\nconditional diffusion model is designed to guarantee head poses that are aware\nof prosody. Thirdly, to estimate lip-synchronized and realistic expressions\nfrom the input audio within limited training data, a two-stage training\nstrategy is devised to decouple frequent and frame-wise lip motion distillation\nfrom the generation of other more temporally dependent but less audio-related\nmotions, e.g., blinks and frowns. Extensive experiments validate GoHD's\nadvanced generalization capabilities, demonstrating its effectiveness in\ngenerating realistic talking face results on arbitrary subjects.\n","authors":["Ziqi Zhou","Weize Quan","Hailin Shi","Wei Li","Lili Wang","Dong-Ming Yan"],"pdf_url":"https://arxiv.org/pdf/2412.09296v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.09945v1","updated":"2024-12-13T08:10:47Z","published":"2024-12-13T08:10:47Z","title":"Going Beyond Feature Similarity: Effective Dataset distillation based on\n  Class-aware Conditional Mutual Information","summary":"  Dataset distillation (DD) aims to minimize the time and memory consumption\nneeded for training deep neural networks on large datasets, by creating a\nsmaller synthetic dataset that has similar performance to that of the full real\ndataset. However, current dataset distillation methods often result in\nsynthetic datasets that are excessively difficult for networks to learn from,\ndue to the compression of a substantial amount of information from the original\ndata through metrics measuring feature similarity, e,g., distribution matching\n(DM). In this work, we introduce conditional mutual information (CMI) to assess\nthe class-aware complexity of a dataset and propose a novel method by\nminimizing CMI. Specifically, we minimize the distillation loss while\nconstraining the class-aware complexity of the synthetic dataset by minimizing\nits empirical CMI from the feature space of pre-trained networks,\nsimultaneously. Conducting on a thorough set of experiments, we show that our\nmethod can serve as a general regularization method to existing DD methods and\nimprove the performance and training efficiency.\n","authors":["Xinhao Zhong","Bin Chen","Hao Fang","Xulin Gu","Shu-Tao Xia","En-Hui Yang"],"pdf_url":"https://arxiv.org/pdf/2412.09945v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09938v1","updated":"2024-12-13T07:57:31Z","published":"2024-12-13T07:57:31Z","title":"Pixel Intensity Tracking for Remote Respiratory Monitoring: A Study on\n  Indonesian Subject","summary":"  Respiratory rate is a vital sign indicating various health conditions.\nTraditional contact-based measurement methods are often uncomfortable, and\nalternatives like respiratory belts and smartwatches have limitations in cost\nand operability. Therefore, a non-contact method based on Pixel Intensity\nChanges (PIC) with RGB camera images is proposed. Experiments involved 3 sizes\nof bounding boxes, 3 filter options (Laplacian, Sobel, and no filter), and 2\ncorner detection algorithms (ShiTomasi and Harris), with tracking using the\nLukas-Kanade algorithm. Eighteen configurations were tested on 67 subjects in\nstatic and dynamic conditions. The best results in static conditions were\nachieved with the Medium Bounding box, Sobel Filter, and Harris Method (MAE:\n0.85, RMSE: 1.49). In dynamic conditions, the Large Bounding box with no filter\nand ShiTomasi, and Medium Bounding box with no filter and Harris, produced the\nlowest MAE (0.81) and RMSE (1.35)\n","authors":["Muhammad Yahya Ayyashy Mujahidan","Martin Clinton Tosima Manullang"],"pdf_url":"https://arxiv.org/pdf/2412.09938v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09936v1","updated":"2024-12-13T07:51:32Z","published":"2024-12-13T07:51:32Z","title":"CaLoRAify: Calorie Estimation with Visual-Text Pairing and LoRA-Driven\n  Visual Language Models","summary":"  The obesity phenomenon, known as the heavy issue, is a leading cause of\npreventable chronic diseases worldwide. Traditional calorie estimation tools\noften rely on specific data formats or complex pipelines, limiting their\npracticality in real-world scenarios. Recently, vision-language models (VLMs)\nhave excelled in understanding real-world contexts and enabling conversational\ninteractions, making them ideal for downstream tasks such as ingredient\nanalysis. However, applying VLMs to calorie estimation requires domain-specific\ndata and alignment strategies. To this end, we curated CalData, a 330K\nimage-text pair dataset tailored for ingredient recognition and calorie\nestimation, combining a large-scale recipe dataset with detailed nutritional\ninstructions for robust vision-language training. Built upon this dataset, we\npresent CaLoRAify, a novel VLM framework aligning ingredient recognition and\ncalorie estimation via training with visual-text pairs. During inference, users\nonly need a single monocular food image to estimate calories while retaining\nthe flexibility of agent-based conversational interaction. With Low-rank\nAdaptation (LoRA) and Retrieve-augmented Generation (RAG) techniques, our\nsystem enhances the performance of foundational VLMs in the vertical domain of\ncalorie estimation. Our code and data are fully open-sourced at\nhttps://github.com/KennyYao2001/16824-CaLORAify.\n","authors":["Dongyu Yao","Keling Yao","Junhong Zhou","Yinghao Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.09936v1.pdf","comment":"Disclaimer: This work is part of a course project and reflects\n  ongoing exploration in the field of vision-language models and calorie\n  estimation. Findings and conclusions are subject to further validation and\n  refinement"},{"id":"http://arxiv.org/abs/2412.09927v1","updated":"2024-12-13T07:28:14Z","published":"2024-12-13T07:28:14Z","title":"Neural Vector Tomography for Reconstructing a Magnetization Vector Field","summary":"  Discretized techniques for vector tomographic reconstructions are prone to\nproducing artifacts in the reconstructions. The quality of these\nreconstructions may further deteriorate as the amount of noise increases. In\nthis work, we instead model the underlying vector fields using smooth neural\nfields. Owing to the fact that the activation functions in the neural network\nmay be chosen to be smooth and the domain is no longer pixelated, the model\nresults in high-quality reconstructions, even under presence of noise. In the\ncase where we have underlying global continuous symmetry, we find that the\nneural network substantially improves the accuracy of the reconstruction over\nthe existing techniques.\n","authors":["Giorgi Butbaia","Jiadong Zang"],"pdf_url":"https://arxiv.org/pdf/2412.09927v1.pdf","comment":"6 pages, 5 figures"},{"id":"http://arxiv.org/abs/2402.09066v3","updated":"2024-12-13T07:26:51Z","published":"2024-02-14T10:24:04Z","title":"Solid Waste Detection, Monitoring and Mapping in Remote Sensing Images:\n  A Survey","summary":"  The detection and characterization of illegal solid waste disposal sites are\nessential for environmental protection, particularly for mitigating pollution\nand health hazards. Improperly managed landfills contaminate soil and\ngroundwater via rainwater infiltration, posing threats to both animals and\nhumans. Traditional landfill identification approaches, such as on-site\ninspections, are time-consuming and expensive. Remote sensing is a\ncost-effective solution for the identification and monitoring of solid waste\ndisposal sites that enables broad coverage and repeated acquisitions over time.\nEarth Observation (EO) satellites, equipped with an array of sensors and\nimaging capabilities, have been providing high-resolution data for several\ndecades. Researchers proposed specialized techniques that leverage remote\nsensing imagery to perform a range of tasks such as waste site detection,\ndumping site monitoring, and assessment of suitable locations for new\nlandfills. This review aims to provide a detailed illustration of the most\nrelevant proposals for the detection and monitoring of solid waste sites by\ndescribing and comparing the approaches, the implemented techniques, and the\nemployed data. Furthermore, since the data sources are of the utmost importance\nfor developing an effective solid waste detection model, a comprehensive\noverview of the satellites and publicly available data sets is presented.\nFinally, this paper identifies the open issues in the state-of-the-art and\ndiscusses the relevant research directions for reducing the costs and improving\nthe effectiveness of novel solid waste detection methods.\n","authors":["Piero Fraternali","Luca Morandini","Sergio Luis Herrera González"],"pdf_url":"https://arxiv.org/pdf/2402.09066v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09921v1","updated":"2024-12-13T07:20:35Z","published":"2024-12-13T07:20:35Z","title":"FaceShield: Defending Facial Image against Deepfake Threats","summary":"  The rising use of deepfakes in criminal activities presents a significant\nissue, inciting widespread controversy. While numerous studies have tackled\nthis problem, most primarily focus on deepfake detection. These reactive\nsolutions are insufficient as a fundamental approach for crimes where\nauthenticity verification is not critical. Existing proactive defenses also\nhave limitations, as they are effective only for deepfake models based on\nspecific Generative Adversarial Networks (GANs), making them less applicable in\nlight of recent advancements in diffusion-based models. In this paper, we\npropose a proactive defense method named FaceShield, which introduces novel\nattack strategies targeting deepfakes generated by Diffusion Models (DMs) and\nfacilitates attacks on various existing GAN-based deepfake models through\nfacial feature extractor manipulations. Our approach consists of three main\ncomponents: (i) manipulating the attention mechanism of DMs to exclude\nprotected facial features during the denoising process, (ii) targeting\nprominent facial feature extraction models to enhance the robustness of our\nadversarial perturbation, and (iii) employing Gaussian blur and low-pass\nfiltering techniques to improve imperceptibility while enhancing robustness\nagainst JPEG distortion. Experimental results on the CelebA-HQ and VGGFace2-HQ\ndatasets demonstrate that our method achieves state-of-the-art performance\nagainst the latest deepfake models based on DMs, while also exhibiting\napplicability to GANs and showcasing greater imperceptibility of noise along\nwith enhanced robustness.\n","authors":["Jaehwan Jeong","Sumin In","Sieun Kim","Hannie Shin","Jongheon Jeong","Sang Ho Yoon","Jaewook Chung","Sangpil Kim"],"pdf_url":"https://arxiv.org/pdf/2412.09921v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09920v1","updated":"2024-12-13T07:15:52Z","published":"2024-12-13T07:15:52Z","title":"Precision-Enhanced Human-Object Contact Detection via Depth-Aware\n  Perspective Interaction and Object Texture Restoration","summary":"  Human-object contact (HOT) is designed to accurately identify the areas where\nhumans and objects come into contact. Current methods frequently fail to\naccount for scenarios where objects are frequently blocking the view, resulting\nin inaccurate identification of contact areas. To tackle this problem, we\nsuggest using a perspective interaction HOT detector called PIHOT, which\nutilizes a depth map generation model to offer depth information of humans and\nobjects related to the camera, thereby preventing false interaction detection.\nFurthermore, we use mask dilatation and object restoration techniques to\nrestore the texture details in covered areas, improve the boundaries between\nobjects, and enhance the perception of humans interacting with objects.\nMoreover, a spatial awareness perception is intended to concentrate on the\ncharacteristic features close to the points of contact. The experimental\nresults show that the PIHOT algorithm achieves state-of-the-art performance on\nthree benchmark datasets for HOT detection tasks. Compared to the most recent\nDHOT, our method enjoys an average improvement of 13%, 27.5%, 16%, and 18.5% on\nSC-Acc., C-Acc., mIoU, and wIoU metrics, respectively.\n","authors":["Yuxiao Wang","Wenpeng Neng","Zhenao Wei","Yu Lei","Weiying Xue","Nan Zhuang","Yanwu Xu","Xinyu Jiang","Qi Liu"],"pdf_url":"https://arxiv.org/pdf/2412.09920v1.pdf","comment":"Accepted by AAAl 2025"},{"id":"http://arxiv.org/abs/2412.09919v1","updated":"2024-12-13T07:13:40Z","published":"2024-12-13T07:13:40Z","title":"B-VLLM: A Vision Large Language Model with Balanced Spatio-Temporal\n  Tokens","summary":"  Recently, Vision Large Language Models (VLLMs) integrated with vision\nencoders have shown promising performance in vision understanding. The key of\nVLLMs is to encode visual content into sequences of visual tokens, enabling\nVLLMs to simultaneously process both visual and textual content. However,\nunderstanding videos, especially long videos, remain a challenge to VLLMs as\nthe number of visual tokens grows rapidly when encoding videos, resulting in\nthe risk of exceeding the context window of VLLMs and introducing heavy\ncomputation burden. To restrict the number of visual tokens, existing VLLMs\neither: (1) uniformly downsample videos into a fixed number of frames or (2)\nreducing the number of visual tokens encoded from each frame. We argue the\nformer solution neglects the rich temporal cue in videos and the later\noverlooks the spatial details in each frame. In this work, we present\nBalanced-VLLM (B-VLLM): a novel VLLM framework that aims to effectively\nleverage task relevant spatio-temporal cues while restricting the number of\nvisual tokens under the VLLM context window length. At the core of our method,\nwe devise a text-conditioned adaptive frame selection module to identify frames\nrelevant to the visual understanding task. The selected frames are then\nde-duplicated using a temporal frame token merging technique. The visual tokens\nof the selected frames are processed through a spatial token sampling module\nand an optional spatial token merging strategy to achieve precise control over\nthe token count. Experimental results show that B-VLLM is effective in\nbalancing the number of frames and visual tokens in video understanding,\nyielding superior performance on various video understanding benchmarks. Our\ncode is available at https://github.com/zhuqiangLu/B-VLLM.\n","authors":["Zhuqiang Lu","Zhenfei Yin","Mengwei He","Zhihui Wang","Zicheng Liu","Zhiyong Wang","Kun Hu"],"pdf_url":"https://arxiv.org/pdf/2412.09919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.01500v2","updated":"2024-12-13T07:05:27Z","published":"2024-12-02T13:51:58Z","title":"SF-Loc: A Visual Mapping and Geo-Localization System based on Sparse\n  Visual Structure Frames","summary":"  For high-level geo-spatial applications and intelligent robotics, accurate\nglobal pose information is of crucial importance. Map-aided localization is a\nuniversal approach to overcome the limitations of global navigation satellite\nsystem (GNSS) in challenging environments. However, current solutions face\nchallenges in terms of mapping flexibility, storage burden and re-localization\nperformance. In this work, we present SF-Loc, a lightweight visual mapping and\nmap-aided localization system, whose core idea is the map representation based\non sparse frames with dense but compact depth, termed as visual structure\nframes. In the mapping phase, multi-sensor dense bundle adjustment (MS-DBA) is\napplied to construct geo-referenced visual structure frames. The local\nco-visbility is checked to keep the map sparsity and achieve incremental\nmapping. In the localization phase, coarse-to-fine vision-based localization is\nperformed, in which multi-frame information and the map distribution are fully\nintegrated. To be specific, the concept of spatially smoothed similarity (SSS)\nis proposed to overcome the place ambiguity, and pairwise frame matching is\napplied for efficient and robust pose estimation. Experimental results on the\ncross-season dataset verify the effectiveness of the system. In complex urban\nroad scenarios, the map size is down to 3 MB per kilometer and stable\ndecimeter-level re-localization can be achieved. The code will be made\nopen-source soon (https://github.com/GREAT-WHU/SF-Loc).\n","authors":["Yuxuan Zhou","Xingxing Li","Shengyu Li","Chunxi Xia","Xuanbin Wang","Shaoquan Feng"],"pdf_url":"https://arxiv.org/pdf/2412.01500v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09912v1","updated":"2024-12-13T06:59:17Z","published":"2024-12-13T06:59:17Z","title":"All-in-One: Transferring Vision Foundation Models into Stereo Matching","summary":"  As a fundamental vision task, stereo matching has made remarkable progress.\nWhile recent iterative optimization-based methods have achieved promising\nperformance, their feature extraction capabilities still have room for\nimprovement. Inspired by the ability of vision foundation models (VFMs) to\nextract general representations, in this work, we propose AIO-Stereo which can\nflexibly select and transfer knowledge from multiple heterogeneous VFMs to a\nsingle stereo matching model. To better reconcile features between\nheterogeneous VFMs and the stereo matching model and fully exploit prior\nknowledge from VFMs, we proposed a dual-level feature utilization mechanism\nthat aligns heterogeneous features and transfers multi-level knowledge. Based\non the mechanism, a dual-level selective knowledge transfer module is designed\nto selectively transfer knowledge and integrate the advantages of multiple\nVFMs. Experimental results show that AIO-Stereo achieves start-of-the-art\nperformance on multiple datasets and ranks $1^{st}$ on the Middlebury dataset\nand outperforms all the published work on the ETH3D benchmark.\n","authors":["Jingyi Zhou","Haoyu Zhang","Jiakang Yuan","Peng Ye","Tao Chen","Hao Jiang","Meiya Chen","Yangyang Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.09912v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.00477v3","updated":"2024-12-13T06:57:07Z","published":"2024-11-30T13:29:36Z","title":"LineGS : 3D Line Segment Representation on 3D Gaussian Splatting","summary":"  Abstract representations of 3D scenes play a crucial role in computer vision,\nenabling a wide range of applications such as mapping, localization, surface\nreconstruction, and even advanced tasks like SLAM and rendering. Among these\nrepresentations, line segments are widely used because of their ability to\nsuccinctly capture the structural features of a scene. However, existing 3D\nreconstruction methods often face significant challenges. Methods relying on 2D\nprojections suffer from instability caused by errors in multi-view matching and\nocclusions, while direct 3D approaches are hampered by noise and sparsity in 3D\npoint cloud data. This paper introduces LineGS, a novel method that combines\ngeometry-guided 3D line reconstruction with a 3D Gaussian splatting model to\naddress these challenges and improve representation ability. The method\nleverages the high-density Gaussian point distributions along the edge of the\nscene to refine and optimize initial line segments generated from traditional\ngeometric approaches. By aligning these segments with the underlying geometric\nfeatures of the scene, LineGS achieves a more precise and reliable\nrepresentation of 3D structures. The results show significant improvements in\nboth geometric accuracy and model compactness compared to baseline methods.\n","authors":["Chenggang Yang","Yuang Shi"],"pdf_url":"https://arxiv.org/pdf/2412.00477v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09910v1","updated":"2024-12-13T06:56:12Z","published":"2024-12-13T06:56:12Z","title":"Prompt2Perturb (P2P): Text-Guided Diffusion-Based Adversarial Attacks on\n  Breast Ultrasound Images","summary":"  Deep neural networks (DNNs) offer significant promise for improving breast\ncancer diagnosis in medical imaging. However, these models are highly\nsusceptible to adversarial attacks--small, imperceptible changes that can\nmislead classifiers--raising critical concerns about their reliability and\nsecurity. Traditional attacks rely on fixed-norm perturbations, misaligning\nwith human perception. In contrast, diffusion-based attacks require pre-trained\nmodels, demanding substantial data when these models are unavailable, limiting\npractical use in data-scarce scenarios. In medical imaging, however, this is\noften unfeasible due to the limited availability of datasets. Building on\nrecent advancements in learnable prompts, we propose Prompt2Perturb (P2P), a\nnovel language-guided attack method capable of generating meaningful attack\nexamples driven by text instructions. During the prompt learning phase, our\napproach leverages learnable prompts within the text encoder to create subtle,\nyet impactful, perturbations that remain imperceptible while guiding the model\ntowards targeted outcomes. In contrast to current prompt learning-based\napproaches, our P2P stands out by directly updating text embeddings, avoiding\nthe need for retraining diffusion models. Further, we leverage the finding that\noptimizing only the early reverse diffusion steps boosts efficiency while\nensuring that the generated adversarial examples incorporate subtle noise, thus\npreserving ultrasound image quality without introducing noticeable artifacts.\nWe show that our method outperforms state-of-the-art attack techniques across\nthree breast ultrasound datasets in FID and LPIPS. Moreover, the generated\nimages are both more natural in appearance and more effective compared to\nexisting adversarial attacks. Our code will be publicly available\nhttps://github.com/yasamin-med/P2P.\n","authors":["Yasamin Medghalchi","Moein Heidari","Clayton Allard","Leonid Sigal","Ilker Hacihaliloglu"],"pdf_url":"https://arxiv.org/pdf/2412.09910v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11930v3","updated":"2024-12-13T06:54:04Z","published":"2024-11-18T11:54:58Z","title":"AtomThink: A Slow Thinking Framework for Multimodal Mathematical\n  Reasoning","summary":"  In this paper, we address the challenging task of multimodal mathematical\nreasoning by incorporating the ability of ``slow thinking\" into multimodal\nlarge language models (MLLMs). Contrary to existing methods that rely on direct\nor fast thinking, our key idea is to construct long chains of thought (CoT)\nconsisting of atomic actions in a step-by-step manner, guiding MLLMs to perform\ncomplex reasoning. To this end, we design a novel AtomThink framework composed\nof three key modules: (i) a CoT annotation engine that automatically generates\nhigh-quality CoT annotations to address the lack of high-quality visual\nmathematical data; (ii) an atomic step fine-tuning strategy that jointly\noptimizes an MLLM and a policy reward model (PRM) for step-wise reasoning; and\n(iii) four different search strategies that can be applied with the PRM to\ncomplete reasoning. Additionally, we propose AtomMATH, a large-scale multimodal\ndataset of long CoTs, and an atomic capability evaluation metric for\nmathematical tasks. Extensive experimental results show that the proposed\nAtomThink significantly improves the performance of baseline MLLMs, achieving\napproximately 50\\% relative accuracy gains on MathVista and 120\\% on MathVerse.\nTo support the advancement of multimodal slow-thinking models, we will make our\ncode and dataset publicly available on https://github.com/Quinn777/AtomThink.\n","authors":["Kun Xiang","Zhili Liu","Zihao Jiang","Yunshuang Nie","Runhui Huang","Haoxiang Fan","Hanhui Li","Weiran Huang","Yihan Zeng","Jianhua Han","Lanqing Hong","Hang Xu","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2411.11930v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09907v1","updated":"2024-12-13T06:52:02Z","published":"2024-12-13T06:52:02Z","title":"IQViC: In-context, Question Adaptive Vision Compressor for Long-term\n  Video Understanding LMMs","summary":"  With the increasing complexity of video data and the need for more efficient\nlong-term temporal understanding, existing long-term video understanding\nmethods often fail to accurately capture and analyze extended video sequences.\nThese methods typically struggle to maintain performance over longer durations\nand to handle the intricate dependencies within the video content. To address\nthese limitations, we propose a simple yet effective large multi-modal model\nframework for long-term video understanding that incorporates a novel visual\ncompressor, the In-context, Question Adaptive Visual Compressor (IQViC). The\nkey idea, inspired by humans' selective attention and in-context memory\nmechanisms, is to introduce a novel visual compressor and incorporate efficient\nmemory management techniques to enhance long-term video question answering. Our\nframework utilizes IQViC, a transformer-based visual compressor, enabling\nquestion-conditioned in-context compression, unlike existing methods that rely\non full video visual features. This selectively extracts relevant information,\nsignificantly reducing memory token requirements. Through extensive experiments\non a new dataset based on InfiniBench for long-term video understanding, and\nstandard benchmarks used for existing methods' evaluation, we demonstrate the\neffectiveness of our proposed IQViC framework and its superiority over\nstate-of-the-art methods in terms of video understanding accuracy and memory\nefficiency.\n","authors":["Sosuke Yamao","Natsuki Miyahara","Yuki Harazono","Shun Takeuchi"],"pdf_url":"https://arxiv.org/pdf/2412.09907v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.03015v2","updated":"2024-12-13T06:46:21Z","published":"2024-12-04T04:03:12Z","title":"Benchmarking Attention Mechanisms and Consistency Regularization\n  Semi-Supervised Learning for Post-Flood Building Damage Assessment in\n  Satellite Images","summary":"  Post-flood building damage assessment is critical for rapid response and\npost-disaster reconstruction planning. Current research fails to consider the\ndistinct requirements of disaster assessment (DA) from change detection (CD) in\nneural network design. This paper focuses on two key differences: 1) building\nchange features in DA satellite images are more subtle than in CD; 2) DA\ndatasets face more severe data scarcity and label imbalance. To address these\nissues, in terms of model architecture, the research explores the benchmark\nperformance of attention mechanisms in post-flood DA tasks and introduces\nSimple Prior Attention UNet (SPAUNet) to enhance the model's ability to\nrecognize subtle changes, in terms of semi-supervised learning (SSL)\nstrategies, the paper constructs four different combinations of image-level\nlabel category reference distributions for consistent training. Experimental\nresults on flood events of xBD dataset show that SPAUNet performs exceptionally\nwell in supervised learning experiments, achieving a recall of 79.10% and an F1\nscore of 71.32% for damaged classification, outperforming CD methods. The\nresults indicate the necessity of DA task-oriented model design. SSL\nexperiments demonstrate the positive impact of image-level consistency\nregularization on the model. Using pseudo-labels to form the reference\ndistribution for consistency training yields the best results, proving the\npotential of using the category distribution of a large amount of unlabeled\ndata for SSL. This paper clarifies the differences between DA and CD tasks. It\npreliminarily explores model design strategies utilizing prior attention\nmechanisms and image-level consistency regularization, establishing new\npost-flood DA task benchmark methods.\n","authors":["Jiaxi Yu","Tomohiro Fukuda","Nobuyoshi Yabuki"],"pdf_url":"https://arxiv.org/pdf/2412.03015v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09901v1","updated":"2024-12-13T06:40:26Z","published":"2024-12-13T06:40:26Z","title":"MulSMo: Multimodal Stylized Motion Generation by Bidirectional Control\n  Flow","summary":"  Generating motion sequences conforming to a target style while adhering to\nthe given content prompts requires accommodating both the content and style. In\nexisting methods, the information usually only flows from style to content,\nwhich may cause conflict between the style and content, harming the\nintegration. Differently, in this work we build a bidirectional control flow\nbetween the style and the content, also adjusting the style towards the\ncontent, in which case the style-content collision is alleviated and the\ndynamics of the style is better preserved in the integration. Moreover, we\nextend the stylized motion generation from one modality, i.e. the style motion,\nto multiple modalities including texts and images through contrastive learning,\nleading to flexible style control on the motion generation. Extensive\nexperiments demonstrate that our method significantly outperforms previous\nmethods across different datasets, while also enabling multimodal signals\ncontrol. The code of our method will be made publicly available.\n","authors":["Zhe Li","Yisheng He","Lei Zhong","Weichao Shen","Qi Zuo","Lingteng Qiu","Zilong Dong","Laurence Tianruo Yang","Weihao Yuan"],"pdf_url":"https://arxiv.org/pdf/2412.09901v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09202v2","updated":"2024-12-13T06:38:45Z","published":"2024-12-12T11:56:24Z","title":"Temporal Action Localization with Cross Layer Task Decoupling and\n  Refinement","summary":"  Temporal action localization (TAL) involves dual tasks to classify and\nlocalize actions within untrimmed videos. However, the two tasks often have\nconflicting requirements for features. Existing methods typically employ\nseparate heads for classification and localization tasks but share the same\ninput feature, leading to suboptimal performance. To address this issue, we\npropose a novel TAL method with Cross Layer Task Decoupling and Refinement\n(CLTDR). Based on the feature pyramid of video, CLTDR strategy integrates\nsemantically strong features from higher pyramid layers and detailed\nboundary-aware boundary features from lower pyramid layers to effectively\ndisentangle the action classification and localization tasks. Moreover, the\nmultiple features from cross layers are also employed to refine and align the\ndisentangled classification and regression results. At last, a lightweight\nGated Multi-Granularity (GMG) module is proposed to comprehensively extract and\naggregate video features at instant, local, and global temporal granularities.\nBenefiting from the CLTDR and GMG modules, our method achieves state-of-the-art\nperformance on five challenging benchmarks: THUMOS14, MultiTHUMOS,\nEPIC-KITCHENS-100, ActivityNet-1.3, and HACS. Our code and pre-trained models\nare publicly available at: https://github.com/LiQiang0307/CLTDR-GMG.\n","authors":["Qiang Li","Di Liu","Jun Kong","Sen Li","Hui Xu","Jianzhong Wang"],"pdf_url":"https://arxiv.org/pdf/2412.09202v2.pdf","comment":"Accepted in AAAI 2025"},{"id":"http://arxiv.org/abs/2305.10769v5","updated":"2024-12-13T06:36:28Z","published":"2023-05-18T07:23:12Z","title":"Catch-Up Distillation: You Only Need to Train Once for Accelerating\n  Sampling","summary":"  Diffusion Probability Models (DPMs) have made impressive advancements in\nvarious machine learning domains. However, achieving high-quality synthetic\nsamples typically involves performing a large number of sampling steps, which\nimpedes the possibility of real-time sample synthesis. Traditional accelerated\nsampling algorithms via knowledge distillation rely on pre-trained model\nweights and discrete time step scenarios, necessitating additional training\nsessions to achieve their goals. To address these issues, we propose the\nCatch-Up Distillation (CUD), which encourages the current moment output of the\nvelocity estimation model ``catch up'' with its previous moment output.\nSpecifically, CUD adjusts the original Ordinary Differential Equation (ODE)\ntraining objective to align the current moment output with both the ground\ntruth label and the previous moment output, utilizing Runge-Kutta-based\nmulti-step alignment distillation for precise ODE estimation while preventing\nasynchronous updates. Furthermore, we investigate the design space for CUDs\nunder continuous time-step scenarios and analyze how to determine the suitable\nstrategies. To demonstrate CUD's effectiveness, we conduct thorough ablation\nand comparison experiments on CIFAR-10, MNIST, and ImageNet-64. On CIFAR-10, we\nobtain a FID of 2.80 by sampling in 15 steps under one-session training and the\nnew state-of-the-art FID of 3.37 by sampling in one step with additional\ntraining. This latter result necessitated only 620k iterations with a batch\nsize of 128, in contrast to Consistency Distillation, which demanded 2100k\niterations with a larger batch size of 256. Our code is released at\nhttps://anonymous.4open.science/r/Catch-Up-Distillation-E31F.\n","authors":["Shitong Shao","Xu Dai","Lujun Li","Huanran Chen","Yang Hu","Shouyi Yin"],"pdf_url":"https://arxiv.org/pdf/2305.10769v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09895v1","updated":"2024-12-13T06:30:52Z","published":"2024-12-13T06:30:52Z","title":"Building a Multi-modal Spatiotemporal Expert for Zero-shot Action\n  Recognition with CLIP","summary":"  Zero-shot action recognition (ZSAR) requires collaborative multi-modal\nspatiotemporal understanding. However, finetuning CLIP directly for ZSAR yields\nsuboptimal performance, given its inherent constraints in capturing essential\ntemporal dynamics from both vision and text perspectives, especially when\nencountering novel actions with fine-grained spatiotemporal discrepancies. In\nthis work, we propose Spatiotemporal Dynamic Duo (STDD), a novel CLIP-based\nframework to comprehend multi-modal spatiotemporal dynamics synergistically.\nFor the vision side, we propose an efficient Space-time Cross Attention, which\ncaptures spatiotemporal dynamics flexibly with simple yet effective operations\napplied before and after spatial attention, without adding additional\nparameters or increasing computational complexity. For the semantic side, we\nconduct spatiotemporal text augmentation by comprehensively constructing an\nAction Semantic Knowledge Graph (ASKG) to derive nuanced text prompts. The ASKG\nelaborates on static and dynamic concepts and their interrelations, based on\nthe idea of decomposing actions into spatial appearances and temporal motions.\nDuring the training phase, the frame-level video representations are\nmeticulously aligned with prompt-level nuanced text representations, which are\nconcurrently regulated by the video representations from the frozen CLIP to\nenhance generalizability. Extensive experiments validate the effectiveness of\nour approach, which consistently surpasses state-of-the-art approaches on\npopular video benchmarks (i.e., Kinetics-600, UCF101, and HMDB51) under\nchallenging ZSAR settings. Code is available at\nhttps://github.com/Mia-YatingYu/STDD.\n","authors":["Yating Yu","Congqi Cao","Yueran Zhang","Qinyi Lv","Lingtong Min","Yanning Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.09895v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.02734v2","updated":"2024-12-13T06:17:48Z","published":"2024-12-03T18:18:33Z","title":"MVCTrack: Boosting 3D Point Cloud Tracking via Multimodal-Guided Virtual\n  Cues","summary":"  3D single object tracking is essential in autonomous driving and robotics.\nExisting methods often struggle with sparse and incomplete point cloud\nscenarios. To address these limitations, we propose a Multimodal-guided Virtual\nCues Projection (MVCP) scheme that generates virtual cues to enrich sparse\npoint clouds. Additionally, we introduce an enhanced tracker MVCTrack based on\nthe generated virtual cues. Specifically, the MVCP scheme seamlessly integrates\nRGB sensors into LiDAR-based systems, leveraging a set of 2D detections to\ncreate dense 3D virtual cues that significantly improve the sparsity of point\nclouds. These virtual cues can naturally integrate with existing LiDAR-based 3D\ntrackers, yielding substantial performance gains. Extensive experiments\ndemonstrate that our method achieves competitive performance on the NuScenes\ndataset.\n","authors":["Zhaofeng Hu","Sifan Zhou","Shibo Zhao","Zhihang Yuan"],"pdf_url":"https://arxiv.org/pdf/2412.02734v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.03937v2","updated":"2024-12-13T06:15:54Z","published":"2024-12-05T07:35:19Z","title":"AIpparel: A Large Multimodal Generative Model for Digital Garments","summary":"  Apparel is essential to human life, offering protection, mirroring cultural\nidentities, and showcasing personal style. Yet, the creation of garments\nremains a time-consuming process, largely due to the manual work involved in\ndesigning them. To simplify this process, we introduce AIpparel, a large\nmultimodal model for generating and editing sewing patterns. Our model\nfine-tunes state-of-the-art large multimodal models (LMMs) on a custom-curated\nlarge-scale dataset of over 120,000 unique garments, each with multimodal\nannotations including text, images, and sewing patterns. Additionally, we\npropose a novel tokenization scheme that concisely encodes these complex sewing\npatterns so that LLMs can learn to predict them efficiently. AIpparelachieves\nstate-of-the-art performance in single-modal tasks, including text-to-garment\nand image-to-garment prediction, and enables novel multimodal garment\ngeneration applications such as interactive garment editing. The project\nwebsite is at georgenakayama.github.io/AIpparel/.\n","authors":["Kiyohiro Nakayama","Jan Ackermann","Timur Levent Kesdogan","Yang Zheng","Maria Korosteleva","Olga Sorkine-Hornung","Leonidas J. Guibas","Guandao Yang","Gordon Wetzstein"],"pdf_url":"https://arxiv.org/pdf/2412.03937v2.pdf","comment":"The project website is at georgenakayama.github.io/AIpparel/"},{"id":"http://arxiv.org/abs/2412.09892v1","updated":"2024-12-13T06:14:57Z","published":"2024-12-13T06:14:57Z","title":"VQTalker: Towards Multilingual Talking Avatars through Facial Motion\n  Tokenization","summary":"  We present VQTalker, a Vector Quantization-based framework for multilingual\ntalking head generation that addresses the challenges of lip synchronization\nand natural motion across diverse languages. Our approach is grounded in the\nphonetic principle that human speech comprises a finite set of distinct sound\nunits (phonemes) and corresponding visual articulations (visemes), which often\nshare commonalities across languages. We introduce a facial motion tokenizer\nbased on Group Residual Finite Scalar Quantization (GRFSQ), which creates a\ndiscretized representation of facial features. This method enables\ncomprehensive capture of facial movements while improving generalization to\nmultiple languages, even with limited training data. Building on this quantized\nrepresentation, we implement a coarse-to-fine motion generation process that\nprogressively refines facial animations. Extensive experiments demonstrate that\nVQTalker achieves state-of-the-art performance in both video-driven and\nspeech-driven scenarios, particularly in multilingual settings. Notably, our\nmethod achieves high-quality results at a resolution of 512*512 pixels while\nmaintaining a lower bitrate of approximately 11 kbps. Our work opens new\npossibilities for cross-lingual talking face generation. Synthetic results can\nbe viewed at https://x-lance.github.io/VQTalker.\n","authors":["Tao Liu","Ziyang Ma","Qi Chen","Feilong Chen","Shuai Fan","Xie Chen","Kai Yu"],"pdf_url":"https://arxiv.org/pdf/2412.09892v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2412.09886v1","updated":"2024-12-13T06:01:39Z","published":"2024-12-13T06:01:39Z","title":"T-GMSI: A transformer-based generative model for spatial interpolation\n  under sparse measurements","summary":"  Generating continuous environmental models from sparsely sampled data is a\ncritical challenge in spatial modeling, particularly for topography.\nTraditional spatial interpolation methods often struggle with handling sparse\nmeasurements. To address this, we propose a Transformer-based Generative Model\nfor Spatial Interpolation (T-GMSI) using a vision transformer (ViT)\narchitecture for digital elevation model (DEM) generation under sparse\nconditions. T-GMSI replaces traditional convolution-based methods with ViT for\nfeature extraction and DEM interpolation while incorporating a terrain\nfeature-aware loss function for enhanced accuracy. T-GMSI excels in producing\nhigh-quality elevation surfaces from datasets with over 70% sparsity and\ndemonstrates strong transferability across diverse landscapes without\nfine-tuning. Its performance is validated through extensive experiments,\noutperforming traditional methods such as ordinary Kriging (OK) and natural\nneighbor (NN) and a conditional generative adversarial network (CGAN)-based\nmodel (CEDGAN). Compared to OK and NN, T-GMSI reduces root mean square error\n(RMSE) by 40% and 25% on airborne lidar data and by 23% and 10% on spaceborne\nlidar data. Against CEDGAN, T-GMSI achieves a 20% RMSE improvement on provided\nDEM data, requiring no fine-tuning. The ability of model on generalizing to\nlarge, unseen terrains underscores its transferability and potential\napplicability beyond topographic modeling. This research establishes T-GMSI as\na state-of-the-art solution for spatial interpolation on sparse datasets and\nhighlights its broader utility for other sparse data interpolation challenges.\n","authors":["Xiangxi Tian","Jie Shan"],"pdf_url":"https://arxiv.org/pdf/2412.09886v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09881v1","updated":"2024-12-13T05:51:03Z","published":"2024-12-13T05:51:03Z","title":"Sharpening Your Density Fields: Spiking Neuron Aided Fast Geometry\n  Learning","summary":"  Neural Radiance Fields (NeRF) have achieved remarkable progress in neural\nrendering. Extracting geometry from NeRF typically relies on the Marching Cubes\nalgorithm, which uses a hand-crafted threshold to define the level set.\nHowever, this threshold-based approach requires laborious and scenario-specific\ntuning, limiting its practicality for real-world applications. In this work, we\nseek to enhance the efficiency of this method during the training time. To this\nend, we introduce a spiking neuron mechanism that dynamically adjusts the\nthreshold, eliminating the need for manual selection. Despite its promise,\ndirectly training with the spiking neuron often results in model collapse and\nnoisy outputs. To overcome these challenges, we propose a round-robin strategy\nthat stabilizes the training process and enables the geometry network to\nachieve a sharper and more precise density distribution with minimal\ncomputational overhead. We validate our approach through extensive experiments\non both synthetic and real-world datasets. The results show that our method\nsignificantly improves the performance of threshold-based techniques, offering\na more robust and efficient solution for NeRF geometry extraction.\n","authors":["Yi Gu","Zhaorui Wang","Dongjun Ye","Renjing Xu"],"pdf_url":"https://arxiv.org/pdf/2412.09881v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09875v1","updated":"2024-12-13T05:40:50Z","published":"2024-12-13T05:40:50Z","title":"Selective State Space Memory for Large Vision-Language Models","summary":"  Large Vision-Language Models (LVLMs) have demonstrated remarkable performance\nacross a wide range of multimodal tasks. However, fine-tuning these models for\ndomain-specific applications remains a computationally intensive challenge.\nThis paper introduces State Space Memory Integration (SSMI), a novel approach\nfor efficient fine-tuning of LVLMs. By integrating lightweight Mamba-based\nstate space modules into the LVLM architecture, SSMI captures long-range\ndependencies and injects task-specific visual and sequential patterns\neffectively. Unlike traditional fine-tuning methods, SSMI requires only a\nfraction of the model's parameters to be updated, making it computationally\nefficient and scalable. Experiments on benchmark datasets, including COCO\nCaptioning, VQA, and Flickr30k, demonstrate that SSMI achieves state-of-the-art\nperformance while maintaining robustness and generalization capabilities.\nComprehensive analysis further validates the advantages of SSMI in terms of\nefficiency, adaptability, and interpretability, positioning it as a compelling\nsolution for fine-tuning large-scale vision-language models.\n","authors":["Chee Ng","Yuen Fung"],"pdf_url":"https://arxiv.org/pdf/2412.09875v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09874v1","updated":"2024-12-13T05:40:20Z","published":"2024-12-13T05:40:20Z","title":"Can Students Beyond The Teacher? Distilling Knowledge from Teacher's\n  Bias","summary":"  Knowledge distillation (KD) is a model compression technique that transfers\nknowledge from a large teacher model to a smaller student model to enhance its\nperformance. Existing methods often assume that the student model is inherently\ninferior to the teacher model. However, we identify that the fundamental issue\naffecting student performance is the bias transferred by the teacher. Current\nKD frameworks transmit both right and wrong knowledge, introducing bias that\nmisleads the student model. To address this issue, we propose a novel strategy\nto rectify bias and greatly improve the student model's performance. Our\nstrategy involves three steps: First, we differentiate knowledge and design a\nbias elimination method to filter out biases, retaining only the right\nknowledge for the student model to learn. Next, we propose a bias rectification\nmethod to rectify the teacher model's wrong predictions, fundamentally\naddressing bias interference. The student model learns from both the right\nknowledge and the rectified biases, greatly improving its prediction accuracy.\nAdditionally, we introduce a dynamic learning approach with a loss function\nthat updates weights dynamically, allowing the student model to quickly learn\nright knowledge-based easy tasks initially and tackle hard tasks corresponding\nto biases later, greatly enhancing the student model's learning efficiency. To\nthe best of our knowledge, this is the first strategy enabling the student\nmodel to surpass the teacher model. Experiments demonstrate that our strategy,\nas a plug-and-play module, is versatile across various mainstream KD\nframeworks. We will release our code after the paper is accepted.\n","authors":["Jianhua Zhang","Yi Gao","Ruyu Liu","Xu Cheng","Houxiang Zhang","Shengyong Chen"],"pdf_url":"https://arxiv.org/pdf/2412.09874v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09870v1","updated":"2024-12-13T05:29:37Z","published":"2024-12-13T05:29:37Z","title":"Dynamic Cross-Modal Alignment for Robust Semantic Location Prediction","summary":"  Semantic location prediction from multimodal social media posts is a critical\ntask with applications in personalized services and human mobility analysis.\nThis paper introduces \\textit{Contextualized Vision-Language Alignment\n(CoVLA)}, a discriminative framework designed to address the challenges of\ncontextual ambiguity and modality discrepancy inherent in this task. CoVLA\nleverages a Contextual Alignment Module (CAM) to enhance cross-modal feature\nalignment and a Cross-modal Fusion Module (CMF) to dynamically integrate\ntextual and visual information. Extensive experiments on a benchmark dataset\ndemonstrate that CoVLA significantly outperforms state-of-the-art methods,\nachieving improvements of 2.3\\% in accuracy and 2.5\\% in F1-score. Ablation\nstudies validate the contributions of CAM and CMF, while human evaluations\nhighlight the contextual relevance of the predictions. Additionally, robustness\nanalysis shows that CoVLA maintains high performance under noisy conditions,\nmaking it a reliable solution for real-world applications. These results\nunderscore the potential of CoVLA in advancing semantic location prediction\nresearch.\n","authors":["Liu Jing","Amirul Rahman"],"pdf_url":"https://arxiv.org/pdf/2412.09870v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09868v1","updated":"2024-12-13T05:27:35Z","published":"2024-12-13T05:27:35Z","title":"RP-SLAM: Real-time Photorealistic SLAM with Efficient 3D Gaussian\n  Splatting","summary":"  3D Gaussian Splatting has emerged as a promising technique for high-quality\n3D rendering, leading to increasing interest in integrating 3DGS into realism\nSLAM systems. However, existing methods face challenges such as Gaussian\nprimitives redundancy, forgetting problem during continuous optimization, and\ndifficulty in initializing primitives in monocular case due to lack of depth\ninformation. In order to achieve efficient and photorealistic mapping, we\npropose RP-SLAM, a 3D Gaussian splatting-based vision SLAM method for monocular\nand RGB-D cameras. RP-SLAM decouples camera poses estimation from Gaussian\nprimitives optimization and consists of three key components. Firstly, we\npropose an efficient incremental mapping approach to achieve a compact and\naccurate representation of the scene through adaptive sampling and Gaussian\nprimitives filtering. Secondly, a dynamic window optimization method is\nproposed to mitigate the forgetting problem and improve map consistency.\nFinally, for the monocular case, a monocular keyframe initialization method\nbased on sparse point cloud is proposed to improve the initialization accuracy\nof Gaussian primitives, which provides a geometric basis for subsequent\noptimization. The results of numerous experiments demonstrate that RP-SLAM\nachieves state-of-the-art map rendering accuracy while ensuring real-time\nperformance and model compactness.\n","authors":["Lizhi Bai","Chunqi Tian","Jun Yang","Siyu Zhang","Masanori Suganuma","Takayuki Okatani"],"pdf_url":"https://arxiv.org/pdf/2412.09868v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08966v3","updated":"2024-12-13T04:59:10Z","published":"2024-02-14T06:20:48Z","title":"Pretraining Vision-Language Model for Difference Visual Question\n  Answering in Longitudinal Chest X-rays","summary":"  Difference visual question answering (diff-VQA) is a challenging task that\nrequires answering complex questions based on differences between a pair of\nimages. This task is particularly important in reading chest X-ray images\nbecause radiologists often compare multiple images of the same patient taken at\ndifferent times to track disease progression and changes in its severity in\ntheir clinical practice. However, previous works focused on designing specific\nnetwork architectures for the diff-VQA task, missing opportunities to enhance\nthe model's performance using a pretrained vision-language model (VLM). Here,\nwe introduce a novel VLM called PLURAL, which is pretrained on natural and\nlongitudinal chest X-ray data for the diff-VQA task. The model is developed\nusing a step-by-step approach, starting with being pretrained on natural images\nand texts, followed by being trained using longitudinal chest X-ray data. The\nlongitudinal data consist of pairs of X-ray images, along with question-answer\nsets and radiologist's reports that describe the changes in lung abnormalities\nand diseases over time. Our experimental results show that the PLURAL model\noutperforms state-of-the-art methods not only in diff-VQA for longitudinal\nX-rays but also in conventional VQA for a single X-ray image. Through extensive\nexperiments, we demonstrate the effectiveness of the proposed VLM architecture\nand pretraining method in improving the model's performance.\n","authors":["Yeongjae Cho","Taehee Kim","Heejun Shin","Sungzoon Cho","Dongmyung Shin"],"pdf_url":"https://arxiv.org/pdf/2402.08966v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09616v2","updated":"2024-12-13T04:58:33Z","published":"2024-12-12T18:59:46Z","title":"V2PE: Improving Multimodal Long-Context Capability of Vision-Language\n  Models with Variable Visual Position Encoding","summary":"  Vision-Language Models (VLMs) have shown promising capabilities in handling\nvarious multimodal tasks, yet they struggle in long-context scenarios,\nparticularly in tasks involving videos, high-resolution images, or lengthy\nimage-text documents. In our work, we first conduct an empirical analysis of\nthe long-context capabilities of VLMs using our augmented long-context\nmultimodal datasets. Our findings reveal that directly applying the positional\nencoding mechanism used for textual tokens to visual tokens is suboptimal, and\nVLM performance degrades sharply when the position encoding exceeds the model's\ncontext window. To address this, we propose Variable Visual Position Encoding\n(V2PE), a novel positional encoding approach that employs variable and smaller\nincrements for visual tokens, enabling more efficient management of long\nmultimodal sequences. Our experiments demonstrate the effectiveness of V2PE to\nenhances VLMs' ability to effectively understand and reason over long\nmultimodal contexts. We further integrate V2PE with our augmented long-context\nmultimodal datasets to fine-tune the open-source VLM, InternVL2. The fine-tuned\nmodel achieves strong performance on both standard and long-context multimodal\ntasks. Notably, when the sequence length of the training dataset is increased\nto 256K tokens, the model is capable of processing multimodal sequences up to\n1M tokens, highlighting its potential for real-world long-context applications.\n","authors":["Junqi Ge","Ziyi Chen","Jintao Lin","Jinguo Zhu","Xihui Liu","Jifeng Dai","Xizhou Zhu"],"pdf_url":"https://arxiv.org/pdf/2412.09616v2.pdf","comment":"The code and models will be available at\n  https://github.com/OpenGVLab/V2PE"},{"id":"http://arxiv.org/abs/2412.09856v1","updated":"2024-12-13T04:55:10Z","published":"2024-12-13T04:55:10Z","title":"LinGen: Towards High-Resolution Minute-Length Text-to-Video Generation\n  with Linear Computational Complexity","summary":"  Text-to-video generation enhances content creation but is highly\ncomputationally intensive: The computational cost of Diffusion Transformers\n(DiTs) scales quadratically in the number of pixels. This makes minute-length\nvideo generation extremely expensive, limiting most existing models to\ngenerating videos of only 10-20 seconds length. We propose a Linear-complexity\ntext-to-video Generation (LinGen) framework whose cost scales linearly in the\nnumber of pixels. For the first time, LinGen enables high-resolution\nminute-length video generation on a single GPU without compromising quality. It\nreplaces the computationally-dominant and quadratic-complexity block,\nself-attention, with a linear-complexity block called MATE, which consists of\nan MA-branch and a TE-branch. The MA-branch targets short-to-long-range\ncorrelations, combining a bidirectional Mamba2 block with our token\nrearrangement method, Rotary Major Scan, and our review tokens developed for\nlong video generation. The TE-branch is a novel TEmporal Swin Attention block\nthat focuses on temporal correlations between adjacent tokens and medium-range\ntokens. The MATE block addresses the adjacency preservation issue of Mamba and\nimproves the consistency of generated videos significantly. Experimental\nresults show that LinGen outperforms DiT (with a 75.6% win rate) in video\nquality with up to 15$\\times$ (11.5$\\times$) FLOPs (latency) reduction.\nFurthermore, both automatic metrics and human evaluation demonstrate our\nLinGen-4B yields comparable video quality to state-of-the-art models (with a\n50.5%, 52.1%, 49.1% win rate with respect to Gen-3, LumaLabs, and Kling,\nrespectively). This paves the way to hour-length movie generation and real-time\ninteractive video generation. We provide 68s video generation results and more\nexamples in our project website: https://lineargen.github.io/.\n","authors":["Hongjie Wang","Chih-Yao Ma","Yen-Cheng Liu","Ji Hou","Tao Xu","Jialiang Wang","Felix Juefei-Xu","Yaqiao Luo","Peizhao Zhang","Tingbo Hou","Peter Vajda","Niraj K. Jha","Xiaoliang Dai"],"pdf_url":"https://arxiv.org/pdf/2412.09856v1.pdf","comment":"20 pages, 20 figures"},{"id":"http://arxiv.org/abs/2412.09324v2","updated":"2024-12-13T04:51:11Z","published":"2024-12-12T14:49:55Z","title":"Are Conditional Latent Diffusion Models Effective for Image Restoration?","summary":"  Recent advancements in image restoration increasingly employ conditional\nlatent diffusion models (CLDMs). While these models have demonstrated notable\nperformance improvements in recent years, this work questions their suitability\nfor IR tasks. CLDMs excel in capturing high-level semantic correlations, making\nthem effective for tasks like text-to-image generation with spatial\nconditioning. However, in IR, where the goal is to enhance image perceptual\nquality, these models face difficulty of modeling the relationship between\ndegraded images and ground truth images using a low-level representation. To\nsupport our claims, we compare state-of-the-art CLDMs with traditional image\nrestoration models through extensive experiments. Results reveal that despite\nthe scaling advantages of CLDMs, they suffer from high distortion and semantic\ndeviation, especially in cases with minimal degradation, where traditional\nmethods outperform them. Additionally, we perform empirical studies to examine\nthe impact of various CLDM design elements on their restoration performance. We\nhope this finding inspires a reexamination of current CLDM-based IR solutions,\nopening up more opportunities in this field.\n","authors":["Yunchen Yuan","Junyuan Xiao","Xinjie Li"],"pdf_url":"https://arxiv.org/pdf/2412.09324v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13139v2","updated":"2024-12-13T04:45:39Z","published":"2024-10-17T01:51:58Z","title":"See Behind Walls in Real-time Using Aerial Drones and Augmented Reality","summary":"  This work presents ARD2, a framework that enables real-time through-wall\nsurveillance using two aerial drones and an augmented reality (AR) device. ARD2\nconsists of two main steps: target direction estimation and contour\nreconstruction. In the first stage, ARD2 leverages geometric relationships\nbetween the drones, the user, and the target to project the target's direction\nonto the user's AR display. In the second stage, images from the drones are\nsynthesized to reconstruct the target's contour, allowing the user to visualize\nthe target behind walls. Experimental results demonstrate the system's accuracy\nin both direction estimation and contour reconstruction.\n","authors":["Sikai Yang","Kang Yang","Yuning Chen","Fan Zhao","Wan Du"],"pdf_url":"https://arxiv.org/pdf/2410.13139v2.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2409.19454v4","updated":"2024-12-13T04:44:31Z","published":"2024-09-28T20:40:18Z","title":"See Where You Read with Eye Gaze Tracking and Large Language Model","summary":"  Losing track of reading progress during line switching can be frustrating.\nEye gaze tracking technology offers a potential solution by highlighting read\nparagraphs, aiding users in avoiding wrong line switches. However, the gap\nbetween gaze tracking accuracy (2-3 cm) and text line spacing (3-5 mm) makes\ndirect application impractical. Existing methods leverage the linear reading\npattern but fail during jump reading. This paper presents a reading tracking\nand highlighting system that supports both linear and jump reading. Based on\nexperimental insights from the gaze nature study of 16 users, two gaze error\nmodels are designed to enable both jump reading detection and relocation. The\nsystem further leverages the large language model's contextual perception\ncapability in aiding reading tracking. A reading tracking domain-specific\nline-gaze alignment opportunity is also exploited to enable dynamic and\nfrequent calibration of the gaze results. Controlled experiments demonstrate\nreliable linear reading tracking, as well as 84% accuracy in tracking jump\nreading. Furthermore, real field tests with 18 volunteers demonstrated the\nsystem's effectiveness in tracking and highlighting read paragraphs, improving\nreading efficiency, and enhancing user experience.\n","authors":["Sikai Yang","Gang Yan","Wan Du"],"pdf_url":"https://arxiv.org/pdf/2409.19454v4.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2409.19554v4","updated":"2024-12-13T04:43:40Z","published":"2024-09-29T04:43:10Z","title":"Tri-Cam: Practical Eye Gaze Tracking via Camera Network","summary":"  As human eyes serve as conduits of rich information, unveiling emotions,\nintentions, and even aspects of an individual's health and overall well-being,\ngaze tracking also enables various human-computer interaction applications, as\nwell as insights in psychological and medical research. However, existing gaze\ntracking solutions fall short at handling free user movement, and also require\nlaborious user effort in system calibration. We introduce Tri-Cam, a practical\ndeep learning-based gaze tracking system using three affordable RGB webcams. It\nfeatures a split network structure for efficient training, as well as\ndesignated network designs to handle the separated gaze tracking tasks. Tri-Cam\nis also equipped with an implicit calibration module, which makes use of mouse\nclick opportunities to reduce calibration overhead on the user's end. We\nevaluate Tri-Cam against Tobii, the state-of-the-art commercial eye tracker,\nachieving comparable accuracy, while supporting a wider free movement area. In\nconclusion, Tri-Cam provides a user-friendly, affordable, and robust gaze\ntracking solution that could practically enable various applications.\n","authors":["Sikai Yang","Wan Du"],"pdf_url":"https://arxiv.org/pdf/2409.19554v4.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2412.09846v1","updated":"2024-12-13T04:29:08Z","published":"2024-12-13T04:29:08Z","title":"A Single-Frame and Multi-Frame Cascaded Image Super-Resolution Method","summary":"  The objective of image super-resolution is to reconstruct a high-resolution\n(HR) image with the prior knowledge from one or several low-resolution (LR)\nimages. However, in the real world, due to the limited complementary\ninformation, the performance of both single-frame and multi-frame\nsuper-resolution reconstruction degrades rapidly as the magnification\nincreases. In this paper, we propose a novel two-step image super resolution\nmethod concatenating multi-frame super-resolution (MFSR) with single-frame\nsuper-resolution (SFSR), to progressively upsample images to the desired\nresolution. The proposed method consisting of an L0-norm constrained\nreconstruction scheme and an enhanced residual back-projection network,\nintegrating the flexibility of the variational modelbased method and the\nfeature learning capacity of the deep learning-based method. To verify the\neffectiveness of the proposed algorithm, extensive experiments with both\nsimulated and real world sequences were implemented. The experimental results\nshow that the proposed method yields superior performance in both objective and\nperceptual quality measurements. The average PSNRs of the cascade model in set5\nand set14 are 33.413 dB and 29.658 dB respectively, which are 0.76 dB and 0.621\ndB more than the baseline method. In addition, the experiment indicates that\nthis cascade model can be robustly applied to different SFSR and MFSR methods.\n","authors":["Jing Sun","Qiangqiang Yuan","Huanfeng Shen","Jie Li","Liangpei Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.09846v1.pdf","comment":"20 pages, 13 figures"},{"id":"http://arxiv.org/abs/2412.09844v1","updated":"2024-12-13T04:27:08Z","published":"2024-12-13T04:27:08Z","title":"Real-time Identity Defenses against Malicious Personalization of\n  Diffusion Models","summary":"  Personalized diffusion models, capable of synthesizing highly realistic\nimages based on a few reference portraits, pose substantial social, ethical,\nand legal risks by enabling identity replication. Existing defense mechanisms\nrely on computationally intensive adversarial perturbations tailored to\nindividual images, rendering them impractical for real-world deployment. This\nstudy introduces Real-time Identity Defender (RID), a neural network designed\nto generate adversarial perturbations through a single forward pass, bypassing\nthe need for image-specific optimization. RID achieves unprecedented\nefficiency, with defense times as low as 0.12 seconds on a single GPU (4,400\ntimes faster than leading methods) and 1.1 seconds per image on a standard\nIntel i9 CPU, making it suitable for edge devices such as smartphones. Despite\nits efficiency, RID matches state-of-the-art performance across visual and\nquantitative benchmarks, effectively mitigating identity replication risks. Our\nanalysis reveals that RID's perturbations mimic the efficacy of traditional\ndefenses while exhibiting properties distinct from natural noise, such as\nGaussian perturbations. To enhance robustness, we extend RID into an ensemble\nframework that integrates multiple pre-trained text-to-image diffusion models,\nensuring resilience against black-box attacks and post-processing techniques,\nincluding JPEG compression and diffusion-based purification.\n","authors":["Hanzhong Guo","Shen Nie","Chao Du","Tianyu Pang","Hao Sun","Chongxuan Li"],"pdf_url":"https://arxiv.org/pdf/2412.09844v1.pdf","comment":"21 pages, 7 figures"},{"id":"http://arxiv.org/abs/2412.09842v1","updated":"2024-12-13T04:22:23Z","published":"2024-12-13T04:22:23Z","title":"Leveraging Programmatically Generated Synthetic Data for Differentially\n  Private Diffusion Training","summary":"  Programmatically generated synthetic data has been used in differential\nprivate training for classification to enhance performance without privacy\nleakage. However, as the synthetic data is generated from a random process, the\ndistribution of real data and the synthetic data are distinguishable and\ndifficult to transfer. Therefore, the model trained with the synthetic data\ngenerates unrealistic random images, raising challenges to adapt the synthetic\ndata for generative models. In this work, we propose DP-SynGen, which leverages\nprogrammatically generated synthetic data in diffusion models to address this\nchallenge. By exploiting the three stages of diffusion models(coarse, context,\nand cleaning) we identify stages where synthetic data can be effectively\nutilized. We theoretically and empirically verified that cleaning and coarse\nstages can be trained without private data, replacing them with synthetic data\nto reduce the privacy budget. The experimental results show that DP-SynGen\nimproves the quality of generative data by mitigating the negative impact of\nprivacy-induced noise on the generation process.\n","authors":["Yujin Choi","Jinseong Park","Junyoung Byun","Jaewook Lee"],"pdf_url":"https://arxiv.org/pdf/2412.09842v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09841v1","updated":"2024-12-13T04:19:48Z","published":"2024-12-13T04:19:48Z","title":"Super-Resolution for Remote Sensing Imagery via the Coupling of a\n  Variational Model and Deep Learning","summary":"  Image super-resolution (SR) is an effective way to enhance the spatial\nresolution and detail information of remote sensing images, to obtain a\nsuperior visual quality. As SR is severely ill-conditioned, effective image\npriors are necessary to regularize the solution space and generate the\ncorresponding high-resolution (HR) image. In this paper, we propose a novel\ngradient-guided multi-frame super-resolution (MFSR) framework for remote\nsensing imagery reconstruction. The framework integrates a learned gradient\nprior as the regularization term into a model-based optimization method.\nSpecifically, the local gradient regularization (LGR) prior is derived from the\ndeep residual attention network (DRAN) through gradient profile transformation.\nThe non-local total variation (NLTV) prior is characterized using the spatial\nstructure similarity of the gradient patches with the maximum a posteriori\n(MAP) model. The modeled prior performs well in preserving edge smoothness and\nsuppressing visual artifacts, while the learned prior is effective in enhancing\nsharp edges and recovering fine structures. By incorporating the two\ncomplementary priors into an adaptive norm based reconstruction framework, the\nmixed L1 and L2 regularization minimization problem is optimized to achieve the\nrequired HR remote sensing image. Extensive experimental results on remote\nsensing data demonstrate that the proposed method can produce visually pleasant\nimages and is superior to several of the state-of-the-art SR algorithms in\nterms of the quantitative evaluation.\n","authors":["Jing Sun","Huanfeng Shen","Qiangqiang Yuan","Liangpei Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.09841v1.pdf","comment":"18 pages, 14 figures"},{"id":"http://arxiv.org/abs/2406.07828v2","updated":"2024-12-13T04:12:25Z","published":"2024-06-12T02:48:52Z","title":"Spatial Annealing for Efficient Few-shot Neural Rendering","summary":"  Neural Radiance Fields (NeRF) with hybrid representations have shown\nimpressive capabilities for novel view synthesis, delivering high efficiency.\nNonetheless, their performance significantly drops with sparse input views.\nVarious regularization strategies have been devised to address these\nchallenges. However, these strategies either require additional rendering costs\nor involve complex pipeline designs, leading to a loss of training efficiency.\nAlthough FreeNeRF has introduced an efficient frequency annealing strategy, its\noperation on frequency positional encoding is incompatible with the efficient\nhybrid representations. In this paper, we introduce an accurate and efficient\nfew-shot neural rendering method named \\textbf{S}patial \\textbf{A}nnealing\nregularized \\textbf{NeRF} (\\textbf{SANeRF}), which adopts the pre-filtering\ndesign of a hybrid representation. We initially establish the analytical\nformulation of the frequency band limit for a hybrid architecture by deducing\nits filtering process. Based on this analysis, we propose a universal form of\nfrequency annealing in the spatial domain, which can be implemented by\nmodulating the sampling kernel to exponentially shrink from an initial one with\na narrow grid tangent kernel spectrum. This methodology is crucial for\nstabilizing the early stages of the training phase and significantly\ncontributes to enhancing the subsequent process of detail refinement. Our\nextensive experiments reveal that, by adding merely one line of code, SANeRF\ndelivers superior rendering quality and much faster reconstruction speed\ncompared to current few-shot neural rendering methods. Notably, SANeRF\noutperforms FreeNeRF on the Blender dataset, achieving 700$\\times$ faster\nreconstruction speed.\n","authors":["Yuru Xiao","Deming Zhai","Wenbo Zhao","Kui Jiang","Junjun Jiang","Xianming Liu"],"pdf_url":"https://arxiv.org/pdf/2406.07828v2.pdf","comment":"AAAI 2025, code available at https://github.com/pulangk97/SANeRF"},{"id":"http://arxiv.org/abs/2409.07040v3","updated":"2024-12-13T04:00:36Z","published":"2024-09-11T06:12:03Z","title":"Retinex-RAWMamba: Bridging Demosaicing and Denoising for Low-Light RAW\n  Image Enhancement","summary":"  Low-light image enhancement, particularly in cross-domain tasks such as\nmapping from the raw domain to the sRGB domain, remains a significant\nchallenge. Many deep learning-based methods have been developed to address this\nissue and have shown promising results in recent years. However, single-stage\nmethods, which attempt to unify the complex mapping across both domains,\nleading to limited denoising performance. In contrast, two-stage approaches\ntypically decompose a raw image with color filter arrays (CFA) into a\nfour-channel RGGB format before feeding it into a neural network. However, this\nstrategy overlooks the critical role of demosaicing within the Image Signal\nProcessing (ISP) pipeline, leading to color distortions under varying lighting\nconditions, especially in low-light scenarios. To address these issues, we\ndesign a novel Mamba scanning mechanism, called RAWMamba, to effectively handle\nraw images with different CFAs. Furthermore, we present a Retinex Decomposition\nModule (RDM) grounded in Retinex prior, which decouples illumination from\nreflectance to facilitate more effective denoising and automatic non-linear\nexposure correction. By bridging demosaicing and denoising, better raw image\nenhancement is achieved. Experimental evaluations conducted on public datasets\nSID and MCR demonstrate that our proposed RAWMamba achieves state-of-the-art\nperformance on cross-domain mapping.\n","authors":["Xianmin Chen","Peiliang Huang","Xiaoxu Feng","Dingwen Zhang","Longfei Han","Junwei Han"],"pdf_url":"https://arxiv.org/pdf/2409.07040v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09835v1","updated":"2024-12-13T03:56:40Z","published":"2024-12-13T03:56:40Z","title":"Which cycling environment appears safer? Learning cycling safety\n  perceptions from pairwise image comparisons","summary":"  Cycling is critical for cities to transition to more sustainable transport\nmodes. Yet, safety concerns remain a critical deterrent for individuals to\ncycle. If individuals perceive an environment as unsafe for cycling, it is\nlikely that they will prefer other means of transportation. Yet, capturing and\nunderstanding how individuals perceive cycling risk is complex and often slow,\nwith researchers defaulting to traditional surveys and in-loco interviews. In\nthis study, we tackle this problem. We base our approach on using pairwise\ncomparisons of real-world images, repeatedly presenting respondents with pairs\nof road environments and asking them to select the one they perceive as safer\nfor cycling, if any. Using the collected data, we train a siamese-convolutional\nneural network using a multi-loss framework that learns from individuals'\nresponses, learns preferences directly from images, and includes ties (often\ndiscarded in the literature). Effectively, this model learns to predict\nhuman-style perceptions, evaluating which cycling environments are perceived as\nsafer. Our model achieves good results, showcasing this approach has a\nreal-life impact, such as improving interventions' effectiveness. Furthermore,\nit facilitates the continuous assessment of changing cycling environments,\npermitting short-term evaluations of measures to enhance perceived cycling\nsafety. Finally, our method can be efficiently deployed in different locations\nwith a growing number of openly available street-view images.\n","authors":["Miguel Costa","Manuel Marques","Carlos Lima Azevedo","Felix Wilhelm Siebert","Filipe Moura"],"pdf_url":"https://arxiv.org/pdf/2412.09835v1.pdf","comment":"\\copyright 2024 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2411.16946v2","updated":"2024-12-13T03:42:00Z","published":"2024-11-25T21:35:42Z","title":"Lens Distortion Encoding System Version 1.0","summary":"  Lens Distortion Encoding System (LDES) allows for a distortion-accurate\nworkflow, with a seamless interchange of high quality motion picture images\nregardless of the lens source. This system is similar in a concept to the\nAcademy Color Encoding System (ACES), but for distortion. Presented solution is\nfully compatible with existing software/plug-in tools for STMapping found in\npopular production software like Adobe After Effects or DaVinci Resolve. LDES\nutilizes common distortion space and produces single high-quality, animatable\nSTMap used for direct transformation of one view to another, neglecting the\nneed of lens-swapping for each shoot. The LDES profile of a lens consist of two\nelements; View Map texture, and Footage Map texture, each labeled with the FOV\nvalue. Direct distortion mapping is produced by sampling of the Footage Map\nthrough the View Map. The result; animatable mapping texture, is then used to\nsample the footage to a desired distortion. While the Footage Map is specific\nto a footage, View Maps can be freely combined/transitioned and animated,\nallowing for effects like smooth shift from anamorphic to spherical distortion,\npreviously impossible to achieve in practice. Presented LDES Version 1.0 uses\ncommon 32-bit STMap format for encoding, supported by most compositing\nsoftware, directly or via plug-ins. The difference between standard STMap\nworkflow and LDES is that it encodes absolute pixel position in the spherical\nimage model. The main benefit of this approach is the ability to achieve a\nsimilar look of a highly expensive lens using some less expensive equipment in\nterms of distortion. It also provides greater artistic control and never seen\nbefore manipulation of footage.\n","authors":["Jakub Maksymilian Fober"],"pdf_url":"https://arxiv.org/pdf/2411.16946v2.pdf","comment":"8 pages, 1 figure, 2 tables"},{"id":"http://arxiv.org/abs/2412.09828v1","updated":"2024-12-13T03:39:09Z","published":"2024-12-13T03:39:09Z","title":"MSC: Multi-Scale Spatio-Temporal Causal Attention for Autoregressive\n  Video Diffusion","summary":"  Diffusion transformers enable flexible generative modeling for video.\nHowever, it is still technically challenging and computationally expensive to\ngenerate high-resolution videos with rich semantics and complex motion. Similar\nto languages, video data are also auto-regressive by nature, so it is\ncounter-intuitive to use attention mechanism with bi-directional dependency in\nthe model. Here we propose a Multi-Scale Causal (MSC) framework to address\nthese problems. Specifically, we introduce multiple resolutions in the spatial\ndimension and high-low frequencies in the temporal dimension to realize\nefficient attention calculation. Furthermore, attention blocks on multiple\nscales are combined in a controlled way to allow causal conditioning on noisy\nimage frames for diffusion training, based on the idea that noise destroys\ninformation at different rates on different resolutions. We theoretically show\nthat our approach can greatly reduce the computational complexity and enhance\nthe efficiency of training. The causal attention diffusion framework can also\nbe used for auto-regressive long video generation, without violating the\nnatural order of frame sequences.\n","authors":["Xunnong Xu","Mengying Cao"],"pdf_url":"https://arxiv.org/pdf/2412.09828v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2412.09827v1","updated":"2024-12-13T03:38:49Z","published":"2024-12-13T03:38:49Z","title":"Low-Rank Adaptation with Task-Relevant Feature Enhancement for\n  Fine-tuning Language Models","summary":"  Fine-tuning pre-trained large language models in a parameter-efficient manner\nis widely studied for its effectiveness and efficiency. LoRA is one of the most\nwidely used methods, which assumes that the optimization process is essentially\nlow dimensional. Although LoRA has demonstrated commendable performance, there\nremains a significant performance gap between LoRA and full fine-tuning when\nlearning new tasks. In this work, we propose Low-Rank Adaptation with\nTask-Relevant Feature Enhancement(LoRATRF) for enhancing task-relevant features\nfrom the perspective of editing neural network representations. To prioritize\ntask-relevant features, a task-aware filter that selectively extracts valuable\nknowledge from hidden representations for the target or current task is\ndesigned. As the experiments on a vareity of datasets including NLU,\ncommonsense reasoning and mathematical reasoning tasks demonstrates, our method\nreduces 33.71% parameters and achieves better performance on a variety of\ndatasets in comparison with SOTA low-rank methods.\n","authors":["Changqun Li","Chaofan Ding","Kexin Luan","Xinhan Di"],"pdf_url":"https://arxiv.org/pdf/2412.09827v1.pdf","comment":"6 Pages, 3 figures accepted by AAAI 2025 CoLoRAI - Connecting\n  Low-Rank Representations in AI Workshop"},{"id":"http://arxiv.org/abs/2412.09349v2","updated":"2024-12-13T03:30:44Z","published":"2024-12-12T15:15:59Z","title":"DisPose: Disentangling Pose Guidance for Controllable Human Image\n  Animation","summary":"  Controllable human image animation aims to generate videos from reference\nimages using driving videos. Due to the limited control signals provided by\nsparse guidance (e.g., skeleton pose), recent works have attempted to introduce\nadditional dense conditions (e.g., depth map) to ensure motion alignment.\nHowever, such strict dense guidance impairs the quality of the generated video\nwhen the body shape of the reference character differs significantly from that\nof the driving video. In this paper, we present DisPose to mine more\ngeneralizable and effective control signals without additional dense input,\nwhich disentangles the sparse skeleton pose in human image animation into\nmotion field guidance and keypoint correspondence. Specifically, we generate a\ndense motion field from a sparse motion field and the reference image, which\nprovides region-level dense guidance while maintaining the generalization of\nthe sparse pose control. We also extract diffusion features corresponding to\npose keypoints from the reference image, and then these point features are\ntransferred to the target pose to provide distinct identity information. To\nseamlessly integrate into existing models, we propose a plug-and-play hybrid\nControlNet that improves the quality and consistency of generated videos while\nfreezing the existing model parameters. Extensive qualitative and quantitative\nexperiments demonstrate the superiority of DisPose compared to current methods.\nCode:\n\\href{https://github.com/lihxxx/DisPose}{https://github.com/lihxxx/DisPose}.\n","authors":["Hongxiang Li","Yaowei Li","Yuhang Yang","Junjie Cao","Zhihong Zhu","Xuxin Cheng","Long Chen"],"pdf_url":"https://arxiv.org/pdf/2412.09349v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09822v1","updated":"2024-12-13T03:20:53Z","published":"2024-12-13T03:20:53Z","title":"Dynamic Try-On: Taming Video Virtual Try-on with Dynamic Attention\n  Mechanism","summary":"  Video try-on stands as a promising area for its tremendous real-world\npotential. Previous research on video try-on has primarily focused on\ntransferring product clothing images to videos with simple human poses, while\nperforming poorly with complex movements. To better preserve clothing details,\nthose approaches are armed with an additional garment encoder, resulting in\nhigher computational resource consumption. The primary challenges in this\ndomain are twofold: (1) leveraging the garment encoder's capabilities in video\ntry-on while lowering computational requirements; (2) ensuring temporal\nconsistency in the synthesis of human body parts, especially during rapid\nmovements. To tackle these issues, we propose a novel video try-on framework\nbased on Diffusion Transformer(DiT), named Dynamic Try-On.\n  To reduce computational overhead, we adopt a straightforward approach by\nutilizing the DiT backbone itself as the garment encoder and employing a\ndynamic feature fusion module to store and integrate garment features. To\nensure temporal consistency of human body parts, we introduce a limb-aware\ndynamic attention module that enforces the DiT backbone to focus on the regions\nof human limbs during the denoising process. Extensive experiments demonstrate\nthe superiority of Dynamic Try-On in generating stable and smooth try-on\nresults, even for videos featuring complicated human postures.\n","authors":["Jun Zheng","Jing Wang","Fuwei Zhao","Xujie Zhang","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2412.09822v1.pdf","comment":"Project Page: https://zhengjun-ai.github.io/dynamic-tryon-page/"},{"id":"http://arxiv.org/abs/2408.03703v2","updated":"2024-12-13T03:19:24Z","published":"2024-08-07T11:33:46Z","title":"CAS-ViT: Convolutional Additive Self-attention Vision Transformers for\n  Efficient Mobile Applications","summary":"  Vision Transformers (ViTs) mark a revolutionary advance in neural networks\nwith their token mixer's powerful global context capability. However, the\npairwise token affinity and complex matrix operations limit its deployment on\nresource-constrained scenarios and real-time applications, such as mobile\ndevices, although considerable efforts have been made in previous works. In\nthis paper, we introduce CAS-ViT: Convolutional Additive Self-attention Vision\nTransformers, to achieve a balance between efficiency and performance in mobile\napplications. Firstly, we argue that the capability of token mixers to obtain\nglobal contextual information hinges on multiple information interactions, such\nas spatial and channel domains. Subsequently, we propose Convolutional Additive\nToken Mixer (CATM) employing underlying spatial and channel attention as novel\ninteraction forms. This module eliminates troublesome complex operations such\nas matrix multiplication and Softmax. We introduce Convolutional Additive\nSelf-attention(CAS) block hybrid architecture and utilize CATM for each block.\nAnd further, we build a family of lightweight networks, which can be easily\nextended to various downstream tasks. Finally, we evaluate CAS-ViT across a\nvariety of vision tasks, including image classification, object detection,\ninstance segmentation, and semantic segmentation. Our M and T model achieves\n83.0\\%/84.1\\% top-1 with only 12M/21M parameters on ImageNet-1K. Meanwhile,\nthroughput evaluations on GPUs, ONNX, and iPhones also demonstrate superior\nresults compared to other state-of-the-art backbones. Extensive experiments\ndemonstrate that our approach achieves a better balance of performance,\nefficient inference and easy-to-deploy. Our code and model are available at:\n\\url{https://github.com/Tianfang-Zhang/CAS-ViT}\n","authors":["Tianfang Zhang","Lei Li","Yang Zhou","Wentao Liu","Chen Qian","Jenq-Neng Hwang","Xiangyang Ji"],"pdf_url":"https://arxiv.org/pdf/2408.03703v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08580v2","updated":"2024-12-13T03:13:47Z","published":"2024-12-11T17:57:10Z","title":"LAION-SG: An Enhanced Large-Scale Dataset for Training Complex\n  Image-Text Models with Structural Annotations","summary":"  Recent advances in text-to-image (T2I) generation have shown remarkable\nsuccess in producing high-quality images from text. However, existing T2I\nmodels show decayed performance in compositional image generation involving\nmultiple objects and intricate relationships. We attribute this problem to\nlimitations in existing datasets of image-text pairs, which lack precise\ninter-object relationship annotations with prompts only. To address this\nproblem, we construct LAION-SG, a large-scale dataset with high-quality\nstructural annotations of scene graphs (SG), which precisely describe\nattributes and relationships of multiple objects, effectively representing the\nsemantic structure in complex scenes. Based on LAION-SG, we train a new\nfoundation model SDXL-SG to incorporate structural annotation information into\nthe generation process. Extensive experiments show advanced models trained on\nour LAION-SG boast significant performance improvements in complex scene\ngeneration over models on existing datasets. We also introduce CompSG-Bench, a\nbenchmark that evaluates models on compositional image generation, establishing\na new standard for this domain. Our annotations with the associated processing\ncode, the foundation model and the benchmark protocol are publicly available at\nhttps://github.com/mengcye/LAION-SG.\n","authors":["Zejian Li","Chenye Meng","Yize Li","Ling Yang","Shengyuan Zhang","Jiarui Ma","Jiayi Li","Guang Yang","Changyuan Yang","Zhiyuan Yang","Jinxiong Chang","Lingyun Sun"],"pdf_url":"https://arxiv.org/pdf/2412.08580v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09817v1","updated":"2024-12-13T03:13:44Z","published":"2024-12-13T03:13:44Z","title":"Enhancing Multimodal Large Language Models Complex Reason via Similarity\n  Computation","summary":"  Multimodal large language models have experienced rapid growth, and numerous\ndifferent models have emerged. The interpretability of LVLMs remains an\nunder-explored area. Especially when faced with more complex tasks such as\nchain-of-thought reasoning, its internal mechanisms still resemble a black box\nthat is difficult to decipher. By studying the interaction and information flow\nbetween images and text, we noticed that in models such as LLaVA1.5, image\ntokens that are semantically related to text are more likely to have\ninformation flow convergence in the LLM decoding layer, and these image tokens\nreceive higher attention scores. However, those image tokens that are less\nrelevant to the text do not have information flow convergence, and they only\nget very small attention scores. To efficiently utilize the image information,\nwe propose a new image token reduction method, Simignore, which aims to improve\nthe complex reasoning ability of LVLMs by computing the similarity between\nimage and text embeddings and ignoring image tokens that are irrelevant and\nunimportant to the text. Through extensive experiments, we demonstrate the\neffectiveness of our method for complex reasoning tasks. The paper's source\ncode can be accessed from \\url{https://github.com/FanshuoZeng/Simignore}.\n","authors":["Xiaofeng Zhang","Fanshuo Zeng","Yihao Quan","Zheng Hui","Jiawei Yao"],"pdf_url":"https://arxiv.org/pdf/2412.09817v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10210v4","updated":"2024-12-13T03:11:30Z","published":"2024-04-16T01:41:22Z","title":"MK-SGN: A Spiking Graph Convolutional Network with Multimodal Fusion and\n  Knowledge Distillation for Skeleton-based Action Recognition","summary":"  In recent years, multimodal Graph Convolutional Networks (GCNs) have achieved\nremarkable performance in skeleton-based action recognition. The reliance on\nhigh-energy-consuming continuous floating-point operations inherent in\nGCN-based methods poses significant challenges for deployment in\nenergy-constrained, battery-powered edge devices. To address these limitations,\nMK-SGN, a Spiking Graph Convolutional Network with Multimodal Fusion and\nKnowledge Distillation, is proposed to leverage the energy efficiency of\nSpiking Neural Networks (SNNs) for skeleton-based action recognition for the\nfirst time. By integrating the energy-saving properties of SNNs with the graph\nrepresentation capabilities of GCNs, MK-SGN achieves significant reductions in\nenergy consumption while maintaining competitive recognition accuracy. Firstly,\nwe formulate a Spiking Multimodal Fusion (SMF) module to effectively fuse\nmultimodal skeleton data represented as spike-form features. Secondly, we\npropose the Self-Attention Spiking Graph Convolution (SA-SGC) module and the\nSpiking Temporal Convolution (STC) module, to capture spatial relationships and\ntemporal dynamics of spike-form features. Finally, we propose an integrated\nknowledge distillation strategy to transfer information from the multimodal GCN\nto the SGN, incorporating both intermediate-layer distillation and soft-label\ndistillation to enhance the performance of the SGN. MK-SGN exhibits substantial\nadvantages, surpassing state-of-the-art GCN frameworks in energy efficiency and\noutperforming state-of-the-art SNN frameworks in recognition accuracy. The\nproposed method achieves a remarkable reduction in energy consumption,\nexceeding 98\\% compared to conventional GCN-based approaches. This research\nestablishes a robust baseline for developing high-performance, energy-efficient\nSNN-based models for skeleton-based action recognition\n","authors":["Naichuan Zheng","Hailun Xia","Zeyu Liang","Yuchen Du"],"pdf_url":"https://arxiv.org/pdf/2404.10210v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09063v2","updated":"2024-12-13T02:41:26Z","published":"2024-12-12T08:46:22Z","title":"An Efficient Framework for Enhancing Discriminative Models via Diffusion\n  Techniques","summary":"  Image classification serves as the cornerstone of computer vision,\ntraditionally achieved through discriminative models based on deep neural\nnetworks. Recent advancements have introduced classification methods derived\nfrom generative models, which offer the advantage of zero-shot classification.\nHowever, these methods suffer from two main drawbacks: high computational\noverhead and inferior performance compared to discriminative models. Inspired\nby the coordinated cognitive processes of rapid-slow pathway interactions in\nthe human brain during visual signal recognition, we propose the\nDiffusion-Based Discriminative Model Enhancement Framework (DBMEF). This\nframework seamlessly integrates discriminative and generative models in a\ntraining-free manner, leveraging discriminative models for initial predictions\nand endowing deep neural networks with rethinking capabilities via diffusion\nmodels. Consequently, DBMEF can effectively enhance the classification accuracy\nand generalization capability of discriminative models in a plug-and-play\nmanner. We have conducted extensive experiments across 17 prevalent deep model\narchitectures with different training methods, including both CNN-based models\nsuch as ResNet and Transformer-based models like ViT, to demonstrate the\neffectiveness of the proposed DBMEF. Specifically, the framework yields a\n1.51\\% performance improvement for ResNet-50 on the ImageNet dataset and 3.02\\%\non the ImageNet-A dataset. In conclusion, our research introduces a novel\nparadigm for image classification, demonstrating stable improvements across\ndifferent datasets and neural networks. The code is available at\nhttps://github.com/ChunXiaostudy/DBMEF.\n","authors":["Chunxiao Li","Xiaoxiao Wang","Boming Miao","Chuanlong Xie","Zizhe Wang","Yao Zhu"],"pdf_url":"https://arxiv.org/pdf/2412.09063v2.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.09799v1","updated":"2024-12-13T02:36:29Z","published":"2024-12-13T02:36:29Z","title":"CP-DETR: Concept Prompt Guide DETR Toward Stronger Universal Object\n  Detection","summary":"  Recent research on universal object detection aims to introduce language in a\nSoTA closed-set detector and then generalize the open-set concepts by\nconstructing large-scale (text-region) datasets for training. However, these\nmethods face two main challenges: (i) how to efficiently use the prior\ninformation in the prompts to genericise objects and (ii) how to reduce\nalignment bias in the downstream tasks, both leading to sub-optimal performance\nin some scenarios beyond pre-training. To address these challenges, we propose\na strong universal detection foundation model called CP-DETR, which is\ncompetitive in almost all scenarios, with only one pre-training weight.\nSpecifically, we design an efficient prompt visual hybrid encoder that enhances\nthe information interaction between prompt and visual through scale-by-scale\nand multi-scale fusion modules. Then, the hybrid encoder is facilitated to\nfully utilize the prompted information by prompt multi-label loss and auxiliary\ndetection head. In addition to text prompts, we have designed two practical\nconcept prompt generation methods, visual prompt and optimized prompt, to\nextract abstract concepts through concrete visual examples and stably reduce\nalignment bias in downstream tasks. With these effective designs, CP-DETR\ndemonstrates superior universal detection performance in a broad spectrum of\nscenarios. For example, our Swin-T backbone model achieves 47.6 zero-shot AP on\nLVIS, and the Swin-L backbone model achieves 32.2 zero-shot AP on ODinW35.\nFurthermore, our visual prompt generation method achieves 68.4 AP on COCO val\nby interactive detection, and the optimized prompt achieves 73.1 fully-shot AP\non ODinW13.\n","authors":["Qibo Chen","Weizhong Jin","Jianyue Ge","Mengdi Liu","Yuchao Yan","Jian Jiang","Li Yu","Xuanjiang Guo","Shuchang Li","Jianzhong Chen"],"pdf_url":"https://arxiv.org/pdf/2412.09799v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2408.10188v6","updated":"2024-12-13T02:32:06Z","published":"2024-08-19T17:48:08Z","title":"LongVILA: Scaling Long-Context Visual Language Models for Long Videos","summary":"  Long-context capability is critical for multi-modal foundation models,\nespecially for long video understanding. We introduce LongVILA, a full-stack\nsolution for long-context visual-language models by co-designing the algorithm\nand system. For model training, we upgrade existing VLMs to support long video\nunderstanding by incorporating two additional stages, i.e., long context\nextension and long video supervised fine-tuning. However, training on long\nvideo is computationally and memory intensive. We introduce the long-context\nMulti-Modal Sequence Parallelism (MM-SP) system that efficiently parallelizes\nlong video training and inference, enabling 2M context length training on 256\nGPUs without any gradient checkpointing. LongVILA efficiently extends the\nnumber of video frames of VILA from 8 to 2048, achieving 99.8% accuracy in\n6,000-frame (more than 1 million tokens) video needle-in-a-haystack.\nLongVILA-7B demonstrates strong accuracy on 9 popular video benchmarks, e.g.\n65.1% VideoMME with subtitle. Besides, MM-SP is 2.1x - 5.7x faster than ring\nstyle sequence parallelism and 1.1x - 1.4x faster than Megatron with a hybrid\ncontext and tensor parallelism. Moreover, it seamlessly integrates with Hugging\nFace Transformers.\n","authors":["Yukang Chen","Fuzhao Xue","Dacheng Li","Qinghao Hu","Ligeng Zhu","Xiuyu Li","Yunhao Fang","Haotian Tang","Shang Yang","Zhijian Liu","Ethan He","Hongxu Yin","Pavlo Molchanov","Jan Kautz","Linxi Fan","Yuke Zhu","Yao Lu","Song Han"],"pdf_url":"https://arxiv.org/pdf/2408.10188v6.pdf","comment":"Code and models are available at\n  https://github.com/NVlabs/VILA/tree/main/longvila"},{"id":"http://arxiv.org/abs/2412.09795v1","updated":"2024-12-13T02:26:58Z","published":"2024-12-13T02:26:58Z","title":"Is it the model or the metric -- On robustness measures of deeplearning\n  models","summary":"  Determining the robustness of deep learning models is an established and\nongoing challenge within automated decision-making systems. With the advent and\nsuccess of techniques that enable advanced deep learning (DL), these models are\nbeing used in widespread applications, including high-stake ones like\nhealthcare, education, border-control. Therefore, it is critical to understand\nthe limitations of these models and predict their regions of failures, in order\nto create the necessary guardrails for their successful and safe deployment. In\nthis work, we revisit robustness, specifically investigating the sufficiency of\nrobust accuracy (RA), within the context of deepfake detection. We present\nrobust ratio (RR) as a complementary metric, that can quantify the changes to\nthe normalized or probability outcomes under input perturbation. We present a\ncomparison of RA and RR and demonstrate that despite similar RA between models,\nthe models show varying RR under different tolerance (perturbation) levels.\n","authors":["Zhijin Lyu","Yutong Jin","Sneha Das"],"pdf_url":"https://arxiv.org/pdf/2412.09795v1.pdf","comment":"Extended abstract at Northern Lights Deep Learning (NLDL) Conference\n  2025"},{"id":"http://arxiv.org/abs/2409.00755v3","updated":"2024-12-13T02:10:35Z","published":"2024-09-01T15:48:20Z","title":"Trusted Unified Feature-Neighborhood Dynamics for Multi-View\n  Classification","summary":"  Multi-view classification (MVC) faces inherent challenges due to domain gaps\nand inconsistencies across different views, often resulting in uncertainties\nduring the fusion process. While Evidential Deep Learning (EDL) has been\neffective in addressing view uncertainty, existing methods predominantly rely\non the Dempster-Shafer combination rule, which is sensitive to conflicting\nevidence and often neglects the critical role of neighborhood structures within\nmulti-view data. To address these limitations, we propose a Trusted Unified\nFeature-NEighborhood Dynamics (TUNED) model for robust MVC. This method\neffectively integrates local and global feature-neighborhood (F-N) structures\nfor robust decision-making. Specifically, we begin by extracting local F-N\nstructures within each view. To further mitigate potential uncertainties and\nconflicts in multi-view fusion, we employ a selective Markov random field that\nadaptively manages cross-view neighborhood dependencies. Additionally, we\nemploy a shared parameterized evidence extractor that learns global consensus\nconditioned on local F-N structures, thereby enhancing the global integration\nof multi-view features. Experiments on benchmark datasets show that our method\nimproves accuracy and robustness over existing approaches, particularly in\nscenarios with high uncertainty and conflicting views. The code will be made\navailable at https://github.com/JethroJames/TUNED.\n","authors":["Haojian Huang","Chuanyu Qin","Zhe Liu","Kaijing Ma","Jin Chen","Han Fang","Chao Ban","Hao Sun","Zhongjiang He"],"pdf_url":"https://arxiv.org/pdf/2409.00755v3.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2412.08410v2","updated":"2024-12-13T02:05:34Z","published":"2024-12-11T14:29:35Z","title":"Physical Informed Driving World Model","summary":"  Autonomous driving requires robust perception models trained on high-quality,\nlarge-scale multi-view driving videos for tasks like 3D object detection,\nsegmentation and trajectory prediction. While world models provide a\ncost-effective solution for generating realistic driving videos, challenges\nremain in ensuring these videos adhere to fundamental physical principles, such\nas relative and absolute motion, spatial relationship like occlusion and\nspatial consistency, and temporal consistency. To address these, we propose\nDrivePhysica, an innovative model designed to generate realistic multi-view\ndriving videos that accurately adhere to essential physical principles through\nthree key advancements: (1) a Coordinate System Aligner module that integrates\nrelative and absolute motion features to enhance motion interpretation, (2) an\nInstance Flow Guidance module that ensures precise temporal consistency via\nefficient 3D flow extraction, and (3) a Box Coordinate Guidance module that\nimproves spatial relationship understanding and accurately resolves occlusion\nhierarchies. Grounded in physical principles, we achieve state-of-the-art\nperformance in driving video generation quality (3.96 FID and 38.06 FVD on the\nNuscenes dataset) and downstream perception tasks. Our project homepage:\nhttps://metadrivescape.github.io/papers_project/DrivePhysica/page.html\n","authors":["Zhuoran Yang","Xi Guo","Chenjing Ding","Chiyu Wang","Wei Wu"],"pdf_url":"https://arxiv.org/pdf/2412.08410v2.pdf","comment":"project homepage:\n  https://metadrivescape.github.io/papers_project/DrivePhysica/page.html"},{"id":"http://arxiv.org/abs/2412.09782v1","updated":"2024-12-13T01:37:44Z","published":"2024-12-13T01:37:44Z","title":"EI-Drive: A Platform for Cooperative Perception with Realistic\n  Communication Models","summary":"  The growing interest in autonomous driving calls for realistic simulation\nplatforms capable of accurately simulating cooperative perception process in\nrealistic traffic scenarios. Existing studies for cooperative perception often\nhave not accounted for transmission latency and errors in real-world\nenvironments. To address this gap, we introduce EI-Drive, an edge-AI based\nautonomous driving simulation platform that integrates advanced cooperative\nperception with more realistic communication models. Built on the CARLA\nframework, EI-Drive features new modules for cooperative perception while\ntaking into account transmission latency and errors, providing a more realistic\nplatform for evaluating cooperative perception algorithms. In particular, the\nplatform enables vehicles to fuse data from multiple sources, improving\nsituational awareness and safety in complex environments. With its modular\ndesign, EI-Drive allows for detailed exploration of sensing, perception,\nplanning, and control in various cooperative driving scenarios. Experiments\nusing EI-Drive demonstrate significant improvements in vehicle safety and\nperformance, particularly in scenarios with complex traffic flow and network\nconditions. All code and documents are accessible on our GitHub page:\n\\url{https://ucd-dare.github.io/eidrive.github.io/}.\n","authors":["Hanchu Zhou","Edward Xie","Wei Shao","Dechen Gao","Michelle Dong","Junshan Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.09782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09775v1","updated":"2024-12-13T00:58:10Z","published":"2024-12-13T00:58:10Z","title":"waveOrder: generalist framework for label-agnostic computational\n  microscopy","summary":"  Correlative computational microscopy is accelerating the mapping of dynamic\nbiological systems by integrating morphological and molecular measurements\nacross spatial scales, from organelles to entire organisms. Visualization,\nmeasurement, and prediction of interactions among the components of biological\nsystems can be accelerated by generalist computational imaging frameworks that\nrelax the trade-offs imposed by multiplex dynamic imaging. This work reports a\ngeneralist framework for wave optical imaging of the architectural order\n(waveOrder) among biomolecules for encoding and decoding multiple specimen\nproperties from a minimal set of acquired channels, with or without fluorescent\nlabels. waveOrder expresses material properties in terms of elegant physically\nmotivated basis vectors directly interpretable as phase, absorption,\nbirefringence, diattenuation, and fluorophore density; and it expresses image\ndata in terms of directly measurable Stokes parameters. We report a\ncorresponding multi-channel reconstruction algorithm to recover specimen\nproperties in multiple contrast modes. With this framework, we implement\nmultiple 3D computational microscopy methods, including quantitative phase\nimaging, quantitative label-free imaging with phase and polarization, and\nfluorescence deconvolution imaging, across scales ranging from organelles to\nwhole zebrafish. These advances are available via an extensible open-source\ncomputational imaging library, waveOrder, and a napari plugin, recOrder.\n","authors":["Talon Chandler","Eduardo Hirata-Miyasaki","Ivan E. Ivanov","Ziwen Liu","Deepika Sundarraman","Allyson Quinn Ryan","Adrian Jacobo","Keir Balla","Shalin B. Mehta"],"pdf_url":"https://arxiv.org/pdf/2412.09775v1.pdf","comment":"11 pages of main text (5 figures, one table); 9 pages of\n  supplementary text (4 figures, one table); 5 ancillary videos"},{"id":"http://arxiv.org/abs/2412.09774v1","updated":"2024-12-13T00:57:47Z","published":"2024-12-13T00:57:47Z","title":"A Differentiable Wave Optics Model for End-to-End Computational Imaging\n  System Optimization","summary":"  End-to-end optimization, which simultaneously optimizes optics and\nalgorithms, has emerged as a powerful data-driven method for computational\nimaging system design. This method achieves joint optimization through\nbackpropagation by incorporating differentiable optics simulators to generate\nmeasurements and algorithms to extract information from measurements. However,\ndue to high computational costs, it is challenging to model both aberration and\ndiffraction in light transport for end-to-end optimization of compound optics.\nTherefore, most existing methods compromise physical accuracy by neglecting\nwave optics effects or off-axis aberrations, which raises concerns about the\nrobustness of the resulting designs. In this paper, we propose a differentiable\noptics simulator that efficiently models both aberration and diffraction for\ncompound optics. Using the simulator, we conduct end-to-end optimization on\nscene reconstruction and classification. Experimental results demonstrate that\nboth lenses and algorithms adopt different configurations depending on whether\nwave optics is modeled. We also show that systems optimized without wave optics\nsuffer from performance degradation when wave optics effects are introduced\nduring testing. These findings underscore the importance of accurate wave\noptics modeling in optimizing imaging systems for robust, high-performance\napplications.\n","authors":["Chi-Jui Ho","Yash Belhe","Steve Rotenberg","Ravi Ramamoorthi","Tzu-Mao Li","Nicholas Antipa"],"pdf_url":"https://arxiv.org/pdf/2412.09774v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14856v4","updated":"2024-12-13T00:46:38Z","published":"2024-06-21T04:02:19Z","title":"Accessible, At-Home Detection of Parkinson's Disease via Multi-task\n  Video Analysis","summary":"  Limited accessibility to neurological care leads to underdiagnosed\nParkinson's Disease (PD), preventing early intervention. Existing AI-based PD\ndetection methods primarily focus on unimodal analysis of motor or speech\ntasks, overlooking the multifaceted nature of the disease. To address this, we\nintroduce a large-scale, multi-task video dataset consisting of 1102 sessions\n(each containing videos of finger tapping, facial expression, and speech tasks\ncaptured via webcam) from 845 participants (272 with PD). We propose a novel\nUncertainty-calibrated Fusion Network (UFNet) that leverages this multimodal\ndata to enhance diagnostic accuracy. UFNet employs independent task-specific\nnetworks, trained with Monte Carlo Dropout for uncertainty quantification,\nfollowed by self-attended fusion of features, with attention weights\ndynamically adjusted based on task-specific uncertainties. To ensure\npatient-centered evaluation, the participants were randomly split into three\nsets: 60% for training, 20% for model selection, and 20% for final performance\nevaluation. UFNet significantly outperformed single-task models in terms of\naccuracy, area under the ROC curve (AUROC), and sensitivity while maintaining\nnon-inferior specificity. Withholding uncertain predictions further boosted the\nperformance, achieving 88.0+-0.3%$ accuracy, 93.0+-0.2% AUROC, 79.3+-0.9%\nsensitivity, and 92.6+-0.3% specificity, at the expense of not being able to\npredict for 2.3+-0.3% data (+- denotes 95% confidence interval). Further\nanalysis suggests that the trained model does not exhibit any detectable bias\nacross sex and ethnic subgroups and is most effective for individuals aged\nbetween 50 and 80. Requiring only a webcam and microphone, our approach\nfacilitates accessible home-based PD screening, especially in regions with\nlimited healthcare resources.\n","authors":["Md Saiful Islam","Tariq Adnan","Jan Freyberg","Sangwu Lee","Abdelrahman Abdelkader","Meghan Pawlik","Cathe Schwartz","Karen Jaffe","Ruth B. Schneider","E Ray Dorsey","Ehsan Hoque"],"pdf_url":"https://arxiv.org/pdf/2406.14856v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09772v1","updated":"2024-12-13T00:39:55Z","published":"2024-12-13T00:39:55Z","title":"Acquisition of Spatially-Varying Reflectance and Surface Normals via\n  Polarized Reflectance Fields","summary":"  Accurately measuring the geometry and spatially-varying reflectance of\nreal-world objects is a complex task due to their intricate shapes formed by\nconcave features, hollow engravings and diverse surfaces, resulting in\ninter-reflection and occlusion when photographed. Moreover, issues like lens\nflare and overexposure can arise from interference from secondary reflections\nand limitations of hardware even in professional studios. In this paper, we\npropose a novel approach using polarized reflectance field capture and a\ncomprehensive statistical analysis algorithm to obtain highly accurate surface\nnormals (within 0.1mm/px) and spatially-varying reflectance data, including\nalbedo, specular separation, roughness, and anisotropy parameters for realistic\nrendering and analysis. Our algorithm removes image artifacts via analytical\nmodeling and further employs both an initial step and an optimization step\ncomputed on the whole image collection to further enhance the precision of\nper-pixel surface reflectance and normal measurement. We showcase the captured\nshapes and reflectance of diverse objects with a wide material range, spanning\nfrom highly diffuse to highly glossy - a challenge unaddressed by prior\ntechniques. Our approach enhances downstream applications by offering precise\nmeasurements for realistic rendering and provides a valuable training dataset\nfor emerging research in inverse rendering. We will release the polarized\nreflectance fields of several captured objects with this work.\n","authors":["Jing Yang","Pratusha Bhuvana Prasad","Qing Zhang","Yajie Zhao"],"pdf_url":"https://arxiv.org/pdf/2412.09772v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10785v2","updated":"2024-12-13T00:17:21Z","published":"2024-07-15T15:03:01Z","title":"Interpretability analysis on a pathology foundation model reveals\n  biologically relevant embeddings across modalities","summary":"  Pathology plays an important role in disease diagnosis, treatment\ndecision-making and drug development. Previous works on interpretability for\nmachine learning models on pathology images have revolved around methods such\nas attention value visualization and deriving human-interpretable features from\nmodel heatmaps. Mechanistic interpretability is an emerging area of model\ninterpretability that focuses on reverse-engineering neural networks. Sparse\nAutoencoders (SAEs) have emerged as a promising direction in terms of\nextracting monosemantic features from polysemantic model activations. In this\nwork, we trained a Sparse Autoencoder on the embeddings of a pathology\npretrained foundation model. We found that Sparse Autoencoder features\nrepresent interpretable and monosemantic biological concepts. In particular,\nindividual SAE dimensions showed strong correlations with cell type counts such\nas plasma cells and lymphocytes. These biological representations were unique\nto the pathology pretrained model and were not found in a self-supervised model\npretrained on natural images. We demonstrated that such biologically-grounded\nmonosemantic representations evolved across the model's depth, and the\npathology foundation model eventually gained robustness to non-biological\nfactors such as scanner type. The emergence of biologically relevant SAE\nfeatures was generalizable to an out-of-domain dataset. Our work paved the way\nfor further exploration around interpretable feature dimensions and their\nutility for medical and clinical applications.\n","authors":["Nhat Le","Ciyue Shen","Chintan Shah","Blake Martin","Daniel Shenker","Harshith Padigela","Jennifer Hipp","Sean Grullon","John Abel","Harsha Vardhan Pokkalla","Dinkar Juyal"],"pdf_url":"https://arxiv.org/pdf/2407.10785v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13911v2","updated":"2024-12-13T23:49:45Z","published":"2024-07-18T21:52:57Z","title":"Continual Distillation Learning: An Empirical Study of Knowledge\n  Distillation in Prompt-based Continual Learning","summary":"  Knowledge Distillation (KD) focuses on using a teacher model to improve a\nstudent model. Traditionally, KD is studied in an offline fashion, where a\ntraining dataset is available before learning. In this work, we introduce the\nproblem of Continual Distillation Learning (CDL) that considers KD in the\nContinual Learning (CL) setup. A teacher model and a student model need to\nlearn a sequence of tasks, and the knowledge of the teacher model will be\ndistilled to the student to improve the student model in an online fashion. The\nCDL problem is valuable to study since for prompt-based continual learning\nmethods, using a larger vision transformer (ViT) leads to better performance in\ncontinual learning. Distilling the knowledge from a large ViT to a small ViT\ncan improve inference efficiency for promptbased CL models. To this end, we\nconducted experiments to study the CDL problem with three prompt-based CL\nmodels, i.e., L2P, DualPrompt and CODA-Prompt, where we utilized logit\ndistillation, feature distillation and prompt distillation for knowledge\ndistillation from a teacher model to a student model. Our findings of this\nstudy can serve as baselines for future CDL work.\n","authors":["Qifan Zhang","Yunhui Guo","Yu Xiang"],"pdf_url":"https://arxiv.org/pdf/2407.13911v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10604v1","updated":"2024-12-13T23:15:35Z","published":"2024-12-13T23:15:35Z","title":"EvalGIM: A Library for Evaluating Generative Image Models","summary":"  As the use of text-to-image generative models increases, so does the adoption\nof automatic benchmarking methods used in their evaluation. However, while\nmetrics and datasets abound, there are few unified benchmarking libraries that\nprovide a framework for performing evaluations across many datasets and\nmetrics. Furthermore, the rapid introduction of increasingly robust\nbenchmarking methods requires that evaluation libraries remain flexible to new\ndatasets and metrics. Finally, there remains a gap in synthesizing evaluations\nin order to deliver actionable takeaways about model performance. To enable\nunified, flexible, and actionable evaluations, we introduce EvalGIM (pronounced\n''EvalGym''), a library for evaluating generative image models. EvalGIM\ncontains broad support for datasets and metrics used to measure quality,\ndiversity, and consistency of text-to-image generative models. In addition,\nEvalGIM is designed with flexibility for user customization as a top priority\nand contains a structure that allows plug-and-play additions of new datasets\nand metrics. To enable actionable evaluation insights, we introduce\n''Evaluation Exercises'' that highlight takeaways for specific evaluation\nquestions. The Evaluation Exercises contain easy-to-use and reproducible\nimplementations of two state-of-the-art evaluation methods of text-to-image\ngenerative models: consistency-diversity-realism Pareto Fronts and\ndisaggregated measurements of performance disparities across groups. EvalGIM\nalso contains Evaluation Exercises that introduce two new analysis methods for\ntext-to-image generative models: robustness analyses of model rankings and\nbalanced evaluations across different prompt styles. We encourage text-to-image\nmodel exploration with EvalGIM and invite contributions at\nhttps://github.com/facebookresearch/EvalGIM/.\n","authors":["Melissa Hall","Oscar Mañas","Reyhane Askari","Mark Ibrahim","Candace Ross","Pietro Astolfi","Tariq Berrada Ifriqi","Marton Havasi","Yohann Benchetrit","Karen Ullrich","Carolina Braga","Abhishek Charnalia","Maeve Ryan","Mike Rabbat","Michal Drozdzal","Jakob Verbeek","Adriana Romero Soriano"],"pdf_url":"https://arxiv.org/pdf/2412.10604v1.pdf","comment":"For code, see https://github.com/facebookresearch/EvalGIM/tree/main"},{"id":"http://arxiv.org/abs/2412.08687v2","updated":"2024-12-13T23:12:23Z","published":"2024-12-11T18:59:46Z","title":"VisionArena: 230K Real World User-VLM Conversations with Preference\n  Labels","summary":"  With the growing adoption and capabilities of vision-language models (VLMs)\ncomes the need for benchmarks that capture authentic user-VLM interactions. In\nresponse, we create VisionArena, a dataset of 230K real-world conversations\nbetween users and VLMs. Collected from Chatbot Arena - an open-source platform\nwhere users interact with VLMs and submit preference votes - VisionArena spans\n73K unique users, 45 VLMs, and 138 languages. Our dataset contains three\nsubsets: VisionArena-Chat, 200k single and multi-turn conversations between a\nuser and a VLM; VisionArena-Battle, 30K conversations comparing two anonymous\nVLMs with user preference votes; and VisionArena-Bench, an automatic benchmark\nof 500 diverse user prompts that efficiently approximate the live Chatbot Arena\nmodel rankings. Additionally, we highlight the types of question asked by\nusers, the influence of response style on preference, and areas where models\noften fail. We find open-ended tasks like captioning and humor are highly\nstyle-dependent, and current VLMs struggle with spatial reasoning and planning\ntasks. Lastly, we show finetuning the same base model on VisionArena-Chat\noutperforms Llava-Instruct-158K, with a 17-point gain on MMMU and a 46-point\ngain on the WildVision benchmark. Dataset at https://huggingface.co/lmarena-ai\n","authors":["Christopher Chou","Lisa Dunlap","Koki Mashita","Krishna Mandal","Trevor Darrell","Ion Stoica","Joseph E. Gonzalez","Wei-Lin Chiang"],"pdf_url":"https://arxiv.org/pdf/2412.08687v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06438v3","updated":"2024-12-13T23:11:31Z","published":"2024-07-08T22:40:15Z","title":"SOLO: A Single Transformer for Scalable Vision-Language Modeling","summary":"  We present SOLO, a single transformer for Scalable visiOn-Language mOdeling.\nCurrent large vision-language models (LVLMs) such as LLaVA mostly employ\nheterogeneous architectures that connect pre-trained visual encoders with large\nlanguage models (LLMs) to facilitate visual recognition and complex reasoning.\nAlthough achieving remarkable performance with relatively lightweight training,\nwe identify four primary scalability limitations: (1) The visual capacity is\nconstrained by pre-trained visual encoders, which are typically an order of\nmagnitude smaller than LLMs. (2) The heterogeneous architecture complicates the\nuse of established hardware and software infrastructure. (3) Study of scaling\nlaws on such architecture must consider three separate components - visual\nencoder, connector, and LLMs, which complicates the analysis. (4) The use of\nexisting visual encoders typically requires following a pre-defined\nspecification of image inputs pre-processing, for example, by reshaping inputs\nto fixed-resolution square images, which presents difficulties in processing\nand training on high-resolution images or those with unusual aspect ratio. A\nunified single Transformer architecture, like SOLO, effectively addresses these\nscalability concerns in LVLMs; however, its limited adoption in the modern\ncontext likely stems from the absence of reliable training recipes that balance\nboth modalities and ensure stable training for billion-scale models. In this\npaper, we introduce the first open-source training recipe for developing SOLO,\nan open-source 7B LVLM using moderate academic resources. The training recipe\ninvolves initializing from LLMs, sequential pre-training on ImageNet and\nweb-scale data, and instruction fine-tuning on our curated high-quality\ndatasets. On extensive evaluation, SOLO demonstrates performance comparable to\nLLaVA-v1.5-7B, particularly excelling in visual mathematical reasoning.\n","authors":["Yangyi Chen","Xingyao Wang","Hao Peng","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2407.06438v3.pdf","comment":"Accepted to TMLR"},{"id":"http://arxiv.org/abs/2407.14616v2","updated":"2024-12-13T23:00:55Z","published":"2024-07-19T18:18:17Z","title":"DeepCA: Deep Learning-based 3D Coronary Artery Tree Reconstruction from\n  Two 2D Non-simultaneous X-ray Angiography Projections","summary":"  Cardiovascular diseases (CVDs) are the most common cause of death worldwide.\nInvasive x-ray coronary angiography (ICA) is one of the most important imaging\nmodalities for the diagnosis of CVDs. ICA typically acquires only two 2D\nprojections, which makes the 3D geometry of coronary vessels difficult to\ninterpret, thus requiring 3D coronary artery tree reconstruction from two\nprojections. State-of-the-art approaches require significant manual\ninteractions and cannot correct the non-rigid cardiac and respiratory motions\nbetween non-simultaneous projections. In this study, we propose a novel deep\nlearning pipeline named \\emph{DeepCA}. We leverage the Wasserstein conditional\ngenerative adversarial network with gradient penalty, latent convolutional\ntransformer layers, and a dynamic snake convolutional critic to implicitly\ncompensate for the non-rigid motion and provide 3D coronary artery tree\nreconstruction. Through simulating projections from coronary computed\ntomography angiography (CCTA), we achieve the generalisation of 3D coronary\ntree reconstruction on real non-simultaneous ICA projections. We incorporate an\napplication-specific evaluation metric to validate our proposed model on both a\nCCTA dataset and a real ICA dataset, together with Chamfer $\\ell_2$ distance.\nThe results demonstrate promising performance of our DeepCA model in vessel\ntopology preservation, recovery of missing features, and generalisation ability\nto real ICA data. To the best of our knowledge, this is the first study that\nleverages deep learning to achieve 3D coronary tree reconstruction from two\nreal non-simultaneous x-ray angiographic projections.\n","authors":["Yiying Wang","Abhirup Banerjee","Robin P. Choudhury","Vicente Grau"],"pdf_url":"https://arxiv.org/pdf/2407.14616v2.pdf","comment":"17 pages, 13 figures, 3 tables; Early accepted to the WACV 2025"},{"id":"http://arxiv.org/abs/2412.10597v1","updated":"2024-12-13T22:53:16Z","published":"2024-12-13T22:53:16Z","title":"Err on the Side of Texture: Texture Bias on Real Data","summary":"  Bias significantly undermines both the accuracy and trustworthiness of\nmachine learning models. To date, one of the strongest biases observed in image\nclassification models is texture bias-where models overly rely on texture\ninformation rather than shape information. Yet, existing approaches for\nmeasuring and mitigating texture bias have not been able to capture how\ntextures impact model robustness in real-world settings. In this work, we\nintroduce the Texture Association Value (TAV), a novel metric that quantifies\nhow strongly models rely on the presence of specific textures when classifying\nobjects. Leveraging TAV, we demonstrate that model accuracy and robustness are\nheavily influenced by texture. Our results show that texture bias explains the\nexistence of natural adversarial examples, where over 90% of these samples\ncontain textures that are misaligned with the learned texture of their true\nlabel, resulting in confident mispredictions.\n","authors":["Blaine Hoak","Ryan Sheatsley","Patrick McDaniel"],"pdf_url":"https://arxiv.org/pdf/2412.10597v1.pdf","comment":"Accepted to IEEE Secure and Trustworthy Machine Learning (SaTML)"},{"id":"http://arxiv.org/abs/2412.10594v1","updated":"2024-12-13T22:38:09Z","published":"2024-12-13T22:38:09Z","title":"Towards Unified Benchmark and Models for Multi-Modal Perceptual Metrics","summary":"  Human perception of similarity across uni- and multimodal inputs is highly\ncomplex, making it challenging to develop automated metrics that accurately\nmimic it. General purpose vision-language models, such as CLIP and large\nmulti-modal models (LMMs), can be applied as zero-shot perceptual metrics, and\nseveral recent works have developed models specialized in narrow perceptual\ntasks. However, the extent to which existing perceptual metrics align with\nhuman perception remains unclear. To investigate this question, we introduce\nUniSim-Bench, a benchmark encompassing 7 multi-modal perceptual similarity\ntasks, with a total of 25 datasets. Our evaluation reveals that while\ngeneral-purpose models perform reasonably well on average, they often lag\nbehind specialized models on individual tasks. Conversely, metrics fine-tuned\nfor specific tasks fail to generalize well to unseen, though related, tasks. As\na first step towards a unified multi-task perceptual similarity metric, we\nfine-tune both encoder-based and generative vision-language models on a subset\nof the UniSim-Bench tasks. This approach yields the highest average\nperformance, and in some cases, even surpasses taskspecific models.\nNevertheless, these models still struggle with generalization to unseen tasks,\nhighlighting the ongoing challenge of learning a robust, unified perceptual\nsimilarity metric capable of capturing the human notion of similarity. The code\nand models are available at https://github.com/SaraGhazanfari/UniSim.\n","authors":["Sara Ghazanfari","Siddharth Garg","Nicolas Flammarion","Prashanth Krishnamurthy","Farshad Khorrami","Francesco Croce"],"pdf_url":"https://arxiv.org/pdf/2412.10594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04244v2","updated":"2024-12-13T22:20:30Z","published":"2024-12-05T15:26:51Z","title":"GigaHands: A Massive Annotated Dataset of Bimanual Hand Activities","summary":"  Understanding bimanual human hand activities is a critical problem in AI and\nrobotics. We cannot build large models of bimanual activities because existing\ndatasets lack the scale, coverage of diverse hand activities, and detailed\nannotations. We introduce GigaHands, a massive annotated dataset capturing 34\nhours of bimanual hand activities from 56 subjects and 417 objects, totaling\n14k motion clips derived from 183 million frames paired with 84k text\nannotations. Our markerless capture setup and data acquisition protocol enable\nfully automatic 3D hand and object estimation while minimizing the effort\nrequired for text annotation. The scale and diversity of GigaHands enable broad\napplications, including text-driven action synthesis, hand motion captioning,\nand dynamic radiance field reconstruction. Our website are avaliable at\nhttps://ivl.cs.brown.edu/research/gigahands.html .\n","authors":["Rao Fu","Dingxi Zhang","Alex Jiang","Wanjia Fu","Austin Funk","Daniel Ritchie","Srinath Sridhar"],"pdf_url":"https://arxiv.org/pdf/2412.04244v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10589v1","updated":"2024-12-13T22:12:37Z","published":"2024-12-13T22:12:37Z","title":"PanSR: An Object-Centric Mask Transformer for Panoptic Segmentation","summary":"  Panoptic segmentation is a fundamental task in computer vision and a crucial\ncomponent for perception in autonomous vehicles. Recent mask-transformer-based\nmethods achieve impressive performance on standard benchmarks but face\nsignificant challenges with small objects, crowded scenes and scenes exhibiting\na wide range of object scales. We identify several fundamental shortcomings of\nthe current approaches: (i) the query proposal generation process is biased\ntowards larger objects, resulting in missed smaller objects, (ii) initially\nwell-localized queries may drift to other objects, resulting in missed\ndetections, (iii) spatially well-separated instances may be merged into a\nsingle mask causing inconsistent and false scene interpretations. To address\nthese issues, we rethink the individual components of the network and its\nsupervision, and propose a novel method for panoptic segmentation PanSR. PanSR\neffectively mitigates instance merging, enhances small-object detection and\nincreases performance in crowded scenes, delivering a notable +3.4 PQ\nimprovement over state-of-the-art on the challenging LaRS benchmark, while\nreaching state-of-the-art performance on Cityscapes. The code and models will\nbe publicly available at https://github.com/lojzezust/PanSR.\n","authors":["Lojze Žust","Matej Kristan"],"pdf_url":"https://arxiv.org/pdf/2412.10589v1.pdf","comment":"8 pages, 9 figures"},{"id":"http://arxiv.org/abs/2412.10587v1","updated":"2024-12-13T22:02:48Z","published":"2024-12-13T22:02:48Z","title":"Evaluation of GPT-4o & GPT-4o-mini's Vision Capabilities for Salt\n  Evaporite Identification","summary":"  Identifying salts from images of their 'stains' has diverse practical\napplications. While specialized AI models are being developed, this paper\nexplores the potential of OpenAI's state-of-the-art vision models (GPT-4o and\nGPT-4o-mini) as an immediate solution. Testing with 12 different types of\nsalts, the GPT-4o model achieved 57% accuracy and a 0.52 F1 score,\nsignificantly outperforming both random chance (8%) and GPT-4o mini (11%\naccuracy). Results suggest that current vision models could serve as an interim\nsolution for salt identification from stain images.\n","authors":["Deven B. Dangi","Beni B. Dangi","Oliver Steinbock"],"pdf_url":"https://arxiv.org/pdf/2412.10587v1.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2412.10573v1","updated":"2024-12-13T21:34:54Z","published":"2024-12-13T21:34:54Z","title":"ExeChecker: Where Did I Go Wrong?","summary":"  In this paper, we present a contrastive learning based framework, ExeChecker,\nfor the interpretation of rehabilitation exercises. Our work builds upon\nstate-of-the-art advances in the area of human pose estimation, graph-attention\nneural networks, and transformer interpretablity. The downstream task is to\nassist rehabilitation by providing informative feedback to users while they are\nperforming prescribed exercises. We utilize a contrastive learning strategy\nduring training. Given a tuple of correctly and incorrectly executed exercises,\nour model is able to identify and highlight those joints that are involved in\nan incorrect movement and thus require the user's attention. We collected an\nin-house dataset, ExeCheck, with paired recordings of both correct and\nincorrect execution of exercises. In our experiments, we tested our method on\nthis dataset as well as the UI-PRMD dataset and found ExeCheck outperformed the\nbaseline method using pairwise sequence alignment in identifying joints of\nphysical relevance in rehabilitation exercises.\n","authors":["Yiwen Gu","Mahir Patel","Margrit Betke"],"pdf_url":"https://arxiv.org/pdf/2412.10573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.02886v3","updated":"2024-12-13T21:27:56Z","published":"2024-12-03T22:46:09Z","title":"Patchfinder: Leveraging Visual Language Models for Accurate Information\n  Retrieval using Model Uncertainty","summary":"  For decades, corporations and governments have relied on scanned documents to\nrecord vast amounts of information. However, extracting this information is a\nslow and tedious process due to the sheer volume and complexity of these\nrecords. The rise of Vision Language Models (VLMs) presents a way to\nefficiently and accurately extract the information out of these documents. The\ncurrent automated workflow often requires a two-step approach involving the\nextraction of information using optical character recognition software and\nsubsequent usage of large language models for processing this information.\nUnfortunately, these methods encounter significant challenges when dealing with\nnoisy scanned documents, often requiring computationally expensive language\nmodels to handle high information density effectively. In this study, we\npropose PatchFinder, an algorithm that builds upon VLMs to improve information\nextraction. First, we devise a confidence-based score, called Patch Confidence,\nbased on the Maximum Softmax Probability of the VLMs' output to measure the\nmodel's confidence in its predictions. Using this metric, PatchFinder\ndetermines a suitable patch size, partitions the input document into\noverlapping patches, and generates confidence-based predictions for the target\ninformation. Our experimental results show that PatchFinder, leveraging Phi-3v,\na 4.2-billion-parameter VLM, achieves an accuracy of 94% on our dataset of 190\nnoisy scanned documents, outperforming ChatGPT-4o by 18.5 percentage points.\n","authors":["Roman Colman","Minh Vu","Manish Bhattarai","Martin Ma","Hari Viswanathan","Daniel O'Malley","Javier E. Santos"],"pdf_url":"https://arxiv.org/pdf/2412.02886v3.pdf","comment":"This paper has been accepted to IEEE/CVF Winter Conference on\n  Applications of Computer Vision (WACV) 2025"},{"id":"http://arxiv.org/abs/2412.10569v1","updated":"2024-12-13T21:17:11Z","published":"2024-12-13T21:17:11Z","title":"Learning to Merge Tokens via Decoupled Embedding for Efficient Vision\n  Transformers","summary":"  Recent token reduction methods for Vision Transformers (ViTs) incorporate\ntoken merging, which measures the similarities between token embeddings and\ncombines the most similar pairs. However, their merging policies are directly\ndependent on intermediate features in ViTs, which prevents exploiting features\ntailored for merging and requires end-to-end training to improve token merging.\nIn this paper, we propose Decoupled Token Embedding for Merging (DTEM) that\nenhances token merging through a decoupled embedding learned via a continuously\nrelaxed token merging process. Our method introduces a lightweight embedding\nmodule decoupled from the ViT forward pass to extract dedicated features for\ntoken merging, thereby addressing the restriction from using intermediate\nfeatures. The continuously relaxed token merging, applied during training,\nenables us to learn the decoupled embeddings in a differentiable manner. Thanks\nto the decoupled structure, our method can be seamlessly integrated into\nexisting ViT backbones and trained either modularly by learning only the\ndecoupled embeddings or end-to-end by fine-tuning. We demonstrate the\napplicability of DTEM on various tasks, including classification, captioning,\nand segmentation, with consistent improvement in token merging. Especially in\nthe ImageNet-1k classification, DTEM achieves a 37.2% reduction in FLOPs while\nmaintaining a top-1 accuracy of 79.85% with DeiT-small. Code is available at\n\\href{https://github.com/movinghoon/dtem}{link}.\n","authors":["Dong Hoon Lee","Seunghoon Hong"],"pdf_url":"https://arxiv.org/pdf/2412.10569v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2412.10566v1","updated":"2024-12-13T21:15:01Z","published":"2024-12-13T21:15:01Z","title":"EVLM: Self-Reflective Multimodal Reasoning for Cross-Dimensional Visual\n  Editing","summary":"  Editing complex visual content based on ambiguous instructions remains a\nchallenging problem in vision-language modeling. While existing models can\ncontextualize content, they often struggle to grasp the underlying intent\nwithin a reference image or scene, leading to misaligned edits. We introduce\nthe Editing Vision-Language Model (EVLM), a system designed to interpret such\ninstructions in conjunction with reference visuals, producing precise and\ncontext-aware editing prompts. Leveraging Chain-of-Thought (CoT) reasoning and\nKL-Divergence Target Optimization (KTO) alignment technique, EVLM captures\nsubjective editing preferences without requiring binary labels. Fine-tuned on a\ndataset of 30,000 CoT examples, with rationale paths rated by human evaluators,\nEVLM demonstrates substantial improvements in alignment with human intentions.\nExperiments across image, video, 3D, and 4D editing tasks show that EVLM\ngenerates coherent, high-quality instructions, supporting a scalable framework\nfor complex vision-language applications.\n","authors":["Umar Khalid","Hasan Iqbal","Azib Farooq","Nazanin Rahnavard","Jing Hua","Chen Chen Umar Khalid","Hasan Iqbal","Azib Farooq","Nazanin Rahnavard","Jing Hua","Chen Chen Umar Khalid","Hasan Iqbal","Azib Farooq","Nazanin Rahnavard","Jing Hua","Chen Chen Umar Khalid","Hasan Iqbal","Azib Farooq","Nazanin Rahnavard","Jing Hua","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2412.10566v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2412.10538v1","updated":"2024-12-13T20:22:35Z","published":"2024-12-13T20:22:35Z","title":"Predictive Pattern Recognition Techniques Towards Spatiotemporal\n  Representation of Plant Growth in Simulated and Controlled Environments: A\n  Comprehensive Review","summary":"  Accurate predictions and representations of plant growth patterns in\nsimulated and controlled environments are important for addressing various\nchallenges in plant phenomics research. This review explores various works on\nstate-of-the-art predictive pattern recognition techniques, focusing on the\nspatiotemporal modeling of plant traits and the integration of dynamic\nenvironmental interactions. We provide a comprehensive examination of\ndeterministic, probabilistic, and generative modeling approaches, emphasizing\ntheir applications in high-throughput phenotyping and simulation-based plant\ngrowth forecasting. Key topics include regressions and neural network-based\nrepresentation models for the task of forecasting, limitations of existing\nexperiment-based deterministic approaches, and the need for dynamic frameworks\nthat incorporate uncertainty and evolving environmental feedback. This review\nsurveys advances in 2D and 3D structured data representations through\nfunctional-structural plant models and conditional generative models. We offer\na perspective on opportunities for future works, emphasizing the integration of\ndomain-specific knowledge to data-driven methods, improvements to available\ndatasets, and the implementation of these techniques toward real-world\napplications.\n","authors":["Mohamed Debbagh","Shangpeng Sun","Mark Lefsrud"],"pdf_url":"https://arxiv.org/pdf/2412.10538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04596v2","updated":"2024-12-13T20:16:52Z","published":"2024-09-06T20:08:21Z","title":"NeCA: 3D Coronary Artery Tree Reconstruction from Two 2D Projections via\n  Neural Implicit Representation","summary":"  Cardiovascular diseases (CVDs) are the most common health threats worldwide.\n2D X-ray invasive coronary angiography (ICA) remains the most widely adopted\nimaging modality for CVD assessment during real-time cardiac interventions.\nHowever, it is often difficult for cardiologists to interpret the 3D geometry\nof coronary vessels based on 2D planes. Moreover, due to the radiation limit,\noften only two angiographic projections are acquired, providing limited\ninformation of the vessel geometry and necessitating 3D coronary tree\nreconstruction based only on two ICA projections. In this paper, we propose a\nself-supervised deep learning method called NeCA, which is based on neural\nimplicit representation using the multiresolution hash encoder and\ndifferentiable cone-beam forward projector layer, in order to achieve 3D\ncoronary artery tree reconstruction from two 2D projections. We validate our\nmethod using six different metrics on a dataset generated from coronary\ncomputed tomography angiography of right coronary artery and left anterior\ndescending artery. The evaluation results demonstrate that our NeCA method,\nwithout requiring 3D ground truth for supervision or large datasets for\ntraining, achieves promising performance in both vessel topology and\nbranch-connectivity preservation compared to the supervised deep learning\nmodel.\n","authors":["Yiying Wang","Abhirup Banerjee","Vicente Grau"],"pdf_url":"https://arxiv.org/pdf/2409.04596v2.pdf","comment":"14 pages, 8 figures, 6 tables"},{"id":"http://arxiv.org/abs/2412.10533v1","updated":"2024-12-13T20:01:51Z","published":"2024-12-13T20:01:51Z","title":"SUGAR: Subject-Driven Video Customization in a Zero-Shot Manner","summary":"  We present SUGAR, a zero-shot method for subject-driven video customization.\nGiven an input image, SUGAR is capable of generating videos for the subject\ncontained in the image and aligning the generation with arbitrary visual\nattributes such as style and motion specified by user-input text. Unlike\nprevious methods, which require test-time fine-tuning or fail to generate\ntext-aligned videos, SUGAR achieves superior results without the need for extra\ncost at test-time. To enable zero-shot capability, we introduce a scalable\npipeline to construct synthetic dataset which is specifically designed for\nsubject-driven customization, leading to 2.5 millions of image-video-text\ntriplets. Additionally, we propose several methods to enhance our model,\nincluding special attention designs, improved training strategies, and a\nrefined sampling algorithm. Extensive experiments are conducted. Compared to\nprevious methods, SUGAR achieves state-of-the-art results in identity\npreservation, video dynamics, and video-text alignment for subject-driven video\ncustomization, demonstrating the effectiveness of our proposed method.\n","authors":["Yufan Zhou","Ruiyi Zhang","Jiuxiang Gu","Nanxuan Zhao","Jing Shi","Tong Sun"],"pdf_url":"https://arxiv.org/pdf/2412.10533v1.pdf","comment":"webpage https://drboog.github.io/SUGAR"},{"id":"http://arxiv.org/abs/2412.10525v1","updated":"2024-12-13T19:38:36Z","published":"2024-12-13T19:38:36Z","title":"RowDetr: End-to-End Row Detection Using Polynomials","summary":"  Crop row detection has garnered significant interest due to its critical role\nin enabling navigation in GPS-denied environments, such as under-canopy\nagricultural settings. To address this challenge, we propose RowDetr, an\nend-to-end neural network that utilizes smooth polynomial functions to\ndelineate crop boundaries in image space. A novel energy-based loss function,\nPolyOptLoss, is introduced to enhance learning robustness, even with noisy\nlabels. The proposed model demonstrates a 3% improvement over Agronav in key\nperformance metrics while being six times faster, making it well-suited for\nreal-time applications. Additionally, metrics from lane detection studies were\nadapted to comprehensively evaluate the system, showcasing its accuracy and\nadaptability in various scenarios.\n","authors":["Rahul Harsha Cheppally","Ajay Sharda"],"pdf_url":"https://arxiv.org/pdf/2412.10525v1.pdf","comment":"Code will be open sourced upon publication"},{"id":"http://arxiv.org/abs/2412.10523v1","updated":"2024-12-13T19:33:48Z","published":"2024-12-13T19:33:48Z","title":"The Language of Motion: Unifying Verbal and Non-verbal Language of 3D\n  Human Motion","summary":"  Human communication is inherently multimodal, involving a combination of\nverbal and non-verbal cues such as speech, facial expressions, and body\ngestures. Modeling these behaviors is essential for understanding human\ninteraction and for creating virtual characters that can communicate naturally\nin applications like games, films, and virtual reality. However, existing\nmotion generation models are typically limited to specific input modalities --\neither speech, text, or motion data -- and cannot fully leverage the diversity\nof available data. In this paper, we propose a novel framework that unifies\nverbal and non-verbal language using multimodal language models for human\nmotion understanding and generation. This model is flexible in taking text,\nspeech, and motion or any combination of them as input. Coupled with our novel\npre-training strategy, our model not only achieves state-of-the-art performance\non co-speech gesture generation but also requires much less data for training.\nOur model also unlocks an array of novel tasks such as editable gesture\ngeneration and emotion prediction from motion. We believe unifying the verbal\nand non-verbal language of human motion is essential for real-world\napplications, and language models offer a powerful approach to achieving this\ngoal. Project page: languageofmotion.github.io.\n","authors":["Changan Chen","Juze Zhang","Shrinidhi K. Lakshmikanth","Yusu Fang","Ruizhi Shao","Gordon Wetzstein","Li Fei-Fei","Ehsan Adeli"],"pdf_url":"https://arxiv.org/pdf/2412.10523v1.pdf","comment":"Project page: languageofmotion.github.io"},{"id":"http://arxiv.org/abs/2311.00810v2","updated":"2024-12-13T19:33:32Z","published":"2023-11-01T19:49:32Z","title":"A Call to Arms: AI Should be Critical for Social Media Analysis of\n  Conflict Zones","summary":"  The massive proliferation of social media data represents a transformative\nopportunity for conflict studies and for tracking the proliferation and use of\nweaponry, as conflicts are increasingly documented in these online spaces. At\nthe same time, the scale and types of data available are problematic for\ntraditional open-source intelligence. This paper focuses on identifying\nspecific weapon systems and the insignias of the armed groups using them as\ndocumented in the Ukraine war, as these tasks are critical to operational\nintelligence and tracking weapon proliferation, especially given the scale of\ninternational military aid given to Ukraine. The large scale of social media\nmakes manual assessment difficult, however, so this paper presents early work\nthat uses computer vision models to support this task. We demonstrate that\nthese models can both identify weapons embedded in images shared in social\nmedia and how the resulting collection of military-relevant images and their\npost times interact with the offline, real-world conflict. Not only can we then\ntrack changes in the prevalence of images of tanks, land mines, military\ntrucks, etc., we find correlations among time series data associated with these\nimages and the daily fatalities in this conflict. This work shows substantial\nopportunity for examining similar online documentation of conflict contexts,\nand we also point to future avenues where computer vision can be further\nimproved for these open-source intelligence tasks.\n","authors":["Afia Abedin","Abdul Bais","Cody Buntain","Laura Courchesne","Brian McQuinn","Matthew E. Taylor","Muhib Ullah"],"pdf_url":"https://arxiv.org/pdf/2311.00810v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.05985v2","updated":"2024-12-13T19:26:00Z","published":"2024-11-08T21:42:50Z","title":"Emotional Images: Assessing Emotions in Images and Potential Biases in\n  Generative Models","summary":"  This paper examines potential biases and inconsistencies in emotional\nevocation of images produced by generative artificial intelligence (AI) models\nand their potential bias toward negative emotions. In particular, we assess\nthis bias by comparing the emotions evoked by an AI-produced image to the\nemotions evoked by prompts used to create those images. As a first step, the\nstudy evaluates three approaches for identifying emotions in images --\ntraditional supervised learning, zero-shot learning with vision-language\nmodels, and cross-modal auto-captioning -- using EmoSet, a large dataset of\nimage-emotion annotations that categorizes images across eight emotional types.\nResults show fine-tuned models, particularly Google's Vision Transformer (ViT),\nsignificantly outperform zero-shot and caption-based methods in recognizing\nemotions in images. For a cross-modality comparison, we then analyze the\ndifferences between emotions in text prompts -- via existing text-based\nemotion-recognition models -- and the emotions evoked in the resulting images.\nFindings indicate that AI-generated images frequently lean toward negative\nemotional content, regardless of the original prompt. This emotional skew in\ngenerative models could amplify negative affective content in digital spaces,\nperpetuating its prevalence and impact. The study advocates for a\nmultidisciplinary approach to better align AI emotion recognition with\npsychological insights and address potential biases in generative AI outputs\nacross digital media.\n","authors":["Maneet Mehta","Cody Buntain"],"pdf_url":"https://arxiv.org/pdf/2411.05985v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10511v1","updated":"2024-12-13T19:12:11Z","published":"2024-12-13T19:12:11Z","title":"Automated Image Captioning with CNNs and Transformers","summary":"  This project aims to create an automated image captioning system that\ngenerates natural language descriptions for input images by integrating\ntechniques from computer vision and natural language processing. We employ\nvarious different techniques, ranging from CNN-RNN to the more advanced\ntransformer-based techniques. Training is carried out on image datasets paired\nwith descriptive captions, and model performance will be evaluated using\nestablished metrics such as BLEU, METEOR, and CIDEr. The project will also\ninvolve experimentation with advanced attention mechanisms, comparisons of\ndifferent architectural choices, and hyperparameter optimization to refine\ncaptioning accuracy and overall system effectiveness.\n","authors":["Joshua Adrian Cahyono","Jeremy Nathan Jusuf"],"pdf_url":"https://arxiv.org/pdf/2412.10511v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10510v1","updated":"2024-12-13T19:11:18Z","published":"2024-12-13T19:11:18Z","title":"DEFAME: Dynamic Evidence-based FAct-checking with Multimodal Experts","summary":"  The proliferation of disinformation presents a growing threat to societal\ntrust and democracy, necessitating robust and scalable Fact-Checking systems.\nIn this work, we present Dynamic Evidence-based FAct-checking with Multimodal\nExperts (DEFAME), a modular, zero-shot MLLM pipeline for open-domain,\ntext-image claim verification. DEFAME frames the problem of fact-checking as a\nsix-stage process, dynamically deciding about the usage of external tools for\nthe retrieval of textual and visual evidence. In addition to the claim's\nveracity, DEFAME returns a justification accompanied by a comprehensive,\nmultimodal fact-checking report. While most alternatives either focus on\nsub-tasks of fact-checking, lack explainability or are limited to text-only\ninputs, DEFAME solves the problem of fact-checking end-to-end, including claims\nwith images or those that require visual evidence. Evaluation on the popular\nbenchmarks VERITE, AVeriTeC, and MOCHEG shows that DEFAME surpasses all\nprevious methods, establishing it as the new state-of-the-art fact-checking\nsystem.\n","authors":["Tobias Braun","Mark Rothermel","Marcus Rohrbach","Anna Rohrbach"],"pdf_url":"https://arxiv.org/pdf/2412.10510v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10494v1","updated":"2024-12-13T18:59:56Z","published":"2024-12-13T18:59:56Z","title":"SnapGen-V: Generating a Five-Second Video within Five Seconds on a\n  Mobile Device","summary":"  We have witnessed the unprecedented success of diffusion-based video\ngeneration over the past year. Recently proposed models from the community have\nwielded the power to generate cinematic and high-resolution videos with smooth\nmotions from arbitrary input prompts. However, as a supertask of image\ngeneration, video generation models require more computation and are thus\nhosted mostly on cloud servers, limiting broader adoption among content\ncreators. In this work, we propose a comprehensive acceleration framework to\nbring the power of the large-scale video diffusion model to the hands of edge\nusers. From the network architecture scope, we initialize from a compact image\nbackbone and search out the design and arrangement of temporal layers to\nmaximize hardware efficiency. In addition, we propose a dedicated adversarial\nfine-tuning algorithm for our efficient model and reduce the denoising steps to\n4. Our model, with only 0.6B parameters, can generate a 5-second video on an\niPhone 16 PM within 5 seconds. Compared to server-side models that take minutes\non powerful GPUs to generate a single video, we accelerate the generation by\nmagnitudes while delivering on-par quality.\n","authors":["Yushu Wu","Zhixing Zhang","Yanyu Li","Yanwu Xu","Anil Kag","Yang Sui","Huseyin Coskun","Ke Ma","Aleksei Lebedev","Ju Hu","Dimitris Metaxas","Yanzhi Wang","Sergey Tulyakov","Jian Ren"],"pdf_url":"https://arxiv.org/pdf/2412.10494v1.pdf","comment":"https://snap-research.github.io/snapgen-v/"},{"id":"http://arxiv.org/abs/2412.10493v1","updated":"2024-12-13T18:59:52Z","published":"2024-12-13T18:59:52Z","title":"SafetyDPO: Scalable Safety Alignment for Text-to-Image Generation","summary":"  Text-to-image (T2I) models have become widespread, but their limited safety\nguardrails expose end users to harmful content and potentially allow for model\nmisuse. Current safety measures are typically limited to text-based filtering\nor concept removal strategies, able to remove just a few concepts from the\nmodel's generative capabilities. In this work, we introduce SafetyDPO, a method\nfor safety alignment of T2I models through Direct Preference Optimization\n(DPO). We enable the application of DPO for safety purposes in T2I models by\nsynthetically generating a dataset of harmful and safe image-text pairs, which\nwe call CoProV2. Using a custom DPO strategy and this dataset, we train safety\nexperts, in the form of low-rank adaptation (LoRA) matrices, able to guide the\ngeneration process away from specific safety-related concepts. Then, we merge\nthe experts into a single LoRA using a novel merging strategy for optimal\nscaling performance. This expert-based approach enables scalability, allowing\nus to remove 7 times more harmful concepts from T2I models compared to\nbaselines. SafetyDPO consistently outperforms the state-of-the-art on many\nbenchmarks and establishes new practices for safety alignment in T2I networks.\nCode and data will be shared at https://safetydpo.github.io/.\n","authors":["Runtao Liu","Chen I Chieh","Jindong Gu","Jipeng Zhang","Renjie Pi","Qifeng Chen","Philip Torr","Ashkan Khakzar","Fabio Pizzati"],"pdf_url":"https://arxiv.org/pdf/2412.10493v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10492v1","updated":"2024-12-13T18:18:00Z","published":"2024-12-13T18:18:00Z","title":"QSM-RimDS: A highly sensitive paramagnetic rim lesion detection and\n  segmentation tool for multiple sclerosis lesions","summary":"  Paramagnetic rim lesions (PRLs) are imaging biomarker of the innate immune\nresponse in MS lesions. QSM-RimNet, a state-of-the-art tool for PRLs detection\non QSM, can identify PRLs but requires precise QSM lesion mask and does not\nprovide rim segmentation. Therefore, the aims of this study are to develop\nQSM-RimDS algorithm to detect PRLs using the readily available FLAIR lesion\nmask and to provide rim segmentation for microglial quantification. QSM-RimDS,\na deep-learning based tool for joint PRL rim segmentation and PRL detection has\nbeen developed. QSM-RimDS has obtained state-of-the art performance in PRL\ndetection and therefore has the potential to be used in clinical practice as a\ntool to assist human readers for the time-consuming PRL detection and\nsegmentation task. QSM-RimDS is made publicly available\n[https://github.com/kennyha85/QSM_RimDS]\n","authors":["Ha Luu","Mert Sisman","Ilhami Kovanlikaya","Tam Vu","Pascal Spincemaille","Yi Wang","Francesca Bagnato","Susan Gauthier","Thanh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2412.10492v1.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.10489v1","updated":"2024-12-13T16:27:54Z","published":"2024-12-13T16:27:54Z","title":"CognitionCapturer: Decoding Visual Stimuli From Human EEG Signal With\n  Multimodal Information","summary":"  Electroencephalogram (EEG) signals have attracted significant attention from\nresearchers due to their non-invasive nature and high temporal sensitivity in\ndecoding visual stimuli. However, most recent studies have focused solely on\nthe relationship between EEG and image data pairs, neglecting the valuable\n``beyond-image-modality\" information embedded in EEG signals. This results in\nthe loss of critical multimodal information in EEG. To address this limitation,\nwe propose CognitionCapturer, a unified framework that fully leverages\nmultimodal data to represent EEG signals. Specifically, CognitionCapturer\ntrains Modality Expert Encoders for each modality to extract cross-modal\ninformation from the EEG modality. Then, it introduces a diffusion prior to map\nthe EEG embedding space to the CLIP embedding space, followed by using a\npretrained generative model, the proposed framework can reconstruct visual\nstimuli with high semantic and structural fidelity. Notably, the framework does\nnot require any fine-tuning of the generative models and can be extended to\nincorporate more modalities. Through extensive experiments, we demonstrate that\nCognitionCapturer outperforms state-of-the-art methods both qualitatively and\nquantitatively. Code: https://github.com/XiaoZhangYES/CognitionCapturer.\n","authors":["Kaifan Zhang","Lihuo He","Xin Jiang","Wen Lu","Di Wang","Xinbo Gao"],"pdf_url":"https://arxiv.org/pdf/2412.10489v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10488v1","updated":"2024-12-13T15:24:11Z","published":"2024-12-13T15:24:11Z","title":"SVGBuilder: Component-Based Colored SVG Generation with Text-Guided\n  Autoregressive Transformers","summary":"  Scalable Vector Graphics (SVG) are essential XML-based formats for versatile\ngraphics, offering resolution independence and scalability. Unlike raster\nimages, SVGs use geometric shapes and support interactivity, animation, and\nmanipulation via CSS and JavaScript. Current SVG generation methods face\nchallenges related to high computational costs and complexity. In contrast,\nhuman designers use component-based tools for efficient SVG creation. Inspired\nby this, SVGBuilder introduces a component-based, autoregressive model for\ngenerating high-quality colored SVGs from textual input. It significantly\nreduces computational overhead and improves efficiency compared to traditional\nmethods. Our model generates SVGs up to 604 times faster than\noptimization-based approaches. To address the limitations of existing SVG\ndatasets and support our research, we introduce ColorSVG-100K, the first\nlarge-scale dataset of colored SVGs, comprising 100,000 graphics. This dataset\nfills the gap in color information for SVG generation models and enhances\ndiversity in model training. Evaluation against state-of-the-art models\ndemonstrates SVGBuilder's superior performance in practical applications,\nhighlighting its efficiency and quality in generating complex SVG graphics.\n","authors":["Zehao Chen","Rong Pan"],"pdf_url":"https://arxiv.org/pdf/2412.10488v1.pdf","comment":"Project: https://svgbuilder.github.io"},{"id":"http://arxiv.org/abs/2412.10482v1","updated":"2024-12-13T10:18:36Z","published":"2024-12-13T10:18:36Z","title":"Dynamic Entity-Masked Graph Diffusion Model for histopathological image\n  Representation Learning","summary":"  Significant disparities between the features of natural images and those\ninherent to histopathological images make it challenging to directly apply and\ntransfer pre-trained models from natural images to histopathology tasks.\nMoreover, the frequent lack of annotations in histopathology patch images has\ndriven researchers to explore self-supervised learning methods like mask\nreconstruction for learning representations from large amounts of unlabeled\ndata. Crucially, previous mask-based efforts in self-supervised learning have\noften overlooked the spatial interactions among entities, which are essential\nfor constructing accurate representations of pathological entities. To address\nthese challenges, constructing graphs of entities is a promising approach. In\naddition, the diffusion reconstruction strategy has recently shown superior\nperformance through its random intensity noise addition technique to enhance\nthe robust learned representation. Therefore, we introduce H-MGDM, a novel\nself-supervised Histopathology image representation learning method through the\nDynamic Entity-Masked Graph Diffusion Model. Specifically, we propose to use\ncomplementary subgraphs as latent diffusion conditions and self-supervised\ntargets respectively during pre-training. We note that the graph can embed\nentities' topological relationships and enhance representation. Dynamic\nconditions and targets can improve pathological fine reconstruction. Our model\nhas conducted pretraining experiments on three large histopathological\ndatasets. The advanced predictive performance and interpretability of H-MGDM\nare clearly evaluated on comprehensive downstream tasks such as classification\nand survival analysis on six datasets. Our code will be publicly available at\nhttps://github.com/centurion-crawler/H-MGDM.\n","authors":["Zhenfeng Zhuang","Min Cen","Yanfeng Li","Fangyu Zhou","Lequan Yu","Baptiste Magnier","Liansheng Wang"],"pdf_url":"https://arxiv.org/pdf/2412.10482v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12191v1","updated":"2024-12-13T23:04:02Z","published":"2024-12-13T23:04:02Z","title":"Vehicle Detection and Classification for Toll collection using YOLOv11\n  and Ensemble OCR","summary":"  Traditional automated toll collection systems depend on complex hardware\nconfigurations, that require huge investments in installation and maintenance.\nThis research paper presents an innovative approach to revolutionize automated\ntoll collection by using a single camera per plaza with the YOLOv11 computer\nvision architecture combined with an ensemble OCR technique. Our system has\nachieved a Mean Average Precision (mAP) of 0.895 over a wide range of\nconditions, demonstrating 98.5% accuracy in license plate recognition, 94.2%\naccuracy in axle detection, and 99.7% OCR confidence scoring. The architecture\nincorporates intelligent vehicle tracking across IOU regions, automatic axle\ncounting by way of spatial wheel detection patterns, and real-time monitoring\nthrough an extended dashboard interface. Extensive training using 2,500 images\nunder various environmental conditions, our solution shows improved performance\nwhile drastically reducing hardware resources compared to conventional systems.\nThis research contributes toward intelligent transportation systems by\nintroducing a scalable, precision-centric solution that improves operational\nefficiency and user experience in modern toll collections.\n","authors":["Karthik Sivakoti"],"pdf_url":"https://arxiv.org/pdf/2412.12191v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12189v1","updated":"2024-12-13T22:00:26Z","published":"2024-12-13T22:00:26Z","title":"Multi-Surrogate-Teacher Assistance for Representation Alignment in\n  Fingerprint-based Indoor Localization","summary":"  Despite remarkable progress in knowledge transfer across visual and textual\ndomains, extending these achievements to indoor localization, particularly for\nlearning transferable representations among Received Signal Strength (RSS)\nfingerprint datasets, remains a challenge. This is due to inherent\ndiscrepancies among these RSS datasets, largely including variations in\nbuilding structure, the input number and disposition of WiFi anchors.\nAccordingly, specialized networks, which were deprived of the ability to\ndiscern transferable representations, readily incorporate environment-sensitive\nclues into the learning process, hence limiting their potential when applied to\nspecific RSS datasets. In this work, we propose a plug-and-play (PnP) framework\nof knowledge transfer, facilitating the exploitation of transferable\nrepresentations for specialized networks directly on target RSS datasets\nthrough two main phases. Initially, we design an Expert Training phase, which\nfeatures multiple surrogate generative teachers, all serving as a global\nadapter that homogenizes the input disparities among independent source RSS\ndatasets while preserving their unique characteristics. In a subsequent Expert\nDistilling phase, we continue introducing a triplet of underlying constraints\nthat requires minimizing the differences in essential knowledge between the\nspecialized network and surrogate teachers through refining its representation\nlearning on the target dataset. This process implicitly fosters a\nrepresentational alignment in such a way that is less sensitive to specific\nenvironmental dynamics. Extensive experiments conducted on three benchmark WiFi\nRSS fingerprint datasets underscore the effectiveness of the framework that\nsignificantly exerts the full potential of specialized networks in\nlocalization.\n","authors":["Son Minh Nguyen","Linh Duy Tran","Duc Viet Le","Paul J. M Havinga"],"pdf_url":"https://arxiv.org/pdf/2412.12189v1.pdf","comment":"Accepted in the 1st round at WACV 2025 (Algorithm Track)"},{"id":"http://arxiv.org/abs/2412.10566v1","updated":"2024-12-13T21:15:01Z","published":"2024-12-13T21:15:01Z","title":"EVLM: Self-Reflective Multimodal Reasoning for Cross-Dimensional Visual\n  Editing","summary":"  Editing complex visual content based on ambiguous instructions remains a\nchallenging problem in vision-language modeling. While existing models can\ncontextualize content, they often struggle to grasp the underlying intent\nwithin a reference image or scene, leading to misaligned edits. We introduce\nthe Editing Vision-Language Model (EVLM), a system designed to interpret such\ninstructions in conjunction with reference visuals, producing precise and\ncontext-aware editing prompts. Leveraging Chain-of-Thought (CoT) reasoning and\nKL-Divergence Target Optimization (KTO) alignment technique, EVLM captures\nsubjective editing preferences without requiring binary labels. Fine-tuned on a\ndataset of 30,000 CoT examples, with rationale paths rated by human evaluators,\nEVLM demonstrates substantial improvements in alignment with human intentions.\nExperiments across image, video, 3D, and 4D editing tasks show that EVLM\ngenerates coherent, high-quality instructions, supporting a scalable framework\nfor complex vision-language applications.\n","authors":["Umar Khalid","Hasan Iqbal","Azib Farooq","Nazanin Rahnavard","Jing Hua","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2412.10566v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2412.12188v1","updated":"2024-12-13T20:20:29Z","published":"2024-12-13T20:20:29Z","title":"Predicting Internet Connectivity in Schools: A Feasibility Study\n  Leveraging Multi-modal Data and Location Encoders in Low-Resource Settings","summary":"  Internet connectivity in schools is critical to provide students with the\ndigital literary skills necessary to compete in modern economies. In order for\ngovernments to effectively implement digital infrastructure development in\nschools, accurate internet connectivity information is required. However,\ntraditional survey-based methods can exceed the financial and capacity limits\nof governments. Open-source Earth Observation (EO) datasets have unlocked our\nability to observe and understand socio-economic conditions on Earth from\nspace, and in combination with Machine Learning (ML), can provide the tools to\ncircumvent costly ground-based survey methods to support infrastructure\ndevelopment. In this paper, we present our work on school internet connectivity\nprediction using EO and ML. We detail the creation of our multi-modal,\nfreely-available satellite imagery and survey information dataset, leverage the\nlatest geographically-aware location encoders, and introduce the first results\nof using the new European Space Agency phi-lab geographically-aware\nfoundational model to predict internet connectivity in Botswana and Rwanda. We\nfind that ML with EO and ground-based auxiliary data yields the best\nperformance in both countries, for accuracy, F1 score, and False Positive\nrates, and highlight the challenges of internet connectivity prediction from\nspace with a case study in Kigali, Rwanda. Our work showcases a practical\napproach to support data-driven digital infrastructure development in\nlow-resource settings, leveraging freely available information, and provide\ncleaned and labelled datasets for future studies to the community through a\nunique collaboration between UNICEF and the European Space Agency phi-lab.\n","authors":["Kelsey Doerksen","Casper Fibaek","Rochelle Schneider","Do-Hyung Kim","Isabelle Tingzon"],"pdf_url":"https://arxiv.org/pdf/2412.12188v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10474v1","updated":"2024-12-13T02:31:48Z","published":"2024-12-13T02:31:48Z","title":"CrossVIT-augmented Geospatial-Intelligence Visualization System for\n  Tracking Economic Development Dynamics","summary":"  Timely and accurate economic data is crucial for effective policymaking.\nCurrent challenges in data timeliness and spatial resolution can be addressed\nwith advancements in multimodal sensing and distributed computing. We introduce\nSenseconomic, a scalable system for tracking economic dynamics via multimodal\nimagery and deep learning. Built on the Transformer framework, it integrates\nremote sensing and street view images using cross-attention, with nighttime\nlight data as weak supervision. The system achieved an R-squared value of\n0.8363 in county-level economic predictions and halved processing time to 23\nminutes using distributed computing. Its user-friendly design includes a\nVue3-based front end with Baidu maps for visualization and a Python-based back\nend automating tasks like image downloads and preprocessing. Senseconomic\nempowers policymakers and researchers with efficient tools for resource\nallocation and economic planning.\n","authors":["Yanbing Bai","Jinhua Su","Bin Qiao","Xiaoran Ma"],"pdf_url":"https://arxiv.org/pdf/2412.10474v1.pdf","comment":null}],"Robotics":[{"id":"http://arxiv.org/abs/2412.10371v1","updated":"2024-12-13T18:59:30Z","published":"2024-12-13T18:59:30Z","title":"GaussianAD: Gaussian-Centric End-to-End Autonomous Driving","summary":"  Vision-based autonomous driving shows great potential due to its satisfactory\nperformance and low costs. Most existing methods adopt dense representations\n(e.g., bird's eye view) or sparse representations (e.g., instance boxes) for\ndecision-making, which suffer from the trade-off between comprehensiveness and\nefficiency. This paper explores a Gaussian-centric end-to-end autonomous\ndriving (GaussianAD) framework and exploits 3D semantic Gaussians to\nextensively yet sparsely describe the scene. We initialize the scene with\nuniform 3D Gaussians and use surrounding-view images to progressively refine\nthem to obtain the 3D Gaussian scene representation. We then use sparse\nconvolutions to efficiently perform 3D perception (e.g., 3D detection, semantic\nmap construction). We predict 3D flows for the Gaussians with dynamic semantics\nand plan the ego trajectory accordingly with an objective of future scene\nforecasting. Our GaussianAD can be trained in an end-to-end manner with\noptional perception labels when available. Extensive experiments on the widely\nused nuScenes dataset verify the effectiveness of our end-to-end GaussianAD on\nvarious tasks including motion planning, 3D occupancy prediction, and 4D\noccupancy forecasting. Code: https://github.com/wzzheng/GaussianAD.\n","authors":["Wenzhao Zheng","Junjie Wu","Yao Zheng","Sicheng Zuo","Zixun Xie","Longchao Yang","Yong Pan","Zhihui Hao","Peng Jia","Xianpeng Lang","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.10371v1.pdf","comment":"Code is available at: https://github.com/wzzheng/GaussianAD"},{"id":"http://arxiv.org/abs/2412.10350v1","updated":"2024-12-13T18:45:39Z","published":"2024-12-13T18:45:39Z","title":"Adaptive Dual-Headway Unicycle Pose Control and Motion Prediction for\n  Optimal Sampling-Based Feedback Motion Planning","summary":"  Safe, smooth, and optimal motion planning for nonholonomically constrained\nmobile robots and autonomous vehicles is essential for achieving reliable,\nseamless, and efficient autonomy in logistics, mobility, and service\nindustries. In many such application settings, nonholonomic robots, like\nunicycles with restricted motion, require precise planning and control of both\ntranslational and orientational motion to approach specific locations in a\ndesignated orientation, such as for approaching changing, parking, and loading\nareas. In this paper, we introduce a new dual-headway unicycle pose control\nmethod by leveraging an adaptively placed headway point in front of the\nunicycle pose and a tailway point behind the goal pose. In summary, the\nunicycle robot continuously follows its headway point, which chases the tailway\npoint of the goal pose and the asymptotic motion of the tailway point towards\nthe goal position guides the unicycle robot to approach the goal location with\nthe correct orientation. The simple and intuitive geometric construction of\ndual-headway unicycle pose control enables an explicit convex feedback motion\nprediction bound on the closed-loop unicycle motion trajectory for fast and\naccurate safety verification. We present an application of dual-headway\nunicycle control for optimal sampling-based motion planning around obstacles.\nIn numerical simulations, we show that optimal unicycle motion planning using\ndual-headway translation and orientation distances significantly outperforms\nEuclidean translation and cosine orientation distances in generating smooth\nmotion with minimal travel and turning effort.\n","authors":["Aykut İşleyen","Abhidnya Kadu","René van de Molengraft","Ömür Arslan"],"pdf_url":"https://arxiv.org/pdf/2412.10350v1.pdf","comment":"12 pages, 7 figures, 1 algorithm, 1 table, an extended version of a\n  paper submitted for publication"},{"id":"http://arxiv.org/abs/2412.10349v1","updated":"2024-12-13T18:45:26Z","published":"2024-12-13T18:45:26Z","title":"Ensuring Force Safety in Vision-Guided Robotic Manipulation via Implicit\n  Tactile Calibration","summary":"  In dynamic environments, robots often encounter constrained movement\ntrajectories when manipulating objects with specific properties, such as doors.\nTherefore, applying the appropriate force is crucial to prevent damage to both\nthe robots and the objects. However, current vision-guided robot state\ngeneration methods often falter in this regard, as they lack the integration of\ntactile perception. To tackle this issue, this paper introduces a novel state\ndiffusion framework termed SafeDiff. It generates a prospective state sequence\nfrom the current robot state and visual context observation while incorporating\nreal-time tactile feedback to refine the sequence. As far as we know, this is\nthe first study specifically focused on ensuring force safety in robotic\nmanipulation. It significantly enhances the rationality of state planning, and\nthe safe action trajectory is derived from inverse dynamics based on this\nrefined planning. In practice, unlike previous approaches that concatenate\nvisual and tactile data to generate future robot state sequences, our method\nemploys tactile data as a calibration signal to adjust the robot's state within\nthe state space implicitly. Additionally, we've developed a large-scale\nsimulation dataset called SafeDoorManip50k, offering extensive multimodal data\nto train and evaluate the proposed method. Extensive experiments show that our\nvisual-tactile model substantially mitigates the risk of harmful forces in the\ndoor opening, across both simulated and real-world settings.\n","authors":["Lai Wei","Jiahua Ma","Yibo Hu","Ruimao Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.10349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10345v1","updated":"2024-12-13T18:40:51Z","published":"2024-12-13T18:40:51Z","title":"TraceVLA: Visual Trace Prompting Enhances Spatial-Temporal Awareness for\n  Generalist Robotic Policies","summary":"  Although large vision-language-action (VLA) models pretrained on extensive\nrobot datasets offer promising generalist policies for robotic learning, they\nstill struggle with spatial-temporal dynamics in interactive robotics, making\nthem less effective in handling complex tasks, such as manipulation. In this\nwork, we introduce visual trace prompting, a simple yet effective approach to\nfacilitate VLA models' spatial-temporal awareness for action prediction by\nencoding state-action trajectories visually. We develop a new TraceVLA model by\nfinetuning OpenVLA on our own collected dataset of 150K robot manipulation\ntrajectories using visual trace prompting. Evaluations of TraceVLA across 137\nconfigurations in SimplerEnv and 4 tasks on a physical WidowX robot demonstrate\nstate-of-the-art performance, outperforming OpenVLA by 10% on SimplerEnv and\n3.5x on real-robot tasks and exhibiting robust generalization across diverse\nembodiments and scenarios. To further validate the effectiveness and generality\nof our method, we present a compact VLA model based on 4B Phi-3-Vision,\npretrained on the Open-X-Embodiment and finetuned on our dataset, rivals the 7B\nOpenVLA baseline while significantly improving inference efficiency.\n","authors":["Ruijie Zheng","Yongyuan Liang","Shuaiyi Huang","Jianfeng Gao","Hal Daumé III","Andrey Kolobov","Furong Huang","Jianwei Yang"],"pdf_url":"https://arxiv.org/pdf/2412.10345v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10320v1","updated":"2024-12-13T18:00:21Z","published":"2024-12-13T18:00:21Z","title":"MeshA*: Efficient Path Planing With Motion Primitives","summary":"  We study a path planning problem where the possible move actions are\nrepresented as a finite set of motion primitives aligned with the grid\nrepresentation of the environment. That is, each primitive corresponds to a\nshort kinodynamically-feasible motion of an agent and is represented as a\nsequence of the swept cells of a grid. Typically heuristic search, i.e. A*, is\nconducted over the lattice induced by these primitives (lattice-based planning)\nto find a path. However due to the large branching factor such search may be\ninefficient in practice. To this end we suggest a novel technique rooted in the\nidea of searching over the grid cells (as in vanilla A*) simultaneously fitting\nthe possible sequences of the motion primitives into these cells. The resultant\nalgorithm, MeshA*, provably preserves the guarantees on completeness and\noptimality, on the one hand, and is shown to notably outperform conventional\nlattice-based planning (x1.5 decrease in the runtime), on the other hand.\nMoreover, we suggest an additional pruning technique that additionally\ndecreases the search space of MeshA*. The resultant planner is combined with\nthe regular A* to retain completeness and is shown to further increase the\nsearch performance at the cost of negligible decrease of the solution quality.\n","authors":["Marat Agranovskiy","Konstantin Yakovlev"],"pdf_url":"https://arxiv.org/pdf/2412.10320v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10239v1","updated":"2024-12-13T16:09:04Z","published":"2024-12-13T16:09:04Z","title":"Variable Stiffness & Dynamic Force Sensor for Tissue Palpation","summary":"  Palpation of human tissue during Minimally Invasive Surgery is hampered due\nto restricted access. In this extended abstract, we present a variable\nstiffness and dynamic force range sensor that has the potential to address this\nchallenge. The sensor utilises light reflection to estimate sensor deformation,\nand from this, the force applied. Experimental testing at different pressures\n(0, 0.5 and 1 PSI) shows that stiffness and force range increases with\npressure. The force calibration results when compared with measured forces\nproduced an average RMSE of 0.016, 0.0715 and 0.1284 N respectively, for these\npressures.\n","authors":["Abu Bakar Dawood","Zhenyu Zhang","Martin Angelmahr","Alberto Arezzo","Kaspar Althoefer"],"pdf_url":"https://arxiv.org/pdf/2412.10239v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.01733v3","updated":"2024-12-13T15:13:48Z","published":"2023-03-03T06:27:38Z","title":"Improving Surgical Situational Awareness with Signed Distance Field: A\n  Pilot Study in Virtual Reality","summary":"  The introduction of image-guided surgical navigation (IGSN) has greatly\nbenefited technically demanding surgical procedures by providing real-time\nsupport and guidance to the surgeon during surgery. To develop effective IGSN,\na careful selection of the surgical information and the medium to present this\ninformation to the surgeon is needed. However, this is not a trivial task due\nto the broad array of available options. To address this problem, we have\ndeveloped an open-source library that facilitates the development of multimodal\nnavigation systems in a wide range of surgical procedures relying on medical\nimaging data. To provide guidance, our system calculates the minimum distance\nbetween the surgical instrument and the anatomy and then presents this\ninformation to the user through different mechanisms. The real-time performance\nof our approach is achieved by calculating Signed Distance Fields at\ninitialization from segmented anatomical volumes. Using this framework, we\ndeveloped a multimodal surgical navigation system to help surgeons navigate\nanatomical variability in a skull base surgery simulation environment. Three\ndifferent feedback modalities were explored: visual, auditory, and haptic. To\nevaluate the proposed system, a pilot user study was conducted in which four\nclinicians performed mastoidectomy procedures with and without guidance. Each\ncondition was assessed using objective performance and subjective workload\nmetrics. This pilot user study showed improvements in procedural safety without\nadditional time or workload. These results demonstrate our pipeline's\nsuccessful use case in the context of mastoidectomy.\n","authors":["Hisashi Ishida","Juan Antonio Barragan","Adnan Munawar","Zhaoshuo Li","Andy Ding","Peter Kazanzides","Danielle Trakimas","Francis X. Creighton","Russell H. Taylor"],"pdf_url":"https://arxiv.org/pdf/2303.01733v3.pdf","comment":"First two authors contributed equally. 6 pages"},{"id":"http://arxiv.org/abs/2410.05095v2","updated":"2024-12-13T15:10:27Z","published":"2024-10-07T14:50:19Z","title":"FIRE-3DV: Framework-Independent Rendering Engine for 3D Graphics using\n  Vulkan","summary":"  Interactive dynamic simulators are an accelerator for developing novel\nrobotic control algorithms and complex systems involving humans and robots. In\nuser training and synthetic data generation applications, high-fidelity\nvisualizations from the simulation are essential. Yet, robotic simulators often\nlimit their rendering algorithms to preserve real-time interaction with the\nsimulation. Advancements in Graphics Processing Units (GPU) enable improved\nvisualization without compromising performance. However, these advancements\ncannot be fully leveraged in simulation frameworks that use legacy graphics\napplication programming interfaces (API) to interface with the GPU. This paper\npresents a performance-focused and lightweight rendering engine supporting the\nmodern Vulkan graphics API that can be easily integrated with other simulation\nframeworks to enhance visualizations. To illustrate the proposed method, our\nengine is used to modernize the legacy rendering pipeline of the Asynchronous\nMulti-Body Framework (AMBF), a dynamic simulation framework used extensively\nfor interactive robotics simulation development. This new rendering engine\nimplements graphical features such as physically based rendering (PBR),\nanti-aliasing, and ray-traced shadows, significantly improving the image\nfidelity of AMBF. Computational experiments show that the engine can render a\nsimulated scene with over seven million triangles while maintaining GPU\ncomputation times within two milliseconds.\n","authors":["Christopher John Allison","Haoying Zhou","Adnan Munawar","Peter Kazanzides","Juan Antonio Barragan"],"pdf_url":"https://arxiv.org/pdf/2410.05095v2.pdf","comment":"8 pages, 8 figures, submitted to the 2024 IEEE International\n  Conference on Robotic Computing (IRC)"},{"id":"http://arxiv.org/abs/2412.10180v1","updated":"2024-12-13T14:52:41Z","published":"2024-12-13T14:52:41Z","title":"A General Safety Framework for Autonomous Manipulation in Human\n  Environments","summary":"  Autonomous robots are projected to augment the manual workforce, especially\nin repetitive and hazardous tasks. For a successful deployment of such robots\nin human environments, it is crucial to guarantee human safety.\nState-of-the-art approaches to ensure human safety are either too restrictive\nto permit a natural human-robot collaboration or make strong assumptions that\ndo not hold when for autonomous robots, e.g., knowledge of a pre-defined\ntrajectory. Therefore, we propose SaRA-shield, a power and force limiting\nframework for AI-based manipulation in human environments that gives formal\nsafety guarantees while allowing for fast robot speeds. As recent studies have\nshown that unconstrained collisions allow for significantly higher contact\nforces than constrained collisions (clamping), we propose to classify contacts\nby their collision type using reachability analysis. We then verify that the\nkinetic energy of the robot is below pain and injury thresholds for the\ndetected collision type of the respective human body part in contact. Our\nreal-world experiments show that SaRA-shield can effectively reduce the speed\nof the robot to adhere to injury-preventing energy limits.\n","authors":["Jakob Thumm","Julian Balletshofer","Leonardo Maglanoc","Luis Muschal","Matthias Althoff"],"pdf_url":"https://arxiv.org/pdf/2412.10180v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13438v2","updated":"2024-12-13T14:27:12Z","published":"2024-11-20T16:26:51Z","title":"Robust Monocular Visual Odometry using Curriculum Learning","summary":"  Curriculum Learning (CL), drawing inspiration from natural learning patterns\nobserved in humans and animals, employs a systematic approach of gradually\nintroducing increasingly complex training data during model development. Our\nwork applies innovative CL methodologies to address the challenging geometric\nproblem of monocular Visual Odometry (VO) estimation, which is essential for\nrobot navigation in constrained environments. The primary objective of our\nresearch is to push the boundaries of current state-of-the-art (SOTA)\nbenchmarks in monocular VO by investigating various curriculum learning\nstrategies. We enhance the end-to-end Deep-Patch-Visual Odometry (DPVO)\nframework through the integration of novel CL approaches, with the goal of\ndeveloping more resilient models capable of maintaining high performance across\nchallenging environments and complex motion scenarios. Our research encompasses\nseveral distinctive CL strategies. We develop methods to evaluate sample\ndifficulty based on trajectory motion characteristics, implement sophisticated\nadaptive scheduling through self-paced weighted loss mechanisms, and utilize\nreinforcement learning agents for dynamic adjustment of training emphasis.\nThrough comprehensive evaluation on the diverse synthetic TartanAir dataset and\ncomplex real-world benchmarks such as EuRoC and TUM-RGBD, our Curriculum\nLearning-based Deep-Patch-Visual Odometry (CL-DPVO) demonstrates superior\nperformance compared to existing SOTA methods, including both feature-based and\nlearning-based VO approaches. The results validate the effectiveness of\nintegrating curriculum learning principles into visual odometry systems.\n","authors":["Assaf Lahiany","Oren Gal"],"pdf_url":"https://arxiv.org/pdf/2411.13438v2.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2412.10154v1","updated":"2024-12-13T14:11:49Z","published":"2024-12-13T14:11:49Z","title":"A Clinical Tuning Framework for Continuous Kinematic and Impedance\n  Control of a Powered Knee-Ankle Prosthesis","summary":"  Objective: Configuring a prosthetic leg is an integral part of the fitting\nprocess, but the personalization of a multi-modal powered knee-ankle prosthesis\nis often too complex to realize in a clinical environment. This paper develops\nboth the technical means to individualize a hybrid kinematic-impedance\ncontroller for variable-incline walking and sit-stand transitions, and an\nintuitive Clinical Tuning Interface (CTI) that allows prosthetists to directly\nmodify the controller behavior.\n  Methods: Utilizing an established method for predicting kinematic gait\nindividuality alongside a new parallel approach for kinetic individuality, we\napplied tuned characteristics exclusively from level-ground walking to\npersonalize continuous-phase/task models of joint kinematics and impedance. To\ntake advantage of this method, we developed a CTI that translates common\nclinical tuning parameters into model adjustments. We then conducted a case\nstudy involving an above-knee amputee participant where a prosthetist\niteratively tuned the prosthesis in a simulated clinical session involving\nwalking and sit-stand transitions.\n  Results: The prosthetist fully tuned the multi-activity prosthesis controller\nin under 20 min. Each iteration of tuning (i.e., observation, parameter\nadjustment, and model reprocessing) took 2 min on average for walking and 1 min\non average for sit-stand. The tuned behavior changes were appropriately\nmanifested in the commanded prosthesis torques, both at the tuned tasks and\nacross untuned tasks (inclines).\n  Conclusion: The CTI leveraged able-bodied trends to efficiently personalize a\nwide array of walking tasks and sit-stand transitions. A case-study validated\nthe CTI tuning method and demonstrated the efficiency necessary for powered\nknee-ankle prostheses to become clinically viable.\n","authors":["Emma Reznick","T. Kevin Best","Robert Gregg"],"pdf_url":"https://arxiv.org/pdf/2412.10154v1.pdf","comment":"9 pages, 8 figures, and Appendix"},{"id":"http://arxiv.org/abs/2412.10137v1","updated":"2024-12-13T13:38:41Z","published":"2024-12-13T13:38:41Z","title":"Constraint-Aware Zero-Shot Vision-Language Navigation in Continuous\n  Environments","summary":"  We address the task of Vision-Language Navigation in Continuous Environments\n(VLN-CE) under the zero-shot setting. Zero-shot VLN-CE is particularly\nchallenging due to the absence of expert demonstrations for training and\nminimal environment structural prior to guide navigation. To confront these\nchallenges, we propose a Constraint-Aware Navigator (CA-Nav), which reframes\nzero-shot VLN-CE as a sequential, constraint-aware sub-instruction completion\nprocess. CA-Nav continuously translates sub-instructions into navigation plans\nusing two core modules: the Constraint-Aware Sub-instruction Manager (CSM) and\nthe Constraint-Aware Value Mapper (CVM). CSM defines the completion criteria\nfor decomposed sub-instructions as constraints and tracks navigation progress\nby switching sub-instructions in a constraint-aware manner. CVM, guided by\nCSM's constraints, generates a value map on the fly and refines it using\nsuperpixel clustering to improve navigation stability. CA-Nav achieves the\nstate-of-the-art performance on two VLN-CE benchmarks, surpassing the previous\nbest method by 12 percent and 13 percent in Success Rate on the validation\nunseen splits of R2R-CE and RxR-CE, respectively. Moreover, CA-Nav demonstrates\nits effectiveness in real-world robot deployments across various indoor scenes\nand instructions.\n","authors":["Kehan Chen","Dong An","Yan Huang","Rongtao Xu","Yifei Su","Yonggen Ling","Ian Reid","Liang Wang"],"pdf_url":"https://arxiv.org/pdf/2412.10137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07205v2","updated":"2024-12-13T12:38:04Z","published":"2024-12-10T05:50:50Z","title":"Crack-EdgeSAM Self-Prompting Crack Segmentation System for Edge Devices","summary":"  Structural health monitoring (SHM) is essential for the early detection of\ninfrastructure defects, such as cracks in concrete bridge pier. but often faces\nchallenges in efficiency and accuracy in complex environments. Although the\nSegment Anything Model (SAM) achieves excellent segmentation performance, its\ncomputational demands limit its suitability for real-time applications on edge\ndevices. To address these challenges, this paper proposes Crack-EdgeSAM, a\nself-prompting crack segmentation system that integrates YOLOv8 for generating\nprompt boxes and a fine-tuned EdgeSAM model for crack segmentation. To ensure\ncomputational efficiency, the method employs ConvLoRA, a Parameter-Efficient\nFine-Tuning (PEFT) technique, along with DiceFocalLoss to fine-tune the EdgeSAM\nmodel. Our experimental results on public datasets and the climbing robot\nautomatic inspections demonstrate that the system achieves high segmentation\naccuracy and significantly enhanced inference speed compared to the most recent\nmethods. Notably, the system processes 1024 x 1024 pixels images at 46 FPS on\nour PC and 8 FPS on Jetson Orin Nano.\n","authors":["Yingchu Wang","Ji He","Shijie Yu"],"pdf_url":"https://arxiv.org/pdf/2412.07205v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10096v1","updated":"2024-12-13T12:32:53Z","published":"2024-12-13T12:32:53Z","title":"Reward Machine Inference for Robotic Manipulation","summary":"  Learning from Demonstrations (LfD) and Reinforcement Learning (RL) have\nenabled robot agents to accomplish complex tasks. Reward Machines (RMs) enhance\nRL's capability to train policies over extended time horizons by structuring\nhigh-level task information. In this work, we introduce a novel LfD approach\nfor learning RMs directly from visual demonstrations of robotic manipulation\ntasks. Unlike previous methods, our approach requires no predefined\npropositions or prior knowledge of the underlying sparse reward signals.\nInstead, it jointly learns the RM structure and identifies key high-level\nevents that drive transitions between RM states. We validate our method on\nvision-based manipulation tasks, showing that the inferred RM accurately\ncaptures task structure and enables an RL agent to effectively learn an optimal\npolicy.\n","authors":["Mattijs Baert","Sam Leroux","Pieter Simoens"],"pdf_url":"https://arxiv.org/pdf/2412.10096v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10087v1","updated":"2024-12-13T12:24:39Z","published":"2024-12-13T12:24:39Z","title":"Consensus-Based Dynamic Task Allocation for Multi-Robot System\n  Considering Payloads Consumption","summary":"  This paper presents a consensus-based payload algorithm (CBPA) to deal with\nthe condition of robots' capability decrease for multi-robot task allocation.\nDuring the execution of complex tasks, robots' capabilities could decrease with\nthe consumption of payloads, which causes a problem that the robot coalition\nwould not meet the tasks' requirements in real time. The proposed CBPA is an\nenhanced version of the consensus-based bundle algorithm (CBBA) and comprises\ntwo primary core phases: the payload bundle construction and consensus phases.\nIn the payload bundle construction phase, CBPA introduces a payload assignment\nmatrix to track the payloads carried by the robots and the demands of\nmulti-robot tasks in real time. Then, robots share their respective payload\nassignment matrix in the consensus phase. These two phases are iterated to\ndynamically adjust the number of robots performing multi-robot tasks and the\nnumber of tasks each robot performs and obtain conflict-free results to ensure\nthat the robot coalition meets the demand and completes all tasks as quickly as\npossible. Physical experiment shows that CBPA is appropriate in complex and\ndynamic scenarios where robots need to collaborate and task requirements are\ntightly coupled to the robots' payloads. Numerical experiments show that CBPA\nhas higher total task gains than CBBA.\n","authors":["Xuekai Qiu","Pengming Zhu","Yiming Hu","Zhiwen Zeng","Huimin Lu"],"pdf_url":"https://arxiv.org/pdf/2412.10087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10083v1","updated":"2024-12-13T12:17:30Z","published":"2024-12-13T12:17:30Z","title":"Heterogeneous Multi-Robot Graph Coverage with Proximity and Movement\n  Constraints","summary":"  Multi-Robot Coverage problems have been extensively studied in robotics,\nplanning and multi-agent systems. In this work, we consider the coverage\nproblem when there are constraints on the proximity (e.g., maximum distance\nbetween the agents, or a blue agent must be adjacent to a red agent) and the\nmovement (e.g., terrain traversability and material load capacity) of the\nrobots. Such constraints naturally arise in many real-world applications, e.g.\nin search-and-rescue and maintenance operations. Given such a setting, the goal\nis to compute a covering tour of the graph with a minimum number of steps, and\nthat adheres to the proximity and movement constraints. For this problem, our\ncontributions are four: (i) a formal formulation of the problem, (ii) an exact\nalgorithm that is FPT in F, d and tw, the set of robot formations that encode\nthe proximity constraints, the maximum nodes degree, and the tree-width of the\ngraph, respectively, (iii) for the case that the graph is a tree: a PTAS\napproximation scheme, that given an approximation parameter epsilon, produces a\ntour that is within a epsilon times error(||F||, d) of the optimal one, and the\ncomputation runs in time poly(n) times h(1/epsilon,||F||). (iv) for the case\nthat the graph is a tree, with $k=3$ robots, and the constraint is that all\nagents are connected: a PTAS scheme with multiplicative approximation error of\n1+O(epsilon), independent of the maximal degree d.\n","authors":["Dolev Mutzari","Yonatan Aumann","Sarit Kraus"],"pdf_url":"https://arxiv.org/pdf/2412.10083v1.pdf","comment":"11 pages, 4 figures, to be published in the 39th Annual AAAI\n  Conference on Artificial Intelligence"},{"id":"http://arxiv.org/abs/2412.10050v1","updated":"2024-12-13T11:22:01Z","published":"2024-12-13T11:22:01Z","title":"ManipGPT: Is Affordance Segmentation by Large Vision Models Enough for\n  Articulated Object Manipulation?","summary":"  Visual actionable affordance has emerged as a transformative approach in\nrobotics, focusing on perceiving interaction areas prior to manipulation.\nTraditional methods rely on pixel sampling to identify successful interaction\nsamples or processing pointclouds for affordance mapping. However, these\napproaches are computationally intensive and struggle to adapt to diverse and\ndynamic environments. This paper introduces ManipGPT, a framework designed to\npredict optimal interaction areas for articulated objects using a large\npre-trained vision transformer (ViT). We created a dataset of 9.9k simulated\nand real images to bridge the sim-to-real gap and enhance real-world\napplicability. By fine-tuning the vision transformer on this small dataset, we\nsignificantly improved part-level affordance segmentation, adapting the model's\nin-context segmentation capabilities to robot manipulation scenarios. This\nenables effective manipulation across simulated and real-world environments by\ngenerating part-level affordance masks, paired with an impedance adaptation\npolicy, sufficiently eliminating the need for complex datasets or perception\nsystems.\n","authors":["Taewhan Kim","Hojin Bae","Zeming Li","Xiaoqi Li","Iaroslav Ponomarenko","Ruihai Wu","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2412.10050v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.10048v1","updated":"2024-12-13T11:20:55Z","published":"2024-12-13T11:20:55Z","title":"BatDeck -- Ultra Low-power Ultrasonic Ego-velocity Estimation and\n  Obstacle Avoidance on Nano-drones","summary":"  Nano-drones, with their small, lightweight design, are ideal for\nconfined-space rescue missions and inherently safe for human interaction.\nHowever, their limited payload restricts the critical sensing needed for\nego-velocity estimation and obstacle detection to single-bean laser-based\ntime-of-flight (ToF) and low-resolution optical sensors. Although those sensors\nhave demonstrated good performance, they fail in some complex real-world\nscenarios, especially when facing transparent or reflective surfaces (ToFs) or\nwhen lacking visual features (optical-flow sensors). Taking inspiration from\nbats, this paper proposes a novel two-way ranging-based method for ego-velocity\nestimation and obstacle avoidance based on down-and-forward facing\nultra-low-power ultrasonic sensors, which improve the performance when the\ndrone faces reflective materials or navigates in complete darkness. Our results\ndemonstrate that our new sensing system achieves a mean square error of 0.019\nm/s on ego-velocity estimation and allows exploration for a flight time of 8\nminutes while covering 136 m on average in a challenging environment with\ntransparent and reflective obstacles. We also compare ultrasonic and\nlaser-based ToF sensing techniques for obstacle avoidance, as well as optical\nflow and ultrasonic-based techniques for ego-velocity estimation, denoting how\nthese systems and methods can be complemented to enhance the robustness of\nnano-drone operations.\n","authors":["Hanna Müller","Victor Kartsch","Michele Magno","Luca Benini"],"pdf_url":"https://arxiv.org/pdf/2412.10048v1.pdf","comment":"This paper is extending \"BatDeck: Advancing Nano-drone Navigation\n  with Low-power Ultrasound-based Obstacle Avoidance\" (SAS 2024), and is\n  submitted to IEEE Transactions on Instrumentation and Measurements. arXiv\n  admin note: text overlap with arXiv:2403.16696"},{"id":"http://arxiv.org/abs/2412.09602v2","updated":"2024-12-13T09:51:22Z","published":"2024-12-12T18:59:13Z","title":"Hidden Biases of End-to-End Driving Datasets","summary":"  End-to-end driving systems have made rapid progress, but have so far not been\napplied to the challenging new CARLA Leaderboard 2.0. Further, while there is a\nlarge body of literature on end-to-end architectures and training strategies,\nthe impact of the training dataset is often overlooked. In this work, we make a\nfirst attempt at end-to-end driving for Leaderboard 2.0. Instead of\ninvestigating architectures, we systematically analyze the training dataset,\nleading to new insights: (1) Expert style significantly affects downstream\npolicy performance. (2) In complex data sets, the frames should not be weighted\non the basis of simplistic criteria such as class frequencies. (3) Instead,\nestimating whether a frame changes the target labels compared to previous\nframes can reduce the size of the dataset without removing important\ninformation. By incorporating these findings, our model ranks first and second\nrespectively on the map and sensors tracks of the 2024 CARLA Challenge, and\nsets a new state-of-the-art on the Bench2Drive test routes. Finally, we uncover\na design flaw in the current evaluation metrics and propose a modification for\nfuture challenges. Our dataset, code, and pre-trained models are publicly\navailable at https://github.com/autonomousvision/carla_garage.\n","authors":["Julian Zimmerlin","Jens Beißwenger","Bernhard Jaeger","Andreas Geiger","Kashyap Chitta"],"pdf_url":"https://arxiv.org/pdf/2412.09602v2.pdf","comment":"Technical report for the CVPR 2024 Workshop on Foundation Models for\n  Autonomous Systems. Runner-up of the track 'CARLA Autonomous Driving\n  Challenge' in the 2024 Autonomous Grand Challenge\n  (https://opendrivelab.com/challenge2024/)"},{"id":"http://arxiv.org/abs/2412.09995v1","updated":"2024-12-13T09:30:02Z","published":"2024-12-13T09:30:02Z","title":"Virtualization & Microservice Architecture for Software-Defined\n  Vehicles: An Evaluation and Exploration","summary":"  The emergence of Software-Defined Vehicles (SDVs) signifies a shift from a\ndistributed network of electronic control units (ECUs) to a centralized\ncomputing architecture within the vehicle's electrical and electronic systems.\nThis transition addresses the growing complexity and demand for enhanced\nfunctionality in traditional E/E architectures, with containerization and\nvirtualization streamlining software development and updates within the SDV\nframework. While widely used in cloud computing, their performance and\nsuitability for intelligent vehicles have yet to be thoroughly evaluated. In\nthis work, we conduct a comprehensive performance evaluation of\ncontainerization and virtualization on embedded and high-performance AMD64 and\nARM64 systems, focusing on CPU, memory, network, and disk metrics. In addition,\nwe assess their impact on real-world automotive applications using the Autoware\nframework and further integrate a microservice-based architecture to evaluate\nits start-up time and resource consumption. Our extensive experiments reveal a\nslight 0-5% performance decline in CPU, memory, and network usage for both\ncontainerization and virtualization compared to bare-metal setups, with more\nsignificant reductions in disk operations-5-15% for containerized environments\nand up to 35% for virtualized setups. Despite these declines, experiments with\nactual vehicle applications demonstrate minimal impact on the Autoware\nframework, and in some cases, a microservice architecture integration improves\nstart-up time by up to 18%.\n","authors":["Long Wen","Markus Rickert","Fengjunjie Pan","Jianjie Lin","Yu Zhang","Tobias Betz","Alois Knoll"],"pdf_url":"https://arxiv.org/pdf/2412.09995v1.pdf","comment":"15 pages, 15 figures"},{"id":"http://arxiv.org/abs/2412.09989v1","updated":"2024-12-13T09:21:02Z","published":"2024-12-13T09:21:02Z","title":"One Filter to Deploy Them All: Robust Safety for Quadrupedal Navigation\n  in Unknown Environments","summary":"  As learning-based methods for legged robots rapidly grow in popularity, it is\nimportant that we can provide safety assurances efficiently across different\ncontrollers and environments. Existing works either rely on a priori knowledge\nof the environment and safety constraints to ensure system safety or provide\nassurances for a specific locomotion policy. To address these limitations, we\npropose an observation-conditioned reachability-based (OCR) safety-filter\nframework. Our key idea is to use an OCR value network (OCR-VN) that predicts\nthe optimal control-theoretic safety value function for new failure regions and\ndynamic uncertainty during deployment time. Specifically, the OCR-VN\nfacilitates rapid safety adaptation through two key components: a LiDAR-based\ninput that allows the dynamic construction of safe regions in light of new\nobstacles and a disturbance estimation module that accounts for dynamics\nuncertainty in the wild. The predicted safety value function is used to\nconstruct an adaptive safety filter that overrides the nominal quadruped\ncontroller when necessary to maintain safety. Through simulation studies and\nhardware experiments on a Unitree Go1 quadruped, we demonstrate that the\nproposed framework can automatically safeguard a wide range of hierarchical\nquadruped controllers, adapts to novel environments, and is robust to unmodeled\ndynamics without a priori access to the controllers or environments - hence,\n\"One Filter to Deploy Them All\". The experiment videos can be found on the\nproject website.\n","authors":["Albert Lin","Shuang Peng","Somil Bansal"],"pdf_url":"https://arxiv.org/pdf/2412.09989v1.pdf","comment":"Project website:\n  https://sia-lab-git.github.io/One_Filter_to_Deploy_Them_All/"},{"id":"http://arxiv.org/abs/2412.09265v2","updated":"2024-12-13T08:44:16Z","published":"2024-12-12T13:22:02Z","title":"Score and Distribution Matching Policy: Advanced Accelerated Visuomotor\n  Policies via Matched Distillation","summary":"  Visual-motor policy learning has advanced with architectures like\ndiffusion-based policies, known for modeling complex robotic trajectories.\nHowever, their prolonged inference times hinder high-frequency control tasks\nrequiring real-time feedback. While consistency distillation (CD) accelerates\ninference, it introduces errors that compromise action quality. To address\nthese limitations, we propose the Score and Distribution Matching Policy (SDM\nPolicy), which transforms diffusion-based policies into single-step generators\nthrough a two-stage optimization process: score matching ensures alignment with\ntrue action distributions, and distribution matching minimizes KL divergence\nfor consistency. A dual-teacher mechanism integrates a frozen teacher for\nstability and an unfrozen teacher for adversarial training, enhancing\nrobustness and alignment with target distributions. Evaluated on a 57-task\nsimulation benchmark, SDM Policy achieves a 6x inference speedup while having\nstate-of-the-art action quality, providing an efficient and reliable framework\nfor high-frequency robotic tasks.\n","authors":["Bofang Jia","Pengxiang Ding","Can Cui","Mingyang Sun","Pengfang Qian","Siteng Huang","Zhaoxin Fan","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2412.09265v2.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2412.07689v3","updated":"2024-12-13T08:13:44Z","published":"2024-12-10T17:27:32Z","title":"DriveMM: All-in-One Large Multimodal Model for Autonomous Driving","summary":"  Large Multimodal Models (LMMs) have demonstrated exceptional comprehension\nand interpretation capabilities in Autonomous Driving (AD) by incorporating\nlarge language models. Despite the advancements, current data-driven AD\napproaches tend to concentrate on a single dataset and specific tasks,\nneglecting their overall capabilities and ability to generalize. To bridge\nthese gaps, we propose DriveMM, a general large multimodal model designed to\nprocess diverse data inputs, such as images and multi-view videos, while\nperforming a broad spectrum of AD tasks, including perception, prediction, and\nplanning. Initially, the model undergoes curriculum pre-training to process\nvaried visual signals and perform basic visual comprehension and perception\ntasks. Subsequently, we augment and standardize various AD-related datasets to\nfine-tune the model, resulting in an all-in-one LMM for autonomous driving. To\nassess the general capabilities and generalization ability, we conduct\nevaluations on six public benchmarks and undertake zero-shot transfer on an\nunseen dataset, where DriveMM achieves state-of-the-art performance across all\ntasks. We hope DriveMM as a promising solution for future end-to-end autonomous\ndriving applications in the real world. Project page with code:\nhttps://github.com/zhijian11/DriveMM.\n","authors":["Zhijian Huang","Chengjian Feng","Feng Yan","Baihui Xiao","Zequn Jie","Yujie Zhong","Xiaodan Liang","Lin Ma"],"pdf_url":"https://arxiv.org/pdf/2412.07689v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.01500v2","updated":"2024-12-13T07:05:27Z","published":"2024-12-02T13:51:58Z","title":"SF-Loc: A Visual Mapping and Geo-Localization System based on Sparse\n  Visual Structure Frames","summary":"  For high-level geo-spatial applications and intelligent robotics, accurate\nglobal pose information is of crucial importance. Map-aided localization is a\nuniversal approach to overcome the limitations of global navigation satellite\nsystem (GNSS) in challenging environments. However, current solutions face\nchallenges in terms of mapping flexibility, storage burden and re-localization\nperformance. In this work, we present SF-Loc, a lightweight visual mapping and\nmap-aided localization system, whose core idea is the map representation based\non sparse frames with dense but compact depth, termed as visual structure\nframes. In the mapping phase, multi-sensor dense bundle adjustment (MS-DBA) is\napplied to construct geo-referenced visual structure frames. The local\nco-visbility is checked to keep the map sparsity and achieve incremental\nmapping. In the localization phase, coarse-to-fine vision-based localization is\nperformed, in which multi-frame information and the map distribution are fully\nintegrated. To be specific, the concept of spatially smoothed similarity (SSS)\nis proposed to overcome the place ambiguity, and pairwise frame matching is\napplied for efficient and robust pose estimation. Experimental results on the\ncross-season dataset verify the effectiveness of the system. In complex urban\nroad scenarios, the map size is down to 3 MB per kilometer and stable\ndecimeter-level re-localization can be achieved. The code will be made\nopen-source soon (https://github.com/GREAT-WHU/SF-Loc).\n","authors":["Yuxuan Zhou","Xingxing Li","Shengyu Li","Chunxi Xia","Xuanbin Wang","Shaoquan Feng"],"pdf_url":"https://arxiv.org/pdf/2412.01500v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09913v1","updated":"2024-12-13T07:03:05Z","published":"2024-12-13T07:03:05Z","title":"Digital Twin Enabled Runtime Verification for Autonomous Mobile Robots\n  under Uncertainty","summary":"  As autonomous robots increasingly navigate complex and unpredictable\nenvironments, ensuring their reliable behavior under uncertainty becomes a\ncritical challenge. This paper introduces a digital twin-based runtime\nverification for an autonomous mobile robot to mitigate the impact posed by\nuncertainty in the deployment environment. The safety and performance\nproperties are specified and synthesized as runtime monitors using TeSSLa. The\nintegration of the executable digital twin, via the MQTT protocol, enables\ncontinuous monitoring and validation of the robot's behavior in real-time. We\nexplore the sources of uncertainties, including sensor noise and environment\nvariations, and analyze their impact on the robot safety and performance.\nEquipped with high computation resources, the cloud-located digital twin serves\nas a watch-dog model to estimate the actual state, check the consistency of the\nrobot's actuations and intervene to override such actuations if a safety or\nperformance property is about to be violated. The experimental analysis\ndemonstrated high efficiency of the proposed approach in ensuring the\nreliability and robustness of the autonomous robot behavior in uncertain\nenvironments and securing high alignment between the actual and expected speeds\nwhere the difference is reduced by up to 41\\% compared to the default robot\nnavigation control.\n","authors":["Joakim Schack Betzer","Jalil Boudjadar","Mirgita Frasheri","Prasad Talasila"],"pdf_url":"https://arxiv.org/pdf/2412.09913v1.pdf","comment":"10 pages, 10 figures, accepted at 2024 28th International Symposium\n  on Distributed Simulation and Real Time Applications"},{"id":"http://arxiv.org/abs/2412.02734v2","updated":"2024-12-13T06:17:48Z","published":"2024-12-03T18:18:33Z","title":"MVCTrack: Boosting 3D Point Cloud Tracking via Multimodal-Guided Virtual\n  Cues","summary":"  3D single object tracking is essential in autonomous driving and robotics.\nExisting methods often struggle with sparse and incomplete point cloud\nscenarios. To address these limitations, we propose a Multimodal-guided Virtual\nCues Projection (MVCP) scheme that generates virtual cues to enrich sparse\npoint clouds. Additionally, we introduce an enhanced tracker MVCTrack based on\nthe generated virtual cues. Specifically, the MVCP scheme seamlessly integrates\nRGB sensors into LiDAR-based systems, leveraging a set of 2D detections to\ncreate dense 3D virtual cues that significantly improve the sparsity of point\nclouds. These virtual cues can naturally integrate with existing LiDAR-based 3D\ntrackers, yielding substantial performance gains. Extensive experiments\ndemonstrate that our method achieves competitive performance on the NuScenes\ndataset.\n","authors":["Zhaofeng Hu","Sifan Zhou","Shibo Zhao","Zhihang Yuan"],"pdf_url":"https://arxiv.org/pdf/2412.02734v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09878v1","updated":"2024-12-13T05:50:13Z","published":"2024-12-13T05:50:13Z","title":"SonicBoom: Contact Localization Using Array of Microphones","summary":"  In cluttered environments where visual sensors encounter heavy occlusion,\nsuch as in agricultural settings, tactile signals can provide crucial spatial\ninformation for the robot to locate rigid objects and maneuver around them. We\nintroduce SonicBoom, a holistic hardware and learning pipeline that enables\ncontact localization through an array of contact microphones. While\nconventional sound source localization methods effectively triangulate sources\nin air, localization through solid media with irregular geometry and structure\npresents challenges that are difficult to model analytically. We address this\nchallenge through a feature engineering and learning based approach,\nautonomously collecting 18,000 robot interaction sound pairs to learn a mapping\nbetween acoustic signals and collision locations on the robot end effector\nlink. By leveraging relative features between microphones, SonicBoom achieves\nlocalization errors of 0.42cm for in distribution interactions and maintains\nrobust performance of 2.22cm error even with novel objects and contact\nconditions. We demonstrate the system's practical utility through haptic\nmapping of occluded branches in mock canopy settings, showing that acoustic\nbased sensing can enable reliable robot navigation in visually challenging\nenvironments.\n","authors":["Moonyoung Lee","Uksang Yoo","Jean Oh","Jeffrey Ichnowski","George Kantor","Oliver Kroemer"],"pdf_url":"https://arxiv.org/pdf/2412.09878v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2412.09877v1","updated":"2024-12-13T05:43:21Z","published":"2024-12-13T05:43:21Z","title":"Optimized Coordination Strategy for Multi-Aerospace Systems in\n  Pick-and-Place Tasks By Deep Neural Network","summary":"  In this paper, we present an advanced strategy for the coordinated control of\na multi-agent aerospace system, utilizing Deep Neural Networks (DNNs) within a\nreinforcement learning framework. Our approach centers on optimizing autonomous\ntask assignment to enhance the system's operational efficiency in object\nrelocation tasks, framed as an aerospace-oriented pick-and-place scenario. By\nmodeling this coordination challenge within a MuJoCo environment, we employ a\ndeep reinforcement learning algorithm to train a DNN-based policy to maximize\ntask completion rates across the multi-agent system. The objective function is\nexplicitly designed to maximize effective object transfer rates, leveraging\nneural network capabilities to handle complex state and action spaces in\nhigh-dimensional aerospace environments. Through extensive simulation, we\nbenchmark the proposed method against a heuristic combinatorial approach rooted\nin game-theoretic principles, demonstrating a marked performance improvement,\nwith the trained policy achieving up to 16\\% higher task efficiency.\nExperimental validation is conducted on a multi-agent hardware setup to\nsubstantiate the efficacy of our approach in a real-world aerospace scenario.\n","authors":["Ye Zhang","Linyue Chu","Letian Xu","Kangtong Mo","Zhengjian Kang","Xingyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.09877v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09868v1","updated":"2024-12-13T05:27:35Z","published":"2024-12-13T05:27:35Z","title":"RP-SLAM: Real-time Photorealistic SLAM with Efficient 3D Gaussian\n  Splatting","summary":"  3D Gaussian Splatting has emerged as a promising technique for high-quality\n3D rendering, leading to increasing interest in integrating 3DGS into realism\nSLAM systems. However, existing methods face challenges such as Gaussian\nprimitives redundancy, forgetting problem during continuous optimization, and\ndifficulty in initializing primitives in monocular case due to lack of depth\ninformation. In order to achieve efficient and photorealistic mapping, we\npropose RP-SLAM, a 3D Gaussian splatting-based vision SLAM method for monocular\nand RGB-D cameras. RP-SLAM decouples camera poses estimation from Gaussian\nprimitives optimization and consists of three key components. Firstly, we\npropose an efficient incremental mapping approach to achieve a compact and\naccurate representation of the scene through adaptive sampling and Gaussian\nprimitives filtering. Secondly, a dynamic window optimization method is\nproposed to mitigate the forgetting problem and improve map consistency.\nFinally, for the monocular case, a monocular keyframe initialization method\nbased on sparse point cloud is proposed to improve the initialization accuracy\nof Gaussian primitives, which provides a geometric basis for subsequent\noptimization. The results of numerous experiments demonstrate that RP-SLAM\nachieves state-of-the-art map rendering accuracy while ensuring real-time\nperformance and model compactness.\n","authors":["Lizhi Bai","Chunqi Tian","Jun Yang","Siyu Zhang","Masanori Suganuma","Takayuki Okatani"],"pdf_url":"https://arxiv.org/pdf/2412.09868v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09858v1","updated":"2024-12-13T04:57:55Z","published":"2024-12-13T04:57:55Z","title":"RLDG: Robotic Generalist Policy Distillation via Reinforcement Learning","summary":"  Recent advances in robotic foundation models have enabled the development of\ngeneralist policies that can adapt to diverse tasks. While these models show\nimpressive flexibility, their performance heavily depends on the quality of\ntheir training data. In this work, we propose Reinforcement Learning Distilled\nGeneralists (RLDG), a method that leverages reinforcement learning to generate\nhigh-quality training data for finetuning generalist policies. Through\nextensive real-world experiments on precise manipulation tasks like connector\ninsertion and assembly, we demonstrate that generalist policies trained with\nRL-generated data consistently outperform those trained with human\ndemonstrations, achieving up to 40% higher success rates while generalizing\nbetter to new tasks. We also provide a detailed analysis that reveals this\nperformance gain stems from both optimized action distributions and improved\nstate coverage. Our results suggest that combining task-specific RL with\ngeneralist policy distillation offers a promising approach for developing more\ncapable and efficient robotic manipulation systems that maintain the\nflexibility of foundation models while achieving the performance of specialized\ncontrollers. Videos and code can be found on our project website\nhttps://generalist-distillation.github.io\n","authors":["Charles Xu","Qiyang Li","Jianlan Luo","Sergey Levine"],"pdf_url":"https://arxiv.org/pdf/2412.09858v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07700v2","updated":"2024-12-13T04:13:03Z","published":"2024-09-12T02:04:00Z","title":"Disturbance-Robust Backup Control Barrier Functions: Safety Under\n  Uncertain Dynamics","summary":"  Obtaining a controlled invariant set is crucial for safety-critical control\nwith control barrier functions (CBFs) but is non-trivial for complex nonlinear\nsystems and constraints. Backup control barrier functions allow such sets to be\nconstructed online in a computationally tractable manner by examining the\nevolution (or flow) of the system under a known backup control law. However,\nfor systems with unmodeled disturbances, this flow cannot be directly computed,\nmaking the current methods inadequate for assuring safety in these scenarios.\nTo address this gap, we leverage bounds on the nominal and disturbed flow to\ncompute a forward invariant set online by ensuring safety of an expanding norm\nball tube centered around the nominal system evolution. We prove that this set\nresults in robust control constraints which guarantee safety of the disturbed\nsystem via our Disturbance-Robust Backup Control Barrier Function (DR-bCBF)\nsolution. The efficacy of the proposed framework is demonstrated in simulation,\napplied to a double integrator problem and a rigid body spacecraft rotation\nproblem with rate constraints.\n","authors":["David E. J. van Wijk","Samuel Coogan","Tamas G. Molnar","Manoranjan Majji","Kerianne L. Hobbs"],"pdf_url":"https://arxiv.org/pdf/2409.07700v2.pdf","comment":"Accepted for publication in IEEE Control Systems Letters (L-CSS). 6\n  pages, 4 figures"},{"id":"http://arxiv.org/abs/2309.09224v2","updated":"2024-12-13T03:35:39Z","published":"2023-09-17T09:34:00Z","title":"CapsuleBot: A Novel Hybrid Aerial-Ground Bi-Copter Robot With Two\n  Actuated-Wheel-Rotors","summary":"  This paper presents the design, modeling, and experimental validation of\nCapsuleBot, a novel hybrid aerial-ground bi-copter robot designed for\nlong-endurance and low-noise operations. CapsuleBot combines the\nmaneuverability of a bi-copter in the air with the low power consumption and\nlow noise of a two-wheel self-balancing robot on the ground. To achieve this,\nwe design an innovative mechanical structure named the actuated-wheel-rotor,\nwhich uses a servo motor and a brushless motor to function as both a tilting\nrotor in the air and an actuated wheel on the ground. CapsuleBot is equipped\nwith two actuated-wheel-rotors, enabling it to achieve hybrid aerial-ground\npropulsion using only four motors, with no additional motors required compared\nto a bi-copter. Additionally, we develop comprehensive dynamics and control\nsystems for both air and wheel mode, based on the bi-copter model and the\ntwo-wheel self-balancing robot model. A prototype of CapsuleBot is constructed,\nand its performance in terms of low power consumption and low noise is\nvalidated through experiments. Challenging tasks demonstrate CapsuleBot's\ncapability to climb steep, fly over cliffs, and traverse rough terrains.\n","authors":["Zhi Zheng","Qifeng Cai","Jin Wang","Xinhang Xu","Muqing Cao","Huan Yu","Jihao Li","Jun Meng","Guodong Lu"],"pdf_url":"https://arxiv.org/pdf/2309.09224v2.pdf","comment":"Accepted by IEEE Robotics and Automation Letters (RA-L)"},{"id":"http://arxiv.org/abs/2412.09816v1","updated":"2024-12-13T03:13:04Z","published":"2024-12-13T03:13:04Z","title":"Distributed Inverse Dynamics Control for Quadruped Robots using\n  Geometric Optimization","summary":"  This paper presents a distributed inverse dynamics controller (DIDC) for\nquadruped robots that addresses the limitations of existing reactive\ncontrollers: simplified dynamical models, the inability to handle exact\nfriction cone constraints, and the high computational requirements of\nwhole-body controllers. Current methods either ignore friction constraints\nentirely or use linear approximations, leading to potential slip and\ninstability, while comprehensive whole-body controllers demand significant\ncomputational resources. Our approach uses full rigid-body dynamics and\nenforces exact friction cone constraints through a novel geometric\noptimization-based solver. DIDC combines the required generalized forces\ncorresponding to the actuated and unactuated spaces by projecting them onto the\nactuated space while satisfying the physical constraints and maintaining\northogonality between the base and joint tracking objectives. Experimental\nvalidation shows that our approach reduces foot slippage, improves orientation\ntracking, and converges at least two times faster than existing reactive\ncontrollers with generic QP-based implementations. The controller enables\nstable omnidirectional trotting at various speeds and consumes less power than\ncomparable methods while running efficiently on embedded processors.\n","authors":["Nimesh Khandelwal","Amritanshu Manu","Shakti S. Gupta","Mangal Kothari","Prashanth Krishnamurthy","Farshad Khorrami"],"pdf_url":"https://arxiv.org/pdf/2412.09816v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13223v2","updated":"2024-12-13T02:31:22Z","published":"2024-06-19T05:25:15Z","title":"Act Better by Timing: A timing-Aware Reinforcement Learning for\n  Autonomous Driving","summary":"  Autonomous vehicles inevitably encounter a vast array of scenarios in\nreal-world environments. Addressing long-tail scenarios, particularly those\ninvolving intensive interactions with numerous traffic participants, remains\none of the most significant challenges in achieving high-level autonomous\ndriving. Reinforcement learning (RL) offers a promising solution for such\nscenarios and allows autonomous vehicles to continuously self-evolve during\ninteractions. However, traditional RL often requires trial and error from\nscratch in new scenarios, resulting in inefficient exploration of unknown\nstates. Integrating RL with planning-based methods can significantly accelerate\nthe learning process. Additionally, conventional RL methods lack robust safety\nmechanisms, making agents prone to collisions in dynamic environments in\npursuit of short-term rewards. Many existing safe RL methods depend on\nenvironment modeling to identify reliable safety boundaries for constraining\nagent behavior. However, explicit environmental models can fail to capture the\ncomplexity of dynamic environments comprehensively. Inspired by the observation\nthat human drivers rarely take risks in uncertain situations, this study\nintroduces the concept of action timing and proposes a timing-aware RL method,\nIn this approach, a \"timing imagination\" process previews the execution results\nof the agent's strategies at different time scales. The optimal execution\ntiming is then projected to each decision moment, generating a dynamic safety\nfactor to constrain actions. A planning-based method serves as a conservative\nbaseline strategy in uncertain states. In two representative interaction\nscenarios, an unsignalized intersection and a roundabout, the proposed model\noutperforms the benchmark models in driving safety.\n","authors":["Guanzhou Li","Jianping Wu","Yujing He"],"pdf_url":"https://arxiv.org/pdf/2406.13223v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09782v1","updated":"2024-12-13T01:37:44Z","published":"2024-12-13T01:37:44Z","title":"EI-Drive: A Platform for Cooperative Perception with Realistic\n  Communication Models","summary":"  The growing interest in autonomous driving calls for realistic simulation\nplatforms capable of accurately simulating cooperative perception process in\nrealistic traffic scenarios. Existing studies for cooperative perception often\nhave not accounted for transmission latency and errors in real-world\nenvironments. To address this gap, we introduce EI-Drive, an edge-AI based\nautonomous driving simulation platform that integrates advanced cooperative\nperception with more realistic communication models. Built on the CARLA\nframework, EI-Drive features new modules for cooperative perception while\ntaking into account transmission latency and errors, providing a more realistic\nplatform for evaluating cooperative perception algorithms. In particular, the\nplatform enables vehicles to fuse data from multiple sources, improving\nsituational awareness and safety in complex environments. With its modular\ndesign, EI-Drive allows for detailed exploration of sensing, perception,\nplanning, and control in various cooperative driving scenarios. Experiments\nusing EI-Drive demonstrate significant improvements in vehicle safety and\nperformance, particularly in scenarios with complex traffic flow and network\nconditions. All code and documents are accessible on our GitHub page:\n\\url{https://ucd-dare.github.io/eidrive.github.io/}.\n","authors":["Hanchu Zhou","Edward Xie","Wei Shao","Dechen Gao","Michelle Dong","Junshan Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.09782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09777v1","updated":"2024-12-13T01:10:56Z","published":"2024-12-13T01:10:56Z","title":"Contingency Constrained Planning with MPPI within MPPI","summary":"  For safety, autonomous systems must be able to consider sudden changes and\nenact contingency plans appropriately. State-of-the-art methods currently find\ntrajectories that balance between nominal and contingency behavior, or plan for\na singular contingency plan; however, this does not guarantee that the\nresulting plan is safe for all time. To address this research gap, this paper\npresents Contingency-MPPI, a data-driven optimization-based strategy that\nembeds contingency planning inside a nominal planner. By learning to\napproximate the optimal contingency-constrained control sequence with adaptive\nimportance sampling, the proposed method's sampling efficiency is further\nimproved with initializations from a lightweight path planner and trajectory\noptimizer. Finally, we present simulated and hardware experiments demonstrating\nour algorithm generating nominal and contingency plans in real time on a mobile\nrobot.\n","authors":["Leonard Jung","Alexander Estornell","Michael Everett"],"pdf_url":"https://arxiv.org/pdf/2412.09777v1.pdf","comment":"12 Pages, 6 Figures"},{"id":"http://arxiv.org/abs/2412.10599v1","updated":"2024-12-13T23:02:15Z","published":"2024-12-13T23:02:15Z","title":"Advances in Transformers for Robotic Applications: A Review","summary":"  The introduction of Transformers architecture has brought about significant\nbreakthroughs in Deep Learning (DL), particularly within Natural Language\nProcessing (NLP). Since their inception, Transformers have outperformed many\ntraditional neural network architectures due to their \"self-attention\"\nmechanism and their scalability across various applications. In this paper, we\ncover the use of Transformers in Robotics. We go through recent advances and\ntrends in Transformer architectures and examine their integration into robotic\nperception, planning, and control for autonomous systems. Furthermore, we\nreview past work and recent research on use of Transformers in Robotics as\npre-trained foundation models and integration of Transformers with Deep\nReinforcement Learning (DRL) for autonomous systems. We discuss how different\nTransformer variants are being adapted in robotics for reliable planning and\nperception, increasing human-robot interaction, long-horizon decision-making,\nand generalization. Finally, we address limitations and challenges, offering\ninsight and suggestions for future research directions.\n","authors":["Nikunj Sanghai","Nik Bear Brown"],"pdf_url":"https://arxiv.org/pdf/2412.10599v1.pdf","comment":"Early preprint, focusing primarily on general purpose robots, more\n  updates to come"},{"id":"http://arxiv.org/abs/2412.10525v1","updated":"2024-12-13T19:38:36Z","published":"2024-12-13T19:38:36Z","title":"RowDetr: End-to-End Row Detection Using Polynomials","summary":"  Crop row detection has garnered significant interest due to its critical role\nin enabling navigation in GPS-denied environments, such as under-canopy\nagricultural settings. To address this challenge, we propose RowDetr, an\nend-to-end neural network that utilizes smooth polynomial functions to\ndelineate crop boundaries in image space. A novel energy-based loss function,\nPolyOptLoss, is introduced to enhance learning robustness, even with noisy\nlabels. The proposed model demonstrates a 3% improvement over Agronav in key\nperformance metrics while being six times faster, making it well-suited for\nreal-time applications. Additionally, metrics from lane detection studies were\nadapted to comprehensively evaluate the system, showcasing its accuracy and\nadaptability in various scenarios.\n","authors":["Rahul Harsha Cheppally","Ajay Sharda"],"pdf_url":"https://arxiv.org/pdf/2412.10525v1.pdf","comment":"Code will be open sourced upon publication"},{"id":"http://arxiv.org/abs/2412.10515v1","updated":"2024-12-13T19:19:10Z","published":"2024-12-13T19:19:10Z","title":"Active Semantic Mapping with Mobile Manipulator in Horticultural\n  Environments","summary":"  Semantic maps are fundamental for robotics tasks such as navigation and\nmanipulation. They also enable yield prediction and phenotyping in agricultural\nsettings. In this paper, we introduce an efficient and scalable approach for\nactive semantic mapping in horticultural environments, employing a mobile robot\nmanipulator equipped with an RGB-D camera. Our method leverages probabilistic\nsemantic maps to detect semantic targets, generate candidate viewpoints, and\ncompute corresponding information gain. We present an efficient ray-casting\nstrategy and a novel information utility function that accounts for both\nsemantics and occlusions. The proposed approach reduces total runtime by 8%\ncompared to previous baselines. Furthermore, our information metric surpasses\nother metrics in reducing multi-class entropy and improving surface coverage,\nparticularly in the presence of segmentation noise. Real-world experiments\nvalidate our method's effectiveness but also reveal challenges such as depth\nsensor noise and varying environmental conditions, requiring further research.\n","authors":["Jose Cuaran","Kulbir Singh Ahluwalia","Kendall Koe","Naveen Kumar Uppalapati","Girish Chowdhary"],"pdf_url":"https://arxiv.org/pdf/2412.10515v1.pdf","comment":"Preprint (under review)"},{"id":"http://arxiv.org/abs/2412.12190v1","updated":"2024-12-13T22:52:47Z","published":"2024-12-13T22:52:47Z","title":"iMoT: Inertial Motion Transformer for Inertial Navigation","summary":"  We propose iMoT, an innovative Transformer-based inertial odometry method\nthat retrieves cross-modal information from motion and rotation modalities for\naccurate positional estimation. Unlike prior work, during the encoding of the\nmotion context, we introduce Progressive Series Decoupler at the beginning of\neach encoder layer to stand out critical motion events inherent in acceleration\nand angular velocity signals. To better aggregate cross-modal interactions, we\npresent Adaptive Positional Encoding, which dynamically modifies positional\nembeddings for temporal discrepancies between different modalities. During\ndecoding, we introduce a small set of learnable query motion particles as\npriors to model motion uncertainties within velocity segments. Each query\nmotion particle is intended to draw cross-modal features dedicated to a\nspecific motion mode, all taken together allowing the model to refine its\nunderstanding of motion dynamics effectively. Lastly, we design a dynamic\nscoring mechanism to stabilize iMoT's optimization by considering all aligned\nmotion particles at the final decoding step, ensuring robust and accurate\nvelocity segment estimation. Extensive evaluations on various inertial datasets\ndemonstrate that iMoT significantly outperforms state-of-the-art methods in\ndelivering superior robustness and accuracy in trajectory reconstruction.\n","authors":["Son Minh Nguyen","Linh Duy Tran","Duc Viet Le","Paul J. M Havinga"],"pdf_url":"https://arxiv.org/pdf/2412.12190v1.pdf","comment":"Accepted as technical research paper in 39th AAAI Conference on\n  Artificial Intelligence, 2025 (AAAI 2025)"}]},"2024-12-16T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2412.12096v1","updated":"2024-12-16T18:59:45Z","published":"2024-12-16T18:59:45Z","title":"PanSplat: 4K Panorama Synthesis with Feed-Forward Gaussian Splatting","summary":"  With the advent of portable 360{\\deg} cameras, panorama has gained\nsignificant attention in applications like virtual reality (VR), virtual tours,\nrobotics, and autonomous driving. As a result, wide-baseline panorama view\nsynthesis has emerged as a vital task, where high resolution, fast inference,\nand memory efficiency are essential. Nevertheless, existing methods are\ntypically constrained to lower resolutions (512 $\\times$ 1024) due to demanding\nmemory and computational requirements. In this paper, we present PanSplat, a\ngeneralizable, feed-forward approach that efficiently supports resolution up to\n4K (2048 $\\times$ 4096). Our approach features a tailored spherical 3D Gaussian\npyramid with a Fibonacci lattice arrangement, enhancing image quality while\nreducing information redundancy. To accommodate the demands of high resolution,\nwe propose a pipeline that integrates a hierarchical spherical cost volume and\nGaussian heads with local operations, enabling two-step deferred\nbackpropagation for memory-efficient training on a single A100 GPU. Experiments\ndemonstrate that PanSplat achieves state-of-the-art results with superior\nefficiency and image quality across both synthetic and real-world datasets.\nCode will be available at \\url{https://github.com/chengzhag/PanSplat}.\n","authors":["Cheng Zhang","Haofei Xu","Qianyi Wu","Camilo Cruz Gambardella","Dinh Phung","Jianfei Cai"],"pdf_url":"https://arxiv.org/pdf/2412.12096v1.pdf","comment":"Project Page: https://chengzhag.github.io/publication/pansplat/ Code:\n  https://github.com/chengzhag/PanSplat"},{"id":"http://arxiv.org/abs/2412.12095v1","updated":"2024-12-16T18:59:29Z","published":"2024-12-16T18:59:29Z","title":"Causal Diffusion Transformers for Generative Modeling","summary":"  We introduce Causal Diffusion as the autoregressive (AR) counterpart of\nDiffusion models. It is a next-token(s) forecasting framework that is friendly\nto both discrete and continuous modalities and compatible with existing\nnext-token prediction models like LLaMA and GPT. While recent works attempt to\ncombine diffusion with AR models, we show that introducing sequential\nfactorization to a diffusion model can substantially improve its performance\nand enables a smooth transition between AR and diffusion generation modes.\nHence, we propose CausalFusion - a decoder-only transformer that\ndual-factorizes data across sequential tokens and diffusion noise levels,\nleading to state-of-the-art results on the ImageNet generation benchmark while\nalso enjoying the AR advantage of generating an arbitrary number of tokens for\nin-context reasoning. We further demonstrate CausalFusion's multimodal\ncapabilities through a joint image generation and captioning model, and\nshowcase CausalFusion's ability for zero-shot in-context image manipulations.\nWe hope that this work could provide the community with a fresh perspective on\ntraining multimodal models over discrete and continuous data.\n","authors":["Chaorui Deng","Deyao Zh","Kunchang Li","Shi Guan","Haoqi Fan"],"pdf_url":"https://arxiv.org/pdf/2412.12095v1.pdf","comment":"21 pages, 22 figures"},{"id":"http://arxiv.org/abs/2412.12093v1","updated":"2024-12-16T18:58:51Z","published":"2024-12-16T18:58:51Z","title":"CAP4D: Creating Animatable 4D Portrait Avatars with Morphable Multi-View\n  Diffusion Models","summary":"  Reconstructing photorealistic and dynamic portrait avatars from images is\nessential to many applications including advertising, visual effects, and\nvirtual reality. Depending on the application, avatar reconstruction involves\ndifferent capture setups and constraints $-$ for example, visual effects\nstudios use camera arrays to capture hundreds of reference images, while\ncontent creators may seek to animate a single portrait image downloaded from\nthe internet. As such, there is a large and heterogeneous ecosystem of methods\nfor avatar reconstruction. Techniques based on multi-view stereo or neural\nrendering achieve the highest quality results, but require hundreds of\nreference images. Recent generative models produce convincing avatars from a\nsingle reference image, but visual fidelity yet lags behind multi-view\ntechniques. Here, we present CAP4D: an approach that uses a morphable\nmulti-view diffusion model to reconstruct photoreal 4D (dynamic 3D) portrait\navatars from any number of reference images (i.e., one to 100) and animate and\nrender them in real time. Our approach demonstrates state-of-the-art\nperformance for single-, few-, and multi-image 4D portrait avatar\nreconstruction, and takes steps to bridge the gap in visual fidelity between\nsingle-image and multi-view reconstruction techniques.\n","authors":["Felix Taubner","Ruihang Zhang","Mathieu Tuli","David B. Lindell"],"pdf_url":"https://arxiv.org/pdf/2412.12093v1.pdf","comment":"23 pages, 15 figures"},{"id":"http://arxiv.org/abs/2412.12091v1","updated":"2024-12-16T18:58:17Z","published":"2024-12-16T18:58:17Z","title":"Wonderland: Navigating 3D Scenes from a Single Image","summary":"  This paper addresses a challenging question: How can we efficiently create\nhigh-quality, wide-scope 3D scenes from a single arbitrary image? Existing\nmethods face several constraints, such as requiring multi-view data,\ntime-consuming per-scene optimization, low visual quality in backgrounds, and\ndistorted reconstructions in unseen areas. We propose a novel pipeline to\novercome these limitations. Specifically, we introduce a large-scale\nreconstruction model that uses latents from a video diffusion model to predict\n3D Gaussian Splattings for the scenes in a feed-forward manner. The video\ndiffusion model is designed to create videos precisely following specified\ncamera trajectories, allowing it to generate compressed video latents that\ncontain multi-view information while maintaining 3D consistency. We train the\n3D reconstruction model to operate on the video latent space with a progressive\ntraining strategy, enabling the efficient generation of high-quality,\nwide-scope, and generic 3D scenes. Extensive evaluations across various\ndatasets demonstrate that our model significantly outperforms existing methods\nfor single-view 3D scene generation, particularly with out-of-domain images.\nFor the first time, we demonstrate that a 3D reconstruction model can be\neffectively built upon the latent space of a diffusion model to realize\nefficient 3D scene generation.\n","authors":["Hanwen Liang","Junli Cao","Vidit Goel","Guocheng Qian","Sergei Korolev","Demetri Terzopoulos","Konstantinos N. Plataniotis","Sergey Tulyakov","Jian Ren"],"pdf_url":"https://arxiv.org/pdf/2412.12091v1.pdf","comment":"Project page: https://snap-research.github.io/wonderland/"},{"id":"http://arxiv.org/abs/2412.12089v1","updated":"2024-12-16T18:56:24Z","published":"2024-12-16T18:56:24Z","title":"Stabilizing Reinforcement Learning in Differentiable Multiphysics\n  Simulation","summary":"  Recent advances in GPU-based parallel simulation have enabled practitioners\nto collect large amounts of data and train complex control policies using deep\nreinforcement learning (RL), on commodity GPUs. However, such successes for RL\nin robotics have been limited to tasks sufficiently simulated by fast\nrigid-body dynamics. Simulation techniques for soft bodies are comparatively\nseveral orders of magnitude slower, thereby limiting the use of RL due to\nsample complexity requirements. To address this challenge, this paper presents\nboth a novel RL algorithm and a simulation platform to enable scaling RL on\ntasks involving rigid bodies and deformables. We introduce Soft Analytic Policy\nOptimization (SAPO), a maximum entropy first-order model-based actor-critic RL\nalgorithm, which uses first-order analytic gradients from differentiable\nsimulation to train a stochastic actor to maximize expected return and entropy.\nAlongside our approach, we develop Rewarped, a parallel differentiable\nmultiphysics simulation platform that supports simulating various materials\nbeyond rigid bodies. We re-implement challenging manipulation and locomotion\ntasks in Rewarped, and show that SAPO outperforms baselines over a range of\ntasks that involve interaction between rigid bodies, articulations, and\ndeformables.\n","authors":["Eliot Xing","Vernon Luk","Jean Oh"],"pdf_url":"https://arxiv.org/pdf/2412.12089v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12087v1","updated":"2024-12-16T18:56:17Z","published":"2024-12-16T18:56:17Z","title":"Instruction-based Image Manipulation by Watching How Things Move","summary":"  This paper introduces a novel dataset construction pipeline that samples\npairs of frames from videos and uses multimodal large language models (MLLMs)\nto generate editing instructions for training instruction-based image\nmanipulation models. Video frames inherently preserve the identity of subjects\nand scenes, ensuring consistent content preservation during editing.\nAdditionally, video data captures diverse, natural dynamics-such as non-rigid\nsubject motion and complex camera movements-that are difficult to model\notherwise, making it an ideal source for scalable dataset construction. Using\nthis approach, we create a new dataset to train InstructMove, a model capable\nof instruction-based complex manipulations that are difficult to achieve with\nsynthetically generated datasets. Our model demonstrates state-of-the-art\nperformance in tasks such as adjusting subject poses, rearranging elements, and\naltering camera perspectives.\n","authors":["Mingdeng Cao","Xuaner Zhang","Yinqiang Zheng","Zhihao Xia"],"pdf_url":"https://arxiv.org/pdf/2412.12087v1.pdf","comment":"Project page: https://ljzycmd.github.io/projects/InstructMove/"},{"id":"http://arxiv.org/abs/2411.17474v2","updated":"2024-12-16T18:55:09Z","published":"2024-11-25T18:59:50Z","title":"Probing the Mid-level Vision Capabilities of Self-Supervised Learning","summary":"  Mid-level vision capabilities - such as generic object localization and 3D\ngeometric understanding - are not only fundamental to human vision but are also\ncrucial for many real-world applications of computer vision. These abilities\nemerge with minimal supervision during the early stages of human visual\ndevelopment. Despite their significance, current self-supervised learning (SSL)\napproaches are primarily designed and evaluated for high-level recognition\ntasks, leaving their mid-level vision capabilities largely unexamined.\n  In this study, we introduce a suite of benchmark protocols to systematically\nassess mid-level vision capabilities and present a comprehensive, controlled\nevaluation of 22 prominent SSL models across 8 mid-level vision tasks. Our\nexperiments reveal a weak correlation between mid-level and high-level task\nperformance. We also identify several SSL methods with highly imbalanced\nperformance across mid-level and high-level capabilities, as well as some that\nexcel in both. Additionally, we investigate key factors contributing to\nmid-level vision performance, such as pretraining objectives and network\narchitectures. Our study provides a holistic and timely view of what SSL models\nhave learned, complementing existing research that primarily focuses on\nhigh-level vision tasks. We hope our findings guide future SSL research to\nbenchmark models not only on high-level vision tasks but on mid-level as well.\n","authors":["Xuweiyi Chen","Markus Marks","Zezhou Cheng"],"pdf_url":"https://arxiv.org/pdf/2411.17474v2.pdf","comment":"Project Page: https://midvision-probe.cs.virginia.edu/"},{"id":"http://arxiv.org/abs/2412.12083v1","updated":"2024-12-16T18:52:56Z","published":"2024-12-16T18:52:56Z","title":"IDArb: Intrinsic Decomposition for Arbitrary Number of Input Views and\n  Illuminations","summary":"  Capturing geometric and material information from images remains a\nfundamental challenge in computer vision and graphics. Traditional\noptimization-based methods often require hours of computational time to\nreconstruct geometry, material properties, and environmental lighting from\ndense multi-view inputs, while still struggling with inherent ambiguities\nbetween lighting and material. On the other hand, learning-based approaches\nleverage rich material priors from existing 3D object datasets but face\nchallenges with maintaining multi-view consistency. In this paper, we introduce\nIDArb, a diffusion-based model designed to perform intrinsic decomposition on\nan arbitrary number of images under varying illuminations. Our method achieves\naccurate and multi-view consistent estimation on surface normals and material\nproperties. This is made possible through a novel cross-view, cross-domain\nattention module and an illumination-augmented, view-adaptive training\nstrategy. Additionally, we introduce ARB-Objaverse, a new dataset that provides\nlarge-scale multi-view intrinsic data and renderings under diverse lighting\nconditions, supporting robust training. Extensive experiments demonstrate that\nIDArb outperforms state-of-the-art methods both qualitatively and\nquantitatively. Moreover, our approach facilitates a range of downstream tasks,\nincluding single-image relighting, photometric stereo, and 3D reconstruction,\nhighlighting its broad applications in realistic 3D content creation.\n","authors":["Zhibing Li","Tong Wu","Jing Tan","Mengchen Zhang","Jiaqi Wang","Dahua Lin"],"pdf_url":"https://arxiv.org/pdf/2412.12083v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12079v1","updated":"2024-12-16T18:48:58Z","published":"2024-12-16T18:48:58Z","title":"UniLoc: Towards Universal Place Recognition Using Any Single Modality","summary":"  To date, most place recognition methods focus on single-modality retrieval.\nWhile they perform well in specific environments, cross-modal methods offer\ngreater flexibility by allowing seamless switching between map and query\nsources. It also promises to reduce computation requirements by having a\nunified model, and achieving greater sample efficiency by sharing parameters.\nIn this work, we develop a universal solution to place recognition, UniLoc,\nthat works with any single query modality (natural language, image, or point\ncloud). UniLoc leverages recent advances in large-scale contrastive learning,\nand learns by matching hierarchically at two levels: instance-level matching\nand scene-level matching. Specifically, we propose a novel Self-Attention based\nPooling (SAP) module to evaluate the importance of instance descriptors when\naggregated into a place-level descriptor. Experiments on the KITTI-360 dataset\ndemonstrate the benefits of cross-modality for place recognition, achieving\nsuperior performance in cross-modal settings and competitive results also for\nuni-modal scenarios. Our project page is publicly available at\nhttps://yan-xia.github.io/projects/UniLoc/.\n","authors":["Yan Xia","Zhendong Li","Yun-Jin Li","Letian Shi","Hu Cao","João F. Henriques","Daniel Cremers"],"pdf_url":"https://arxiv.org/pdf/2412.12079v1.pdf","comment":"14 pages, 10 figures"},{"id":"http://arxiv.org/abs/2412.12077v1","updated":"2024-12-16T18:46:58Z","published":"2024-12-16T18:46:58Z","title":"CPath-Omni: A Unified Multimodal Foundation Model for Patch and Whole\n  Slide Image Analysis in Computational Pathology","summary":"  The emergence of large multimodal models (LMMs) has brought significant\nadvancements to pathology. Previous research has primarily focused on\nseparately training patch-level and whole-slide image (WSI)-level models,\nlimiting the integration of learned knowledge across patches and WSIs, and\nresulting in redundant models. In this work, we introduce CPath-Omni, the first\n15-billion-parameter LMM designed to unify both patch and WSI level image\nanalysis, consolidating a variety of tasks at both levels, including\nclassification, visual question answering, captioning, and visual referring\nprompting. Extensive experiments demonstrate that CPath-Omni achieves\nstate-of-the-art (SOTA) performance across seven diverse tasks on 39 out of 42\ndatasets, outperforming or matching task-specific models trained for individual\ntasks. Additionally, we develop a specialized pathology CLIP-based visual\nprocessor for CPath-Omni, CPath-CLIP, which, for the first time, integrates\ndifferent vision models and incorporates a large language model as a text\nencoder to build a more powerful CLIP model, which achieves SOTA performance on\nnine zero-shot and four few-shot datasets. Our findings highlight CPath-Omni's\nability to unify diverse pathology tasks, demonstrating its potential to\nstreamline and advance the field of foundation model in pathology.\n","authors":["Yuxuan Sun","Yixuan Si","Chenglu Zhu","Xuan Gong","Kai Zhang","Pingyi Chen","Ye Zhang","Zhongyi Shui","Tao Lin","Lin Yang"],"pdf_url":"https://arxiv.org/pdf/2412.12077v1.pdf","comment":"22 pages, 13 figures"},{"id":"http://arxiv.org/abs/2412.12075v1","updated":"2024-12-16T18:46:45Z","published":"2024-12-16T18:46:45Z","title":"CG-Bench: Clue-grounded Question Answering Benchmark for Long Video\n  Understanding","summary":"  Most existing video understanding benchmarks for multimodal large language\nmodels (MLLMs) focus only on short videos. The limited number of benchmarks for\nlong video understanding often rely solely on multiple-choice questions (MCQs).\nHowever, because of the inherent limitation of MCQ-based evaluation and the\nincreasing reasoning ability of MLLMs, models can give the current answer\npurely by combining short video understanding with elimination, without\ngenuinely understanding the video content. To address this gap, we introduce\nCG-Bench, a novel benchmark designed for clue-grounded question answering in\nlong videos. CG-Bench emphasizes the model's ability to retrieve relevant clues\nfor questions, enhancing evaluation credibility. It features 1,219 manually\ncurated videos categorized by a granular system with 14 primary categories, 171\nsecondary categories, and 638 tertiary categories, making it the largest\nbenchmark for long video analysis. The benchmark includes 12,129 QA pairs in\nthree major question types: perception, reasoning, and hallucination.\nCompensating the drawbacks of pure MCQ-based evaluation, we design two novel\nclue-based evaluation methods: clue-grounded white box and black box\nevaluations, to assess whether the model generates answers based on the correct\nunderstanding of the video. We evaluate multiple closed-source and open-source\nMLLMs on CG-Bench. Results indicate that current models significantly\nunderperform in understanding long videos compared to short ones, and a\nsignificant gap exists between open-source and commercial models. We hope\nCG-Bench can advance the development of more trustworthy and capable MLLMs for\nlong video understanding. All annotations and video data are released at\nhttps://cg-bench.github.io/leaderboard/.\n","authors":["Guo Chen","Yicheng Liu","Yifei Huang","Yuping He","Baoqi Pei","Jilan Xu","Yali Wang","Tong Lu","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2412.12075v1.pdf","comment":"14 pages, 9 figures"},{"id":"http://arxiv.org/abs/2412.12068v1","updated":"2024-12-16T18:42:05Z","published":"2024-12-16T18:42:05Z","title":"SPADE: Spectroscopic Photoacoustic Denoising using an Analytical and\n  Data-free Enhancement Framework","summary":"  Spectroscopic photoacoustic (sPA) imaging uses multiple wavelengths to\ndifferentiate chromophores based on their unique optical absorption spectra.\nThis technique has been widely applied in areas such as vascular mapping, tumor\ndetection, and therapeutic monitoring. However, sPA imaging is highly\nsusceptible to noise, leading to poor signal-to-noise ratio (SNR) and\ncompromised image quality. Traditional denoising techniques like frame\naveraging, though effective in improving SNR, can be impractical for dynamic\nimaging scenarios due to reduced frame rates. Advanced methods, including\nlearning-based approaches and analytical algorithms, have demonstrated promise\nbut often require extensive training data and parameter tuning, limiting their\nadaptability for real-time clinical use. In this work, we propose a sPA\ndenoising using a tuning-free analytical and data-free enhancement (SPADE)\nframework for denoising sPA images. This framework integrates a data-free\nlearning-based method with an efficient BM3D-based analytical approach while\npreserves spectral linearity, providing noise reduction and ensuring that\nfunctional information is maintained. The SPADE framework was validated through\nsimulation, phantom, ex vivo, and in vivo experiments. Results demonstrated\nthat SPADE improved SNR and preserved spectral information, outperforming\nconventional methods, especially in challenging imaging conditions. SPADE\npresents a promising solution for enhancing sPA imaging quality in clinical\napplications where noise reduction and spectral preservation are critical.\n","authors":["Fangzhou Lin","Shang Gao","Yichuan Tang","Xihan Ma","Ryo Murakami","Ziming Zhang","John D. Obayemic","Winston W. Soboyejo","Haichong K. Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.12068v1.pdf","comment":"20 pages, 7 figures"},{"id":"http://arxiv.org/abs/2412.12050v1","updated":"2024-12-16T18:20:06Z","published":"2024-12-16T18:20:06Z","title":"Exploring Semantic Consistency and Style Diversity for Domain\n  Generalized Semantic Segmentation","summary":"  Domain Generalized Semantic Segmentation (DGSS) seeks to utilize source\ndomain data exclusively to enhance the generalization of semantic segmentation\nacross unknown target domains. Prevailing studies predominantly concentrate on\nfeature normalization and domain randomization, these approaches exhibit\nsignificant limitations. Feature normalization-based methods tend to confuse\nsemantic features in the process of constraining the feature space\ndistribution, resulting in classification misjudgment. Domain\nrandomization-based methods frequently incorporate domain-irrelevant noise due\nto the uncontrollability of style transformations, resulting in segmentation\nambiguity. To address these challenges, we introduce a novel framework, named\nSCSD for Semantic Consistency prediction and Style Diversity generalization. It\ncomprises three pivotal components: Firstly, a Semantic Query Booster is\ndesigned to enhance the semantic awareness and discrimination capabilities of\nobject queries in the mask decoder, enabling cross-domain semantic consistency\nprediction. Secondly, we develop a Text-Driven Style Transform module that\nutilizes domain difference text embeddings to controllably guide the style\ntransformation of image features, thereby increasing inter-domain style\ndiversity. Lastly, to prevent the collapse of similar domain feature spaces, we\nintroduce a Style Synergy Optimization mechanism that fortifies the separation\nof inter-domain features and the aggregation of intra-domain features by\nsynergistically weighting style contrastive loss and style aggregation loss.\nExtensive experiments demonstrate that the proposed SCSD significantly\noutperforms existing state-of-theart methods. Notably, SCSD trained on GTAV\nachieved an average of 49.11 mIoU on the four unseen domain datasets,\nsurpassing the previous state-of-the-art method by +4.08 mIoU. Code is\navailable at https://github.com/nhw649/SCSD.\n","authors":["Hongwei Niu","Linhuang Xie","Jianghang Lin","Shengchuan Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.12050v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.12048v1","updated":"2024-12-16T18:18:17Z","published":"2024-12-16T18:18:17Z","title":"A LoRA is Worth a Thousand Pictures","summary":"  Recent advances in diffusion models and parameter-efficient fine-tuning\n(PEFT) have made text-to-image generation and customization widely accessible,\nwith Low Rank Adaptation (LoRA) able to replicate an artist's style or subject\nusing minimal data and computation. In this paper, we examine the relationship\nbetween LoRA weights and artistic styles, demonstrating that LoRA weights alone\ncan serve as an effective descriptor of style, without the need for additional\nimage generation or knowledge of the original training set. Our findings show\nthat LoRA weights yield better performance in clustering of artistic styles\ncompared to traditional pre-trained features, such as CLIP and DINO, with\nstrong structural similarities between LoRA-based and conventional image-based\nembeddings observed both qualitatively and quantitatively. We identify various\nretrieval scenarios for the growing collection of customized models and show\nthat our approach enables more accurate retrieval in real-world settings where\nknowledge of the training images is unavailable and additional generation is\nrequired. We conclude with a discussion on potential future applications, such\nas zero-shot LoRA fine-tuning and model attribution.\n","authors":["Chenxi Liu","Towaki Takikawa","Alec Jacobson"],"pdf_url":"https://arxiv.org/pdf/2412.12048v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08628v2","updated":"2024-12-16T18:16:14Z","published":"2024-12-11T18:48:20Z","title":"EOV-Seg: Efficient Open-Vocabulary Panoptic Segmentation","summary":"  Open-vocabulary panoptic segmentation aims to segment and classify everything\nin diverse scenes across an unbounded vocabulary. Existing methods typically\nemploy two-stage or single-stage framework. The two-stage framework involves\ncropping the image multiple times using masks generated by a mask generator,\nfollowed by feature extraction, while the single-stage framework relies on a\nheavyweight mask decoder to make up for the lack of spatial position\ninformation through self-attention and cross-attention in multiple stacked\nTransformer blocks. Both methods incur substantial computational overhead,\nthereby hindering the efficiency of model inference. To fill the gap in\nefficiency, we propose EOV-Seg, a novel single-stage, shared, efficient, and\nspatialaware framework designed for open-vocabulary panoptic segmentation.\nSpecifically, EOV-Seg innovates in two aspects. First, a Vocabulary-Aware\nSelection (VAS) module is proposed to improve the semantic comprehension of\nvisual aggregated features and alleviate the feature interaction burden on the\nmask decoder. Second, we introduce a Two-way Dynamic Embedding Experts (TDEE),\nwhich efficiently utilizes the spatial awareness capabilities of ViT-based CLIP\nbackbone. To the best of our knowledge, EOV-Seg is the first open-vocabulary\npanoptic segmentation framework towards efficiency, which runs faster and\nachieves competitive performance compared with state-of-the-art methods.\nSpecifically, with COCO training only, EOV-Seg achieves 24.5 PQ, 32.1 mIoU, and\n11.6 FPS on the ADE20K dataset and the inference time of EOV-Seg is 4-19 times\nfaster than state-of-theart methods. Especially, equipped with ResNet50\nbackbone, EOV-Seg runs 23.8 FPS with only 71M parameters on a single RTX 3090\nGPU. Code is available at https://github.com/nhw649/EOV-Seg.\n","authors":["Hongwei Niu","Jie Hu","Jianghang Lin","Guannan Jiang","Shengchuan Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.08628v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.12032v1","updated":"2024-12-16T17:58:45Z","published":"2024-12-16T17:58:45Z","title":"FSFM: A Generalizable Face Security Foundation Model via Self-Supervised\n  Facial Representation Learning","summary":"  This work asks: with abundant, unlabeled real faces, how to learn a robust\nand transferable facial representation that boosts various face security tasks\nwith respect to generalization performance? We make the first attempt and\npropose a self-supervised pretraining framework to learn fundamental\nrepresentations of real face images, FSFM, that leverages the synergy between\nmasked image modeling (MIM) and instance discrimination (ID). We explore\nvarious facial masking strategies for MIM and present a simple yet powerful\nCRFR-P masking, which explicitly forces the model to capture meaningful\nintra-region consistency and challenging inter-region coherency. Furthermore,\nwe devise the ID network that naturally couples with MIM to establish\nunderlying local-to-global correspondence via tailored self-distillation. These\nthree learning objectives, namely 3C, empower encoding both local features and\nglobal semantics of real faces. After pretraining, a vanilla ViT serves as a\nuniversal vision foundation model for downstream face security tasks:\ncross-dataset deepfake detection, cross-domain face anti-spoofing, and unseen\ndiffusion facial forgery detection. Extensive experiments on 10 public datasets\ndemonstrate that our model transfers better than supervised pretraining, visual\nand facial self-supervised learning arts, and even outperforms task-specialized\nSOTA methods.\n","authors":["Gaojian Wang","Feng Lin","Tong Wu","Zhenguang Liu","Zhongjie Ba","Kui Ren"],"pdf_url":"https://arxiv.org/pdf/2412.12032v1.pdf","comment":"21 pages, 11 figures, project page: https://fsfm-3c.github.io"},{"id":"http://arxiv.org/abs/2412.12031v1","updated":"2024-12-16T17:57:33Z","published":"2024-12-16T17:57:33Z","title":"RepFace: Refining Closed-Set Noise with Progressive Label Correction for\n  Face Recognition","summary":"  Face recognition has made remarkable strides, driven by the expanding scale\nof datasets, advancements in various backbone and discriminative losses.\nHowever, face recognition performance is heavily affected by the label noise,\nespecially closed-set noise. While numerous studies have focused on handling\nlabel noise, addressing closed-set noise still poses challenges. This paper\nidentifies this challenge as training isn't robust to noise at the early-stage\ntraining, and necessitating an appropriate learning strategy for samples with\nlow confidence, which are often misclassified as closed-set noise in later\ntraining phases. To address these issues, we propose a new framework to\nstabilize the training at early stages and split the samples into clean,\nambiguous and noisy groups which are devised with separate training strategies.\nInitially, we employ generated auxiliary closed-set noisy samples to enable the\nmodel to identify noisy data at the early stages of training. Subsequently, we\nintroduce how samples are split into clean, ambiguous and noisy groups by their\nsimilarity to the positive and nearest negative centers. Then we perform label\nfusion for ambiguous samples by incorporating accumulated model predictions.\nFinally, we apply label smoothing within the closed set, adjusting the label to\na point between the nearest negative class and the initially assigned label.\nExtensive experiments validate the effectiveness of our method on mainstream\nface datasets, achieving state-of-the-art results. The code will be released\nupon acceptance.\n","authors":["Jie Zhang","Xun Gong","Zhonglin Sun"],"pdf_url":"https://arxiv.org/pdf/2412.12031v1.pdf","comment":"11 pages, 5 figures, AAAI2025"},{"id":"http://arxiv.org/abs/2412.10316v2","updated":"2024-12-16T17:54:44Z","published":"2024-12-13T17:58:06Z","title":"BrushEdit: All-In-One Image Inpainting and Editing","summary":"  Image editing has advanced significantly with the development of diffusion\nmodels using both inversion-based and instruction-based methods. However,\ncurrent inversion-based approaches struggle with big modifications (e.g.,\nadding or removing objects) due to the structured nature of inversion noise,\nwhich hinders substantial changes. Meanwhile, instruction-based methods often\nconstrain users to black-box operations, limiting direct interaction for\nspecifying editing regions and intensity. To address these limitations, we\npropose BrushEdit, a novel inpainting-based instruction-guided image editing\nparadigm, which leverages multimodal large language models (MLLMs) and image\ninpainting models to enable autonomous, user-friendly, and interactive\nfree-form instruction editing. Specifically, we devise a system enabling\nfree-form instruction editing by integrating MLLMs and a dual-branch image\ninpainting model in an agent-cooperative framework to perform editing category\nclassification, main object identification, mask acquisition, and editing area\ninpainting. Extensive experiments show that our framework effectively combines\nMLLMs and inpainting models, achieving superior performance across seven\nmetrics including mask region preservation and editing effect coherence.\n","authors":["Yaowei Li","Yuxuan Bian","Xuan Ju","Zhaoyang Zhang","Ying Shan","Yuexian Zou","Qiang Xu"],"pdf_url":"https://arxiv.org/pdf/2412.10316v2.pdf","comment":"WebPage available at\n  https://liyaowei-stu.github.io/project/BrushEdit/"},{"id":"http://arxiv.org/abs/2409.00397v2","updated":"2024-12-16T17:43:04Z","published":"2024-08-31T09:14:54Z","title":"COSMo: CLIP Talks on Open-Set Multi-Target Domain Adaptation","summary":"  Multi-Target Domain Adaptation (MTDA) entails learning domain-invariant\ninformation from a single source domain and applying it to multiple unlabeled\ntarget domains. Yet, existing MTDA methods predominantly focus on addressing\ndomain shifts within visual features, often overlooking semantic features and\nstruggling to handle unknown classes, resulting in what is known as Open-Set\n(OS) MTDA. While large-scale vision-language foundation models like CLIP show\npromise, their potential for MTDA remains largely unexplored. This paper\nintroduces COSMo, a novel method that learns domain-agnostic prompts through\nsource domain-guided prompt learning to tackle the MTDA problem in the prompt\nspace. By leveraging a domain-specific bias network and separate prompts for\nknown and unknown classes, COSMo effectively adapts across domain and class\nshifts. To the best of our knowledge, COSMo is the first method to address\nOpen-Set Multi-Target DA (OSMTDA), offering a more realistic representation of\nreal-world scenarios and addressing the challenges of both open-set and\nmulti-target DA. COSMo demonstrates an average improvement of $5.1\\%$ across\nthree challenging datasets: Mini-DomainNet, Office-31, and Office-Home,\ncompared to other related DA methods adapted to operate within the OSMTDA\nsetting. Code is available at: https://github.com/munish30monga/COSMo\n","authors":["Munish Monga","Sachin Kumar Giroh","Ankit Jha","Mainak Singha","Biplab Banerjee","Jocelyn Chanussot"],"pdf_url":"https://arxiv.org/pdf/2409.00397v2.pdf","comment":"Accepted in BMVC 2024"},{"id":"http://arxiv.org/abs/2412.12001v1","updated":"2024-12-16T17:29:51Z","published":"2024-12-16T17:29:51Z","title":"LLM-RG4: Flexible and Factual Radiology Report Generation across Diverse\n  Input Contexts","summary":"  Drafting radiology reports is a complex task requiring flexibility, where\nradiologists tail content to available information and particular clinical\ndemands. However, most current radiology report generation (RRG) models are\nconstrained to a fixed task paradigm, such as predicting the full ``finding''\nsection from a single image, inherently involving a mismatch between inputs and\noutputs. The trained models lack the flexibility for diverse inputs and could\ngenerate harmful, input-agnostic hallucinations. To bridge the gap between\ncurrent RRG models and the clinical demands in practice, we first develop a\ndata generation pipeline to create a new MIMIC-RG4 dataset, which considers\nfour common radiology report drafting scenarios and has perfectly corresponded\ninput and output. Secondly, we propose a novel large language model (LLM) based\nRRG framework, namely LLM-RG4, which utilizes LLM's flexible\ninstruction-following capabilities and extensive general knowledge. We further\ndevelop an adaptive token fusion module that offers flexibility to handle\ndiverse scenarios with different input combinations, while minimizing the\nadditional computational burden associated with increased input volumes.\nBesides, we propose a token-level loss weighting strategy to direct the model's\nattention towards positive and uncertain descriptions. Experimental results\ndemonstrate that LLM-RG4 achieves state-of-the-art performance in both clinical\nefficiency and natural language generation on the MIMIC-RG4 and MIMIC-CXR\ndatasets. We quantitatively demonstrate that our model has minimal\ninput-agnostic hallucinations, whereas current open-source models commonly\nsuffer from this problem.\n","authors":["Zhuhao Wang","Yihua Sun","Zihan Li","Xuan Yang","Fang Chen","Hongen Liao"],"pdf_url":"https://arxiv.org/pdf/2412.12001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11998v1","updated":"2024-12-16T17:26:06Z","published":"2024-12-16T17:26:06Z","title":"SAMIC: Segment Anything with In-Context Spatial Prompt Engineering","summary":"  Few-shot segmentation is the problem of learning to identify specific types\nof objects (e.g., airplanes) in images from a small set of labeled reference\nimages. The current state of the art is driven by resource-intensive\nconstruction of models for every new domain-specific application. Such models\nmust be trained on enormous labeled datasets of unrelated objects (e.g., cars,\ntrains, animals) so that their ``knowledge'' can be transferred to new types of\nobjects. In this paper, we show how to leverage existing vision foundation\nmodels (VFMs) to reduce the incremental cost of creating few-shot segmentation\nmodels for new domains. Specifically, we introduce SAMIC, a small network that\nlearns how to prompt VFMs in order to segment new types of objects in\ndomain-specific applications. SAMIC enables any task to be approached as a\nfew-shot learning problem. At 2.6 million parameters, it is 94% smaller than\nthe leading models (e.g., having ResNet 101 backbone with 45+ million\nparameters). Even using 1/5th of the training data provided by one-shot\nbenchmarks, SAMIC is competitive with, or sets the state of the art, on a\nvariety of few-shot and semantic segmentation datasets including COCO-$20^i$,\nPascal-$5^i$, PerSeg, FSS-1000, and NWPU VHR-10.\n","authors":["Savinay Nagendra","Kashif Rashid","Chaopeng Shen","Daniel Kifer"],"pdf_url":"https://arxiv.org/pdf/2412.11998v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16726v2","updated":"2024-12-16T17:11:49Z","published":"2024-11-23T04:38:51Z","title":"EmotiveTalk: Expressive Talking Head Generation through Audio\n  Information Decoupling and Emotional Video Diffusion","summary":"  Diffusion models have revolutionized the field of talking head generation,\nyet still face challenges in expressiveness, controllability, and stability in\nlong-time generation. In this research, we propose an EmotiveTalk framework to\naddress these issues. Firstly, to realize better control over the generation of\nlip movement and facial expression, a Vision-guided Audio Information\nDecoupling (V-AID) approach is designed to generate audio-based decoupled\nrepresentations aligned with lip movements and expression. Specifically, to\nachieve alignment between audio and facial expression representation spaces, we\npresent a Diffusion-based Co-speech Temporal Expansion (Di-CTE) module within\nV-AID to generate expression-related representations under multi-source emotion\ncondition constraints. Then we propose a well-designed Emotional Talking Head\nDiffusion (ETHD) backbone to efficiently generate highly expressive talking\nhead videos, which contains an Expression Decoupling Injection (EDI) module to\nautomatically decouple the expressions from reference portraits while\nintegrating the target expression information, achieving more expressive\ngeneration performance. Experimental results show that EmotiveTalk can generate\nexpressive talking head videos, ensuring the promised controllability of\nemotions and stability during long-time generation, yielding state-of-the-art\nperformance compared to existing methods.\n","authors":["Haotian Wang","Yuzhe Weng","Yueyan Li","Zilu Guo","Jun Du","Shutong Niu","Jiefeng Ma","Shan He","Xiaoyan Wu","Qiming Hu","Bing Yin","Cong Liu","Qingfeng Liu"],"pdf_url":"https://arxiv.org/pdf/2411.16726v2.pdf","comment":"https://emotivetalk.github.io/"},{"id":"http://arxiv.org/abs/2412.11974v1","updated":"2024-12-16T16:58:28Z","published":"2024-12-16T16:58:28Z","title":"Emma-X: An Embodied Multimodal Action Model with Grounded Chain of\n  Thought and Look-ahead Spatial Reasoning","summary":"  Traditional reinforcement learning-based robotic control methods are often\ntask-specific and fail to generalize across diverse environments or unseen\nobjects and instructions. Visual Language Models (VLMs) demonstrate strong\nscene understanding and planning capabilities but lack the ability to generate\nactionable policies tailored to specific robotic embodiments. To address this,\nVisual-Language-Action (VLA) models have emerged, yet they face challenges in\nlong-horizon spatial reasoning and grounded task planning. In this work, we\npropose the Embodied Multimodal Action Model with Grounded Chain of Thought and\nLook-ahead Spatial Reasoning, Emma-X. Emma-X leverages our constructed\nhierarchical embodiment dataset based on BridgeV2, containing 60,000 robot\nmanipulation trajectories auto-annotated with grounded task reasoning and\nspatial guidance. Additionally, we introduce a trajectory segmentation strategy\nbased on gripper states and motion trajectories, which can help mitigate\nhallucination in grounding subtask reasoning generation. Experimental results\ndemonstrate that Emma-X achieves superior performance over competitive\nbaselines, particularly in real-world robotic tasks requiring spatial\nreasoning.\n","authors":["Qi Sun","Pengfei Hong","Tej Deep Pala","Vernon Toh","U-Xuan Tan","Deepanway Ghosal","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2412.11974v1.pdf","comment":"https://github.com/declare-lab/Emma-X,\n  https://huggingface.co/declare-lab/Emma-X"},{"id":"http://arxiv.org/abs/2412.11972v1","updated":"2024-12-16T16:55:22Z","published":"2024-12-16T16:55:22Z","title":"Controllable Shadow Generation with Single-Step Diffusion Models from\n  Synthetic Data","summary":"  Realistic shadow generation is a critical component for high-quality image\ncompositing and visual effects, yet existing methods suffer from certain\nlimitations: Physics-based approaches require a 3D scene geometry, which is\noften unavailable, while learning-based techniques struggle with control and\nvisual artifacts. We introduce a novel method for fast, controllable, and\nbackground-free shadow generation for 2D object images. We create a large\nsynthetic dataset using a 3D rendering engine to train a diffusion model for\ncontrollable shadow generation, generating shadow maps for diverse light source\nparameters. Through extensive ablation studies, we find that rectified flow\nobjective achieves high-quality results with just a single sampling step\nenabling real-time applications. Furthermore, our experiments demonstrate that\nthe model generalizes well to real-world images. To facilitate further research\nin evaluating quality and controllability in shadow generation, we release a\nnew public benchmark containing a diverse set of object images and shadow maps\nin various settings. The project page is available at\nhttps://gojasper.github.io/controllable-shadow-generation-project/\n","authors":["Onur Tasar","Clément Chadebec","Benjamin Aubin"],"pdf_url":"https://arxiv.org/pdf/2412.11972v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16258v2","updated":"2024-12-16T16:46:03Z","published":"2024-08-29T04:40:31Z","title":"GSDiff: Synthesizing Vector Floorplans via Geometry-enhanced Structural\n  Graph Generation","summary":"  Automating architectural floorplan design is vital for housing and interior\ndesign, offering a faster, cost-effective alternative to manual sketches by\narchitects. However, existing methods, including rule-based and learning-based\napproaches, face challenges in design complexity and constrained generation\nwith extensive post-processing, and tend to obvious geometric inconsistencies\nsuch as misalignment, overlap, and gaps. In this work, we propose a novel\ngenerative framework for vector floorplan design via structural graph\ngeneration, called GSDiff, focusing on wall junction generation and wall\nsegment prediction to capture both geometric and semantic aspects of structural\ngraphs. To improve the geometric rationality of generated structural graphs, we\npropose two innovative geometry enhancement methods. In wall junction\ngeneration, we propose a novel alignment loss function to improve geometric\nconsistency. In wall segment prediction, we propose a random self-supervision\nmethod to enhance the model's perception of the overall geometric structure,\nthereby promoting the generation of reasonable geometric structures. Employing\nthe diffusion model and the Transformer model, as well as the geometry\nenhancement strategies, our framework can generate wall junctions, wall\nsegments and room polygons with structural and semantic information, resulting\nin structural graphs that accurately represent floorplans. Extensive\nexperiments show that the proposed method surpasses existing techniques,\nenabling free generation and constrained generation, marking a shift towards\nstructure generation in architectural design. Code and data are available at\nhttps://github.com/SizheHu/GSDiff.\n","authors":["Sizhe Hu","Wenming Wu","Yuntao Wang","Benzhu Xu","Liping Zheng"],"pdf_url":"https://arxiv.org/pdf/2408.16258v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11959v1","updated":"2024-12-16T16:41:51Z","published":"2024-12-16T16:41:51Z","title":"Gramian Multimodal Representation Learning and Alignment","summary":"  Human perception integrates multiple modalities, such as vision, hearing, and\nlanguage, into a unified understanding of the surrounding reality. While recent\nmultimodal models have achieved significant progress by aligning pairs of\nmodalities via contrastive learning, their solutions are unsuitable when\nscaling to multiple modalities. These models typically align each modality to a\ndesignated anchor without ensuring the alignment of all modalities with each\nother, leading to suboptimal performance in tasks requiring a joint\nunderstanding of multiple modalities. In this paper, we structurally rethink\nthe pairwise conventional approach to multimodal learning and we present the\nnovel Gramian Representation Alignment Measure (GRAM), which overcomes the\nabove-mentioned limitations. GRAM learns and then aligns $n$ modalities\ndirectly in the higher-dimensional space in which modality embeddings lie by\nminimizing the Gramian volume of the $k$-dimensional parallelotope spanned by\nthe modality vectors, ensuring the geometric alignment of all modalities\nsimultaneously. GRAM can replace cosine similarity in any downstream method,\nholding for 2 to $n$ modality and providing more meaningful alignment with\nrespect to previous similarity measures. The novel GRAM-based contrastive loss\nfunction enhances the alignment of multimodal models in the higher-dimensional\nembedding space, leading to new state-of-the-art performance in downstream\ntasks such as video-audio-text retrieval and audio-video classification. The\nproject page, the code, and the pretrained models are available at\nhttps://ispamm.github.io/GRAM/.\n","authors":["Giordano Cicchetti","Eleonora Grassucci","Luigi Sigillo","Danilo Comminiello"],"pdf_url":"https://arxiv.org/pdf/2412.11959v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11953v1","updated":"2024-12-16T16:37:03Z","published":"2024-12-16T16:37:03Z","title":"Reliable Breast Cancer Molecular Subtype Prediction based on\n  uncertainty-aware Bayesian Deep Learning by Mammography","summary":"  Breast cancer is a heterogeneous disease with different molecular subtypes,\nclinical behavior, treatment responses as well as survival outcomes. The\ndevelopment of a reliable, accurate, available and inexpensive method to\npredict the molecular subtypes using medical images plays an important role in\nthe diagnosis and prognosis of breast cancer. Recently, deep learning methods\nhave shown good performance in the breast cancer classification tasks using\nvarious medical images. Despite all that success, classical deep learning\ncannot deliver the predictive uncertainty. The uncertainty represents the\nvalidity of the predictions.Therefore, the high predicted uncertainty might\ncause a negative effect in the accurate diagnosis of breast cancer molecular\nsubtypes. To overcome this, uncertainty quantification methods are used to\ndetermine the predictive uncertainty. Accordingly, in this study, we proposed\nan uncertainty-aware Bayesian deep learning model using the full mammogram\nimages. In addition, to increase the performance of the multi-class molecular\nsubtype classification task, we proposed a novel hierarchical classification\nstrategy, named the two-stage classification strategy. The separate AUC of the\nproposed model for each subtype was 0.71, 0.75 and 0.86 for HER2-enriched,\nluminal and triple-negative classes, respectively. The proposed model not only\nhas a comparable performance to other studies in the field of breast cancer\nmolecular subtypes prediction, even using full mammography images, but it is\nalso more reliable, due to quantify the predictive uncertainty.\n","authors":["Mohaddeseh Chegini","Ali Mahloojifar"],"pdf_url":"https://arxiv.org/pdf/2412.11953v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11952v1","updated":"2024-12-16T16:35:35Z","published":"2024-12-16T16:35:35Z","title":"Advancing Comprehensive Aesthetic Insight with Multi-Scale Text-Guided\n  Self-Supervised Learning","summary":"  Image Aesthetic Assessment (IAA) is a vital and intricate task that entails\nanalyzing and assessing an image's aesthetic values, and identifying its\nhighlights and areas for improvement. Traditional methods of IAA often\nconcentrate on a single aesthetic task and suffer from inadequate labeled\ndatasets, thus impairing in-depth aesthetic comprehension. Despite efforts to\novercome this challenge through the application of Multi-modal Large Language\nModels (MLLMs), such models remain underdeveloped for IAA purposes. To address\nthis, we propose a comprehensive aesthetic MLLM capable of nuanced aesthetic\ninsight. Central to our approach is an innovative multi-scale text-guided\nself-supervised learning technique. This technique features a multi-scale\nfeature alignment module and capitalizes on a wealth of unlabeled data in a\nself-supervised manner to structurally and functionally enhance aesthetic\nability. The empirical evidence indicates that accompanied with extensive\ninstruct-tuning, our model sets new state-of-the-art benchmarks across multiple\ntasks, including aesthetic scoring, aesthetic commenting, and personalized\nimage aesthetic assessment. Remarkably, it also demonstrates zero-shot learning\ncapabilities in the emerging task of aesthetic suggesting. Furthermore, for\npersonalized image aesthetic assessment, we harness the potential of in-context\nlearning and showcase its inherent advantages.\n","authors":["Yuti Liu","Shice Liu","Junyuan Gao","Pengtao Jiang","Hao Zhang","Jinwei Chen","Bo Li"],"pdf_url":"https://arxiv.org/pdf/2412.11952v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.11949v1","updated":"2024-12-16T16:33:28Z","published":"2024-12-16T16:33:28Z","title":"Coconut Palm Tree Counting on Drone Images with Deep Object Detection\n  and Synthetic Training Data","summary":"  Drones have revolutionized various domains, including agriculture. Recent\nadvances in deep learning have propelled among other things object detection in\ncomputer vision. This study utilized YOLO, a real-time object detector, to\nidentify and count coconut palm trees in Ghanaian farm drone footage. The farm\npresented has lost track of its trees due to different planting phases. While\nmanual counting would be very tedious and error-prone, accurately determining\nthe number of trees is crucial for efficient planning and management of\nagricultural processes, especially for optimizing yields and predicting\nproduction. We assessed YOLO for palm detection within a semi-automated\nframework, evaluated accuracy augmentations, and pondered its potential for\nfarmers. Data was captured in September 2022 via drones. To optimize YOLO with\nscarce data, synthetic images were created for model training and validation.\nThe YOLOv7 model, pretrained on the COCO dataset (excluding coconut palms), was\nadapted using tailored data. Trees from footage were repositioned on synthetic\nimages, with testing on distinct authentic images. In our experiments, we\nadjusted hyperparameters, improving YOLO's mean average precision (mAP). We\nalso tested various altitudes to determine the best drone height. From an\ninitial mAP@.5 of $0.65$, we achieved 0.88, highlighting the value of synthetic\nimages in agricultural scenarios.\n","authors":["Tobias Rohe","Barbara Böhm","Michael Kölle","Jonas Stein","Robert Müller","Claudia Linnhoff-Popien"],"pdf_url":"https://arxiv.org/pdf/2412.11949v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2412.11938v1","updated":"2024-12-16T16:23:05Z","published":"2024-12-16T16:23:05Z","title":"Are the Latent Representations of Foundation Models for Pathology\n  Invariant to Rotation?","summary":"  Self-supervised foundation models for digital pathology encode small patches\nfrom H\\&E whole slide images into latent representations used for downstream\ntasks. However, the invariance of these representations to patch rotation\nremains unexplored. This study investigates the rotational invariance of latent\nrepresentations across twelve foundation models by quantifying the alignment\nbetween non-rotated and rotated patches using mutual $k$-nearest neighbours and\ncosine distance. Models that incorporated rotation augmentation during\nself-supervised training exhibited significantly greater invariance to\nrotations. We hypothesise that the absence of rotational inductive bias in the\ntransformer architecture necessitates rotation augmentation during training to\nachieve learned invariance. Code:\nhttps://github.com/MatousE/rot-invariance-analysis.\n","authors":["Matouš Elphick","Samra Turajlic","Guang Yang"],"pdf_url":"https://arxiv.org/pdf/2412.11938v1.pdf","comment":"Samra Turajlic and Guang Yang are joint last authors"},{"id":"http://arxiv.org/abs/2402.12121v2","updated":"2024-12-16T16:09:47Z","published":"2024-02-19T13:16:10Z","title":"IRR: Image Review Ranking Framework for Evaluating Vision-Language\n  Models","summary":"  Large-scale Vision-Language Models (LVLMs) process both images and text,\nexcelling in multimodal tasks such as image captioning and description\ngeneration. However, while these models excel at generating factual content,\ntheir ability to generate and evaluate texts reflecting perspectives on the\nsame image, depending on the context, has not been sufficiently explored. To\naddress this, we propose IRR: Image Review Rank, a novel evaluation framework\ndesigned to assess critic review texts from multiple perspectives. IRR\nevaluates LVLMs by measuring how closely their judgments align with human\ninterpretations. We validate it using a dataset of images from 15 categories,\neach with five critic review texts and annotated rankings in both English and\nJapanese, totaling over 2,000 data instances. The datasets are available at\nhttps://hf.co/datasets/naist-nlp/Wiki-ImageReview1.0. Our results indicate\nthat, although LVLMs exhibited consistent performance across languages, their\ncorrelation with human annotations was insufficient, highlighting the need for\nfurther advancements. These findings highlight the limitations of current\nevaluation methods and the need for approaches that better capture human\nreasoning in Vision & Language tasks.\n","authors":["Kazuki Hayashi","Kazuma Onishi","Toma Suzuki","Yusuke Ide","Seiji Gobara","Shigeki Saito","Yusuke Sakai","Hidetaka Kamigaito","Katsuhiko Hayashi","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2402.12121v2.pdf","comment":"18pages, Accepted at COLING25"},{"id":"http://arxiv.org/abs/2412.11917v1","updated":"2024-12-16T16:01:18Z","published":"2024-12-16T16:01:18Z","title":"Does VLM Classification Benefit from LLM Description Semantics?","summary":"  Accurately describing images via text is a foundation of explainable AI.\nVision-Language Models (VLMs) like CLIP have recently addressed this by\naligning images and texts in a shared embedding space, expressing semantic\nsimilarities between vision and language embeddings. VLM classification can be\nimproved with descriptions generated by Large Language Models (LLMs). However,\nit is difficult to determine the contribution of actual description semantics,\nas the performance gain may also stem from a semantic-agnostic ensembling\neffect. Considering this, we ask how to distinguish the actual discriminative\npower of descriptions from performance boosts that potentially rely on an\nensembling effect. To study this, we propose an alternative evaluation scenario\nthat shows a characteristic behavior if the used descriptions have\ndiscriminative power. Furthermore, we propose a training-free method to select\ndiscriminative descriptions that work independently of classname ensembling\neffects. The training-free method works in the following way: A test image has\na local CLIP label neighborhood, i.e., its top-$k$ label predictions. Then,\nw.r.t. to a small selection set, we extract descriptions that distinguish each\nclass well in the local neighborhood. Using the selected descriptions, we\ndemonstrate improved classification accuracy across seven datasets and provide\nin-depth analysis and insights into the explainability of description-based\nimage classification by VLMs.\n","authors":["Pingchuan Ma","Lennart Rietdorf","Dmytro Kotovenko","Vincent Tao Hu","Björn Ommer"],"pdf_url":"https://arxiv.org/pdf/2412.11917v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11906v1","updated":"2024-12-16T15:52:59Z","published":"2024-12-16T15:52:59Z","title":"PunchBench: Benchmarking MLLMs in Multimodal Punchline Comprehension","summary":"  Multimodal punchlines, which involve humor or sarcasm conveyed in\nimage-caption pairs, are a popular way of communication on online multimedia\nplatforms. With the rapid development of multimodal large language models\n(MLLMs), it is essential to assess their ability to effectively comprehend\nthese punchlines. However, existing benchmarks on punchline comprehension\nsuffer from three major limitations: 1) language shortcuts that allow models to\nsolely rely on text, 2) lack of question diversity, and 3) narrow focus on a\nspecific domain of multimodal content (e.g., cartoon). To address these\nlimitations, we introduce a multimodal \\textbf{Punch}line comprehension\n\\textbf{Bench}mark, named \\textbf{PunchBench}, which is tailored for accurate\nand comprehensive evaluation of punchline comprehension. To enhance the\nevaluation accuracy, we generate synonymous and antonymous captions by\nmodifying original captions, which mitigates the impact of shortcuts in the\ncaptions. To provide a comprehensive evaluation, PunchBench incorporates\ndiverse question formats and image-captions from various domains. On this\nbasis, we conduct extensive evaluations and reveal a significant gap between\nstate-of-the-art MLLMs and humans in punchline comprehension. To improve\npunchline comprehension, we propose Simple-to-Complex Chain-of-Question\n(SC-CoQ) strategy, enabling the models to incrementally address complicated\nquestions by first mastering simple ones. SC-CoQ effectively enhances the\nperformance of various MLLMs on PunchBench, surpassing in-context learning and\nchain-of-thought.\n","authors":["Kun Ouyang","Yuanxin Liu","Shicheng Li","Yi Liu","Hao Zhou","Fandong Meng","Jie Zhou","Xu Sun"],"pdf_url":"https://arxiv.org/pdf/2412.11906v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02329v3","updated":"2024-12-16T15:42:53Z","published":"2024-01-04T16:06:31Z","title":"Exploring Vacant Classes in Label-Skewed Federated Learning","summary":"  Label skews, characterized by disparities in local label distribution across\nclients, pose a significant challenge in federated learning. As minority\nclasses suffer from worse accuracy due to overfitting on local imbalanced data,\nprior methods often incorporate class-balanced learning techniques during local\ntraining. Although these methods improve the mean accuracy across all classes,\nwe observe that vacant classes-referring to categories absent from a client's\ndata distribution-remain poorly recognized. Besides, there is still a gap in\nthe accuracy of local models on minority classes compared to the global model.\nThis paper introduces FedVLS, a novel approach to label-skewed federated\nlearning that integrates both vacant-class distillation and logit suppression\nsimultaneously. Specifically, vacant-class distillation leverages knowledge\ndistillation during local training on each client to retain essential\ninformation related to vacant classes from the global model. Moreover, logit\nsuppression directly penalizes network logits for non-label classes,\neffectively addressing misclassifications in minority classes that may be\nbiased toward majority classes. Extensive experiments validate the efficacy of\nFedVLS, demonstrating superior performance compared to previous\nstate-of-the-art (SOTA) methods across diverse datasets with varying degrees of\nlabel skews. Our code is available at https://github.com/krumpguo/FedVLS.\n","authors":["Kuangpu Guo","Yuhe Ding","Jian Liang","Ran He","Zilei Wang","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2401.02329v3.pdf","comment":"AAAI2025"},{"id":"http://arxiv.org/abs/2412.11892v1","updated":"2024-12-16T15:41:14Z","published":"2024-12-16T15:41:14Z","title":"From 2D CAD Drawings to 3D Parametric Models: A Vision-Language Approach","summary":"  In this paper, we present CAD2Program, a new method for reconstructing 3D\nparametric models from 2D CAD drawings. Our proposed method is inspired by\nrecent successes in vision-language models (VLMs), and departs from traditional\nmethods which rely on task-specific data representations and/or algorithms.\nSpecifically, on the input side, we simply treat the 2D CAD drawing as a raster\nimage, regardless of its original format, and encode the image with a standard\nViT model. We show that such an encoding scheme achieves competitive\nperformance against existing methods that operate on vector-graphics inputs,\nwhile imposing substantially fewer restrictions on the 2D drawings. On the\noutput side, our method auto-regressively predicts a general-purpose language\ndescribing 3D parametric models in text form. Compared to other sequence\nmodeling methods for CAD which use domain-specific sequence representations\nwith fixed-size slots, our text-based representation is more flexible, and can\nbe easily extended to arbitrary geometric entities and semantic or functional\nproperties. Experimental results on a large-scale dataset of cabinet models\ndemonstrate the effectiveness of our method.\n","authors":["Xilin Wang","Jia Zheng","Yuanchao Hu","Hao Zhu","Qian Yu","Zihan Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.11892v1.pdf","comment":"To Appear in AAAI 2025. The project page is at\n  https://manycore-research.github.io/CAD2Program"},{"id":"http://arxiv.org/abs/2412.11890v1","updated":"2024-12-16T15:38:25Z","published":"2024-12-16T15:38:25Z","title":"SegMAN: Omni-scale Context Modeling with State Space Models and Local\n  Attention for Semantic Segmentation","summary":"  High-quality semantic segmentation relies on three key capabilities: global\ncontext modeling, local detail encoding, and multi-scale feature extraction.\nHowever, recent methods struggle to possess all these capabilities\nsimultaneously. Hence, we aim to empower segmentation networks to\nsimultaneously carry out efficient global context modeling, high-quality local\ndetail encoding, and rich multi-scale feature representation for varying input\nresolutions. In this paper, we introduce SegMAN, a novel linear-time model\ncomprising a hybrid feature encoder dubbed SegMAN Encoder, and a decoder based\non state space models. Specifically, the SegMAN Encoder synergistically\nintegrates sliding local attention with dynamic state space models, enabling\nhighly efficient global context modeling while preserving fine-grained local\ndetails. Meanwhile, the MMSCopE module in our decoder enhances multi-scale\ncontext feature extraction and adaptively scales with the input resolution. We\ncomprehensively evaluate SegMAN on three challenging datasets: ADE20K,\nCityscapes, and COCO-Stuff. For instance, SegMAN-B achieves 52.6% mIoU on\nADE20K, outperforming SegNeXt-L by 1.6% mIoU while reducing computational\ncomplexity by over 15% GFLOPs. On Cityscapes, SegMAN-B attains 83.8% mIoU,\nsurpassing SegFormer-B3 by 2.1% mIoU with approximately half the GFLOPs.\nSimilarly, SegMAN-B improves upon VWFormer-B3 by 1.6% mIoU with lower GFLOPs on\nthe COCO-Stuff dataset. Our code is available at\nhttps://github.com/yunxiangfu2001/SegMAN.\n","authors":["Yunxiang Fu","Meng Lou","Yizhou Yu"],"pdf_url":"https://arxiv.org/pdf/2412.11890v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11883v1","updated":"2024-12-16T15:32:05Z","published":"2024-12-16T15:32:05Z","title":"Towards Physically-Based Sky-Modeling","summary":"  Accurate environment maps are a key component in rendering photorealistic\noutdoor scenes with coherent illumination. They enable captivating visual arts,\nimmersive virtual reality and a wide range of engineering and scientific\napplications. Recent works have extended sky-models to be more comprehensive\nand inclusive of cloud formations but existing approaches fall short in\nfaithfully recreating key-characteristics in physically captured HDRI. As we\ndemonstrate, environment maps produced by sky-models do not relight scenes with\nthe same tones, shadows, and illumination coherence as physically captured HDR\nimagery. Though the visual quality of DNN-generated LDR and HDR imagery has\ngreatly progressed in recent years, we demonstrate this progress to be\ntangential to sky-modelling. Due to the Extended Dynamic Range (EDR) of 14EV\nrequired for outdoor environment maps inclusive of the sun, sky-modelling\nextends beyond the conventional paradigm of High Dynamic Range Imagery (HDRI).\nIn this work, we propose an all-weather sky-model, learning weathered-skies\ndirectly from physically captured HDR imagery. Per user-controlled positioning\nof the sun and cloud formations, our model (AllSky) allows for emulation of\nphysically captured environment maps with improved retention of the Extended\nDynamic Range (EDR) of the sky.\n","authors":["Ian J. Maquignaz"],"pdf_url":"https://arxiv.org/pdf/2412.11883v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11866v1","updated":"2024-12-16T15:20:54Z","published":"2024-12-16T15:20:54Z","title":"Event-based Motion Deblurring via Multi-Temporal Granularity Fusion","summary":"  Conventional frame-based cameras inevitably produce blurry effects due to\nmotion occurring during the exposure time. Event camera, a bio-inspired sensor\noffering continuous visual information could enhance the deblurring\nperformance. Effectively utilizing the high-temporal-resolution event data is\ncrucial for extracting precise motion information and enhancing deblurring\nperformance. However, existing event-based image deblurring methods usually\nutilize voxel-based event representations, losing the fine-grained temporal\ndetails that are mathematically essential for fast motion deblurring. In this\npaper, we first introduce point cloud-based event representation into the image\ndeblurring task and propose a Multi-Temporal Granularity Network (MTGNet). It\ncombines the spatially dense but temporally coarse-grained voxel-based event\nrepresentation and the temporally fine-grained but spatially sparse point\ncloud-based event. To seamlessly integrate such complementary representations,\nwe design a Fine-grained Point Branch. An Aggregation and Mapping Module (AMM)\nis proposed to align the low-level point-based features with frame-based\nfeatures and an Adaptive Feature Diffusion Module (AFDM) is designed to manage\nthe resolution discrepancies between event data and image data by enriching the\nsparse point feature. Extensive subjective and objective evaluations\ndemonstrate that our method outperforms current state-of-the-art approaches on\nboth synthetic and real-world datasets.\n","authors":["Xiaopeng Lin","Hongwei Ren","Yulong Huang","Zunchang Liu","Yue Zhou","Haotian Fu","Biao Pan","Bojun Cheng"],"pdf_url":"https://arxiv.org/pdf/2412.11866v1.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2412.11863v1","updated":"2024-12-16T15:20:03Z","published":"2024-12-16T15:20:03Z","title":"GeoX: Geometric Problem Solving Through Unified Formalized\n  Vision-Language Pre-training","summary":"  Despite their proficiency in general tasks, Multi-modal Large Language Models\n(MLLMs) struggle with automatic Geometry Problem Solving (GPS), which demands\nunderstanding diagrams, interpreting symbols, and performing complex reasoning.\nThis limitation arises from their pre-training on natural images and texts,\nalong with the lack of automated verification in the problem-solving process.\nBesides, current geometric specialists are limited by their task-specific\ndesigns, making them less effective for broader geometric problems. To this\nend, we present GeoX, a multi-modal large model focusing on geometric\nunderstanding and reasoning tasks. Given the significant differences between\ngeometric diagram-symbol and natural image-text, we introduce unimodal\npre-training to develop a diagram encoder and symbol decoder, enhancing the\nunderstanding of geometric images and corpora. Furthermore, we introduce\ngeometry-language alignment, an effective pre-training paradigm that bridges\nthe modality gap between unimodal geometric experts. We propose a\nGenerator-And-Sampler Transformer (GS-Former) to generate discriminative\nqueries and eliminate uninformative representations from unevenly distributed\ngeometric signals. Finally, GeoX benefits from visual instruction tuning,\nempowering it to take geometric images and questions as input and generate\nverifiable solutions. Experiments show that GeoX outperforms both generalists\nand geometric specialists on publicly recognized benchmarks, such as GeoQA,\nUniGeo, Geometry3K, and PGPS9k.\n","authors":["Renqiu Xia","Mingsheng Li","Hancheng Ye","Wenjie Wu","Hongbin Zhou","Jiakang Yuan","Tianshuo Peng","Xinyu Cai","Xiangchao Yan","Bin Wang","Conghui He","Botian Shi","Tao Chen","Junchi Yan","Bo Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.11863v1.pdf","comment":"Our code is available at https://github.com/UniModal4Reasoning/GeoX"},{"id":"http://arxiv.org/abs/2409.12784v5","updated":"2024-12-16T15:13:24Z","published":"2024-09-19T13:51:21Z","title":"Evaluating Image Hallucination in Text-to-Image Generation with\n  Question-Answering","summary":"  Despite the impressive success of text-to-image (TTI) generation models,\nexisting studies overlook the issue of whether these models accurately convey\nfactual information. In this paper, we focus on the problem of image\nhallucination, where images created by generation models fail to faithfully\ndepict factual content. To address this, we introduce I-HallA (Image\nHallucination evaluation with Question Answering), a novel automated evaluation\nmetric that measures the factuality of generated images through visual question\nanswering (VQA). We also introduce I-HallA v1.0, a curated benchmark dataset\nfor this purpose. As part of this process, we develop a pipeline that generates\nhigh-quality question-answer pairs using multiple GPT-4 Omni-based agents, with\nhuman judgments to ensure accuracy. Our evaluation protocols measure image\nhallucination by testing if images from existing text-to-image models can\ncorrectly respond to these questions. The I-HallA v1.0 dataset comprises 1.2K\ndiverse image-text pairs across nine categories with 1,000 rigorously curated\nquestions covering various compositional challenges. We evaluate five\ntext-to-image models using I-HallA and reveal that these state-of-the-art\nmodels often fail to accurately convey factual information. Moreover, we\nvalidate the reliability of our metric by demonstrating a strong Spearman\ncorrelation (rho=0.95) with human judgments. We believe our benchmark dataset\nand metric can serve as a foundation for developing factually accurate\ntext-to-image generation models.\n","authors":["Youngsun Lim","Hojun Choi","Hyunjung Shim"],"pdf_url":"https://arxiv.org/pdf/2409.12784v5.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2412.11849v1","updated":"2024-12-16T15:10:53Z","published":"2024-12-16T15:10:53Z","title":"Ensemble Learning and 3D Pix2Pix for Comprehensive Brain Tumor Analysis\n  in Multimodal MRI","summary":"  Motivated by the need for advanced solutions in the segmentation and\ninpainting of glioma-affected brain regions in multi-modal magnetic resonance\nimaging (MRI), this study presents an integrated approach leveraging the\nstrengths of ensemble learning with hybrid transformer models and convolutional\nneural networks (CNNs), alongside the innovative application of 3D Pix2Pix\nGenerative Adversarial Network (GAN). Our methodology combines robust tumor\nsegmentation capabilities, utilizing axial attention and transformer encoders\nfor enhanced spatial relationship modeling, with the ability to synthesize\nbiologically plausible brain tissue through 3D Pix2Pix GAN. This integrated\napproach addresses the BraTS 2023 cluster challenges by offering precise\nsegmentation and realistic inpainting, tailored for diverse tumor types and\nsub-regions. The results demonstrate outstanding performance, evidenced by\nquantitative evaluations such as the Dice Similarity Coefficient (DSC),\nHausdorff Distance (HD95) for segmentation, and Structural Similarity Index\nMeasure (SSIM), Peak Signal-to-Noise Ratio (PSNR), and Mean-Square Error (MSE)\nfor inpainting. Qualitative assessments further validate the high-quality,\nclinically relevant outputs. In conclusion, this study underscores the\npotential of combining advanced machine learning techniques for comprehensive\nbrain tumor analysis, promising significant advancements in clinical\ndecision-making and patient care within the realm of medical imaging.\n","authors":["Ramy A. Zeineldin","Franziska Mathis-Ullrich"],"pdf_url":"https://arxiv.org/pdf/2412.11849v1.pdf","comment":"Accepted at the MICCAI BraTS Challenge 2023"},{"id":"http://arxiv.org/abs/2412.11840v1","updated":"2024-12-16T15:03:08Z","published":"2024-12-16T15:03:08Z","title":"Sonar-based Deep Learning in Underwater Robotics: Overview, Robustness\n  and Challenges","summary":"  With the growing interest in underwater exploration and monitoring,\nAutonomous Underwater Vehicles (AUVs) have become essential. The recent\ninterest in onboard Deep Learning (DL) has advanced real-time environmental\ninteraction capabilities relying on efficient and accurate vision-based DL\nmodels. However, the predominant use of sonar in underwater environments,\ncharacterized by limited training data and inherent noise, poses challenges to\nmodel robustness. This autonomy improvement raises safety concerns for\ndeploying such models during underwater operations, potentially leading to\nhazardous situations. This paper aims to provide the first comprehensive\noverview of sonar-based DL under the scope of robustness. It studies\nsonar-based DL perception task models, such as classification, object\ndetection, segmentation, and SLAM. Furthermore, the paper systematizes\nsonar-based state-of-the-art datasets, simulators, and robustness methods such\nas neural network verification, out-of-distribution, and adversarial attacks.\nThis paper highlights the lack of robustness in sonar-based DL research and\nsuggests future research pathways, notably establishing a baseline sonar-based\ndataset and bridging the simulation-to-reality gap.\n","authors":["Martin Aubard","Ana Madureira","Luís Teixeira","José Pinto"],"pdf_url":"https://arxiv.org/pdf/2412.11840v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11836v1","updated":"2024-12-16T14:57:40Z","published":"2024-12-16T14:57:40Z","title":"UnMA-CapSumT: Unified and Multi-Head Attention-driven Caption\n  Summarization Transformer","summary":"  Image captioning is the generation of natural language descriptions of images\nwhich have increased immense popularity in the recent past. With this different\ndeep-learning techniques are devised for the development of factual and\nstylized image captioning models. Previous models focused more on the\ngeneration of factual and stylized captions separately providing more than one\ncaption for a single image. The descriptions generated from these suffer from\nout-of-vocabulary and repetition issues. To the best of our knowledge, no such\nwork exists that provided a description that integrates different captioning\nmethods to describe the contents of an image with factual and stylized\n(romantic and humorous) elements. To overcome these limitations, this paper\npresents a novel Unified Attention and Multi-Head Attention-driven Caption\nSummarization Transformer (UnMA-CapSumT) based Captioning Framework. It\nutilizes both factual captions and stylized captions generated by the Modified\nAdaptive Attention-based factual image captioning model (MAA-FIC) and Style\nFactored Bi-LSTM with attention (SF-Bi-ALSTM) driven stylized image captioning\nmodel respectively. SF-Bi-ALSTM-based stylized IC model generates two prominent\nstyles of expression- {romance, and humor}. The proposed summarizer UnMHA-ST\ncombines both factual and stylized descriptions of an input image to generate\nstyled rich coherent summarized captions. The proposed UnMHA-ST transformer\nlearns and summarizes different linguistic styles efficiently by incorporating\nproposed word embedding fastText with Attention Word Embedding (fTA-WE) and\npointer-generator network with coverage mechanism concept to solve the\nout-of-vocabulary issues and repetition problem. Extensive experiments are\nconducted on Flickr8K and a subset of FlickrStyle10K with supporting ablation\nstudies to prove the efficiency and efficacy of the proposed framework.\n","authors":["Dhruv Sharma","Chhavi Dhiman","Dinesh Kumar"],"pdf_url":"https://arxiv.org/pdf/2412.11836v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17629v4","updated":"2024-12-16T14:56:52Z","published":"2023-11-29T13:43:17Z","title":"RQFormer: Rotated Query Transformer for End-to-End Oriented Object\n  Detection","summary":"  Oriented object detection presents a challenging task due to the presence of\nobject instances with multiple orientations, varying scales, and dense\ndistributions. Recently, end-to-end detectors have made significant strides by\nemploying attention mechanisms and refining a fixed number of queries through\nconsecutive decoder layers. However, existing end-to-end oriented object\ndetectors still face two primary challenges: 1) misalignment between positional\nqueries and keys, leading to inconsistency between classification and\nlocalization; and 2) the presence of a large number of similar queries, which\ncomplicates one-to-one label assignments and optimization. To address these\nlimitations, we propose an end-to-end oriented detector called the Rotated\nQuery Transformer, which integrates two key technologies: Rotated RoI Attention\n(RRoI Attention) and Selective Distinct Queries (SDQ). First, RRoI Attention\naligns positional queries and keys from oriented regions of interest through\ncross-attention. Second, SDQ collects queries from intermediate decoder layers\nand filters out similar ones to generate distinct queries, thereby facilitating\nthe optimization of one-to-one label assignments. Finally, extensive\nexperiments conducted on four remote sensing datasets and one scene text\ndataset demonstrate the effectiveness of our method. To further validate its\ngeneralization capability, we also extend our approach to horizontal object\ndetection The code is available at\n\\url{https://github.com/wokaikaixinxin/RQFormer}.\n","authors":["Jiaqi Zhao","Zeyu Ding","Yong Zhou","Hancheng Zhu","Wenliang Du","Rui Yao","Abdulmotaleb El Saddik"],"pdf_url":"https://arxiv.org/pdf/2311.17629v4.pdf","comment":"This article is accepted by Expert Systems With Applications (ESWA)\n  2024"},{"id":"http://arxiv.org/abs/2403.13804v2","updated":"2024-12-16T14:53:21Z","published":"2024-03-20T17:59:43Z","title":"Learning from Synthetic Data for Visual Grounding","summary":"  This paper extensively investigates the effectiveness of synthetic training\ndata to improve the capabilities of vision-and-language models for grounding\ntextual descriptions to image regions. We explore various strategies to best\ngenerate image-text pairs and image-text-box triplets using a series of\npretrained models under different settings and varying degrees of reliance on\nreal data. Through comparative analyses with synthetic, real, and web-crawled\ndata, we identify factors that contribute to performance differences, and\npropose SynGround, an effective pipeline for generating useful synthetic data\nfor visual grounding. Our findings show that SynGround can improve the\nlocalization capabilities of off-the-shelf vision-and-language models and\noffers the potential for arbitrarily large scale data generation. Particularly,\ndata generated with SynGround improves the pointing game accuracy of a\npretrained ALBEF and BLIP models by 4.81% and 17.11% absolute percentage\npoints, respectively, across the RefCOCO+ and the Flickr30k benchmarks.\n","authors":["Ruozhen He","Ziyan Yang","Paola Cascante-Bonilla","Alexander C. Berg","Vicente Ordonez"],"pdf_url":"https://arxiv.org/pdf/2403.13804v2.pdf","comment":"Project Page: https://catherine-r-he.github.io/SynGround/"},{"id":"http://arxiv.org/abs/2412.07527v2","updated":"2024-12-16T14:43:29Z","published":"2024-12-10T14:03:41Z","title":"Deep Joint Unrolling for Deblurring and Low-Light Image Enhancement\n  (JUDE)","summary":"  Low-light and blurring issues are prevalent when capturing photos at night,\noften due to the use of long exposure to address dim environments. Addressing\nthese joint problems can be challenging and error-prone if an end-to-end model\nis trained without incorporating an appropriate physical model. In this paper,\nwe introduce JUDE, a Deep Joint Unrolling for Deblurring and Low-Light Image\nEnhancement, inspired by the image physical model. Based on Retinex theory and\nthe blurring model, the low-light blurry input is iteratively deblurred and\ndecomposed, producing sharp low-light reflectance and illuminance through an\nunrolling mechanism. Additionally, we incorporate various modules to estimate\nthe initial blur kernel, enhance brightness, and eliminate noise in the final\nimage. Comprehensive experiments on LOL-Blur and Real-LOL-Blur demonstrate that\nour method outperforms existing techniques both quantitatively and\nqualitatively.\n","authors":["Tu Vo","Chan Y. Park"],"pdf_url":"https://arxiv.org/pdf/2412.07527v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2412.11820v1","updated":"2024-12-16T14:37:16Z","published":"2024-12-16T14:37:16Z","title":"Spatiotemporal Blind-Spot Network with Calibrated Flow Alignment for\n  Self-Supervised Video Denoising","summary":"  Self-supervised video denoising aims to remove noise from videos without\nrelying on ground truth data, leveraging the video itself to recover clean\nframes. Existing methods often rely on simplistic feature stacking or apply\noptical flow without thorough analysis. This results in suboptimal utilization\nof both inter-frame and intra-frame information, and it also neglects the\npotential of optical flow alignment under self-supervised conditions, leading\nto biased and insufficient denoising outcomes. To this end, we first explore\nthe practicality of optical flow in the self-supervised setting and introduce a\nSpatioTemporal Blind-spot Network (STBN) for global frame feature utilization.\nIn the temporal domain, we utilize bidirectional blind-spot feature propagation\nthrough the proposed blind-spot alignment block to ensure accurate temporal\nalignment and effectively capture long-range dependencies. In the spatial\ndomain, we introduce the spatial receptive field expansion module, which\nenhances the receptive field and improves global perception capabilities.\nAdditionally, to reduce the sensitivity of optical flow estimation to noise, we\npropose an unsupervised optical flow distillation mechanism that refines\nfine-grained inter-frame interactions during optical flow alignment. Our method\ndemonstrates superior performance across both synthetic and real-world video\ndenoising datasets. The source code is publicly available at\nhttps://github.com/ZKCCZ/STBN.\n","authors":["Zikang Chen","Tao Jiang","Xiaowan Hu","Wang Zhang","Huaqiu Li","Haoqian Wang"],"pdf_url":"https://arxiv.org/pdf/2412.11820v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11819v1","updated":"2024-12-16T14:35:52Z","published":"2024-12-16T14:35:52Z","title":"HiGDA: Hierarchical Graph of Nodes to Learn Local-to-Global Topology for\n  Semi-Supervised Domain Adaptation","summary":"  The enhanced representational power and broad applicability of deep learning\nmodels have attracted significant interest from the research community in\nrecent years. However, these models often struggle to perform effectively under\ndomain shift conditions, where the training data (the source domain) is related\nto but exhibits different distributions from the testing data (the target\ndomain). To address this challenge, previous studies have attempted to reduce\nthe domain gap between source and target data by incorporating a few labeled\ntarget samples during training - a technique known as semi-supervised domain\nadaptation (SSDA). While this strategy has demonstrated notable improvements in\nclassification performance, the network architectures used in these approaches\nprimarily focus on exploiting the features of individual images, leaving room\nfor improvement in capturing rich representations. In this study, we introduce\na Hierarchical Graph of Nodes designed to simultaneously present\nrepresentations at both feature and category levels. At the feature level, we\nintroduce a local graph to identify the most relevant patches within an image,\nfacilitating adaptability to defined main object representations. At the\ncategory level, we employ a global graph to aggregate the features from samples\nwithin the same category, thereby enriching overall representations. Extensive\nexperiments on widely used SSDA benchmark datasets, including Office-Home,\nDomainNet, and VisDA2017, demonstrate that both quantitative and qualitative\nresults substantiate the effectiveness of HiGDA, establishing it as a new\nstate-of-the-art method.\n","authors":["Ba Hung Ngo","Doanh C. Bui","Nhat-Tuong Do-Tran","Tae Jong Choi"],"pdf_url":"https://arxiv.org/pdf/2412.11819v1.pdf","comment":"Accepted for presentation at AAAI2025"},{"id":"http://arxiv.org/abs/2404.13282v2","updated":"2024-12-16T14:33:03Z","published":"2024-04-20T06:01:09Z","title":"Wills Aligner: Multi-Subject Collaborative Brain Visual Decoding","summary":"  Decoding visual information from human brain activity has seen remarkable\nadvancements in recent research. However, the diversity in cortical\nparcellation and fMRI patterns across individuals has prompted the development\nof deep learning models tailored to each subject. The personalization limits\nthe broader applicability of brain visual decoding in real-world scenarios. To\naddress this issue, we introduce Wills Aligner, a novel approach designed to\nachieve multi-subject collaborative brain visual decoding. Wills Aligner begins\nby aligning the fMRI data from different subjects at the anatomical level. It\nthen employs delicate mixture-of-brain-expert adapters and a meta-learning\nstrategy to account for individual fMRI pattern differences. Additionally,\nWills Aligner leverages the semantic relation of visual stimuli to guide the\nlearning of inter-subject commonality, enabling visual decoding for each\nsubject to draw insights from other subjects' data. We rigorously evaluate our\nWills Aligner across various visual decoding tasks, including classification,\ncross-modal retrieval, and image reconstruction. The experimental results\ndemonstrate that Wills Aligner achieves promising performance.\n","authors":["Guangyin Bao","Qi Zhang","Zixuan Gong","Jialei Zhou","Wei Fan","Kun Yi","Usman Naseem","Liang Hu","Duoqian Miao"],"pdf_url":"https://arxiv.org/pdf/2404.13282v2.pdf","comment":"AAAI 2025, 16 pages"},{"id":"http://arxiv.org/abs/2412.11815v1","updated":"2024-12-16T14:32:49Z","published":"2024-12-16T14:32:49Z","title":"ColorFlow: Retrieval-Augmented Image Sequence Colorization","summary":"  Automatic black-and-white image sequence colorization while preserving\ncharacter and object identity (ID) is a complex task with significant market\ndemand, such as in cartoon or comic series colorization. Despite advancements\nin visual colorization using large-scale generative models like diffusion\nmodels, challenges with controllability and identity consistency persist,\nmaking current solutions unsuitable for industrial application.To address this,\nwe propose ColorFlow, a three-stage diffusion-based framework tailored for\nimage sequence colorization in industrial applications. Unlike existing methods\nthat require per-ID finetuning or explicit ID embedding extraction, we propose\na novel robust and generalizable Retrieval Augmented Colorization pipeline for\ncolorizing images with relevant color references. Our pipeline also features a\ndual-branch design: one branch for color identity extraction and the other for\ncolorization, leveraging the strengths of diffusion models. We utilize the\nself-attention mechanism in diffusion models for strong in-context learning and\ncolor identity matching. To evaluate our model, we introduce ColorFlow-Bench, a\ncomprehensive benchmark for reference-based colorization. Results show that\nColorFlow outperforms existing models across multiple metrics, setting a new\nstandard in sequential image colorization and potentially benefiting the art\nindustry. We release our codes and models on our project page:\nhttps://zhuang2002.github.io/ColorFlow/.\n","authors":["Junhao Zhuang","Xuan Ju","Zhaoyang Zhang","Yong Liu","Shiyi Zhang","Chun Yuan","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2412.11815v1.pdf","comment":"Project Page: https://zhuang2002.github.io/ColorFlow/"},{"id":"http://arxiv.org/abs/2412.11813v1","updated":"2024-12-16T14:29:31Z","published":"2024-12-16T14:29:31Z","title":"Designing Semi-Structured Pruning of Graph Convolutional Networks for\n  Skeleton-based Recognition","summary":"  Deep neural networks (DNNs) are nowadays witnessing a major success in\nsolving many pattern recognition tasks including skeleton-based classification.\nThe deployment of DNNs on edge-devices, endowed with limited time and memory\nresources, requires designing lightweight and efficient variants of these\nnetworks. Pruning is one of the lightweight network design techniques that\noperate by removing unnecessary network parts, in a structured or an\nunstructured manner, including individual weights, neurons or even entire\nchannels. Nonetheless, structured and unstructured pruning methods, when\napplied separately, may either be inefficient or ineffective. In this paper, we\ndevise a novel semi-structured method that discards the downsides of structured\nand unstructured pruning while gathering their upsides to some extent. The\nproposed solution is based on a differentiable cascaded parametrization which\ncombines (i) a band-stop mechanism that prunes weights depending on their\nmagnitudes, (ii) a weight-sharing parametrization that prunes connections\neither individually or group-wise, and (iii) a gating mechanism which\narbitrates between different group-wise and entry-wise pruning. All these\ncascaded parametrizations are built upon a common latent tensor which is\ntrained end-to-end by minimizing a classification loss and a surrogate tensor\nrank regularizer. Extensive experiments, conducted on the challenging tasks of\naction and hand-gesture recognition, show the clear advantage of our proposed\nsemi-structured pruning approach against both structured and unstructured\npruning, when taken separately, as well as the related work.\n","authors":["Hichem Sahbi"],"pdf_url":"https://arxiv.org/pdf/2412.11813v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11812v1","updated":"2024-12-16T14:25:52Z","published":"2024-12-16T14:25:52Z","title":"CLDA-YOLO: Visual Contrastive Learning Based Domain Adaptive YOLO\n  Detector","summary":"  Unsupervised domain adaptive (UDA) algorithms can markedly enhance the\nperformance of object detectors under conditions of domain shifts, thereby\nreducing the necessity for extensive labeling and retraining. Current domain\nadaptive object detection algorithms primarily cater to two-stage detectors,\nwhich tend to offer minimal improvements when directly applied to single-stage\ndetectors such as YOLO. Intending to benefit the YOLO detector from UDA, we\nbuild a comprehensive domain adaptive architecture using a teacher-student\ncooperative system for the YOLO detector. In this process, we propose\nuncertainty learning to cope with pseudo-labeling generated by the teacher\nmodel with extreme uncertainty and leverage dynamic data augmentation to\nasymptotically adapt the teacher-student system to the environment. To address\nthe inability of single-stage object detectors to align at multiple stages, we\nutilize a unified visual contrastive learning paradigm that aligns instance at\nbackbone and head respectively, which steadily improves the robustness of the\ndetectors in cross-domain tasks. In summary, we present an unsupervised domain\nadaptive YOLO detector based on visual contrastive learning (CLDA-YOLO), which\nachieves highly competitive results across multiple domain adaptive datasets\nwithout any reduction in inference speed.\n","authors":["Tianheng Qiu","Ka Lung Law","Guanghua Pan","Jufei Wang","Xin Gao","Xuan Huang","Hu Wei"],"pdf_url":"https://arxiv.org/pdf/2412.11812v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11807v1","updated":"2024-12-16T14:18:01Z","published":"2024-12-16T14:18:01Z","title":"PhysAug: A Physical-guided and Frequency-based Data Augmentation for\n  Single-Domain Generalized Object Detection","summary":"  Single-Domain Generalized Object Detection~(S-DGOD) aims to train on a single\nsource domain for robust performance across a variety of unseen target domains\nby taking advantage of an object detector. Existing S-DGOD approaches often\nrely on data augmentation strategies, including a composition of visual\ntransformations, to enhance the detector's generalization ability. However, the\nabsence of real-world prior knowledge hinders data augmentation from\ncontributing to the diversity of training data distributions. To address this\nissue, we propose PhysAug, a novel physical model-based non-ideal imaging\ncondition data augmentation method, to enhance the adaptability of the S-DGOD\ntasks. Drawing upon the principles of atmospheric optics, we develop a\nuniversal perturbation model that serves as the foundation for our proposed\nPhysAug. Given that visual perturbations typically arise from the interaction\nof light with atmospheric particles, the image frequency spectrum is harnessed\nto simulate real-world variations during training. This approach fosters the\ndetector to learn domain-invariant representations, thereby enhancing its\nability to generalize across various settings. Without altering the network\narchitecture or loss function, our approach significantly outperforms the\nstate-of-the-art across various S-DGOD datasets. In particular, it achieves a\nsubstantial improvement of $7.3\\%$ and $7.2\\%$ over the baseline on DWD and\nCityscape-C, highlighting its enhanced generalizability in real-world settings.\n","authors":["Xiaoran Xu","Jiangang Yang","Wenhui Shi","Siyuan Ding","Luqing Luo","Jian Liu"],"pdf_url":"https://arxiv.org/pdf/2412.11807v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11802v1","updated":"2024-12-16T14:12:06Z","published":"2024-12-16T14:12:06Z","title":"AMI-Net: Adaptive Mask Inpainting Network for Industrial Anomaly\n  Detection and Localization","summary":"  Unsupervised visual anomaly detection is crucial for enhancing industrial\nproduction quality and efficiency. Among unsupervised methods, reconstruction\napproaches are popular due to their simplicity and effectiveness. The key\naspect of reconstruction methods lies in the restoration of anomalous regions,\nwhich current methods have not satisfactorily achieved. To tackle this issue,\nwe introduce a novel \\uline{A}daptive \\uline{M}ask \\uline{I}npainting\n\\uline{Net}work (AMI-Net) from the perspective of adaptive mask-inpainting. In\ncontrast to traditional reconstruction methods that treat non-semantic image\npixels as targets, our method uses a pre-trained network to extract multi-scale\nsemantic features as reconstruction targets. Given the multiscale nature of\nindustrial defects, we incorporate a training strategy involving random\npositional and quantitative masking. Moreover, we propose an innovative\nadaptive mask generator capable of generating adaptive masks that effectively\nmask anomalous regions while preserving normal regions. In this manner, the\nmodel can leverage the visible normal global contextual information to restore\nthe masked anomalous regions, thereby effectively suppressing the\nreconstruction of defects. Extensive experimental results on the MVTec AD and\nBTAD industrial datasets validate the effectiveness of the proposed method.\nAdditionally, AMI-Net exhibits exceptional real-time performance, striking a\nfavorable balance between detection accuracy and speed, rendering it highly\nsuitable for industrial applications. Code is available at:\nhttps://github.com/luow23/AMI-Net\n","authors":["Wei Luo","Haiming Yao","Wenyong Yu","Zhengyong Li"],"pdf_url":"https://arxiv.org/pdf/2412.11802v1.pdf","comment":"Accepted by IEEE Transactions on Automation Science and\n  Engineering.Code is available at: https://github.com/luow23/AMI-Net"},{"id":"http://arxiv.org/abs/2409.09424v2","updated":"2024-12-16T14:05:25Z","published":"2024-09-14T12:25:14Z","title":"NBBOX: Noisy Bounding Box Improves Remote Sensing Object Detection","summary":"  Data augmentation has shown significant advancements in computer vision to\nimprove model performance over the years, particularly in scenarios with\nlimited and insufficient data. Currently, most studies focus on adjusting the\nimage or its features to expand the size, quality, and variety of samples\nduring training in various tasks including object detection. However, we argue\nthat it is necessary to investigate bounding box transformations as a data\naugmentation technique rather than image-level transformations, especially in\naerial imagery due to potentially inconsistent bounding box annotations. Hence,\nthis letter presents a thorough investigation of bounding box transformation in\nterms of scaling, rotation, and translation for remote sensing object\ndetection. We call this augmentation strategy NBBOX (Noise Injection into\nBounding Box). We conduct extensive experiments on DOTA and DIOR-R, both\nwell-known datasets that include a variety of rotated generic objects in aerial\nimages. Experimental results show that our approach significantly improves\nremote sensing object detection without whistles and bells and it is more\ntime-efficient than other state-of-the-art augmentation strategies.\n","authors":["Yechan Kim","SooYeon Kim","Moongu Jeon"],"pdf_url":"https://arxiv.org/pdf/2409.09424v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11788v1","updated":"2024-12-16T14:00:30Z","published":"2024-12-16T14:00:30Z","title":"Neural Collapse Inspired Knowledge Distillation","summary":"  Existing knowledge distillation (KD) methods have demonstrated their ability\nin achieving student network performance on par with their teachers. However,\nthe knowledge gap between the teacher and student remains significant and may\nhinder the effectiveness of the distillation process. In this work, we\nintroduce the structure of Neural Collapse (NC) into the KD framework. NC\ntypically occurs in the final phase of training, resulting in a graceful\ngeometric structure where the last-layer features form a simplex equiangular\ntight frame. Such phenomenon has improved the generalization of deep network\ntraining. We hypothesize that NC can also alleviate the knowledge gap in\ndistillation, thereby enhancing student performance. This paper begins with an\nempirical analysis to bridge the connection between knowledge distillation and\nneural collapse. Through this analysis, we establish that transferring the\nteacher's NC structure to the student benefits the distillation process.\nTherefore, instead of merely transferring instance-level logits or features, as\ndone by existing distillation methods, we encourage students to learn the\nteacher's NC structure. Thereby, we propose a new distillation paradigm termed\nNeural Collapse-inspired Knowledge Distillation (NCKD). Comprehensive\nexperiments demonstrate that NCKD is simple yet effective, improving the\ngeneralization of all distilled student models and achieving state-of-the-art\naccuracy performance.\n","authors":["Shuoxi Zhang","Zijian Song","Kun He"],"pdf_url":"https://arxiv.org/pdf/2412.11788v1.pdf","comment":"13 pages, 7 figures. Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2404.12630v2","updated":"2024-12-16T13:59:51Z","published":"2024-04-19T05:12:04Z","title":"MindTuner: Cross-Subject Visual Decoding with Visual Fingerprint and\n  Semantic Correction","summary":"  Decoding natural visual scenes from brain activity has flourished, with\nextensive research in single-subject tasks and, however, less in cross-subject\ntasks. Reconstructing high-quality images in cross-subject tasks is a\nchallenging problem due to profound individual differences between subjects and\nthe scarcity of data annotation. In this work, we proposed MindTuner for\ncross-subject visual decoding, which achieves high-quality and rich semantic\nreconstructions using only 1 hour of fMRI training data benefiting from the\nphenomena of visual fingerprint in the human visual system and a novel\nfMRI-to-text alignment paradigm. Firstly, we pre-train a multi-subject model\namong 7 subjects and fine-tune it with scarce data on new subjects, where LoRAs\nwith Skip-LoRAs are utilized to learn the visual fingerprint. Then, we take the\nimage modality as the intermediate pivot modality to achieve fMRI-to-text\nalignment, which achieves impressive fMRI-to-text retrieval performance and\ncorrects fMRI-to-image reconstruction with fine-tuned semantics. The results of\nboth qualitative and quantitative analyses demonstrate that MindTuner surpasses\nstate-of-the-art cross-subject visual decoding models on the Natural Scenes\nDataset (NSD), whether using training data of 1 hour or 40 hours.\n","authors":["Zixuan Gong","Qi Zhang","Guangyin Bao","Lei Zhu","Ke Liu","Liang Hu","Duoqian Miao"],"pdf_url":"https://arxiv.org/pdf/2404.12630v2.pdf","comment":"AAAI 2025, 14 pages"},{"id":"http://arxiv.org/abs/2412.11785v1","updated":"2024-12-16T13:57:02Z","published":"2024-12-16T13:57:02Z","title":"InterDyn: Controllable Interactive Dynamics with Video Diffusion Models","summary":"  Predicting the dynamics of interacting objects is essential for both humans\nand intelligent systems. However, existing approaches are limited to\nsimplified, toy settings and lack generalizability to complex, real-world\nenvironments. Recent advances in generative models have enabled the prediction\nof state transitions based on interventions, but focus on generating a single\nfuture state which neglects the continuous motion and subsequent dynamics\nresulting from the interaction. To address this gap, we propose InterDyn, a\nnovel framework that generates videos of interactive dynamics given an initial\nframe and a control signal encoding the motion of a driving object or actor.\nOur key insight is that large video foundation models can act as both neural\nrenderers and implicit physics simulators by learning interactive dynamics from\nlarge-scale video data. To effectively harness this capability, we introduce an\ninteractive control mechanism that conditions the video generation process on\nthe motion of the driving entity. Qualitative results demonstrate that InterDyn\ngenerates plausible, temporally consistent videos of complex object\ninteractions while generalizing to unseen objects. Quantitative evaluations\nshow that InterDyn outperforms baselines that focus on static state\ntransitions. This work highlights the potential of leveraging video generative\nmodels as implicit physics engines.\n","authors":["Rick Akkerman","Haiwen Feng","Michael J. Black","Dimitrios Tzionas","Victoria Fernández Abrevaya"],"pdf_url":"https://arxiv.org/pdf/2412.11785v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19746v2","updated":"2024-12-16T13:53:50Z","published":"2024-05-30T06:49:59Z","title":"DenseSeg: Joint Learning for Semantic Segmentation and Landmark\n  Detection Using Dense Image-to-Shape Representation","summary":"  Purpose: Semantic segmentation and landmark detection are fundamental tasks\nof medical image processing, facilitating further analysis of anatomical\nobjects. Although deep learning-based pixel-wise classification has set a\nnew-state-of-the-art for segmentation, it falls short in landmark detection, a\nstrength of shape-based approaches. Methods: In this work, we propose a dense\nimage-to-shape representation that enables the joint learning of landmarks and\nsemantic segmentation by employing a fully convolutional architecture. Our\nmethod intuitively allows the extraction of arbitrary landmarks due to its\nrepresentation of anatomical correspondences. We benchmark our method against\nthe state-of-the-art for semantic segmentation (nnUNet), a shape-based approach\nemploying geometric deep learning and a convolutional neural network-based\nmethod for landmark detection. Results: We evaluate our method on two medical\ndataset: one common benchmark featuring the lungs, heart, and clavicle from\nthorax X-rays, and another with 17 different bones in the paediatric wrist.\nWhile our method is on pair with the landmark detection baseline in the thorax\nsetting (error in mm of $2.6\\pm0.9$ vs $2.7\\pm0.9$), it substantially surpassed\nit in the more complex wrist setting ($1.1\\pm0.6$ vs $1.9\\pm0.5$). Conclusion:\nWe demonstrate that dense geometric shape representation is beneficial for\nchallenging landmark detection tasks and outperforms previous state-of-the-art\nusing heatmap regression. While it does not require explicit training on the\nlandmarks themselves, allowing for the addition of new landmarks without\nnecessitating retraining.}\n","authors":["Ron Keuth","Lasse Hansen","Maren Balks","Ronja Jäger","Anne-Nele Schröder","Ludger Tüshaus","Mattias Heinrich"],"pdf_url":"https://arxiv.org/pdf/2405.19746v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11779v1","updated":"2024-12-16T13:49:57Z","published":"2024-12-16T13:49:57Z","title":"Impact of Face Alignment on Face Image Quality","summary":"  Face alignment is a crucial step in preparing face images for feature\nextraction in facial analysis tasks. For applications such as face recognition,\nfacial expression recognition, and facial attribute classification, alignment\nis widely utilized during both training and inference to standardize the\npositions of key landmarks in the face. It is well known that the application\nand method of face alignment significantly affect the performance of facial\nanalysis models. However, the impact of alignment on face image quality has not\nbeen thoroughly investigated. Current FIQA studies often assume alignment as a\nprerequisite but do not explicitly evaluate how alignment affects quality\nmetrics, especially with the advent of modern deep learning-based detectors\nthat integrate detection and landmark localization. To address this need, our\nstudy examines the impact of face alignment on face image quality scores. We\nconducted experiments on the LFW, IJB-B, and SCFace datasets, employing MTCNN\nand RetinaFace models for face detection and alignment. To evaluate face image\nquality, we utilized several assessment methods, including SER-FIQ, FaceQAN,\nDifFIQA, and SDD-FIQA. Our analysis included examining quality score\ndistributions for the LFW and IJB-B datasets and analyzing average quality\nscores at varying distances in the SCFace dataset. Our findings reveal that\nface image quality assessment methods are sensitive to alignment. Moreover,\nthis sensitivity increases under challenging real-life conditions, highlighting\nthe importance of evaluating alignment's role in quality assessment.\n","authors":["Eren Onaran","Erdi Sarıtaş","Hazım Kemal Ekenel"],"pdf_url":"https://arxiv.org/pdf/2412.11779v1.pdf","comment":"Accepted at EAI ROSENET 2024 - 8th EAI International Conference on\n  Robotic Sensor Networks"},{"id":"http://arxiv.org/abs/2412.04903v2","updated":"2024-12-16T13:47:29Z","published":"2024-12-06T09:59:47Z","title":"EACO: Enhancing Alignment in Multimodal LLMs via Critical Observation","summary":"  Multimodal large language models (MLLMs) have achieved remarkable progress on\nvarious visual question answering and reasoning tasks leveraging instruction\nfine-tuning specific datasets. They can also learn from preference data\nannotated by human to enhance their reasoning ability and mitigate\nhallucinations. Most of preference data is generated from the model itself.\nHowever, existing methods require high-quality critical labels, which are\ncostly and rely on human or proprietary models like GPT-4V. In this work, we\npropose Enhancing Alignment in MLLMs via Critical Observation (EACO), which\naligns MLLMs by self-generated preference data using only 5k images\neconomically. Our approach begins with collecting and refining a Scoring\nEvaluation Instruction-tuning dataset to train a critical evaluation model,\ntermed the Critic. This Critic observes model responses across multiple\ndimensions, selecting preferred and non-preferred outputs for refined Direct\nPreference Optimization (DPO) tuning. To further enhance model performance, we\nemploy an additional supervised fine-tuning stage after preference tuning. EACO\nreduces the overall hallucinations by 65.6% on HallusionBench and improves the\nreasoning ability by 21.8% on MME-Cognition. EACO achieves an 8.5% improvement\nover LLaVA-v1.6-Mistral-7B across multiple benchmarks. Remarkably, EACO also\nshows the potential critical ability in open-source MLLMs, demonstrating that\nEACO is a viable path to boost the competence of MLLMs.\n","authors":["Yongxin Wang","Meng Cao","Haokun Lin","Mingfei Han","Liang Ma","Jin Jiang","Yuhao Cheng","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2412.04903v2.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2412.11771v1","updated":"2024-12-16T13:44:26Z","published":"2024-12-16T13:44:26Z","title":"Point Cloud-Assisted Neural Image Compression","summary":"  High-efficient image compression is a critical requirement. In several\nscenarios where multiple modalities of data are captured by different sensors,\nthe auxiliary information from other modalities are not fully leveraged by\nexisting image-only codecs, leading to suboptimal compression efficiency. In\nthis paper, we increase image compression performance with the assistance of\npoint cloud, which is widely adopted in the area of autonomous driving. We\nfirst unify the data representation for both modalities to facilitate data\nprocessing. Then, we propose the point cloud-assisted neural image codec\n(PCA-NIC) to enhance the preservation of image texture and structure by\nutilizing the high-dimensional point cloud information. We further introduce a\nmulti-modal feature fusion transform module (MMFFT) to capture more\nrepresentative image features, remove redundant information between channels\nand modalities that are not relevant to the image content. Our work is the\nfirst to improve image compression performance using point cloud and achieves\nstate-of-the-art performance.\n","authors":["Ziqun Li","Qi Zhang","Xiaofeng Huang","Zhao Wang","Siwei Ma","Wei Yan"],"pdf_url":"https://arxiv.org/pdf/2412.11771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11767v1","updated":"2024-12-16T13:39:32Z","published":"2024-12-16T13:39:32Z","title":"IDEA-Bench: How Far are Generative Models from Professional Designing?","summary":"  Real-world design tasks - such as picture book creation, film storyboard\ndevelopment using character sets, photo retouching, visual effects, and font\ntransfer - are highly diverse and complex, requiring deep interpretation and\nextraction of various elements from instructions, descriptions, and reference\nimages. The resulting images often implicitly capture key features from\nreferences or user inputs, making it challenging to develop models that can\neffectively address such varied tasks. While existing visual generative models\ncan produce high-quality images based on prompts, they face significant\nlimitations in professional design scenarios that involve varied forms and\nmultiple inputs and outputs, even when enhanced with adapters like ControlNets\nand LoRAs. To address this, we introduce IDEA-Bench, a comprehensive benchmark\nencompassing 100 real-world design tasks, including rendering, visual effects,\nstoryboarding, picture books, fonts, style-based, and identity-preserving\ngeneration, with 275 test cases to thoroughly evaluate a model's\ngeneral-purpose generation capabilities. Notably, even the best-performing\nmodel only achieves 22.48 on IDEA-Bench, while the best general-purpose model\nonly achieves 6.81. We provide a detailed analysis of these results,\nhighlighting the inherent challenges and providing actionable directions for\nimprovement. Additionally, we provide a subset of 18 representative tasks\nequipped with multimodal large language model (MLLM)-based auto-evaluation\ntechniques to facilitate rapid model development and comparison. We releases\nthe benchmark data, evaluation toolkits, and an online leaderboard at\nhttps://github.com/ali-vilab/IDEA-Bench, aiming to drive the advancement of\ngenerative models toward more versatile and applicable intelligent design\nsystems.\n","authors":["Chen Liang","Lianghua Huang","Jingwu Fang","Huanzhang Dou","Wei Wang","Zhi-Fan Wu","Yupeng Shi","Junge Zhang","Xin Zhao","Yu Liu"],"pdf_url":"https://arxiv.org/pdf/2412.11767v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11762v1","updated":"2024-12-16T13:26:52Z","published":"2024-12-16T13:26:52Z","title":"GS-ProCams: Gaussian Splatting-based Projector-Camera Systems","summary":"  We present GS-ProCams, the first Gaussian Splatting-based framework for\nprojector-camera systems (ProCams). GS-ProCams significantly enhances the\nefficiency of projection mapping (PM) that requires establishing geometric and\nradiometric mappings between the projector and the camera. Previous CNN-based\nProCams are constrained to a specific viewpoint, limiting their applicability\nto novel perspectives. In contrast, NeRF-based ProCams support view-agnostic\nprojection mapping, however, they require an additional colocated light source\nand demand significant computational and memory resources. To address this\nissue, we propose GS-ProCams that employs 2D Gaussian for scene\nrepresentations, and enables efficient view-agnostic ProCams applications. In\nparticular, we explicitly model the complex geometric and photometric mappings\nof ProCams using projector responses, the target surface's geometry and\nmaterials represented by Gaussians, and global illumination component. Then, we\nemploy differentiable physically-based rendering to jointly estimate them from\ncaptured multi-view projections. Compared to state-of-the-art NeRF-based\nmethods, our GS-ProCams eliminates the need for additional devices, achieving\nsuperior ProCams simulation quality. It is also 600 times faster and uses only\n1/10 of the GPU memory.\n","authors":["Qingyue Deng","Jijiang Li","Haibin Ling","Bingyao Huang"],"pdf_url":"https://arxiv.org/pdf/2412.11762v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11755v1","updated":"2024-12-16T13:19:41Z","published":"2024-12-16T13:19:41Z","title":"Generative Inbetweening through Frame-wise Conditions-Driven Video\n  Generation","summary":"  Generative inbetweening aims to generate intermediate frame sequences by\nutilizing two key frames as input. Although remarkable progress has been made\nin video generation models, generative inbetweening still faces challenges in\nmaintaining temporal stability due to the ambiguous interpolation path between\ntwo key frames. This issue becomes particularly severe when there is a large\nmotion gap between input frames. In this paper, we propose a straightforward\nyet highly effective Frame-wise Conditions-driven Video Generation (FCVG)\nmethod that significantly enhances the temporal stability of interpolated video\nframes. Specifically, our FCVG provides an explicit condition for each frame,\nmaking it much easier to identify the interpolation path between two input\nframes and thus ensuring temporally stable production of visually plausible\nvideo frames. To achieve this, we suggest extracting matched lines from two\ninput frames that can then be easily interpolated frame by frame, serving as\nframe-wise conditions seamlessly integrated into existing video generation\nmodels. In extensive evaluations covering diverse scenarios such as natural\nlandscapes, complex human poses, camera movements and animations, existing\nmethods often exhibit incoherent transitions across frames. In contrast, our\nFCVG demonstrates the capability to generate temporally stable videos using\nboth linear and non-linear interpolation curves. Our project page and code are\navailable at \\url{https://fcvg-inbetween.github.io/}.\n","authors":["Tianyi Zhu","Dongwei Ren","Qilong Wang","Xiaohe Wu","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2412.11755v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11610v3","updated":"2024-12-16T13:16:45Z","published":"2024-10-15T13:46:19Z","title":"Enhanced Encoder-Decoder Architecture for Accurate Monocular Depth\n  Estimation","summary":"  Estimating depth from a single 2D image is a challenging task due to the lack\nof stereo or multi-view data, which are typically required for depth\nperception. This paper introduces a novel deep learning-based approach using an\nenhanced encoder-decoder architecture, where the Inception-ResNet-v2 model\nserves as the encoder. This is the first instance of utilizing\nInception-ResNet-v2 as an encoder for monocular depth estimation, demonstrating\nimproved performance over previous models. Our model effectively captures\ncomplex objects and fine-grained details, which are generally difficult to\npredict. Additionally, it incorporates multi-scale feature extraction to\nenhance depth prediction accuracy across various object sizes and distances. We\npropose a composite loss function comprising depth loss, gradient edge loss,\nand Structural Similarity Index Measure (SSIM) loss, with fine-tuned weights to\noptimize the weighted sum, ensuring a balance across different aspects of depth\nestimation. Experimental results on the NYU Depth V2 dataset show that our\nmodel achieves state-of-the-art performance, with an Absolute Relative Error\n(ARE) of 0.064, Root Mean Square Error (RMSE) of 0.228, and accuracy ($\\delta$\n< 1.25) of 89.3%. These metrics demonstrate that our model can accurately\npredict depth even in challenging scenarios, providing a scalable solution for\nreal-world applications in robotics, 3D reconstruction, and augmented reality.\n","authors":["Dabbrata Das","Argho Deb Das","Farhan Sadaf"],"pdf_url":"https://arxiv.org/pdf/2410.11610v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.11251v2","updated":"2024-12-16T13:15:13Z","published":"2023-06-20T03:05:28Z","title":"Lipschitz Singularities in Diffusion Models","summary":"  Diffusion models, which employ stochastic differential equations to sample\nimages through integrals, have emerged as a dominant class of generative\nmodels. However, the rationality of the diffusion process itself receives\nlimited attention, leaving the question of whether the problem is well-posed\nand well-conditioned. In this paper, we explore a perplexing tendency of\ndiffusion models: they often display the infinite Lipschitz property of the\nnetwork with respect to time variable near the zero point. We provide\ntheoretical proofs to illustrate the presence of infinite Lipschitz constants\nand empirical results to confirm it. The Lipschitz singularities pose a threat\nto the stability and accuracy during both the training and inference processes\nof diffusion models. Therefore, the mitigation of Lipschitz singularities holds\ngreat potential for enhancing the performance of diffusion models. To address\nthis challenge, we propose a novel approach, dubbed E-TSDM, which alleviates\nthe Lipschitz singularities of the diffusion model near the zero point of\ntimesteps. Remarkably, our technique yields a substantial improvement in\nperformance. Moreover, as a byproduct of our method, we achieve a dramatic\nreduction in the Fr\\'echet Inception Distance of acceleration methods relying\non network Lipschitz, including DDIM and DPM-Solver, by over 33%. Extensive\nexperiments on diverse datasets validate our theory and method. Our work may\nadvance the understanding of the general diffusion process, and also provide\ninsights for the design of diffusion models.\n","authors":["Zhantao Yang","Ruili Feng","Han Zhang","Yujun Shen","Kai Zhu","Lianghua Huang","Yifei Zhang","Yu Liu","Deli Zhao","Jingren Zhou","Fan Cheng"],"pdf_url":"https://arxiv.org/pdf/2306.11251v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11753v1","updated":"2024-12-16T13:12:11Z","published":"2024-12-16T13:12:11Z","title":"DriveGazen: Event-Based Driving Status Recognition using Conventional\n  Camera","summary":"  We introduce a wearable driving status recognition device and our open-source\ndataset, along with a new real-time method robust to changes in lighting\nconditions for identifying driving status from eye observations of drivers. The\ncore of our method is generating event frames from conventional intensity\nframes, and the other is a newly designed Attention Driving State Network\n(ADSN). Compared to event cameras, conventional cameras offer complete\ninformation and lower hardware costs, enabling captured frames to encode rich\nspatial information. However, these textures lack temporal information, posing\nchallenges in effectively identifying driving status. DriveGazen addresses this\nissue from three perspectives. First, we utilize video frames to generate\nrealistic synthetic dynamic vision sensor (DVS) events. Second, we adopt a\nspiking neural network to decode pertinent temporal information. Lastly, ADSN\nextracts crucial spatial cues from corresponding intensity frames and conveys\nspatial attention to convolutional spiking layers during both training and\ninference through a novel guide attention module to guide the feature learning\nand feature enhancement of the event frame. We specifically collected the\nDriving Status (DriveGaze) dataset to demonstrate the effectiveness of our\napproach. Additionally, we validate the superiority of the DriveGazen on the\nSingle-eye Event-based Emotion (SEE) dataset. To the best of our knowledge, our\nmethod is the first to utilize guide attention spiking neural networks and\neye-based event frames generated from conventional cameras for driving status\nrecognition. Please refer to our project page for more details:\nhttps://github.com/TooyoungALEX/AAAI25-DriveGazen.\n","authors":["Xiaoyin Yang"],"pdf_url":"https://arxiv.org/pdf/2412.11753v1.pdf","comment":"9 pages, 4 figures, (AAAI25)The 39th Annual AAAI Conference on\n  Artificial Intelligence"},{"id":"http://arxiv.org/abs/2412.11752v1","updated":"2024-12-16T13:11:02Z","published":"2024-12-16T13:11:02Z","title":"Deformable Radial Kernel Splatting","summary":"  Recently, Gaussian splatting has emerged as a robust technique for\nrepresenting 3D scenes, enabling real-time rasterization and high-fidelity\nrendering. However, Gaussians' inherent radial symmetry and smoothness\nconstraints limit their ability to represent complex shapes, often requiring\nthousands of primitives to approximate detailed geometry. We introduce\nDeformable Radial Kernel (DRK), which extends Gaussian splatting into a more\ngeneral and flexible framework. Through learnable radial bases with adjustable\nangles and scales, DRK efficiently models diverse shape primitives while\nenabling precise control over edge sharpness and boundary curvature. iven DRK's\nplanar nature, we further develop accurate ray-primitive intersection\ncomputation for depth sorting and introduce efficient kernel culling strategies\nfor improved rasterization efficiency. Extensive experiments demonstrate that\nDRK outperforms existing methods in both representation efficiency and\nrendering quality, achieving state-of-the-art performance while dramatically\nreducing primitive count.\n","authors":["Yi-Hua Huang","Ming-Xian Lin","Yang-Tian Sun","Ziyi Yang","Xiaoyang Lyu","Yan-Pei Cao","Xiaojuan Qi"],"pdf_url":"https://arxiv.org/pdf/2412.11752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11735v1","updated":"2024-12-16T12:56:57Z","published":"2024-12-16T12:56:57Z","title":"Transferable Adversarial Face Attack with Text Controlled Attribute","summary":"  Traditional adversarial attacks typically produce adversarial examples under\nnorm-constrained conditions, whereas unrestricted adversarial examples are\nfree-form with semantically meaningful perturbations. Current unrestricted\nadversarial impersonation attacks exhibit limited control over adversarial face\nattributes and often suffer from low transferability. In this paper, we propose\na novel Text Controlled Attribute Attack (TCA$^2$) to generate photorealistic\nadversarial impersonation faces guided by natural language. Specifically, the\ncategory-level personal softmax vector is employed to precisely guide the\nimpersonation attacks. Additionally, we propose both data and model\naugmentation strategies to achieve transferable attacks on unknown target\nmodels. Finally, a generative model, \\textit{i.e}, Style-GAN, is utilized to\nsynthesize impersonated faces with desired attributes. Extensive experiments on\ntwo high-resolution face recognition datasets validate that our TCA$^2$ method\ncan generate natural text-guided adversarial impersonation faces with high\ntransferability. We also evaluate our method on real-world face recognition\nsystems, \\textit{i.e}, Face++ and Aliyun, further demonstrating the practical\npotential of our approach.\n","authors":["Wenyun Li","Zheng Zhang","Xiangyuan Lan","Dongmei Jiang"],"pdf_url":"https://arxiv.org/pdf/2412.11735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01756v2","updated":"2024-12-16T12:53:25Z","published":"2024-11-04T02:43:55Z","title":"ChatTracker: Enhancing Visual Tracking Performance via Chatting with\n  Multimodal Large Language Model","summary":"  Visual object tracking aims to locate a targeted object in a video sequence\nbased on an initial bounding box. Recently, Vision-Language~(VL) trackers have\nproposed to utilize additional natural language descriptions to enhance\nversatility in various applications. However, VL trackers are still inferior to\nState-of-The-Art (SoTA) visual trackers in terms of tracking performance. We\nfound that this inferiority primarily results from their heavy reliance on\nmanual textual annotations, which include the frequent provision of ambiguous\nlanguage descriptions. In this paper, we propose ChatTracker to leverage the\nwealth of world knowledge in the Multimodal Large Language Model (MLLM) to\ngenerate high-quality language descriptions and enhance tracking performance.\nTo this end, we propose a novel reflection-based prompt optimization module to\niteratively refine the ambiguous and inaccurate descriptions of the target with\ntracking feedback. To further utilize semantic information produced by MLLM, a\nsimple yet effective VL tracking framework is proposed and can be easily\nintegrated as a plug-and-play module to boost the performance of both VL and\nvisual trackers. Experimental results show that our proposed ChatTracker\nachieves a performance comparable to existing methods.\n","authors":["Yiming Sun","Fan Yu","Shaoxiang Chen","Yu Zhang","Junwei Huang","Chenhui Li","Yang Li","Changbo Wang"],"pdf_url":"https://arxiv.org/pdf/2411.01756v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11715v1","updated":"2024-12-16T12:35:56Z","published":"2024-12-16T12:35:56Z","title":"Discrepancy-Aware Attention Network for Enhanced Audio-Visual Zero-Shot\n  Learning","summary":"  Audio-visual Zero-Shot Learning (ZSL) has attracted significant attention for\nits ability to identify unseen classes and perform well in video classification\ntasks. However, modal imbalance in (G)ZSL leads to over-reliance on the optimal\nmodality, reducing discriminative capabilities for unseen classes. Some studies\nhave attempted to address this issue by modifying parameter gradients, but two\nchallenges still remain: (a) Quality discrepancies, where modalities offer\ndiffering quantities and qualities of information for the same concept. (b)\nContent discrepancies, where sample contributions within a modality vary\nsignificantly. To address these challenges, we propose a Discrepancy-Aware\nAttention Network (DAAN) for Enhanced Audio-Visual ZSL. Our approach introduces\na Quality-Discrepancy Mitigation Attention (QDMA) unit to minimize redundant\ninformation in the high-quality modality and a Contrastive Sample-level\nGradient Modulation (CSGM) block to adjust gradient magnitudes and balance\ncontent discrepancies. We quantify modality contributions by integrating\noptimization and convergence rate for more precise gradient modulation in CSGM.\nExperiments demonstrates DAAN achieves state-of-the-art performance on\nbenchmark datasets, with ablation studies validating the effectiveness of\nindividual modules.\n","authors":["RunLin Yu","Yipu Gong","Wenrui Li","Aiwen Sun","Mengren Zheng"],"pdf_url":"https://arxiv.org/pdf/2412.11715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11710v1","updated":"2024-12-16T12:32:21Z","published":"2024-12-16T12:32:21Z","title":"Re-Attentional Controllable Video Diffusion Editing","summary":"  Editing videos with textual guidance has garnered popularity due to its\nstreamlined process which mandates users to solely edit the text prompt\ncorresponding to the source video. Recent studies have explored and exploited\nlarge-scale text-to-image diffusion models for text-guided video editing,\nresulting in remarkable video editing capabilities. However, they may still\nsuffer from some limitations such as mislocated objects, incorrect number of\nobjects. Therefore, the controllability of video editing remains a formidable\nchallenge. In this paper, we aim to challenge the above limitations by\nproposing a Re-Attentional Controllable Video Diffusion Editing (ReAtCo)\nmethod. Specially, to align the spatial placement of the target objects with\nthe edited text prompt in a training-free manner, we propose a Re-Attentional\nDiffusion (RAD) to refocus the cross-attention activation responses between the\nedited text prompt and the target video during the denoising stage, resulting\nin a spatially location-aligned and semantically high-fidelity manipulated\nvideo. In particular, to faithfully preserve the invariant region content with\nless border artifacts, we propose an Invariant Region-guided Joint Sampling\n(IRJS) strategy to mitigate the intrinsic sampling errors w.r.t the invariant\nregions at each denoising timestep and constrain the generated content to be\nharmonized with the invariant region content. Experimental results verify that\nReAtCo consistently improves the controllability of video diffusion editing and\nachieves superior video editing performance.\n","authors":["Yuanzhi Wang","Yong Li","Mengyi Liu","Xiaoya Zhang","Xin Liu","Zhen Cui","Antoni B. Chan"],"pdf_url":"https://arxiv.org/pdf/2412.11710v1.pdf","comment":"Accepted by AAAI 2025. Codes are released at:\n  https://github.com/mdswyz/ReAtCo"},{"id":"http://arxiv.org/abs/2412.11706v1","updated":"2024-12-16T12:28:22Z","published":"2024-12-16T12:28:22Z","title":"AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration","summary":"  Video Diffusion Transformers (DiTs) have demonstrated significant potential\nfor generating high-fidelity videos but are computationally intensive. Existing\nacceleration methods include distillation, which requires costly retraining,\nand feature caching, which is highly sensitive to network architecture. Recent\ntoken reduction methods are training-free and architecture-agnostic, offering\ngreater flexibility and wider applicability. However, they enforce the same\nsequence length across different components, constraining their acceleration\npotential. We observe that intra-sequence redundancy in video DiTs varies\nacross features, blocks, and denoising timesteps. Building on this observation,\nwe propose Asymmetric Reduction and Restoration (AsymRnR), a training-free\napproach to accelerate video DiTs. It offers a flexible and adaptive strategy\nthat reduces the number of tokens based on their redundancy to enhance both\nacceleration and generation quality. We further propose matching cache to\nfacilitate faster processing. Integrated into state-of-the-art video DiTs,\nAsymRnR achieves a superior speedup without compromising the quality.\n","authors":["Wenhao Sun","Rong-Cheng Tu","Jingyi Liao","Zhao Jin","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2412.11706v1.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2412.11702v1","updated":"2024-12-16T12:25:57Z","published":"2024-12-16T12:25:57Z","title":"Flex-PE: Flexible and SIMD Multi-Precision Processing Element for AI\n  Workloads","summary":"  The rapid adaptation of data driven AI models, such as deep learning\ninference, training, Vision Transformers (ViTs), and other HPC applications,\ndrives a strong need for runtime precision configurable different non linear\nactivation functions (AF) hardware support. Existing solutions support diverse\nprecision or runtime AF reconfigurability but fail to address both\nsimultaneously. This work proposes a flexible and SIMD multiprecision\nprocessing element (FlexPE), which supports diverse runtime configurable AFs,\nincluding sigmoid, tanh, ReLU and softmax, and MAC operation. The proposed\ndesign achieves an improved throughput of up to 16X FxP4, 8X FxP8, 4X FxP16 and\n1X FxP32 in pipeline mode with 100% time multiplexed hardware. This work\nproposes an area efficient multiprecision iterative mode in the SIMD systolic\narrays for edge AI use cases. The design delivers superior performance with up\nto 62X and 371X reductions in DMA reads for input feature maps and weight\nfilters in VGG16, with an energy efficiency of 8.42 GOPS / W within the\naccuracy loss of 2%. The proposed architecture supports emerging 4-bit\ncomputations for DL inference while enhancing throughput in FxP8/16 modes for\ntransformers and other HPC applications. The proposed approach enables future\nenergy-efficient AI accelerators in edge and cloud environments.\n","authors":["Mukul Lokhande","Gopal Raut","Santosh Kumar Vishvakarma"],"pdf_url":"https://arxiv.org/pdf/2412.11702v1.pdf","comment":"10 pages, 5 figures, Preprint, Submitted to TVLSI Regular papers"},{"id":"http://arxiv.org/abs/2311.02778v3","updated":"2024-12-16T12:22:12Z","published":"2023-11-05T21:46:12Z","title":"MuSHRoom: Multi-Sensor Hybrid Room Dataset for Joint 3D Reconstruction\n  and Novel View Synthesis","summary":"  Metaverse technologies demand accurate, real-time, and immersive modeling on\nconsumer-grade hardware for both non-human perception (e.g.,\ndrone/robot/autonomous car navigation) and immersive technologies like AR/VR,\nrequiring both structural accuracy and photorealism. However, there exists a\nknowledge gap in how to apply geometric reconstruction and photorealism\nmodeling (novel view synthesis) in a unified framework. To address this gap and\npromote the development of robust and immersive modeling and rendering with\nconsumer-grade devices, we propose a real-world Multi-Sensor Hybrid Room\nDataset (MuSHRoom). Our dataset presents exciting challenges and requires\nstate-of-the-art methods to be cost-effective, robust to noisy data and\ndevices, and can jointly learn 3D reconstruction and novel view synthesis\ninstead of treating them as separate tasks, making them ideal for real-world\napplications. We benchmark several famous pipelines on our dataset for joint 3D\nmesh reconstruction and novel view synthesis. Our dataset and benchmark show\ngreat potential in promoting the improvements for fusing 3D reconstruction and\nhigh-quality rendering in a robust and computationally efficient end-to-end\nfashion. The dataset and code are available at the project website:\nhttps://xuqianren.github.io/publications/MuSHRoom/.\n","authors":["Xuqian Ren","Wenjia Wang","Dingding Cai","Tuuli Tuominen","Juho Kannala","Esa Rahtu"],"pdf_url":"https://arxiv.org/pdf/2311.02778v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08509v2","updated":"2024-12-16T12:19:58Z","published":"2024-07-11T13:46:47Z","title":"Haar Nuclear Norms with Applications to Remote Sensing Imagery\n  Restoration","summary":"  Remote sensing image restoration aims to reconstruct missing or corrupted\nareas within images. To date, low-rank based models have garnered significant\ninterest in this field. This paper proposes a novel low-rank regularization\nterm, named the Haar nuclear norm (HNN), for efficient and effective remote\nsensing image restoration. It leverages the low-rank properties of wavelet\ncoefficients derived from the 2-D frontal slice-wise Haar discrete wavelet\ntransform, effectively modeling the low-rank prior for separated coarse-grained\nstructure and fine-grained textures in the image. Experimental evaluations\nconducted on hyperspectral image inpainting, multi-temporal image cloud\nremoval, and hyperspectral image denoising have revealed the HNN's potential.\nTypically, HNN achieves a performance improvement of 1-4 dB and a speedup of\n10-28x compared to some state-of-the-art methods (e.g., tensor correlated total\nvariation, and fully-connected tensor network) for inpainting tasks.\n","authors":["Shuang Xu","Chang Yu","Jiangjun Peng","Xiangyong Cao","Deyu Meng"],"pdf_url":"https://arxiv.org/pdf/2407.08509v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19271v2","updated":"2024-12-16T12:14:57Z","published":"2024-11-28T17:04:32Z","title":"AGS-Mesh: Adaptive Gaussian Splatting and Meshing with Geometric Priors\n  for Indoor Room Reconstruction Using Smartphones","summary":"  Geometric priors are often used to enhance 3D reconstruction. With many\nsmartphones featuring low-resolution depth sensors and the prevalence of\noff-the-shelf monocular geometry estimators, incorporating geometric priors as\nregularization signals has become common in 3D vision tasks. However, the\naccuracy of depth estimates from mobile devices is typically poor for highly\ndetailed geometry, and monocular estimators often suffer from poor multi-view\nconsistency and precision. In this work, we propose an approach for joint\nsurface depth and normal refinement of Gaussian Splatting methods for accurate\n3D reconstruction of indoor scenes. We develop supervision strategies that\nadaptively filters low-quality depth and normal estimates by comparing the\nconsistency of the priors during optimization. We mitigate regularization in\nregions where prior estimates have high uncertainty or ambiguities. Our\nfiltering strategy and optimization design demonstrate significant improvements\nin both mesh estimation and novel-view synthesis for both 3D and 2D Gaussian\nSplatting-based methods on challenging indoor room datasets. Furthermore, we\nexplore the use of alternative meshing strategies for finer geometry\nextraction. We develop a scale-aware meshing strategy inspired by TSDF and\noctree-based isosurface extraction, which recovers finer details from Gaussian\nmodels compared to other commonly used open-source meshing tools. Our code is\nreleased in https://xuqianren.github.io/ags_mesh_website/.\n","authors":["Xuqian Ren","Matias Turkulainen","Jiepeng Wang","Otto Seiskari","Iaroslav Melekhov","Juho Kannala","Esa Rahtu"],"pdf_url":"https://arxiv.org/pdf/2411.19271v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11685v1","updated":"2024-12-16T11:55:26Z","published":"2024-12-16T11:55:26Z","title":"Ultra-High-Definition Dynamic Multi-Exposure Image Fusion via Infinite\n  Pixel Learning","summary":"  With the continuous improvement of device imaging resolution, the popularity\nof Ultra-High-Definition (UHD) images is increasing. Unfortunately, existing\nmethods for fusing multi-exposure images in dynamic scenes are designed for\nlow-resolution images, which makes them inefficient for generating high-quality\nUHD images on a resource-constrained device. To alleviate the limitations of\nextremely long-sequence inputs, inspired by the Large Language Model (LLM) for\nprocessing infinitely long texts, we propose a novel learning paradigm to\nachieve UHD multi-exposure dynamic scene image fusion on a single\nconsumer-grade GPU, named Infinite Pixel Learning (IPL). The design of our\napproach comes from three key components: The first step is to slice the input\nsequences to relieve the pressure generated by the model processing the data\nstream; Second, we develop an attention cache technique, which is similar to KV\ncache for infinite data stream processing; Finally, we design a method for\nattention cache compression to alleviate the storage burden of the cache on the\ndevice. In addition, we provide a new UHD benchmark to evaluate the\neffectiveness of our method. Extensive experimental results show that our\nmethod maintains high-quality visual performance while fusing UHD dynamic\nmulti-exposure images in real-time (>40fps) on a single consumer-grade GPU.\n","authors":["Xingchi Chen","Zhuoran Zheng","Xuerui Li","Yuying Chen","Shu Wang","Wenqi Ren"],"pdf_url":"https://arxiv.org/pdf/2412.11685v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11681v1","updated":"2024-12-16T11:47:07Z","published":"2024-12-16T11:47:07Z","title":"Fast-staged CNN Model for Accurate pulmonary diseases and Lung cancer\n  detection","summary":"  Pulmonary pathologies are a significant global health concern, often leading\nto fatal outcomes if not diagnosed and treated promptly. Chest radiography\nserves as a primary diagnostic tool, but the availability of experienced\nradiologists remains limited. Advances in Artificial Intelligence (AI) and\nmachine learning, particularly in computer vision, offer promising solutions to\naddress this challenge.\n  This research evaluates a deep learning model designed to detect lung cancer,\nspecifically pulmonary nodules, along with eight other lung pathologies, using\nchest radiographs. The study leverages diverse datasets comprising over 135,120\nfrontal chest radiographs to train a Convolutional Neural Network (CNN). A\ntwo-stage classification system, utilizing ensemble methods and transfer\nlearning, is employed to first triage images into Normal or Abnormal categories\nand then identify specific pathologies, including lung nodules.\n  The deep learning model achieves notable results in nodule classification,\nwith a top-performing accuracy of 77%, a sensitivity of 0.713, a specificity of\n0.776 during external validation, and an AUC score of 0.888. Despite these\nsuccesses, some misclassifications were observed, primarily false negatives.\n  In conclusion, the model demonstrates robust potential for generalization\nacross diverse patient populations, attributed to the geographic diversity of\nthe training dataset. Future work could focus on integrating ETL data\ndistribution strategies and expanding the dataset with additional nodule-type\nsamples to further enhance diagnostic accuracy.\n","authors":["Abdelbaki Souid","Mohamed Hamroun","Soufiene Ben Othman","Hedi Sakli","Naceur Abdelkarim"],"pdf_url":"https://arxiv.org/pdf/2412.11681v1.pdf","comment":"IEEE International Workshop on Mechatronic Systems Supervision 2023"},{"id":"http://arxiv.org/abs/2403.05435v5","updated":"2024-12-16T11:43:39Z","published":"2024-03-08T16:38:11Z","title":"OmniCount: Multi-label Object Counting with Semantic-Geometric Priors","summary":"  Object counting is pivotal for understanding the composition of scenes.\nPreviously, this task was dominated by class-specific methods, which have\ngradually evolved into more adaptable class-agnostic strategies. However, these\nstrategies come with their own set of limitations, such as the need for manual\nexemplar input and multiple passes for multiple categories, resulting in\nsignificant inefficiencies. This paper introduces a more practical approach\nenabling simultaneous counting of multiple object categories using an\nopen-vocabulary framework. Our solution, OmniCount, stands out by using\nsemantic and geometric insights (priors) from pre-trained models to count\nmultiple categories of objects as specified by users, all without additional\ntraining. OmniCount distinguishes itself by generating precise object masks and\nleveraging varied interactive prompts via the Segment Anything Model for\nefficient counting. To evaluate OmniCount, we created the OmniCount-191\nbenchmark, a first-of-its-kind dataset with multi-label object counts,\nincluding points, bounding boxes, and VQA annotations. Our comprehensive\nevaluation in OmniCount-191, alongside other leading benchmarks, demonstrates\nOmniCount's exceptional performance, significantly outpacing existing\nsolutions. The project webpage is available at\nhttps://mondalanindya.github.io/OmniCount.\n","authors":["Anindya Mondal","Sauradip Nag","Xiatian Zhu","Anjan Dutta"],"pdf_url":"https://arxiv.org/pdf/2403.05435v5.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2412.11680v1","updated":"2024-12-16T11:42:25Z","published":"2024-12-16T11:42:25Z","title":"EGP3D: Edge-guided Geometric Preserving 3D Point Cloud Super-resolution\n  for RGB-D camera","summary":"  Point clouds or depth images captured by current RGB-D cameras often suffer\nfrom low resolution, rendering them insufficient for applications such as 3D\nreconstruction and robots. Existing point cloud super-resolution (PCSR) methods\nare either constrained by geometric artifacts or lack attention to edge\ndetails. To address these issues, we propose an edge-guided\ngeometric-preserving 3D point cloud super-resolution (EGP3D) method tailored\nfor RGB-D cameras. Our approach innovatively optimizes the point cloud with an\nedge constraint on a projected 2D space, thereby ensuring high-quality edge\npreservation in the 3D PCSR task. To tackle geometric optimization challenges\nin super-resolution point clouds, particularly preserving edge shapes and\nsmoothness, we introduce a multi-faceted loss function that simultaneously\noptimizes the Chamfer distance, Hausdorff distance, and gradient smoothness.\nExisting datasets used for point cloud upsampling are predominantly synthetic\nand inadequately represent real-world scenarios, neglecting noise and stray\nlight effects. To address the scarcity of realistic RGB-D data for PCSR tasks,\nwe built a dataset that captures real-world noise and stray-light effects,\noffering a more accurate representation of authentic environments. Validated\nthrough simulations and real-world experiments, the proposed method exhibited\nsuperior performance in preserving edge clarity and geometric details.\n","authors":["Zheng Fang","Ke Ye","Yaofang Liu","Gongzhe Li","Xianhong Zhao","Jialong Li","Ruxin Wang","Yuchen Zhang","Xiangyang Ji","Qilin Sun"],"pdf_url":"https://arxiv.org/pdf/2412.11680v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.06727v2","updated":"2024-12-16T11:28:49Z","published":"2024-12-09T18:16:50Z","title":"Take Fake as Real: Realistic-like Robust Black-box Adversarial Attack to\n  Evade AIGC Detection","summary":"  The security of AI-generated content (AIGC) detection is crucial for ensuring\nmultimedia content credibility. To enhance detector security, research on\nadversarial attacks has become essential. However, most existing adversarial\nattacks focus only on GAN-generated facial images detection, struggle to be\neffective on multi-class natural images and diffusion-based detectors, and\nexhibit poor invisibility. To fill this gap, we first conduct an in-depth\nanalysis of the vulnerability of AIGC detectors and discover the feature that\ndetectors vary in vulnerability to different post-processing. Then, considering\nthat the detector is agnostic in real-world scenarios and given this discovery,\nwe propose a Realistic-like Robust Black-box Adversarial attack (R$^2$BA) with\npost-processing fusion optimization. Unlike typical perturbations, R$^2$BA uses\nreal-world post-processing, i.e., Gaussian blur, JPEG compression, Gaussian\nnoise and light spot to generate adversarial examples. Specifically, we use a\nstochastic particle swarm algorithm with inertia decay to optimize\npost-processing fusion intensity and explore the detector's decision boundary.\nGuided by the detector's fake probability, R$^2$BA enhances/weakens the\ndetector-vulnerable/detector-robust post-processing intensity to strike a\nbalance between adversariality and invisibility. Extensive experiments on\npopular/commercial AIGC detectors and datasets demonstrate that R$^2$BA\nexhibits impressive anti-detection performance, excellent invisibility, and\nstrong robustness in GAN-based and diffusion-based cases. Compared to\nstate-of-the-art white-box and black-box attacks, R$^2$BA shows significant\nimprovements of 15\\%--72\\% and 21\\%--47\\% in anti-detection performance under\nthe original and robust scenario respectively, offering valuable insights for\nthe security of AIGC detection in real-world applications.\n","authors":["Caiyun Xie","Dengpan Ye","Yunming Zhang","Long Tang","Yunna Lv","Jiacheng Deng","Jiawei Song"],"pdf_url":"https://arxiv.org/pdf/2412.06727v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11673v1","updated":"2024-12-16T11:26:46Z","published":"2024-12-16T11:26:46Z","title":"$\\texttt{DINO-Foresight}$: Looking into the Future with DINO","summary":"  Predicting future dynamics is crucial for applications like autonomous\ndriving and robotics, where understanding the environment is key. Existing\npixel-level methods are computationally expensive and often focus on irrelevant\ndetails. To address these challenges, we introduce $\\texttt{DINO-Foresight}$, a\nnovel framework that operates in the semantic feature space of pretrained\nVision Foundation Models (VFMs). Our approach trains a masked feature\ntransformer in a self-supervised manner to predict the evolution of VFM\nfeatures over time. By forecasting these features, we can apply off-the-shelf,\ntask-specific heads for various scene understanding tasks. In this framework,\nVFM features are treated as a latent space, to which different heads attach to\nperform specific tasks for future-frame analysis. Extensive experiments show\nthat our framework outperforms existing methods, demonstrating its robustness\nand scalability. Additionally, we highlight how intermediate transformer\nrepresentations in $\\texttt{DINO-Foresight}$ improve downstream task\nperformance, offering a promising path for the self-supervised enhancement of\nVFM features. We provide the implementation code at\nhttps://github.com/Sta8is/DINO-Foresight .\n","authors":["Efstathios Karypidis","Ioannis Kakogeorgiou","Spyros Gidaris","Nikos Komodakis"],"pdf_url":"https://arxiv.org/pdf/2412.11673v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.09441v2","updated":"2024-12-16T11:26:26Z","published":"2024-08-18T11:23:21Z","title":"CLIP-CID: Efficient CLIP Distillation via Cluster-Instance\n  Discrimination","summary":"  Contrastive Language-Image Pre-training (CLIP) has achieved excellent\nperformance over a wide range of tasks. However, the effectiveness of CLIP\nheavily relies on a substantial corpus of pre-training data, resulting in\nnotable consumption of computational resources. Although knowledge distillation\nhas been widely applied in single modality models, how to efficiently expand\nknowledge distillation to vision-language foundation models with extensive data\nremains relatively unexplored. In this paper, we introduce CLIP-CID, a novel\ndistillation mechanism that effectively transfers knowledge from a large\nvision-language foundation model to a smaller model. We initially propose a\nsimple but efficient image semantic balance method to reduce transfer learning\nbias and improve distillation efficiency. This method filters out 43.7% of\nimage-text pairs from the LAION400M while maintaining superior performance.\nAfter that, we leverage cluster-instance discrimination to facilitate knowledge\ntransfer from the teacher model to the student model, thereby empowering the\nstudent model to acquire a holistic semantic comprehension of the pre-training\ndata. Experimental results demonstrate that CLIP-CID achieves state-of-the-art\nperformance on various downstream tasks including linear probe and zero-shot\nclassification.\n","authors":["Kaicheng Yang","Tiancheng Gu","Xiang An","Haiqiang Jiang","Xiangzi Dai","Ziyong Feng","Weidong Cai","Jiankang Deng"],"pdf_url":"https://arxiv.org/pdf/2408.09441v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.11668v1","updated":"2024-12-16T11:19:22Z","published":"2024-12-16T11:19:22Z","title":"Online Writer Retrieval with Chinese Handwritten Phrases: A Synergistic\n  Temporal-Frequency Representation Learning Approach","summary":"  Currently, the prevalence of online handwriting has spurred a critical need\nfor effective retrieval systems to accurately search relevant handwriting\ninstances from specific writers, known as online writer retrieval. Despite the\ngrowing demand, this field suffers from a scarcity of well-established\nmethodologies and public large-scale datasets. This paper tackles these\nchallenges with a focus on Chinese handwritten phrases. First, we propose\nDOLPHIN, a novel retrieval model designed to enhance handwriting\nrepresentations through synergistic temporal-frequency analysis. For frequency\nfeature learning, we propose the HFGA block, which performs gated\ncross-attention between the vanilla temporal handwriting sequence and its\nhigh-frequency sub-bands to amplify salient writing details. For temporal\nfeature learning, we propose the CAIR block, tailored to promote channel\ninteraction and reduce channel redundancy. Second, to address data deficit, we\nintroduce OLIWER, a large-scale online writer retrieval dataset encompassing\nover 670,000 Chinese handwritten phrases from 1,731 individuals. Through\nextensive evaluations, we demonstrate the superior performance of DOLPHIN over\nexisting methods. In addition, we explore cross-domain writer retrieval and\nreveal the pivotal role of increasing feature alignment in bridging the\ndistributional gap between different handwriting data. Our findings emphasize\nthe significance of point sampling frequency and pressure features in improving\nhandwriting representation quality and retrieval performance. Code and dataset\nare available at https://github.com/SCUT-DLVCLab/DOLPHIN.\n","authors":["Peirong Zhang","Lianwen Jin"],"pdf_url":"https://arxiv.org/pdf/2412.11668v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11663v1","updated":"2024-12-16T11:11:23Z","published":"2024-12-16T11:11:23Z","title":"LMM-Regularized CLIP Embeddings for Image Classification","summary":"  In this paper we deal with image classification tasks using the powerful CLIP\nvision-language model. Our goal is to advance the classification performance\nusing the CLIP's image encoder, by proposing a novel Large Multimodal Model\n(LMM) based regularization method. The proposed method uses an LMM to extract\nsemantic descriptions for the images of the dataset. Then, it uses the CLIP's\ntext encoder, frozen, in order to obtain the corresponding text embeddings and\ncompute the mean semantic class descriptions. Subsequently, we adapt the CLIP's\nimage encoder by adding a classification head, and we train it along with the\nimage encoder output, apart from the main classification objective, with an\nadditional auxiliary objective. The additional objective forces the embeddings\nat the image encoder's output to become similar to their corresponding\nLMM-generated mean semantic class descriptions. In this way, it produces\nembeddings with enhanced discrimination ability, leading to improved\nclassification performance. The effectiveness of the proposed regularization\nmethod is validated through extensive experiments on three image classification\ndatasets.\n","authors":["Maria Tzelepi","Vasileios Mezaris"],"pdf_url":"https://arxiv.org/pdf/2412.11663v1.pdf","comment":"Accepted for publication, 26th Int. Symp. on Multimedia (IEEE ISM\n  2024), Tokyo, Japan, Dec. 2024. This is the authors' \"accepted version\""},{"id":"http://arxiv.org/abs/2411.14280v2","updated":"2024-12-16T11:06:19Z","published":"2024-11-21T16:33:35Z","title":"EasyHOI: Unleashing the Power of Large Models for Reconstructing\n  Hand-Object Interactions in the Wild","summary":"  Our work aims to reconstruct hand-object interactions from a single-view\nimage, which is a fundamental but ill-posed task. Unlike methods that\nreconstruct from videos, multi-view images, or predefined 3D templates,\nsingle-view reconstruction faces significant challenges due to inherent\nambiguities and occlusions. These challenges are further amplified by the\ndiverse nature of hand poses and the vast variety of object shapes and sizes.\nOur key insight is that current foundational models for segmentation,\ninpainting, and 3D reconstruction robustly generalize to in-the-wild images,\nwhich could provide strong visual and geometric priors for reconstructing\nhand-object interactions. Specifically, given a single image, we first design a\nnovel pipeline to estimate the underlying hand pose and object shape using\noff-the-shelf large models. Furthermore, with the initial reconstruction, we\nemploy a prior-guided optimization scheme, which optimizes hand pose to comply\nwith 3D physical constraints and the 2D input image content. We perform\nexperiments across several datasets and show that our method consistently\noutperforms baselines and faithfully reconstructs a diverse set of hand-object\ninteractions. Here is the link of our project page:\nhttps://lym29.github.io/EasyHOI-page/\n","authors":["Yumeng Liu","Xiaoxiao Long","Zemin Yang","Yuan Liu","Marc Habermann","Christian Theobalt","Yuexin Ma","Wenping Wang"],"pdf_url":"https://arxiv.org/pdf/2411.14280v2.pdf","comment":"Project page: https://lym29.github.io/EasyHOI-page/"},{"id":"http://arxiv.org/abs/2412.11657v1","updated":"2024-12-16T11:00:02Z","published":"2024-12-16T11:00:02Z","title":"CNNtention: Can CNNs do better with Attention?","summary":"  Convolutional Neural Networks (CNNs) have been the standard for image\nclassification tasks for a long time, but more recently attention-based\nmechanisms have gained traction. This project aims to compare traditional CNNs\nwith attention-augmented CNNs across an image classification task. By\nevaluating and comparing their performance, accuracy and computational\nefficiency, the project will highlight benefits and trade-off of the localized\nfeature extraction of traditional CNNs and the global context capture in\nattention-augmented CNNs. By doing this, we can reveal further insights into\ntheir respective strengths and weaknesses, guide the selection of models based\non specific application needs and ultimately, enhance understanding of these\narchitectures in the deep learning community.\n  This was our final project for CS7643 Deep Learning course at Georgia Tech.\n","authors":["Julian Glattki","Nikhil Kapila","Tejas Rathi"],"pdf_url":"https://arxiv.org/pdf/2412.11657v1.pdf","comment":"10 pages, 11 figures"},{"id":"http://arxiv.org/abs/2407.15500v2","updated":"2024-12-16T10:53:08Z","published":"2024-07-22T09:31:30Z","title":"TextureCrop: Enhancing Synthetic Image Detection through Texture-based\n  Cropping","summary":"  Generative AI technologies produce increasingly realistic imagery, which,\ndespite its potential for creative applications, can also be misused to produce\nmisleading and harmful content. This renders Synthetic Image Detection (SID)\nmethods essential for identifying AI-generated content online. State-of-the-art\nSID methods typically resize or center-crop input images due to architectural\nor computational constraints, which hampers the detection of artifacts that\nappear in high-resolution images. To address this limitation, we propose\nTextureCrop, an image pre-processing component that can be plugged in any\npre-trained SID model to improve its performance. By focusing on high-frequency\nimage parts where generative artifacts are prevalent, TextureCrop enhances SID\nperformance with manageable memory requirements. Experimental results\ndemonstrate a consistent improvement in AUC across various detectors by 6.1%\ncompared to center cropping and by 15% compared to resizing, across\nhigh-resolution images from the Forensynths, Synthbuster and TWIGMA datasets.\n","authors":["Despina Konstantinidou","Christos Koutlis","Symeon Papadopoulos"],"pdf_url":"https://arxiv.org/pdf/2407.15500v2.pdf","comment":"10 pages, 6 images"},{"id":"http://arxiv.org/abs/2412.11650v1","updated":"2024-12-16T10:50:52Z","published":"2024-12-16T10:50:52Z","title":"Image Gradient-Aided Photometric Stereo Network","summary":"  Photometric stereo (PS) endeavors to ascertain surface normals using shading\nclues from photometric images under various illuminations. Recent deep\nlearning-based PS methods often overlook the complexity of object surfaces.\nThese neural network models, which exclusively rely on photometric images for\ntraining, often produce blurred results in high-frequency regions characterized\nby local discontinuities, such as wrinkles and edges with significant gradient\nchanges. To address this, we propose the Image Gradient-Aided Photometric\nStereo Network (IGA-PSN), a dual-branch framework extracting features from both\nphotometric images and their gradients. Furthermore, we incorporate an\nhourglass regression network along with supervision to regularize normal\nregression. Experiments on DiLiGenT benchmarks show that IGA-PSN outperforms\nprevious methods in surface normal estimation, achieving a mean angular error\nof 6.46 while preserving textures and geometric shapes in complex regions.\n","authors":["Kaixuan Wang","Lin Qi","Shiyu Qin","Kai Luo","Yakun Ju","Xia Li","Junyu Dong"],"pdf_url":"https://arxiv.org/pdf/2412.11650v1.pdf","comment":"13 pages, 5 figures, published to Springer"},{"id":"http://arxiv.org/abs/2412.09319v2","updated":"2024-12-16T10:45:46Z","published":"2024-12-12T14:44:05Z","title":"FAMNet: Frequency-aware Matching Network for Cross-domain Few-shot\n  Medical Image Segmentation","summary":"  Existing few-shot medical image segmentation (FSMIS) models fail to address a\npractical issue in medical imaging: the domain shift caused by different\nimaging techniques, which limits the applicability to current FSMIS tasks. To\novercome this limitation, we focus on the cross-domain few-shot medical image\nsegmentation (CD-FSMIS) task, aiming to develop a generalized model capable of\nadapting to a broader range of medical image segmentation scenarios with\nlimited labeled data from the novel target domain. Inspired by the\ncharacteristics of frequency domain similarity across different domains, we\npropose a Frequency-aware Matching Network (FAMNet), which includes two key\ncomponents: a Frequency-aware Matching (FAM) module and a Multi-Spectral Fusion\n(MSF) module. The FAM module tackles two problems during the meta-learning\nphase: 1) intra-domain variance caused by the inherent support-query bias, due\nto the different appearances of organs and lesions, and 2) inter-domain\nvariance caused by different medical imaging techniques. Additionally, we\ndesign an MSF module to integrate the different frequency features decoupled by\nthe FAM module, and further mitigate the impact of inter-domain variance on the\nmodel's segmentation performance. Combining these two modules, our FAMNet\nsurpasses existing FSMIS models and Cross-domain Few-shot Semantic Segmentation\nmodels on three cross-domain datasets, achieving state-of-the-art performance\nin the CD-FSMIS task.\n","authors":["Yuntian Bo","Yazhou Zhu","Lunbo Li","Haofeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.09319v2.pdf","comment":"Accepted by the 39th Annual AAAI Conference on Artificial\n  Intelligence (AAAI-25)"},{"id":"http://arxiv.org/abs/2411.13623v2","updated":"2024-12-16T10:34:06Z","published":"2024-11-20T13:12:43Z","title":"Unsupervised Foundation Model-Agnostic Slide-Level Representation\n  Learning","summary":"  Representation learning of pathology whole-slide images(WSIs) has primarily\nrelied on weak supervision with Multiple Instance Learning (MIL). This approach\nleads to slide representations highly tailored to a specific clinical task.\nSelf-supervised learning (SSL) has been successfully applied to train\nhistopathology foundation models (FMs) for patch embedding generation. However,\ngenerating patient or slide level embeddings remains challenging. Existing\napproaches for slide representation learning extend the principles of SSL from\npatch level learning to entire slides by aligning different augmentations of\nthe slide or by utilizing multimodal data. By integrating tile embeddings from\nmultiple FMs, we propose a new single modality SSL method in feature space that\ngenerates useful slide representations. Our contrastive pretraining strategy,\ncalled COBRA, employs multiple FMs and an architecture based on Mamba-2. COBRA\nexceeds performance of state-of-the-art slide encoders on four different public\nClinical Protemic Tumor Analysis Consortium (CPTAC) cohorts on average by at\nleast +4.5% AUC, despite only being pretrained on 3048 WSIs from The Cancer\nGenome Atlas (TCGA). Additionally, COBRA is readily compatible at inference\ntime with previously unseen feature extractors. Code available at\nhttps://github.com/KatherLab/COBRA.\n","authors":["Tim Lenz","Peter Neidlinger","Marta Ligero","Georg Wölflein","Marko van Treeck","Jakob Nikolas Kather"],"pdf_url":"https://arxiv.org/pdf/2411.13623v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11639v1","updated":"2024-12-16T10:33:10Z","published":"2024-12-16T10:33:10Z","title":"High-speed and High-quality Vision Reconstruction of Spike Camera with\n  Spike Stability Theorem","summary":"  Neuromorphic vision sensors, such as the dynamic vision sensor (DVS) and\nspike camera, have gained increasing attention in recent years. The spike\ncamera can detect fine textures by mimicking the fovea in the human visual\nsystem, and output a high-frequency spike stream. Real-time high-quality vision\nreconstruction from the spike stream can build a bridge to high-level vision\ntask applications of the spike camera. To realize high-speed and high-quality\nvision reconstruction of the spike camera, we propose a new spike stability\ntheorem that reveals the relationship between spike stream characteristics and\nstable light intensity. Based on the spike stability theorem, two\nparameter-free algorithms are designed for the real-time vision reconstruction\nof the spike camera. To demonstrate the performances of our algorithms, two\ndatasets (a public dataset PKU-Spike-High-Speed and a newly constructed dataset\nSpikeCityPCL) are used to compare the reconstruction quality and speed of\nvarious reconstruction methods. Experimental results show that, compared with\nthe current state-of-the-art (SOTA) reconstruction methods, our reconstruction\nmethods obtain the best tradeoff between the reconstruction quality and speed.\nAdditionally, we design the FPGA implementation method of our algorithms to\nrealize the real-time (running at 20,000 FPS) visual reconstruction. Our work\nprovides new theorem and algorithm foundations for the real-time edge-end\nvision processing of the spike camera.\n","authors":["Wei Zhang","Weiquan Yan","Yun Zhao","Wenxiang Cheng","Gang Chen","Huihui Zhou","Yonghong Tian"],"pdf_url":"https://arxiv.org/pdf/2412.11639v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08925v2","updated":"2024-12-16T10:28:02Z","published":"2024-10-11T15:50:31Z","title":"HyperPg -- Prototypical Gaussians on the Hypersphere for Interpretable\n  Deep Learning","summary":"  Prototype Learning methods provide an interpretable alternative to black-box\ndeep learning models. Approaches such as ProtoPNet learn, which part of a test\nimage \"look like\" known prototypical parts from training images, combining\npredictive power with the inherent interpretability of case-based reasoning.\nHowever, existing approaches have two main drawbacks: A) They rely solely on\ndeterministic similarity scores without statistical confidence. B) The\nprototypes are learned in a black-box manner without human input. This work\nintroduces HyperPg, a new prototype representation leveraging Gaussian\ndistributions on a hypersphere in latent space, with learnable mean and\nvariance. HyperPg prototypes adapt to the spread of clusters in the latent\nspace and output likelihood scores. The new architecture, HyperPgNet, leverages\nHyperPg to learn prototypes aligned with human concepts from pixel-level\nannotations. Consequently, each prototype represents a specific concept such as\ncolor, image texture, or part of the image subject. A concept extraction\npipeline built on foundation models provides pixel-level annotations,\nsignificantly reducing human labeling effort. Experiments on CUB-200-2011 and\nStanford Cars datasets demonstrate that HyperPgNet outperforms other prototype\nlearning architectures while using fewer parameters and training steps.\nAdditionally, the concept-aligned HyperPg prototypes are learned transparently,\nenhancing model interpretability.\n","authors":["Maximilian Xiling Li","Korbinian Franz Rudolf","Nils Blank","Rudolf Lioutikov"],"pdf_url":"https://arxiv.org/pdf/2410.08925v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11638v1","updated":"2024-12-16T10:27:48Z","published":"2024-12-16T10:27:48Z","title":"IDProtector: An Adversarial Noise Encoder to Protect Against\n  ID-Preserving Image Generation","summary":"  Recently, zero-shot methods like InstantID have revolutionized\nidentity-preserving generation. Unlike multi-image finetuning approaches such\nas DreamBooth, these zero-shot methods leverage powerful facial encoders to\nextract identity information from a single portrait photo, enabling efficient\nidentity-preserving generation through a single inference pass. However, this\nconvenience introduces new threats to the facial identity protection. This\npaper aims to safeguard portrait photos from unauthorized encoder-based\ncustomization. We introduce IDProtector, an adversarial noise encoder that\napplies imperceptible adversarial noise to portrait photos in a single forward\npass. Our approach offers universal protection for portraits against multiple\nstate-of-the-art encoder-based methods, including InstantID, IP-Adapter, and\nPhotoMaker, while ensuring robustness to common image transformations such as\nJPEG compression, resizing, and affine transformations. Experiments across\ndiverse portrait datasets and generative models reveal that IDProtector\ngeneralizes effectively to unseen data and even closed-source proprietary\nmodels.\n","authors":["Yiren Song","Pei Yang","Hai Ci","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2412.11638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17821v2","updated":"2024-12-16T10:27:35Z","published":"2024-05-28T04:41:02Z","title":"RITUAL: Random Image Transformations as a Universal Anti-hallucination\n  Lever in Large Vision Language Models","summary":"  Recent advancements in Large Vision Language Models (LVLMs) have\nrevolutionized how machines understand and generate textual responses based on\nvisual inputs, yet they often produce \"hallucinatory\" outputs that misinterpret\nvisual information, posing challenges in reliability and trustworthiness. We\npropose RITUAL, a simple decoding method that reduces hallucinations by\nleveraging randomly transformed images as complementary inputs during decoding,\nadjusting the output probability distribution without additional training or\nexternal models. Our key insight is that random transformations expose the\nmodel to diverse visual perspectives, enabling it to correct misinterpretations\nthat lead to hallucinations. Specifically, when a model hallucinates based on\nthe original image, the transformed images -- altered in aspects such as\norientation, scale, or color -- provide alternative viewpoints that help\nrecalibrate the model's predictions. By integrating the probability\ndistributions from both the original and transformed images, RITUAL effectively\nreduces hallucinations. To further improve reliability and address potential\ninstability from arbitrary transformations, we introduce RITUAL+, an extension\nthat selects image transformations based on self-feedback from the LVLM.\nInstead of applying transformations randomly, RITUAL+ uses the LVLM to evaluate\nand choose transformations that are most beneficial for reducing hallucinations\nin a given context. This self-adaptive approach mitigates the potential\nnegative impact of certain transformations on specific tasks, ensuring more\nconsistent performance across different scenarios. Experiments demonstrate that\nRITUAL and RITUAL+ significantly reduce hallucinations across several object\nhallucination benchmarks.\n","authors":["Sangmin Woo","Jaehyuk Jang","Donguk Kim","Yubin Choi","Changick Kim"],"pdf_url":"https://arxiv.org/pdf/2405.17821v2.pdf","comment":"Project: https://sangminwoo.github.io/RITUAL/"},{"id":"http://arxiv.org/abs/2412.11634v1","updated":"2024-12-16T10:25:03Z","published":"2024-12-16T10:25:03Z","title":"Predicting the Original Appearance of Damaged Historical Documents","summary":"  Historical documents encompass a wealth of cultural treasures but suffer from\nsevere damages including character missing, paper damage, and ink erosion over\ntime. However, existing document processing methods primarily focus on\nbinarization, enhancement, etc., neglecting the repair of these damages. To\nthis end, we present a new task, termed Historical Document Repair (HDR), which\naims to predict the original appearance of damaged historical documents. To\nfill the gap in this field, we propose a large-scale dataset HDR28K and a\ndiffusion-based network DiffHDR for historical document repair. Specifically,\nHDR28K contains 28,552 damaged-repaired image pairs with character-level\nannotations and multi-style degradations. Moreover, DiffHDR augments the\nvanilla diffusion framework with semantic and spatial information and a\nmeticulously designed character perceptual loss for contextual and visual\ncoherence. Experimental results demonstrate that the proposed DiffHDR trained\nusing HDR28K significantly surpasses existing approaches and exhibits\nremarkable performance in handling real damaged documents. Notably, DiffHDR can\nalso be extended to document editing and text block generation, showcasing its\nhigh flexibility and generalization capacity. We believe this study could\npioneer a new direction of document processing and contribute to the\ninheritance of invaluable cultures and civilizations. The dataset and code is\navailable at https://github.com/yeungchenwa/HDR.\n","authors":["Zhenhua Yang","Dezhi Peng","Yongxin Shi","Yuyi Zhang","Chongyu Liu","Lianwen Jin"],"pdf_url":"https://arxiv.org/pdf/2412.11634v1.pdf","comment":"Accepted to AAAI 2025; Github Page:\n  https://github.com/yeungchenwa/HDR"},{"id":"http://arxiv.org/abs/2408.05092v2","updated":"2024-12-16T10:10:10Z","published":"2024-08-09T14:33:34Z","title":"PriPHiT: Privacy-Preserving Hierarchical Training of Deep Neural\n  Networks","summary":"  The training phase of deep neural networks requires substantial resources and\nas such is often performed on cloud servers. However, this raises privacy\nconcerns when the training dataset contains sensitive content, e.g., facial or\nmedical images. In this work, we propose a method to perform the training phase\nof a deep learning model on both an edge device and a cloud server that\nprevents sensitive content being transmitted to the cloud while retaining the\ndesired information. The proposed privacy-preserving method uses adversarial\nearly exits to suppress the sensitive content at the edge and transmits the\ntask-relevant information to the cloud. This approach incorporates noise\naddition during the training phase to provide a differential privacy guarantee.\nWe extensively test our method on different facial and medical datasets with\ndiverse attributes using various deep learning architectures, showcasing its\noutstanding performance. We also demonstrate the effectiveness of privacy\npreservation through successful defenses against different white-box, deep and\nGAN-based reconstruction attacks. This approach is designed for\nresource-constrained edge devices, ensuring minimal memory usage and\ncomputational overhead.\n","authors":["Yamin Sepehri","Pedram Pad","Pascal Frossard","L. Andrea Dunbar"],"pdf_url":"https://arxiv.org/pdf/2408.05092v2.pdf","comment":"21 pages, 19 figures, 11 tables"},{"id":"http://arxiv.org/abs/2412.11621v1","updated":"2024-12-16T10:08:38Z","published":"2024-12-16T10:08:38Z","title":"VG-TVP: Multimodal Procedural Planning via Visually Grounded Text-Video\n  Prompting","summary":"  Large Language Model (LLM)-based agents have shown promise in procedural\ntasks, but the potential of multimodal instructions augmented by texts and\nvideos to assist users remains under-explored. To address this gap, we propose\nthe Visually Grounded Text-Video Prompting (VG-TVP) method which is a novel\nLLM-empowered Multimodal Procedural Planning (MPP) framework. It generates\ncohesive text and video procedural plans given a specified high-level\nobjective. The main challenges are achieving textual and visual\ninformativeness, temporal coherence, and accuracy in procedural plans. VG-TVP\nleverages the zero-shot reasoning capability of LLMs, the video-to-text\ngeneration ability of the video captioning models, and the text-to-video\ngeneration ability of diffusion models. VG-TVP improves the interaction between\nmodalities by proposing a novel Fusion of Captioning (FoC) method and using\nText-to-Video Bridge (T2V-B) and Video-to-Text Bridge (V2T-B). They allow LLMs\nto guide the generation of visually-grounded text plans and textual-grounded\nvideo plans. To address the scarcity of datasets suitable for MPP, we have\ncurated a new dataset called Daily-Life Task Procedural Plans (Daily-PP). We\nconduct comprehensive experiments and benchmarks to evaluate human preferences\n(regarding textual and visual informativeness, temporal coherence, and plan\naccuracy). Our VG-TVP method outperforms unimodal baselines on the Daily-PP\ndataset.\n","authors":["Muhammet Furkan Ilaslan","Ali Koksal","Kevin Qinhong Lin","Burak Satar","Mike Zheng Shou","Qianli Xu"],"pdf_url":"https://arxiv.org/pdf/2412.11621v1.pdf","comment":"Accepted for The 39th Annual AAAI Conference on Artificial\n  Intelligence 2025 in Main Track, 19 pages, 24 figures"},{"id":"http://arxiv.org/abs/2412.11620v1","updated":"2024-12-16T10:07:15Z","published":"2024-12-16T10:07:15Z","title":"Combating Semantic Contamination in Learning with Label Noise","summary":"  Noisy labels can negatively impact the performance of deep neural networks.\nOne common solution is label refurbishment, which involves reconstructing noisy\nlabels through predictions and distributions. However, these methods may\nintroduce problematic semantic associations, a phenomenon that we identify as\nSemantic Contamination. Through an analysis of Robust LR, a representative\nlabel refurbishment method, we found that utilizing the logits of views for\nrefurbishment does not adequately balance the semantic information of\nindividual classes. Conversely, using the logits of models fails to maintain\nconsistent semantic relationships across models, which explains why label\nrefurbishment methods frequently encounter issues related to Semantic\nContamination. To address this issue, we propose a novel method called\nCollaborative Cross Learning, which utilizes semi-supervised learning on\nrefurbished labels to extract appropriate semantic associations from embeddings\nacross views and models. Experimental results show that our method outperforms\nexisting approaches on both synthetic and real-world noisy datasets,\neffectively mitigating the impact of label noise and Semantic Contamination.\n","authors":["Wenxiao Fan","Kan Li"],"pdf_url":"https://arxiv.org/pdf/2412.11620v1.pdf","comment":"AAAI2025"},{"id":"http://arxiv.org/abs/2308.14930v2","updated":"2024-12-16T10:04:38Z","published":"2023-08-28T23:08:32Z","title":"Application of Quantum Pre-Processing Filter for Binary Image\n  Classification with Small Samples","summary":"  Over the past few years, there has been significant interest in Quantum\nMachine Learning (QML) among researchers, as it has the potential to transform\nthe field of machine learning. Several models that exploit the properties of\nquantum mechanics have been developed for practical applications. In this\nstudy, we investigated the application of our previously proposed quantum\npre-processing filter (QPF) to binary image classification. We evaluated the\nQPF on four datasets: MNIST (handwritten digits), EMNIST (handwritten digits\nand alphabets), CIFAR-10 (photographic images) and GTSRB (real-life traffic\nsign images). Similar to our previous multi-class classification results, the\napplication of QPF improved the binary image classification accuracy using\nneural network against MNIST, EMNIST, and CIFAR-10 from 98.9% to 99.2%, 97.8%\nto 98.3%, and 71.2% to 76.1%, respectively, but degraded it against GTSRB from\n93.5% to 92.0%. We then applied QPF in cases using a smaller number of training\nand testing samples, i.e. 80 and 20 samples per class, respectively. In order\nto derive statistically stable results, we conducted the experiment with 100\ntrials choosing randomly different training and testing samples and averaging\nthe results. The result showed that the application of QPF did not improve the\nimage classification accuracy against MNIST and EMNIST but improved it against\nCIFAR-10 and GTSRB from 65.8% to 67.2% and 90.5% to 91.8%, respectively.\nFurther research will be conducted as part of future work to investigate the\npotential of QPF to assess the scalability of the proposed approach to larger\nand complex datasets.\n","authors":["Farina Riaz","Shahab Abdulla","Hajime Suzuki","Srinjoy Ganguly","Ravinesh C. Deo","Susan Hopkins"],"pdf_url":"https://arxiv.org/pdf/2308.14930v2.pdf","comment":"This paper is accepted by Journal of Data Science and Intelligent\n  Systems (JDSIS)"},{"id":"http://arxiv.org/abs/2412.08221v2","updated":"2024-12-16T09:54:46Z","published":"2024-12-11T09:17:39Z","title":"Generate Any Scene: Evaluating and Improving Text-to-Vision Generation\n  with Scene Graph Programming","summary":"  DALL-E and Sora have gained attention by producing implausible images, such\nas \"astronauts riding a horse in space.\" Despite the proliferation of\ntext-to-vision models that have inundated the internet with synthetic visuals,\nfrom images to 3D assets, current benchmarks predominantly evaluate these\nmodels on real-world scenes paired with captions. We introduce Generate Any\nScene, a framework that systematically enumerates scene graphs representing a\nvast array of visual scenes, spanning realistic to imaginative compositions.\nGenerate Any Scene leverages 'scene graph programming', a method for\ndynamically constructing scene graphs of varying complexity from a structured\ntaxonomy of visual elements. This taxonomy includes numerous objects,\nattributes, and relations, enabling the synthesis of an almost infinite variety\nof scene graphs. Using these structured representations, Generate Any Scene\ntranslates each scene graph into a caption, enabling scalable evaluation of\ntext-to-vision models through standard metrics. We conduct extensive\nevaluations across multiple text-to-image, text-to-video, and text-to-3D\nmodels, presenting key findings on model performance. We find that DiT-backbone\ntext-to-image models align more closely with input captions than UNet-backbone\nmodels. Text-to-video models struggle with balancing dynamics and consistency,\nwhile both text-to-video and text-to-3D models show notable gaps in human\npreference alignment. We demonstrate the effectiveness of Generate Any Scene by\nconducting three practical applications leveraging captions generated by\nGenerate Any Scene: 1) a self-improving framework where models iteratively\nenhance their performance using generated data, 2) a distillation process to\ntransfer specific strengths from proprietary models to open-source\ncounterparts, and 3) improvements in content moderation by identifying and\ngenerating challenging synthetic data.\n","authors":["Ziqi Gao","Weikai Huang","Jieyu Zhang","Aniruddha Kembhavi","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2412.08221v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06618v2","updated":"2024-12-16T09:52:32Z","published":"2024-10-09T07:14:49Z","title":"Text Proxy: Decomposing Retrieval from a 1-to-N Relationship into N\n  1-to-1 Relationships for Text-Video Retrieval","summary":"  Text-video retrieval (TVR) has seen substantial advancements in recent years,\nfueled by the utilization of pre-trained models and large language models\n(LLMs). Despite these advancements, achieving accurate matching in TVR remains\nchallenging due to inherent disparities between video and textual modalities\nand irregularities in data representation. In this paper, we propose\nText-Video-ProxyNet (TV-ProxyNet), a novel framework designed to decompose the\nconventional 1-to-N relationship of TVR into N distinct 1-to-1 relationships.\nBy replacing a single text query with a series of text proxies, TV-ProxyNet not\nonly broadens the query scope but also achieves a more precise expansion. Each\ntext proxy is crafted through a refined iterative process, controlled by\nmechanisms we term as the director and dash, which regulate the proxy's\ndirection and distance relative to the original text query. This setup not only\nfacilitates more precise semantic alignment but also effectively manages the\ndisparities and noise inherent in multimodal data. Our experiments on three\nrepresentative video-text retrieval benchmarks, MSRVTT, DiDeMo, and ActivityNet\nCaptions, demonstrate the effectiveness of TV-ProxyNet. The results show an\nimprovement of 2.0% to 3.3% in R@1 over the baseline. TV-ProxyNet achieved\nstate-of-the-art performance on MSRVTT and ActivityNet Captions, and a 2.0%\nimprovement on DiDeMo compared to existing methods, validating our approach's\nability to enhance semantic mapping and reduce error propensity.\n","authors":["Jian Xiao","Zhenzhen Hu","Jia Li","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2410.06618v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11609v1","updated":"2024-12-16T09:50:09Z","published":"2024-12-16T09:50:09Z","title":"CLIP-SR: Collaborative Linguistic and Image Processing for\n  Super-Resolution","summary":"  Convolutional Neural Networks (CNNs) have advanced Image Super-Resolution\n(SR), but most CNN-based methods rely solely on pixel-based transformations,\noften leading to artifacts and blurring, particularly with severe downsampling\n(e.g., 8x or 16x). Recent text-guided SR methods attempt to leverage textual\ninformation for enhanced detail, but they frequently struggle with effective\nalignment, resulting in inconsistent semantic coherence. To address these\nlimitations, we introduce a multi-modal semantic enhancement approach that\ncombines textual semantics with visual features, effectively tackling semantic\nmismatches and detail loss in highly degraded LR images. Our proposed\nmulti-modal collaborative framework enables the production of realistic and\nhigh-quality SR images at significant up-scaling factors. The framework\nintegrates text and image inputs, employing a prompt predictor, Text-Image\nFusion Block (TIFBlock), and Iterative Refinement Module alongside CLIP\n(Contrastive Language-Image Pretraining) features to guide a progressive\nenhancement process with fine-grained alignment. This alignment produces\nhigh-resolution outputs with crisp details and semantic coherence, even at\nlarge scaling factors. Through extensive comparative experiments and ablation\nstudies, we validate the effectiveness of our approach. Additionally, by\nincorporating textual semantic guidance, our technique enables a degree of\nsuper-resolution editability while maintaining semantic coherence.\n","authors":["Bingwen Hu","Heng Liu","Zhedong Zheng","Ping Liu"],"pdf_url":"https://arxiv.org/pdf/2412.11609v1.pdf","comment":"11 pages, 10 figures"},{"id":"http://arxiv.org/abs/2412.11608v1","updated":"2024-12-16T09:49:59Z","published":"2024-12-16T09:49:59Z","title":"Towards Adversarial Robustness of Model-Level Mixture-of-Experts\n  Architectures for Semantic Segmentation","summary":"  Vulnerability to adversarial attacks is a well-known deficiency of deep\nneural networks. Larger networks are generally more robust, and ensembling is\none method to increase adversarial robustness: each model's weaknesses are\ncompensated by the strengths of others. While an ensemble uses a deterministic\nrule to combine model outputs, a mixture of experts (MoE) includes an\nadditional learnable gating component that predicts weights for the outputs of\nthe expert models, thus determining their contributions to the final\nprediction. MoEs have been shown to outperform ensembles on specific tasks, yet\ntheir susceptibility to adversarial attacks has not been studied yet. In this\nwork, we evaluate the adversarial vulnerability of MoEs for semantic\nsegmentation of urban and highway traffic scenes. We show that MoEs are, in\nmost cases, more robust to per-instance and universal white-box adversarial\nattacks and can better withstand transfer attacks. Our code is available at\n\\url{https://github.com/KASTEL-MobilityLab/mixtures-of-experts/}.\n","authors":["Svetlana Pavlitska","Enrico Eisen","J. Marius Zöllner"],"pdf_url":"https://arxiv.org/pdf/2412.11608v1.pdf","comment":"Accepted for publication at ICMLA 2024"},{"id":"http://arxiv.org/abs/2309.08927v4","updated":"2024-12-16T09:49:16Z","published":"2023-09-16T08:46:59Z","title":"DynaMoN: Motion-Aware Fast and Robust Camera Localization for Dynamic\n  Neural Radiance Fields","summary":"  The accurate reconstruction of dynamic scenes with neural radiance fields is\nsignificantly dependent on the estimation of camera poses. Widely used\nstructure-from-motion pipelines encounter difficulties in accurately tracking\nthe camera trajectory when faced with separate dynamics of the scene content\nand the camera movement. To address this challenge, we propose Dynamic\nMotion-Aware Fast and Robust Camera Localization for Dynamic Neural Radiance\nFields (DynaMoN). DynaMoN utilizes semantic segmentation and generic motion\nmasks to handle dynamic content for initial camera pose estimation and\nstatics-focused ray sampling for fast and accurate novel-view synthesis. Our\nnovel iterative learning scheme switches between training the NeRF and updating\nthe pose parameters for an improved reconstruction and trajectory estimation\nquality. The proposed pipeline shows significant acceleration of the training\nprocess. We extensively evaluate our approach on two real-world dynamic\ndatasets, the TUM RGB-D dataset and the BONN RGB-D Dynamic dataset. DynaMoN\nimproves over the state-of-the-art both in terms of reconstruction quality and\ntrajectory accuracy. We plan to make our code public to enhance research in\nthis area.\n","authors":["Nicolas Schischka","Hannah Schieber","Mert Asim Karaoglu","Melih Görgülü","Florian Grötzner","Alexander Ladikos","Daniel Roth","Nassir Navab","Benjamin Busam"],"pdf_url":"https://arxiv.org/pdf/2309.08927v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11599v1","updated":"2024-12-16T09:37:52Z","published":"2024-12-16T09:37:52Z","title":"3D$^2$-Actor: Learning Pose-Conditioned 3D-Aware Denoiser for Realistic\n  Gaussian Avatar Modeling","summary":"  Advancements in neural implicit representations and differentiable rendering\nhave markedly improved the ability to learn animatable 3D avatars from sparse\nmulti-view RGB videos. However, current methods that map observation space to\ncanonical space often face challenges in capturing pose-dependent details and\ngeneralizing to novel poses. While diffusion models have demonstrated\nremarkable zero-shot capabilities in 2D image generation, their potential for\ncreating animatable 3D avatars from 2D inputs remains underexplored. In this\nwork, we introduce 3D$^2$-Actor, a novel approach featuring a pose-conditioned\n3D-aware human modeling pipeline that integrates iterative 2D denoising and 3D\nrectifying steps. The 2D denoiser, guided by pose cues, generates detailed\nmulti-view images that provide the rich feature set necessary for high-fidelity\n3D reconstruction and pose rendering. Complementing this, our Gaussian-based 3D\nrectifier renders images with enhanced 3D consistency through a two-stage\nprojection strategy and a novel local coordinate representation. Additionally,\nwe propose an innovative sampling strategy to ensure smooth temporal continuity\nacross frames in video synthesis. Our method effectively addresses the\nlimitations of traditional numerical solutions in handling ill-posed mappings,\nproducing realistic and animatable 3D human avatars. Experimental results\ndemonstrate that 3D$^2$-Actor excels in high-fidelity avatar modeling and\nrobustly generalizes to novel poses. Code is available at:\nhttps://github.com/silence-tang/GaussianActor.\n","authors":["Zichen Tang","Hongyu Yang","Hanchen Zhang","Jiaxin Chen","Di Huang"],"pdf_url":"https://arxiv.org/pdf/2412.11599v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2405.01124v5","updated":"2024-12-16T09:37:17Z","published":"2024-05-02T09:38:07Z","title":"Investigating Self-Supervised Image Denoising with Denaturation","summary":"  Self-supervised learning for image denoising problems in the presence of\ndenaturation for noisy data is a crucial approach in machine learning. However,\ntheoretical understanding of the performance of the approach that uses\ndenatured data is lacking. To provide better understanding of the approach, in\nthis paper, we analyze a self-supervised denoising algorithm that uses\ndenatured data in depth through theoretical analysis and numerical experiments.\nThrough the theoretical analysis, we discuss that the algorithm finds desired\nsolutions to the optimization problem with the population risk, while the\nguarantee for the empirical risk depends on the hardness of the denoising task\nin terms of denaturation levels. We also conduct several experiments to\ninvestigate the performance of an extended algorithm in practice. The results\nindicate that the algorithm training with denatured images works, and the\nempirical performance aligns with the theoretical results. These results\nsuggest several insights for further improvement of self-supervised image\ndenoising that uses denatured data in future directions.\n","authors":["Hiroki Waida","Kimihiro Yamazaki","Atsushi Tokuhisa","Mutsuyo Wada","Yuichiro Wada"],"pdf_url":"https://arxiv.org/pdf/2405.01124v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11596v1","updated":"2024-12-16T09:35:08Z","published":"2024-12-16T09:35:08Z","title":"MeshArt: Generating Articulated Meshes with Structure-guided\n  Transformers","summary":"  Articulated 3D object generation is fundamental for creating realistic,\nfunctional, and interactable virtual assets which are not simply static. We\nintroduce MeshArt, a hierarchical transformer-based approach to generate\narticulated 3D meshes with clean, compact geometry, reminiscent of\nhuman-crafted 3D models. We approach articulated mesh generation in a\npart-by-part fashion across two stages. First, we generate a high-level\narticulation-aware object structure; then, based on this structural\ninformation, we synthesize each part's mesh faces. Key to our approach is\nmodeling both articulation structures and part meshes as sequences of quantized\ntriangle embeddings, leading to a unified hierarchical framework with\ntransformers for autoregressive generation. Object part structures are first\ngenerated as their bounding primitives and articulation modes; a second\ntransformer, guided by these articulation structures, then generates each\npart's mesh triangles. To ensure coherency among generated parts, we introduce\nstructure-guided conditioning that also incorporates local part mesh\nconnectivity. MeshArt shows significant improvements over state of the art,\nwith 57.1% improvement in structure coverage and a 209-point improvement in\nmesh generation FID.\n","authors":["Daoyi Gao","Yawar Siddiqui","Lei Li","Angela Dai"],"pdf_url":"https://arxiv.org/pdf/2412.11596v1.pdf","comment":"Project Page: https://daoyig.github.io/Mesh_Art/"},{"id":"http://arxiv.org/abs/2412.11594v1","updated":"2024-12-16T09:32:23Z","published":"2024-12-16T09:32:23Z","title":"VersaGen: Unleashing Versatile Visual Control for Text-to-Image\n  Synthesis","summary":"  Despite the rapid advancements in text-to-image (T2I) synthesis, enabling\nprecise visual control remains a significant challenge. Existing works\nattempted to incorporate multi-facet controls (text and sketch), aiming to\nenhance the creative control over generated images. However, our pilot study\nreveals that the expressive power of humans far surpasses the capabilities of\ncurrent methods. Users desire a more versatile approach that can accommodate\ntheir diverse creative intents, ranging from controlling individual subjects to\nmanipulating the entire scene composition. We present VersaGen, a generative AI\nagent that enables versatile visual control in T2I synthesis. VersaGen admits\nfour types of visual controls: i) single visual subject; ii) multiple visual\nsubjects; iii) scene background; iv) any combination of the three above or\nmerely no control at all. We train an adaptor upon a frozen T2I model to\naccommodate the visual information into the text-dominated diffusion process.\nWe introduce three optimization strategies during the inference phase of\nVersaGen to improve generation results and enhance user experience.\nComprehensive experiments on COCO and Sketchy validate the effectiveness and\nflexibility of VersaGen, as evidenced by both qualitative and quantitative\nresults.\n","authors":["Zhipeng Chen","Lan Yang","Yonggang Qi","Honggang Zhang","Kaiyue Pang","Ke Li","Yi-Zhe Song"],"pdf_url":"https://arxiv.org/pdf/2412.11594v1.pdf","comment":"The paper has been accepted by AAAI 2025. Paper code:\n  https://github.com/FelixChan9527/VersaGen_official"},{"id":"http://arxiv.org/abs/2412.07147v2","updated":"2024-12-16T09:28:53Z","published":"2024-12-10T03:12:35Z","title":"MIT-10M: A Large Scale Parallel Corpus of Multilingual Image Translation","summary":"  Image Translation (IT) holds immense potential across diverse domains,\nenabling the translation of textual content within images into various\nlanguages. However, existing datasets often suffer from limitations in scale,\ndiversity, and quality, hindering the development and evaluation of IT models.\nTo address this issue, we introduce MIT-10M, a large-scale parallel corpus of\nmultilingual image translation with over 10M image-text pairs derived from\nreal-world data, which has undergone extensive data cleaning and multilingual\ntranslation validation. It contains 840K images in three sizes, 28 categories,\ntasks with three levels of difficulty and 14 languages image-text pairs, which\nis a considerable improvement on existing datasets. We conduct extensive\nexperiments to evaluate and train models on MIT-10M. The experimental results\nclearly indicate that our dataset has higher adaptability when it comes to\nevaluating the performance of the models in tackling challenging and complex\nimage translation tasks in the real world. Moreover, the performance of the\nmodel fine-tuned with MIT-10M has tripled compared to the baseline model,\nfurther confirming its superiority.\n","authors":["Bo Li","Shaolin Zhu","Lijie Wen"],"pdf_url":"https://arxiv.org/pdf/2412.07147v2.pdf","comment":"Accepted in COLING 2025"},{"id":"http://arxiv.org/abs/2412.06365v2","updated":"2024-12-16T09:25:31Z","published":"2024-12-09T10:35:39Z","title":"Is Self-Supervision Enough? Benchmarking Foundation Models Against\n  End-to-End Training for Mitotic Figure Classification","summary":"  Foundation models (FMs), i.e., models trained on a vast amount of typically\nunlabeled data, have become popular and available recently for the domain of\nhistopathology. The key idea is to extract semantically rich vectors from any\ninput patch, allowing for the use of simple subsequent classification networks\npotentially reducing the required amounts of labeled data, and increasing\ndomain robustness. In this work, we investigate to which degree this also holds\nfor mitotic figure classification. Utilizing two popular public mitotic figure\ndatasets, we compared linear probing of five publicly available FMs against\nmodels trained on ImageNet and a simple ResNet50 end-to-end-trained baseline.\nWe found that the end-to-end-trained baseline outperformed all FM-based\nclassifiers, regardless of the amount of data provided. Additionally, we did\nnot observe the FM-based classifiers to be more robust against domain shifts,\nrendering both of the above assumptions incorrect.\n","authors":["Jonathan Ganz","Jonas Ammeling","Emely Rosbach","Ludwig Lausser","Christof A. Bertram","Katharina Breininger","Marc Aubreville"],"pdf_url":"https://arxiv.org/pdf/2412.06365v2.pdf","comment":"6 pages, 2 figures"},{"id":"http://arxiv.org/abs/2404.16471v6","updated":"2024-12-16T09:21:03Z","published":"2024-04-25T09:55:35Z","title":"COBRA -- COnfidence score Based on shape Regression Analysis for\n  method-independent quality assessment of object pose estimation from single\n  images","summary":"  We propose a generic procedure for assessing 6D object pose estimates. Our\napproach relies on the evaluation of discrepancies in the geometry of the\nobserved object, in particular its respective estimated back-projection in 3D,\nagainst a putative functional shape representation comprising mixtures of\nGaussian Processes, that act as a template. Each Gaussian Process is trained to\nyield a fragment of the object's surface in a radial fashion with respect to\ndesignated reference points. We further define a pose confidence measure as the\naverage probability of pixel back-projections in the Gaussian mixture. The goal\nof our experiments is two-fold. a) We demonstrate that our functional\nrepresentation is sufficiently accurate as a shape template on which the\nprobability of back-projected object points can be evaluated, and, b) we show\nthat the resulting confidence scores based on these probabilities are indeed a\nconsistent quality measure of pose.\n","authors":["Panagiotis Sapoutzoglou","Georgios Giapitzakis","Georgios Floros","George Terzakis","Maria Pateraki"],"pdf_url":"https://arxiv.org/pdf/2404.16471v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.09126v5","updated":"2024-12-16T09:18:51Z","published":"2024-08-17T07:27:14Z","title":"Barbie: Text to Barbie-Style 3D Avatars","summary":"  Recent advances in text-guided 3D avatar generation have made substantial\nprogress by distilling knowledge from diffusion models. Despite the plausible\ngenerated appearance, existing methods cannot achieve fine-grained\ndisentanglement or high-fidelity modeling between inner body and outfit. In\nthis paper, we propose Barbie, a novel framework for generating 3D avatars that\ncan be dressed in diverse and high-quality Barbie-like garments and\naccessories. Instead of relying on a holistic model, Barbie achieves\nfine-grained disentanglement on avatars by semantic-aligned separated models\nfor human body and outfits. These disentangled 3D representations are then\noptimized by different expert models to guarantee the domain-specific fidelity.\nTo balance geometry diversity and reasonableness, we propose a series of losses\nfor template-preserving and human-prior evolving. The final avatar is enhanced\nby unified texture refinement for superior texture consistency. Extensive\nexperiments demonstrate that Barbie outperforms existing methods in both\ndressed human and outfit generation, supporting flexible apparel combination\nand animation. Our project page is:\nhttps://xiaokunsun.github.io/Barbie.github.io.\n","authors":["Xiaokun Sun","Zhenyu Zhang","Ying Tai","Qian Wang","Hao Tang","Zili Yi","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2408.09126v5.pdf","comment":"Project page: https://xiaokunsun.github.io/Barbie.github.io"},{"id":"http://arxiv.org/abs/2405.03959v4","updated":"2024-12-16T09:18:28Z","published":"2024-05-07T02:45:50Z","title":"Joint Identity Verification and Pose Alignment for Partial Fingerprints","summary":"  Currently, portable electronic devices are becoming more and more popular.\nFor lightweight considerations, their fingerprint recognition modules usually\nuse limited-size sensors. However, partial fingerprints have few matchable\nfeatures, especially when there are differences in finger pressing posture or\nimage quality, which makes partial fingerprint verification challenging. Most\nexisting methods regard fingerprint position rectification and identity\nverification as independent tasks, ignoring the coupling relationship between\nthem -- relative pose estimation typically relies on paired features as\nanchors, and authentication accuracy tends to improve with more precise pose\nalignment. In this paper, we propose a novel framework for joint identity\nverification and pose alignment of partial fingerprint pairs, aiming to\nleverage their inherent correlation to improve each other. To achieve this, we\npresent a multi-task CNN (Convolutional Neural Network)-Transformer hybrid\nnetwork, and design a pre-training task to enhance the feature extraction\ncapability. Experiments on multiple public datasets (NIST SD14, FVC2002 DB1A &\nDB3A, FVC2004 DB1A & DB2A, FVC2006 DB1A) and an in-house dataset show that our\nmethod achieves state-of-the-art performance in both partial fingerprint\nverification and relative pose estimation, while being more efficient than\nprevious methods.\n","authors":["Xiongjun Guan","Zhiyu Pan","Jianjiang Feng","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2405.03959v4.pdf","comment":"15 pages, in IEEE Transactions on Information Forensics and Security,\n  2024"},{"id":"http://arxiv.org/abs/2412.11586v1","updated":"2024-12-16T09:17:36Z","published":"2024-12-16T09:17:36Z","title":"StrandHead: Text to Strand-Disentangled 3D Head Avatars Using Hair\n  Geometric Priors","summary":"  While haircut indicates distinct personality, existing avatar generation\nmethods fail to model practical hair due to the general or entangled\nrepresentation. We propose StrandHead, a novel text to 3D head avatar\ngeneration method capable of generating disentangled 3D hair with strand\nrepresentation. Without using 3D data for supervision, we demonstrate that\nrealistic hair strands can be generated from prompts by distilling 2D\ngenerative diffusion models. To this end, we propose a series of reliable\npriors on shape initialization, geometric primitives, and statistical haircut\nfeatures, leading to a stable optimization and text-aligned performance.\nExtensive experiments show that StrandHead achieves the state-of-the-art\nreality and diversity of generated 3D head and hair. The generated 3D hair can\nalso be easily implemented in the Unreal Engine for physical simulation and\nother applications. The code will be available at\nhttps://xiaokunsun.github.io/StrandHead.github.io.\n","authors":["Xiaokun Sun","Zeyu Cai","Zhenyu Zhang","Ying Tai","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2412.11586v1.pdf","comment":"Project page: https://xiaokunsun.github.io/StrandHead.github.io"},{"id":"http://arxiv.org/abs/2412.05271v2","updated":"2024-12-16T09:14:43Z","published":"2024-12-06T18:57:08Z","title":"Expanding Performance Boundaries of Open-Source Multimodal Models with\n  Model, Data, and Test-Time Scaling","summary":"  We introduce InternVL 2.5, an advanced multimodal large language model (MLLM)\nseries that builds upon InternVL 2.0, maintaining its core model architecture\nwhile introducing significant enhancements in training and testing strategies\nas well as data quality. In this work, we delve into the relationship between\nmodel scaling and performance, systematically exploring the performance trends\nin vision encoders, language models, dataset sizes, and test-time\nconfigurations. Through extensive evaluations on a wide range of benchmarks,\nincluding multi-discipline reasoning, document understanding, multi-image /\nvideo understanding, real-world comprehension, multimodal hallucination\ndetection, visual grounding, multilingual capabilities, and pure language\nprocessing, InternVL 2.5 exhibits competitive performance, rivaling leading\ncommercial models such as GPT-4o and Claude-3.5-Sonnet. Notably, our model is\nthe first open-source MLLMs to surpass 70% on the MMMU benchmark, achieving a\n3.7-point improvement through Chain-of-Thought (CoT) reasoning and showcasing\nstrong potential for test-time scaling. We hope this model contributes to the\nopen-source community by setting new standards for developing and applying\nmultimodal AI systems. HuggingFace demo see\nhttps://huggingface.co/spaces/OpenGVLab/InternVL\n","authors":["Zhe Chen","Weiyun Wang","Yue Cao","Yangzhou Liu","Zhangwei Gao","Erfei Cui","Jinguo Zhu","Shenglong Ye","Hao Tian","Zhaoyang Liu","Lixin Gu","Xuehui Wang","Qingyun Li","Yimin Ren","Zixuan Chen","Jiapeng Luo","Jiahao Wang","Tan Jiang","Bo Wang","Conghui He","Botian Shi","Xingcheng Zhang","Han Lv","Yi Wang","Wenqi Shao","Pei Chu","Zhongying Tu","Tong He","Zhiyong Wu","Huipeng Deng","Jiaye Ge","Kai Chen","Min Dou","Lewei Lu","Xizhou Zhu","Tong Lu","Dahua Lin","Yu Qiao","Jifeng Dai","Wenhai Wang"],"pdf_url":"https://arxiv.org/pdf/2412.05271v2.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2412.11582v1","updated":"2024-12-16T09:14:32Z","published":"2024-12-16T09:14:32Z","title":"Oriented Tiny Object Detection: A Dataset, Benchmark, and Dynamic\n  Unbiased Learning","summary":"  Detecting oriented tiny objects, which are limited in appearance information\nyet prevalent in real-world applications, remains an intricate and\nunder-explored problem. To address this, we systemically introduce a new\ndataset, benchmark, and a dynamic coarse-to-fine learning scheme in this study.\nOur proposed dataset, AI-TOD-R, features the smallest object sizes among all\noriented object detection datasets. Based on AI-TOD-R, we present a benchmark\nspanning a broad range of detection paradigms, including both fully-supervised\nand label-efficient approaches. Through investigation, we identify a learning\nbias presents across various learning pipelines: confident objects become\nincreasingly confident, while vulnerable oriented tiny objects are further\nmarginalized, hindering their detection performance. To mitigate this issue, we\npropose a Dynamic Coarse-to-Fine Learning (DCFL) scheme to achieve unbiased\nlearning. DCFL dynamically updates prior positions to better align with the\nlimited areas of oriented tiny objects, and it assigns samples in a way that\nbalances both quantity and quality across different object shapes, thus\nmitigating biases in prior settings and sample selection. Extensive experiments\nacross eight challenging object detection datasets demonstrate that DCFL\nachieves state-of-the-art accuracy, high efficiency, and remarkable\nversatility. The dataset, benchmark, and code are available at\nhttps://chasel-tsui.github.io/AI-TOD-R/.\n","authors":["Chang Xu","Ruixiang Zhang","Wen Yang","Haoran Zhu","Fang Xu","Jian Ding","Gui-Song Xia"],"pdf_url":"https://arxiv.org/pdf/2412.11582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11579v1","updated":"2024-12-16T09:09:42Z","published":"2024-12-16T09:09:42Z","title":"SweepEvGS: Event-Based 3D Gaussian Splatting for Macro and Micro\n  Radiance Field Rendering from a Single Sweep","summary":"  Recent advancements in 3D Gaussian Splatting (3D-GS) have demonstrated the\npotential of using 3D Gaussian primitives for high-speed, high-fidelity, and\ncost-efficient novel view synthesis from continuously calibrated input views.\nHowever, conventional methods require high-frame-rate dense and high-quality\nsharp images, which are time-consuming and inefficient to capture, especially\nin dynamic environments. Event cameras, with their high temporal resolution and\nability to capture asynchronous brightness changes, offer a promising\nalternative for more reliable scene reconstruction without motion blur. In this\npaper, we propose SweepEvGS, a novel hardware-integrated method that leverages\nevent cameras for robust and accurate novel view synthesis across various\nimaging settings from a single sweep. SweepEvGS utilizes the initial static\nframe with dense event streams captured during a single camera sweep to\neffectively reconstruct detailed scene views. We also introduce different\nreal-world hardware imaging systems for real-world data collection and\nevaluation for future research. We validate the robustness and efficiency of\nSweepEvGS through experiments in three different imaging settings: synthetic\nobjects, real-world macro-level, and real-world micro-level view synthesis. Our\nresults demonstrate that SweepEvGS surpasses existing methods in visual\nrendering quality, rendering speed, and computational efficiency, highlighting\nits potential for dynamic practical applications.\n","authors":["Jingqian Wu","Shuo Zhu","Chutian Wang","Boxin Shi","Edmund Y. Lam"],"pdf_url":"https://arxiv.org/pdf/2412.11579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11578v1","updated":"2024-12-16T09:09:10Z","published":"2024-12-16T09:09:10Z","title":"DVP-MVS: Synergize Depth-Edge and Visibility Prior for Multi-View Stereo","summary":"  Patch deformation-based methods have recently exhibited substantial\neffectiveness in multi-view stereo, due to the incorporation of deformable and\nexpandable perception to reconstruct textureless areas. However, such\napproaches typically focus on exploring correlative reliable pixels to\nalleviate match ambiguity during patch deformation, but ignore the deformation\ninstability caused by mistaken edge-skipping and visibility occlusion, leading\nto potential estimation deviation. To remedy the above issues, we propose\nDVP-MVS, which innovatively synergizes depth-edge aligned and cross-view prior\nfor robust and visibility-aware patch deformation. Specifically, to avoid\nunexpected edge-skipping, we first utilize Depth Anything V2 followed by the\nRoberts operator to initialize coarse depth and edge maps respectively, both of\nwhich are further aligned through an erosion-dilation strategy to generate\nfine-grained homogeneous boundaries for guiding patch deformation. In addition,\nwe reform view selection weights as visibility maps and restore visible areas\nby cross-view depth reprojection, then regard them as cross-view prior to\nfacilitate visibility-aware patch deformation. Finally, we improve propagation\nand refinement with multi-view geometry consistency by introducing aggregated\nvisible hemispherical normals based on view selection and local projection\ndepth differences based on epipolar lines, respectively. Extensive evaluations\non ETH3D and Tanks & Temples benchmarks demonstrate that our method can achieve\nstate-of-the-art performance with excellent robustness and generalization.\n","authors":["Zhenlong Yuan","Jinguo Luo","Fei Shen","Zhaoxin Li","Cong Liu","Tianlu Mao","Zhaoqi Wang"],"pdf_url":"https://arxiv.org/pdf/2412.11578v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11576v1","updated":"2024-12-16T09:04:58Z","published":"2024-12-16T09:04:58Z","title":"Aligning Visual and Semantic Interpretability through Visually Grounded\n  Concept Bottleneck Models","summary":"  The performance of neural networks increases steadily, but our understanding\nof their decision-making lags behind. Concept Bottleneck Models (CBMs) address\nthis issue by incorporating human-understandable concepts into the prediction\nprocess, thereby enhancing transparency and interpretability. Since existing\napproaches often rely on large language models (LLMs) to infer concepts, their\nresults may contain inaccurate or incomplete mappings, especially in complex\nvisual domains. We introduce visually Grounded Concept Bottleneck Models\n(GCBM), which derive concepts on the image level using segmentation and\ndetection foundation models. Our method generates inherently interpretable\nconcepts, which can be grounded in the input image using attribution methods,\nallowing interpretations to be traced back to the image plane. We show that\nGCBM concepts are meaningful interpretability vehicles, which aid our\nunderstanding of model embedding spaces. GCBMs allow users to control the\ngranularity, number, and naming of concepts, providing flexibility and are\neasily adaptable to new datasets without pre-training or additional data\nneeded. Prediction accuracy is within 0.3-6% of the linear probe and GCBMs\nperform especially well for fine-grained classification interpretability on\nCUB, due to their dataset specificity. Our code is available on\nhttps://github.com/KathPra/GCBM.\n","authors":["Patrick Knab","Katharina Prasse","Sascha Marton","Christian Bartelt","Margret Keuper"],"pdf_url":"https://arxiv.org/pdf/2412.11576v1.pdf","comment":"*Equal contribution"},{"id":"http://arxiv.org/abs/2407.00676v2","updated":"2024-12-16T09:04:24Z","published":"2024-06-30T12:13:34Z","title":"Instruct-IPT: All-in-One Image Processing Transformer via Weight\n  Modulation","summary":"  Due to the unaffordable size and intensive computation costs of low-level\nvision models, All-in-One models that are designed to address a handful of\nlow-level vision tasks simultaneously have been popular. However, existing\nAll-in-One models are limited in terms of the range of tasks and performance.\nTo overcome these limitations, we propose Instruct-IPT -- an All-in-One Image\nProcessing Transformer (IPT) that could effectively address manifold image\nrestoration tasks with large inter-task gaps, such as denoising, deblurring,\nderaining, dehazing, and desnowing. While most research propose feature\nadaptation methods, we reveal their failure in addressing highly distinct\ntasks, and suggest weight modulation that adapts weights to specific tasks.\nFirstly, we search for task-sensitive weights and introduce task-specific\nbiases on top of them. Secondly, we conduct rank analysis for a good\ncompression strategy and perform low-rank decomposition on the biases. Thirdly,\nwe propose synchronous training that updates the task-general backbone model\nand the task-specific biases simultaneously. In this way, the model is\ninstructed to learn both general and task-specific knowledge. Via our simple\nyet effective method that instructs the IPT to be task experts, Instruct-IPT\ncould better cooperate between tasks with distinct characteristics at humble\ncosts. As an additional feature, we enable Instruct-IPT to receive human\nprompts. We have conducted experiments on Instruct-IPT to demonstrate the\neffectiveness of our method on manifold tasks, and we have effectively extended\nour method to diffusion denoisers as well. The code is available at\nhttps://github.com/huawei-noah/Pretrained-IPT.\n","authors":["Yuchuan Tian","Jianhong Han","Hanting Chen","Yuanyuan Xi","Ning Ding","Jie Hu","Chao Xu","Yunhe Wang"],"pdf_url":"https://arxiv.org/pdf/2407.00676v2.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2412.11574v1","updated":"2024-12-16T09:01:32Z","published":"2024-12-16T09:01:32Z","title":"PyPotteryLens: An Open-Source Deep Learning Framework for Automated\n  Digitisation of Archaeological Pottery Documentation","summary":"  Archaeological pottery documentation and study represents a crucial but\ntime-consuming aspect of archaeology. While recent years have seen advances in\ndigital documentation methods, vast amounts of legacy data remain locked in\ntraditional publications. This paper introduces PyPotteryLens, an open-source\nframework that leverages deep learning to automate the digitisation and\nprocessing of archaeological pottery drawings from published sources. The\nsystem combines state-of-the-art computer vision models (YOLO for instance\nsegmentation and EfficientNetV2 for classification) with an intuitive user\ninterface, making advanced digital methods accessible to archaeologists\nregardless of technical expertise. The framework achieves over 97\\% precision\nand recall in pottery detection and classification tasks, while reducing\nprocessing time by up to 5x to 20x compared to manual methods. Testing across\ndiverse archaeological contexts demonstrates robust generalisation\ncapabilities. Also, the system's modular architecture facilitates extension to\nother archaeological materials, while its standardised output format ensures\nlong-term preservation and reusability of digitised data as well as solid basis\nfor training machine learning algorithms. The software, documentation, and\nexamples are available on GitHub\n(https://github.com/lrncrd/PyPottery/tree/PyPotteryLens).\n","authors":["Lorenzo Cardarelli"],"pdf_url":"https://arxiv.org/pdf/2412.11574v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10261v2","updated":"2024-12-16T08:54:43Z","published":"2024-12-13T16:30:35Z","title":"MVQ:Towards Efficient DNN Compression and Acceleration with Masked\n  Vector Quantization","summary":"  Vector quantization(VQ) is a hardware-friendly DNN compression method that\ncan reduce the storage cost and weight-loading datawidth of hardware\naccelerators. However, conventional VQ techniques lead to significant accuracy\nloss because the important weights are not well preserved. To tackle this\nproblem, a novel approach called MVQ is proposed, which aims at better\napproximating important weights with a limited number of codewords. At the\nalgorithm level, our approach removes the less important weights through N:M\npruning and then minimizes the vector clustering error between the remaining\nweights and codewords by the masked k-means algorithm. Only distances between\nthe unpruned weights and the codewords are computed, which are then used to\nupdate the codewords. At the architecture level, our accelerator implements\nvector quantization on an EWS (Enhanced weight stationary) CNN accelerator and\nproposes a sparse systolic array design to maximize the benefits brought by\nmasked vector quantization.\\\\ Our algorithm is validated on various models for\nimage classification, object detection, and segmentation tasks. Experimental\nresults demonstrate that MVQ not only outperforms conventional vector\nquantization methods at comparable compression ratios but also reduces FLOPs.\nUnder ASIC evaluation, our MVQ accelerator boosts energy efficiency by\n2.3$\\times$ and reduces the size of the systolic array by 55\\% when compared\nwith the base EWS accelerator. Compared to the previous sparse accelerators,\nMVQ achieves 1.73$\\times$ higher energy efficiency.\n","authors":["Shuaiting Li","Chengxuan Wang","Juncan Deng","Zeyu Wang","Zewen Ye","Zongsheng Wang","Haibin Shen","Kejie Huang"],"pdf_url":"https://arxiv.org/pdf/2412.10261v2.pdf","comment":"Accepted by ASPLOS '25"},{"id":"http://arxiv.org/abs/2412.11561v1","updated":"2024-12-16T08:47:55Z","published":"2024-12-16T08:47:55Z","title":"RADARSAT Constellation Mission Compact Polarisation SAR Data for Burned\n  Area Mapping with Deep Learning","summary":"  Monitoring wildfires has become increasingly critical due to the sharp rise\nin wildfire incidents in recent years. Optical satellites like Sentinel-2 and\nLandsat are extensively utilized for mapping burned areas. However, the\neffectiveness of optical sensors is compromised by clouds and smoke, which\nobstruct the detection of burned areas. Thus, satellites equipped with\nSynthetic Aperture Radar (SAR), such as dual-polarization Sentinel-1 and\nquad-polarization RADARSAT-1/-2 C-band SAR, which can penetrate clouds and\nsmoke, are investigated for mapping burned areas. However, there is limited\nresearch on using compact polarisation (compact-pol) C-band RADARSAT\nConstellation Mission (RCM) SAR data for this purpose. This study aims to\ninvestigate the capacity of compact polarisation RCM data for burned area\nmapping through deep learning. Compact-pol m-chi decomposition and Compact-pol\nRadar Vegetation Index (CpRVI) are derived from the RCM Multi-look Complex\nproduct. A deep-learning-based processing pipeline incorporating ConvNet-based\nand Transformer-based models is applied for burned area mapping, with three\ndifferent input settings: using only log-ratio dual-polarization intensity\nimages images, using only compact-pol decomposition plus CpRVI, and using all\nthree data sources. The results demonstrate that compact-pol m-chi\ndecomposition and CpRVI images significantly complement log-ratio images for\nburned area mapping. The best-performing Transformer-based model, UNETR,\ntrained with log-ratio, m-chi decomposition, and CpRVI data, achieved an F1\nScore of 0.718 and an IoU Score of 0.565, showing a notable improvement\ncompared to the same model trained using only log-ratio images.\n","authors":["Yu Zhao","Yifang Ban"],"pdf_url":"https://arxiv.org/pdf/2412.11561v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11555v1","updated":"2024-12-16T08:40:12Z","published":"2024-12-16T08:40:12Z","title":"TS-SatFire: A Multi-Task Satellite Image Time-Series Dataset for\n  Wildfire Detection and Prediction","summary":"  Wildfire monitoring and prediction are essential for understanding wildfire\nbehaviour. With extensive Earth observation data, these tasks can be integrated\nand enhanced through multi-task deep learning models. We present a\ncomprehensive multi-temporal remote sensing dataset for active fire detection,\ndaily wildfire monitoring, and next-day wildfire prediction. Covering wildfire\nevents in the contiguous U.S. from January 2017 to October 2021, the dataset\nincludes 3552 surface reflectance images and auxiliary data such as weather,\ntopography, land cover, and fuel information, totalling 71 GB. The lifecycle of\neach wildfire is documented, with labels for active fires (AF) and burned areas\n(BA), supported by manual quality assurance of AF and BA test labels. The\ndataset supports three tasks: a) active fire detection, b) daily burned area\nmapping, and c) wildfire progression prediction. Detection tasks use pixel-wise\nclassification of multi-spectral, multi-temporal images, while prediction tasks\nintegrate satellite and auxiliary data to model fire dynamics. This dataset and\nits benchmarks provide a foundation for advancing wildfire research using deep\nlearning.\n","authors":["Yu Zhao","Sebastian Gerard","Yifang Ban"],"pdf_url":"https://arxiv.org/pdf/2412.11555v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11553v1","updated":"2024-12-16T08:37:58Z","published":"2024-12-16T08:37:58Z","title":"Training Strategies for Isolated Sign Language Recognition","summary":"  This paper introduces a comprehensive model training pipeline for Isolated\nSign Language Recognition (ISLR) designed to accommodate the distinctive\ncharacteristics and constraints of the Sign Language (SL) domain. The\nconstructed pipeline incorporates carefully selected image and video\naugmentations to tackle the challenges of low data quality and varying sign\nspeeds. Including an additional regression head combined with IoU-balanced\nclassification loss enhances the model's awareness of the gesture and\nsimplifies capturing temporal information. Extensive experiments demonstrate\nthat the developed training pipeline easily adapts to different datasets and\narchitectures. Additionally, the ablation study shows that each proposed\ncomponent expands the potential to consider ISLR task specifics. The presented\nstrategies improve recognition performance on a broad set of ISLR benchmarks.\nMoreover, we achieved a state-of-the-art result on the WLASL and Slovo\nbenchmarks with 1.63% and 14.12% improvements compared to the previous best\nsolution, respectively.\n","authors":["Karina Kvanchiani","Roman Kraynov","Elizaveta Petrova","Petr Surovcev","Aleksandr Nagaev","Alexander Kapitanov"],"pdf_url":"https://arxiv.org/pdf/2412.11553v1.pdf","comment":"sign language recognition, training strategies, computer vision"},{"id":"http://arxiv.org/abs/2310.19180v3","updated":"2024-12-16T08:34:52Z","published":"2023-10-29T22:51:49Z","title":"JEN-1 Composer: A Unified Framework for High-Fidelity Multi-Track Music\n  Generation","summary":"  With rapid advances in generative artificial intelligence, the text-to-music\nsynthesis task has emerged as a promising direction for music generation.\nNevertheless, achieving precise control over multi-track generation remains an\nopen challenge. While existing models excel in directly generating multi-track\nmix, their limitations become evident when it comes to composing individual\ntracks and integrating them in a controllable manner. This departure from the\ntypical workflows of professional composers hinders the ability to refine\ndetails in specific tracks. To address this gap, we propose JEN-1 Composer, a\nunified framework designed to efficiently model marginal, conditional, and\njoint distributions over multi-track music using a single model. Building upon\nan audio latent diffusion model, JEN-1 Composer extends the versatility of\nmulti-track music generation. We introduce a progressive curriculum training\nstrategy, which gradually escalates the difficulty of training tasks while\nensuring the model's generalization ability and facilitating smooth transitions\nbetween different scenarios. During inference, users can iteratively generate\nand select music tracks, thus incrementally composing entire musical pieces in\naccordance with the Human-AI co-composition workflow. Our approach demonstrates\nstate-of-the-art performance in controllable and high-fidelity multi-track\nmusic synthesis, marking a significant advancement in interactive AI-assisted\nmusic creation. Our demo pages are available at www.jenmusic.ai/research.\n","authors":["Yao Yao","Peike Li","Boyu Chen","Alex Wang"],"pdf_url":"https://arxiv.org/pdf/2310.19180v3.pdf","comment":"9 pages, 3 figures, accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.11549v1","updated":"2024-12-16T08:31:55Z","published":"2024-12-16T08:31:55Z","title":"MPQ-DM: Mixed Precision Quantization for Extremely Low Bit Diffusion\n  Models","summary":"  Diffusion models have received wide attention in generation tasks. However,\nthe expensive computation cost prevents the application of diffusion models in\nresource-constrained scenarios. Quantization emerges as a practical solution\nthat significantly saves storage and computation by reducing the bit-width of\nparameters. However, the existing quantization methods for diffusion models\nstill cause severe degradation in performance, especially under extremely low\nbit-widths (2-4 bit). The primary decrease in performance comes from the\nsignificant discretization of activation values at low bit quantization. Too\nfew activation candidates are unfriendly for outlier significant weight channel\nquantization, and the discretized features prevent stable learning over\ndifferent time steps of the diffusion model. This paper presents MPQ-DM, a\nMixed-Precision Quantization method for Diffusion Models. The proposed MPQ-DM\nmainly relies on two techniques:(1) To mitigate the quantization error caused\nby outlier severe weight channels, we propose an Outlier-Driven Mixed\nQuantization (OMQ) technique that uses $Kurtosis$ to quantify outlier salient\nchannels and apply optimized intra-layer mixed-precision bit-width allocation\nto recover accuracy performance within target efficiency.(2) To robustly learn\nrepresentations crossing time steps, we construct a Time-Smoothed Relation\nDistillation (TRD) scheme between the quantized diffusion model and its\nfull-precision counterpart, transferring discrete and continuous latent to a\nunified relation space to reduce the representation inconsistency.\nComprehensive experiments demonstrate that MPQ-DM achieves significant accuracy\ngains under extremely low bit-widths compared with SOTA quantization methods.\nMPQ-DM achieves a 58\\% FID decrease under W2A4 setting compared with baseline,\nwhile all other methods even collapse.\n","authors":["Weilun Feng","Haotong Qin","Chuanguang Yang","Zhulin An","Libo Huang","Boyu Diao","Fei Wang","Renshuai Tao","Yongjun Xu","Michele Magno"],"pdf_url":"https://arxiv.org/pdf/2412.11549v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.11542v1","updated":"2024-12-16T08:22:23Z","published":"2024-12-16T08:22:23Z","title":"Meta Curvature-Aware Minimization for Domain Generalization","summary":"  Domain generalization (DG) aims to enhance the ability of models trained on\nsource domains to generalize effectively to unseen domains. Recently,\nSharpness-Aware Minimization (SAM) has shown promise in this area by reducing\nthe sharpness of the loss landscape to obtain more generalized models. However,\nSAM and its variants sometimes fail to guide the model toward a flat minimum,\nand their training processes exhibit limitations, hindering further\nimprovements in model generalization. In this paper, we first propose an\nimproved model training process aimed at encouraging the model to converge to a\nflat minima. To achieve this, we design a curvature metric that has a minimal\neffect when the model is far from convergence but becomes increasingly\ninfluential in indicating the curvature of the minima as the model approaches a\nlocal minimum. Then we derive a novel algorithm from this metric, called Meta\nCurvature-Aware Minimization (MeCAM), to minimize the curvature around the\nlocal minima. Specifically, the optimization objective of MeCAM simultaneously\nminimizes the regular training loss, the surrogate gap of SAM, and the\nsurrogate gap of meta-learning. We provide theoretical analysis on MeCAM's\ngeneralization error and convergence rate, and demonstrate its superiority over\nexisting DG methods through extensive experiments on five benchmark DG\ndatasets, including PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet. Code\nwill be available on GitHub.\n","authors":["Ziyang Chen","Yiwen Ye","Feilong Tang","Yongsheng Pan","Yong Xia"],"pdf_url":"https://arxiv.org/pdf/2412.11542v1.pdf","comment":"21 pages, 5 figures, 17 tables"},{"id":"http://arxiv.org/abs/2412.11540v1","updated":"2024-12-16T08:21:09Z","published":"2024-12-16T08:21:09Z","title":"SP$^2$T: Sparse Proxy Attention for Dual-stream Point Transformer","summary":"  In 3D understanding, point transformers have yielded significant advances in\nbroadening the receptive field. However, further enhancement of the receptive\nfield is hindered by the constraints of grouping attention. The proxy-based\nmodel, as a hot topic in image and language feature extraction, uses global or\nlocal proxies to expand the model's receptive field. But global proxy-based\nmethods fail to precisely determine proxy positions and are not suited for\ntasks like segmentation and detection in the point cloud, and exist local\nproxy-based methods for image face difficulties in global-local balance, proxy\nsampling in various point clouds, and parallel cross-attention computation for\nsparse association. In this paper, we present SP$^2$T, a local proxy-based dual\nstream point transformer, which promotes global receptive field while\nmaintaining a balance between local and global information. To tackle robust 3D\nproxy sampling, we propose a spatial-wise proxy sampling with vertex-based\npoint proxy associations, ensuring robust point-cloud sampling in many scales\nof point cloud. To resolve economical association computation, we introduce\nsparse proxy attention combined with table-based relative bias, which enables\nlow-cost and precise interactions between proxy and point features.\nComprehensive experiments across multiple datasets reveal that our model\nachieves SOTA performance in downstream tasks. The code has been released in\nhttps://github.com/TerenceWallel/Sparse-Proxy-Point-Transformer .\n","authors":["Jiaxu Wan","Hong Zhang","Ziqi He","Qishu Wang","Ding Yuan","Yifan Yang"],"pdf_url":"https://arxiv.org/pdf/2412.11540v1.pdf","comment":"13 pages, 14 figures, 14 tables"},{"id":"http://arxiv.org/abs/2412.11535v1","updated":"2024-12-16T08:13:05Z","published":"2024-12-16T08:13:05Z","title":"Near Large Far Small: Relative Distance Based Partition Learning for\n  UAV-view Geo-Localization","summary":"  UAV-view Geo-Localization (UVGL) presents substantial challenges, primarily\ndue to appearance differences between drone-view and satellite-view. Existing\nmethods develop partition learning strategies aimed at mining more\ncomprehensive information by constructing diverse part-level feature\nrepresentations, which rely on consistent cross-view scales. However,\nvariations of UAV flight state leads to the scale mismatch of cross-views,\nresulting in serious performance degradation of partition-based methods. To\novercome this issue, we propose a partition learning framework based on\nrelative distance, which alleviates the dependence on scale consistency while\nmining fine-grained features. Specifically, we propose a distance guided\ndynamic partition learning strategy (DGDPL), consisting of a square partition\nstrategy and a dynamic-guided adjustment strategy. The former is utilized to\nextract fine-grained features and global features in a simple manner. The\nlatter calculates the relative distance ratio between drone- and satellite-view\nto adjust the partition size, thereby aligning the semantic information between\npartition pairs. Furthermore, we propose a saliency-guided refinement strategy\nto refine part-level features, so as to further improve the retrieval accuracy.\nExtensive experiments show that our approach achieves superior geo-localization\naccuracy across various scale-inconsistent scenarios, and exhibits remarkable\nrobustness against scale variations. The code will be released.\n","authors":["Quan Chen","Tingyu Wang","Rongfeng Lu","Bolun Zheng","Zhedong Zheng","Chenggang Yan"],"pdf_url":"https://arxiv.org/pdf/2412.11535v1.pdf","comment":"In Peer Review"},{"id":"http://arxiv.org/abs/2411.18203v3","updated":"2024-12-16T08:12:17Z","published":"2024-11-27T10:28:57Z","title":"Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning","summary":"  Vision-language models (VLMs) have shown remarkable advancements in\nmultimodal reasoning tasks. However, they still often generate inaccurate or\nirrelevant responses due to issues like hallucinated image understandings or\nunrefined reasoning paths. To address these challenges, we introduce Critic-V,\na novel framework inspired by the Actor-Critic paradigm to boost the reasoning\ncapability of VLMs. This framework decouples the reasoning process and critic\nprocess by integrating two independent components: the Reasoner, which\ngenerates reasoning paths based on visual and textual inputs, and the Critic,\nwhich provides constructive critique to refine these paths. In this approach,\nthe Reasoner generates reasoning responses according to text prompts, which can\nevolve iteratively as a policy based on feedback from the Critic. This\ninteraction process was theoretically driven by a reinforcement learning\nframework where the Critic offers natural language critiques instead of scalar\nrewards, enabling more nuanced feedback to boost the Reasoner's capability on\ncomplex reasoning tasks. The Critic model is trained using Direct Preference\nOptimization (DPO), leveraging a preference dataset of critiques ranked by\nRule-based Reward~(RBR) to enhance its critic capabilities. Evaluation results\nshow that the Critic-V framework significantly outperforms existing methods,\nincluding GPT-4V, on 5 out of 8 benchmarks, especially regarding reasoning\naccuracy and efficiency. Combining a dynamic text-based policy for the Reasoner\nand constructive feedback from the preference-optimized Critic enables a more\nreliable and context-sensitive multimodal reasoning process. Our approach\nprovides a promising solution to enhance the reliability of VLMs, improving\ntheir performance in real-world reasoning-heavy multimodal applications such as\nautonomous driving and embodied intelligence.\n","authors":["Di Zhang","Junxian Li","Jingdi Lei","Xunzhi Wang","Yujie Liu","Zonglin Yang","Jiatong Li","Weida Wang","Suorong Yang","Jianbo Wu","Peng Ye","Wanli Ouyang","Dongzhan Zhou"],"pdf_url":"https://arxiv.org/pdf/2411.18203v3.pdf","comment":"16 pages, 11 figures"},{"id":"http://arxiv.org/abs/2412.11530v1","updated":"2024-12-16T08:08:35Z","published":"2024-12-16T08:08:35Z","title":"RoMeO: Robust Metric Visual Odometry","summary":"  Visual odometry (VO) aims to estimate camera poses from visual inputs -- a\nfundamental building block for many applications such as VR/AR and robotics.\nThis work focuses on monocular RGB VO where the input is a monocular RGB video\nwithout IMU or 3D sensors. Existing approaches lack robustness under this\nchallenging scenario and fail to generalize to unseen data (especially\noutdoors); they also cannot recover metric-scale poses. We propose Robust\nMetric Visual Odometry (RoMeO), a novel method that resolves these issues\nleveraging priors from pre-trained depth models. RoMeO incorporates both\nmonocular metric depth and multi-view stereo (MVS) models to recover\nmetric-scale, simplify correspondence search, provide better initialization and\nregularize optimization. Effective strategies are proposed to inject noise\nduring training and adaptively filter noisy depth priors, which ensure the\nrobustness of RoMeO on in-the-wild data. As shown in Fig.1, RoMeO advances the\nstate-of-the-art (SOTA) by a large margin across 6 diverse datasets covering\nboth indoor and outdoor scenes. Compared to the current SOTA DPVO, RoMeO\nreduces the relative (align the trajectory scale with GT) and absolute\ntrajectory errors both by >50%. The performance gain also transfers to the full\nSLAM pipeline (with global BA & loop closure). Code will be released upon\nacceptance.\n","authors":["Junda Cheng","Zhipeng Cai","Zhaoxing Zhang","Wei Yin","Matthias Muller","Michael Paulitsch","Xin Yang"],"pdf_url":"https://arxiv.org/pdf/2412.11530v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11529v1","updated":"2024-12-16T08:07:53Z","published":"2024-12-16T08:07:53Z","title":"Cross-View Geo-Localization with Street-View and VHR Satellite Imagery\n  in Decentrality Settings","summary":"  Cross-View Geo-Localization tackles the problem of image geo-localization in\nGNSS-denied environments by matching street-view query images with geo-tagged\naerial-view reference images. However, existing datasets and methods often\nassume center-aligned settings or only consider limited decentrality (i.e., the\noffset of the query image from the reference image center). This assumption\noverlooks the challenges present in real-world applications, where large\ndecentrality can significantly enhance localization efficiency but\nsimultaneously lead to a substantial degradation in localization accuracy. To\naddress this limitation, we introduce CVSat, a novel dataset designed to\nevaluate cross-view geo-localization with a large geographic scope and diverse\nlandscapes, emphasizing the decentrality issue. Meanwhile, we propose AuxGeo\n(Auxiliary Enhanced Geo-Localization), which leverages a multi-metric\noptimization strategy with two novel modules: the Bird's-eye view Intermediary\nModule (BIM) and the Position Constraint Module (PCM). BIM uses bird's-eye view\nimages derived from street-view panoramas as an intermediary, simplifying the\ncross-view challenge with decentrality to a cross-view problem and a\ndecentrality problem. PCM leverages position priors between cross-view images\nto establish multi-grained alignment constraints. These modules improve the\nperformance of cross-view geo-localization with the decentrality problem.\nExtensive experiments demonstrate that AuxGeo outperforms previous methods on\nour proposed CVSat dataset, mitigating the issue of large decentrality, and\nalso achieves state-of-the-art performance on existing public datasets such as\nCVUSA, CVACT, and VIGOR.\n","authors":["Panwang Xia","Lei Yu","Yi Wan","Qiong Wu","Peiqi Chen","Liheng Zhong","Yongxiang Yao","Dong Wei","Xinyi Liu","Lixiang Ru","Yingying Zhang","Jiangwei Lao","Jingdong Chen","Ming Yang","Yongjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.11529v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11525v1","updated":"2024-12-16T08:00:50Z","published":"2024-12-16T08:00:50Z","title":"Sequence Matters: Harnessing Video Models in Super-Resolution","summary":"  3D super-resolution aims to reconstruct high-fidelity 3D models from\nlow-resolution (LR) multi-view images. Early studies primarily focused on\nsingle-image super-resolution (SISR) models to upsample LR images into\nhigh-resolution images. However, these methods often lack view consistency\nbecause they operate independently on each image. Although various\npost-processing techniques have been extensively explored to mitigate these\ninconsistencies, they have yet to fully resolve the issues. In this paper, we\nperform a comprehensive study of 3D super-resolution by leveraging video\nsuper-resolution (VSR) models. By utilizing VSR models, we ensure a higher\ndegree of spatial consistency and can reference surrounding spatial\ninformation, leading to more accurate and detailed reconstructions. Our\nfindings reveal that VSR models can perform remarkably well even on sequences\nthat lack precise spatial alignment. Given this observation, we propose a\nsimple yet practical approach to align LR images without involving fine-tuning\nor generating 'smooth' trajectory from the trained 3D models over LR images.\nThe experimental results show that the surprisingly simple algorithms can\nachieve the state-of-the-art results of 3D super-resolution tasks on standard\nbenchmark datasets, such as the NeRF-synthetic and MipNeRF-360 datasets.\nProject page: https://ko-lani.github.io/Sequence-Matters\n","authors":["Hyun-kyu Ko","Dongheok Park","Youngin Park","Byeonghyeon Lee","Juhee Han","Eunbyung Park"],"pdf_url":"https://arxiv.org/pdf/2412.11525v1.pdf","comment":"Project page: https://ko-lani.github.io/Sequence-Matters"},{"id":"http://arxiv.org/abs/2412.08929v2","updated":"2024-12-16T07:56:34Z","published":"2024-12-12T04:34:28Z","title":"CAPrompt: Cyclic Prompt Aggregation for Pre-Trained Model Based Class\n  Incremental Learning","summary":"  Recently, prompt tuning methods for pre-trained models have demonstrated\npromising performance in Class Incremental Learning (CIL). These methods\ntypically involve learning task-specific prompts and predicting the task ID to\nselect the appropriate prompts for inference. However, inaccurate task ID\npredictions can cause severe inconsistencies between the prompts used during\ntraining and inference, leading to knowledge forgetting and performance\ndegradation. Additionally, existing prompt tuning methods rely solely on the\npre-trained model to predict task IDs, without fully leveraging the knowledge\nembedded in the learned prompt parameters, resulting in inferior prediction\nperformance. To address these issues, we propose a novel Cyclic Prompt\nAggregation (CAPrompt) method that eliminates the dependency on task ID\nprediction by cyclically aggregating the knowledge from different prompts.\nSpecifically, rather than predicting task IDs, we introduce an innovative\nprompt aggregation strategy during both training and inference to overcome\nprompt inconsistency by utilizing a weighted sum of different prompts. Thorough\ntheoretical analysis demonstrates that under concave conditions, the aggregated\nprompt achieves lower error compared to selecting a single task-specific\nprompt. Consequently, we incorporate a concave constraint and a linear\nconstraint to guide prompt learning, ensuring compliance with the concave\ncondition requirement. Furthermore, to fully exploit the prompts and achieve\nmore accurate prompt weights, we develop a cyclic weight prediction strategy.\nThis strategy begins with equal weights for each task and automatically adjusts\nthem to more appropriate values in a cyclical manner. Experiments on various\ndatasets demonstrate that our proposed CAPrompt outperforms state-of-the-art\nmethods by 2%-3%. Our code is available at\nhttps://github.com/zhoujiahuan1991/AAAI2025-CAPrompt.\n","authors":["Qiwei Li","Jiahuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.08929v2.pdf","comment":"in Proceedings of the 39th AAAI Conference on Artificial Intelligence\n  (AAAI-25)"},{"id":"http://arxiv.org/abs/2412.11520v1","updated":"2024-12-16T07:56:04Z","published":"2024-12-16T07:56:04Z","title":"EditSplat: Multi-View Fusion and Attention-Guided Optimization for\n  View-Consistent 3D Scene Editing with 3D Gaussian Splatting","summary":"  Recent advancements in 3D editing have highlighted the potential of\ntext-driven methods in real-time, user-friendly AR/VR applications. However,\ncurrent methods rely on 2D diffusion models without adequately considering\nmulti-view information, resulting in multi-view inconsistency. While 3D\nGaussian Splatting (3DGS) significantly improves rendering quality and speed,\nits 3D editing process encounters difficulties with inefficient optimization,\nas pre-trained Gaussians retain excessive source information, hindering\noptimization. To address these limitations, we propose \\textbf{EditSplat}, a\nnovel 3D editing framework that integrates Multi-view Fusion Guidance (MFG) and\nAttention-Guided Trimming (AGT). Our MFG ensures multi-view consistency by\nincorporating essential multi-view information into the diffusion process,\nleveraging classifier-free guidance from the text-to-image diffusion model and\nthe geometric properties of 3DGS. Additionally, our AGT leverages the explicit\nrepresentation of 3DGS to selectively prune and optimize 3D Gaussians,\nenhancing optimization efficiency and enabling precise, semantically rich local\nedits. Through extensive qualitative and quantitative evaluations, EditSplat\nachieves superior multi-view consistency and editing quality over existing\nmethods, significantly enhancing overall efficiency.\n","authors":["Dong In Lee","Hyeongcheol Park","Jiyoung Seo","Eunbyung Park","Hyunje Park","Ha Dam Baek","Shin Sangheon","Sangmin kim","Sangpil Kim"],"pdf_url":"https://arxiv.org/pdf/2412.11520v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11519v1","updated":"2024-12-16T07:54:45Z","published":"2024-12-16T07:54:45Z","title":"LineArt: A Knowledge-guided Training-free High-quality Appearance\n  Transfer for Design Drawing with Diffusion Model","summary":"  Image rendering from line drawings is vital in design and image generation\ntechnologies reduce costs, yet professional line drawings demand preserving\ncomplex details. Text prompts struggle with accuracy, and image translation\nstruggles with consistency and fine-grained control. We present LineArt, a\nframework that transfers complex appearance onto detailed design drawings,\nfacilitating design and artistic creation. It generates high-fidelity\nappearance while preserving structural accuracy by simulating hierarchical\nvisual cognition and integrating human artistic experience to guide the\ndiffusion process. LineArt overcomes the limitations of current methods in\nterms of difficulty in fine-grained control and style degradation in design\ndrawings. It requires no precise 3D modeling, physical property specs, or\nnetwork training, making it more convenient for design tasks. LineArt consists\nof two stages: a multi-frequency lines fusion module to supplement the input\ndesign drawing with detailed structural information and a two-part painting\nprocess for Base Layer Shaping and Surface Layer Coloring. We also present a\nnew design drawing dataset ProLines for evaluation. The experiments show that\nLineArt performs better in accuracy, realism, and material precision compared\nto SOTAs.\n","authors":["Xi Wang","Hongzhen Li","Heng Fang","Yichen Peng","Haoran Xie","Xi Yang","Chuntao Li"],"pdf_url":"https://arxiv.org/pdf/2412.11519v1.pdf","comment":"Project Page: https://meaoxixi.github.io/LineArt/"},{"id":"http://arxiv.org/abs/2412.09920v2","updated":"2024-12-16T07:50:46Z","published":"2024-12-13T07:15:52Z","title":"Precision-Enhanced Human-Object Contact Detection via Depth-Aware\n  Perspective Interaction and Object Texture Restoration","summary":"  Human-object contact (HOT) is designed to accurately identify the areas where\nhumans and objects come into contact. Current methods frequently fail to\naccount for scenarios where objects are frequently blocking the view, resulting\nin inaccurate identification of contact areas. To tackle this problem, we\nsuggest using a perspective interaction HOT detector called PIHOT, which\nutilizes a depth map generation model to offer depth information of humans and\nobjects related to the camera, thereby preventing false interaction detection.\nFurthermore, we use mask dilatation and object restoration techniques to\nrestore the texture details in covered areas, improve the boundaries between\nobjects, and enhance the perception of humans interacting with objects.\nMoreover, a spatial awareness perception is intended to concentrate on the\ncharacteristic features close to the points of contact. The experimental\nresults show that the PIHOT algorithm achieves state-of-the-art performance on\nthree benchmark datasets for HOT detection tasks. Compared to the most recent\nDHOT, our method enjoys an average improvement of 13%, 27.5%, 16%, and 18.5% on\nSC-Acc., C-Acc., mIoU, and wIoU metrics, respectively.\n","authors":["Yuxiao Wang","Wenpeng Neng","Zhenao Wei","Yu Lei","Weiying Xue","Nan Zhuang","Yanwu Xu","Xinyu Jiang","Qi Liu"],"pdf_url":"https://arxiv.org/pdf/2412.09920v2.pdf","comment":"Accepted by AAAl 2025"},{"id":"http://arxiv.org/abs/2412.11513v1","updated":"2024-12-16T07:48:30Z","published":"2024-12-16T07:48:30Z","title":"IGR: Improving Diffusion Model for Garment Restoration from Person Image","summary":"  Garment restoration, the inverse of virtual try-on task, focuses on restoring\nstandard garment from a person image, requiring accurate capture of garment\ndetails. However, existing methods often fail to preserve the identity of the\ngarment or rely on complex processes. To address these limitations, we propose\nan improved diffusion model for restoring authentic garments. Our approach\nemploys two garment extractors to independently capture low-level features and\nhigh-level semantics from the person image. Leveraging a pretrained latent\ndiffusion model, these features are integrated into the denoising process\nthrough garment fusion blocks, which combine self-attention and cross-attention\nlayers to align the restored garment with the person image. Furthermore, a\ncoarse-to-fine training strategy is introduced to enhance the fidelity and\nauthenticity of the generated garments. Experimental results demonstrate that\nour model effectively preserves garment identity and generates high-quality\nrestorations, even in challenging scenarios such as complex garments or those\nwith occlusions.\n","authors":["Le Shen","Rong Huang","Zhijie Wang"],"pdf_url":"https://arxiv.org/pdf/2412.11513v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11512v1","updated":"2024-12-16T07:42:49Z","published":"2024-12-16T07:42:49Z","title":"SpatialMe: Stereo Video Conversion Using Depth-Warping and\n  Blend-Inpainting","summary":"  Stereo video conversion aims to transform monocular videos into immersive\nstereo format. Despite the advancements in novel view synthesis, it still\nremains two major challenges: i) difficulty of achieving high-fidelity and\nstable results, and ii) insufficiency of high-quality stereo video data. In\nthis paper, we introduce SpatialMe, a novel stereo video conversion framework\nbased on depth-warping and blend-inpainting. Specifically, we propose a\nmask-based hierarchy feature update (MHFU) refiner, which integrate and refine\nthe outputs from designed multi-branch inpainting module, using feature update\nunit (FUU) and mask mechanism. We also propose a disparity expansion strategy\nto address the problem of foreground bleeding. Furthermore, we conduct a\nhigh-quality real-world stereo video dataset -- StereoV1K, to alleviate the\ndata shortage. It contains 1000 stereo videos captured in real-world at a\nresolution of 1180 x 1180, covering various indoor and outdoor scenes.\nExtensive experiments demonstrate the superiority of our approach in generating\nstereo videos over state-of-the-art methods.\n","authors":["Jiale Zhang","Qianxi Jia","Yang Liu","Wei Zhang","Wei Wei","Xin Tian"],"pdf_url":"https://arxiv.org/pdf/2412.11512v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11509v1","updated":"2024-12-16T07:33:23Z","published":"2024-12-16T07:33:23Z","title":"Skip Tuning: Pre-trained Vision-Language Models are Effective and\n  Efficient Adapters Themselves","summary":"  Prompt tuning (PT) has long been recognized as an effective and efficient\nparadigm for transferring large pre-trained vision-language models (VLMs) to\ndownstream tasks by learning a tiny set of context vectors. Nevertheless, in\nthis work, we reveal that freezing the parameters of VLMs during learning the\ncontext vectors neither facilitates the transferability of pre-trained\nknowledge nor improves the memory and time efficiency significantly. Upon\nfurther investigation, we find that reducing both the length and width of the\nfeature-gradient propagation flows of the full fine-tuning (FT) baseline is key\nto achieving effective and efficient knowledge transfer. Motivated by this, we\npropose Skip Tuning, a novel paradigm for adapting VLMs to downstream tasks.\nUnlike existing PT or adapter-based methods, Skip Tuning applies Layer-wise\nSkipping (LSkip) and Class-wise Skipping (CSkip) upon the FT baseline without\nintroducing extra context vectors or adapter modules. Extensive experiments\nacross a wide spectrum of benchmarks demonstrate the superior effectiveness and\nefficiency of our Skip Tuning over both PT and adapter-based methods. Code:\nhttps://github.com/Koorye/SkipTuning.\n","authors":["Shihan Wu","Ji Zhang","Pengpeng Zeng","Lianli Gao","Jingkuan Song","Heng Tao Shen"],"pdf_url":"https://arxiv.org/pdf/2412.11509v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06367v3","updated":"2024-12-16T07:32:01Z","published":"2024-06-10T15:26:48Z","title":"MVGamba: Unify 3D Content Generation as State Space Sequence Modeling","summary":"  Recent 3D large reconstruction models (LRMs) can generate high-quality 3D\ncontent in sub-seconds by integrating multi-view diffusion models with scalable\nmulti-view reconstructors. Current works further leverage 3D Gaussian Splatting\nas 3D representation for improved visual quality and rendering efficiency.\nHowever, we observe that existing Gaussian reconstruction models often suffer\nfrom multi-view inconsistency and blurred textures. We attribute this to the\ncompromise of multi-view information propagation in favor of adopting powerful\nyet computationally intensive architectures (e.g., Transformers). To address\nthis issue, we introduce MVGamba, a general and lightweight Gaussian\nreconstruction model featuring a multi-view Gaussian reconstructor based on the\nRNN-like State Space Model (SSM). Our Gaussian reconstructor propagates causal\ncontext containing multi-view information for cross-view self-refinement while\ngenerating a long sequence of Gaussians for fine-detail modeling with linear\ncomplexity. With off-the-shelf multi-view diffusion models integrated, MVGamba\nunifies 3D generation tasks from a single image, sparse images, or text\nprompts. Extensive experiments demonstrate that MVGamba outperforms\nstate-of-the-art baselines in all 3D content generation scenarios with\napproximately only $0.1\\times$ of the model size.\n","authors":["Xuanyu Yi","Zike Wu","Qiuhong Shen","Qingshan Xu","Pan Zhou","Joo-Hwee Lim","Shuicheng Yan","Xinchao Wang","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.06367v3.pdf","comment":"Accepted by NeurIPS 2024. Code is included in\n  https://github.com/SkyworkAI/MVGamba"},{"id":"http://arxiv.org/abs/2412.11495v1","updated":"2024-12-16T07:15:13Z","published":"2024-12-16T07:15:13Z","title":"Exploring More from Multiple Gait Modalities for Human Identification","summary":"  The gait, as a kind of soft biometric characteristic, can reflect the\ndistinct walking patterns of individuals at a distance, exhibiting a promising\ntechnique for unrestrained human identification. With largely excluding\ngait-unrelated cues hidden in RGB videos, the silhouette and skeleton, though\nvisually compact, have acted as two of the most prevailing gait modalities for\na long time. Recently, several attempts have been made to introduce more\ninformative data forms like human parsing and optical flow images to capture\ngait characteristics, along with multi-branch architectures. However, due to\nthe inconsistency within model designs and experiment settings, we argue that a\ncomprehensive and fair comparative study among these popular gait modalities,\ninvolving the representational capacity and fusion strategy exploration, is\nstill lacking. From the perspectives of fine vs. coarse-grained shape and whole\nvs. pixel-wise motion modeling, this work presents an in-depth investigation of\nthree popular gait representations, i.e., silhouette, human parsing, and\noptical flow, with various fusion evaluations, and experimentally exposes their\nsimilarities and differences. Based on the obtained insights, we further\ndevelop a C$^2$Fusion strategy, consequently building our new framework\nMultiGait++. C$^2$Fusion preserves commonalities while highlighting differences\nto enrich the learning of gait features. To verify our findings and\nconclusions, extensive experiments on Gait3D, GREW, CCPG, and SUSTech1K are\nconducted. The code is available at https://github.com/ShiqiYu/OpenGait.\n","authors":["Dongyang Jin","Chao Fan","Weihua Chen","Shiqi Yu"],"pdf_url":"https://arxiv.org/pdf/2412.11495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11489v1","updated":"2024-12-16T07:06:17Z","published":"2024-12-16T07:06:17Z","title":"HGSFusion: Radar-Camera Fusion with Hybrid Generation and\n  Synchronization for 3D Object Detection","summary":"  Millimeter-wave radar plays a vital role in 3D object detection for\nautonomous driving due to its all-weather and all-lighting-condition\ncapabilities for perception. However, radar point clouds suffer from pronounced\nsparsity and unavoidable angle estimation errors. To address these limitations,\nincorporating a camera may partially help mitigate the shortcomings.\nNevertheless, the direct fusion of radar and camera data can lead to negative\nor even opposite effects due to the lack of depth information in images and\nlow-quality image features under adverse lighting conditions. Hence, in this\npaper, we present the radar-camera fusion network with Hybrid Generation and\nSynchronization (HGSFusion), designed to better fuse radar potentials and image\nfeatures for 3D object detection. Specifically, we propose the Radar Hybrid\nGeneration Module (RHGM), which fully considers the Direction-Of-Arrival (DOA)\nestimation errors in radar signal processing. This module generates denser\nradar points through different Probability Density Functions (PDFs) with the\nassistance of semantic information. Meanwhile, we introduce the Dual Sync\nModule (DSM), comprising spatial sync and modality sync, to enhance image\nfeatures with radar positional information and facilitate the fusion of\ndistinct characteristics in different modalities. Extensive experiments\ndemonstrate the effectiveness of our approach, outperforming the\nstate-of-the-art methods in the VoD and TJ4DRadSet datasets by $6.53\\%$ and\n$2.03\\%$ in RoI AP and BEV AP, respectively. The code is available at\nhttps://github.com/garfield-cpp/HGSFusion.\n","authors":["Zijian Gu","Jianwei Ma","Yan Huang","Honghao Wei","Zhanye Chen","Hui Zhang","Wei Hong"],"pdf_url":"https://arxiv.org/pdf/2412.11489v1.pdf","comment":"12 pages, 8 figures, 7 tables. Accepted by AAAI 2025 , the 39th\n  Annual AAAI Conference on Artificial Intelligence"},{"id":"http://arxiv.org/abs/2408.07343v3","updated":"2024-12-16T07:06:15Z","published":"2024-08-14T07:37:07Z","title":"Gradient Alignment Improves Test-Time Adaptation for Medical Image\n  Segmentation","summary":"  Although recent years have witnessed significant advancements in medical\nimage segmentation, the pervasive issue of domain shift among medical images\nfrom diverse centres hinders the effective deployment of pre-trained models.\nMany Test-time Adaptation (TTA) methods have been proposed to address this\nissue by fine-tuning pre-trained models with test data during inference. These\nmethods, however, often suffer from less-satisfactory optimization due to\nsuboptimal optimization direction (dictated by the gradient) and fixed\nstep-size (predicated on the learning rate). In this paper, we propose the\nGradient alignment-based Test-time adaptation (GraTa) method to improve both\nthe gradient direction and learning rate in the optimization procedure. Unlike\nconventional TTA methods, which primarily optimize the pseudo gradient derived\nfrom a self-supervised objective, our method incorporates an auxiliary gradient\nwith the pseudo one to facilitate gradient alignment. Such gradient alignment\nenables the model to excavate the similarities between different gradients and\ncorrect the gradient direction to approximate the empirical gradient related to\nthe current segmentation task. Additionally, we design a dynamic learning rate\nbased on the cosine similarity between the pseudo and auxiliary gradients,\nthereby empowering the adaptive fine-tuning of pre-trained models on diverse\ntest data. Extensive experiments establish the effectiveness of the proposed\ngradient alignment and dynamic learning rate and substantiate the superiority\nof our GraTa method over other state-of-the-art TTA methods on a benchmark\nmedical image segmentation task. The code and weights of pre-trained source\nmodels are available at https://github.com/Chen-Ziyang/GraTa.\n","authors":["Ziyang Chen","Yiwen Ye","Yongsheng Pan","Yong Xia"],"pdf_url":"https://arxiv.org/pdf/2408.07343v3.pdf","comment":"9 pages, 3 figures, 5 tables"},{"id":"http://arxiv.org/abs/2406.18844v4","updated":"2024-12-16T06:59:33Z","published":"2024-06-27T02:31:03Z","title":"Revisiting Backdoor Attacks against Large Vision-Language Models from\n  Domain Shift","summary":"  Instruction tuning enhances large vision-language models (LVLMs) but\nincreases their vulnerability to backdoor attacks due to their open design.\nUnlike prior studies in static settings, this paper explores backdoor attacks\nin LVLM instruction tuning across mismatched training and testing domains. We\nintroduce a new evaluation dimension, backdoor domain generalization, to assess\nattack robustness under visual and text domain shifts. Our findings reveal two\ninsights: (1) backdoor generalizability improves when distinctive trigger\npatterns are independent of specific data domains or model architectures, and\n(2) the competitive interaction between trigger patterns and clean semantic\nregions, where guiding the model to predict triggers enhances attack\ngeneralizability. Based on these insights, we propose a multimodal attribution\nbackdoor attack (MABA) that injects domain-agnostic triggers into critical\nareas using attributional interpretation. Experiments with OpenFlamingo,\nBlip-2, and Otter show that MABA significantly boosts the attack success rate\nof generalization by 36.4%, achieving a 97% success rate at a 0.2% poisoning\nrate. This study reveals limitations in current evaluations and highlights how\nenhanced backdoor generalizability poses a security threat to LVLMs, even\nwithout test data access.\n","authors":["Siyuan Liang","Jiawei Liang","Tianyu Pang","Chao Du","Aishan Liu","Mingli Zhu","Xiaochun Cao","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2406.18844v4.pdf","comment":"11 pages, 9 figures"},{"id":"http://arxiv.org/abs/2412.11484v1","updated":"2024-12-16T06:53:00Z","published":"2024-12-16T06:53:00Z","title":"Efficient Policy Adaptation with Contrastive Prompt Ensemble for\n  Embodied Agents","summary":"  For embodied reinforcement learning (RL) agents interacting with the\nenvironment, it is desirable to have rapid policy adaptation to unseen visual\nobservations, but achieving zero-shot adaptation capability is considered as a\nchallenging problem in the RL context. To address the problem, we present a\nnovel contrastive prompt ensemble (ConPE) framework which utilizes a pretrained\nvision-language model and a set of visual prompts, thus enabling efficient\npolicy learning and adaptation upon a wide range of environmental and physical\nchanges encountered by embodied agents. Specifically, we devise a\nguided-attention-based ensemble approach with multiple visual prompts on the\nvision-language model to construct robust state representations. Each prompt is\ncontrastively learned in terms of an individual domain factor that\nsignificantly affects the agent's egocentric perception and observation. For a\ngiven task, the attention-based ensemble and policy are jointly learned so that\nthe resulting state representations not only generalize to various domains but\nare also optimized for learning the task. Through experiments, we show that\nConPE outperforms other state-of-the-art algorithms for several embodied agent\ntasks including navigation in AI2THOR, manipulation in egocentric-Metaworld,\nand autonomous driving in CARLA, while also improving the sample efficiency of\npolicy learning and adaptation.\n","authors":["Wonje Choi","Woo Kyung Kim","SeungHyun Kim","Honguk Woo"],"pdf_url":"https://arxiv.org/pdf/2412.11484v1.pdf","comment":"Accepted at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2412.11480v1","updated":"2024-12-16T06:48:30Z","published":"2024-12-16T06:48:30Z","title":"Data-driven Precipitation Nowcasting Using Satellite Imagery","summary":"  Accurate precipitation forecasting is crucial for early warnings of\ndisasters, such as floods and landslides. Traditional forecasts rely on\nground-based radar systems, which are space-constrained and have high\nmaintenance costs. Consequently, most developing countries depend on a global\nnumerical model with low resolution, instead of operating their own radar\nsystems. To mitigate this gap, we propose the Neural Precipitation Model (NPM),\nwhich uses global-scale geostationary satellite imagery. NPM predicts\nprecipitation for up to six hours, with an update every hour. We take three key\nchannels to discriminate rain clouds as input: infrared radiation (at a\nwavelength of 10.5 $\\mu m$), upper- (6.3 $\\mu m$), and lower- (7.3 $\\mu m$)\nlevel water vapor channels. Additionally, NPM introduces positional encoders to\ncapture seasonal and temporal patterns, accounting for variations in\nprecipitation. Our experimental results demonstrate that NPM can predict\nrainfall in real-time with a resolution of 2 km. The code and dataset are\navailable at\nhttps://github.com/seominseok0429/Data-driven-Precipitation-Nowcasting-Using-Satellite-Imagery.\n","authors":["Young-Jae Park","Doyi Kim","Minseok Seo","Hae-Gon Jeon","Yeji Choi"],"pdf_url":"https://arxiv.org/pdf/2412.11480v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.11475v1","updated":"2024-12-16T06:38:00Z","published":"2024-12-16T06:38:00Z","title":"OmniVLM: A Token-Compressed, Sub-Billion-Parameter Vision-Language Model\n  for Efficient On-Device Inference","summary":"  We present OmniVLM, a sub-billion-parameter vision-language model for\nefficient on-device inference. OmniVLM introduces a token compression mechanism\nthat reduces visual token sequence length from 729 to 81 tokens, significantly\nreducing computational overhead while preserving visual-semantic fidelity.\nThrough a multi-stage training pipeline of pretraining, supervised fine-tuning,\nand minimal-edit Direct Preference Optimization (DPO), OmniVLM matches the\nperformance of larger models. On multiple benchmarks including ScienceQA, POPE,\nand MMMU, OmniVLM outperforms existing baselines like nanoLLAVA within a\n968M-parameter footprint. Empirical results on the same laptop demonstrate 9.1x\nfaster time-to-first-token (0.75s vs 6.82s) and 1.5x higher decoding speed\n(29.41 vs 19.20 tokens/s) compared to nanoLLAVA, enabling efficient deployment\non edge devices. The model weights can be accessed on huggingface:\n\\url{https://huggingface.co/NexaAIDev/OmniVLM-968M}, and the inference examples\ncan be find in Appendix B.\n","authors":["Wei Chen","Zhiyuan Li","Shuo Xin"],"pdf_url":"https://arxiv.org/pdf/2412.11475v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12730v3","updated":"2024-12-16T06:16:27Z","published":"2024-07-17T16:49:34Z","title":"RoDE: Linear Rectified Mixture of Diverse Experts for Food Large\n  Multi-Modal Models","summary":"  Large Multi-modal Models (LMMs) have significantly advanced a variety of\nvision-language tasks. The scalability and availability of high-quality\ntraining data play a pivotal role in the success of LMMs. In the realm of food,\nwhile comprehensive food datasets such as Recipe1M offer an abundance of\ningredient and recipe information, they often fall short of providing ample\ndata for nutritional analysis. The Recipe1M+ dataset, despite offering a subset\nfor nutritional evaluation, is limited in the scale and accuracy of nutrition\ninformation. To bridge this gap, we introduce Uni-Food, a unified food dataset\nthat comprises over 100,000 images with various food labels, including\ncategories, ingredients, recipes, and ingredient-level nutritional information.\nUni-Food is designed to provide a more holistic approach to food data analysis,\nthereby enhancing the performance and capabilities of LMMs in this domain. To\nmitigate the conflicts arising from multi-task supervision during fine-tuning\nof LMMs, we introduce a novel Linear Rectification Mixture of Diverse Experts\n(RoDE) approach. RoDE utilizes a diverse array of experts to address tasks of\nvarying complexity, thereby facilitating the coordination of trainable\nparameters, i.e., it allocates more parameters for more complex tasks and,\nconversely, fewer parameters for simpler tasks. RoDE implements linear\nrectification union to refine the router's functionality, thereby enhancing the\nefficiency of sparse task allocation. These design choices endow RoDE with\nfeatures that ensure GPU memory efficiency and ease of optimization. Our\nexperimental results validate the effectiveness of our proposed approach in\naddressing the inherent challenges of food-related multitasking.\n","authors":["Pengkun Jiao","Xinlan Wu","Bin Zhu","Jingjing Chen","Chong-Wah Ngo","Yugang Jiang"],"pdf_url":"https://arxiv.org/pdf/2407.12730v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.16387v4","updated":"2024-12-16T06:09:40Z","published":"2023-10-25T05:59:25Z","title":"Frequency-Aware Transformer for Learned Image Compression","summary":"  Learned image compression (LIC) has gained traction as an effective solution\nfor image storage and transmission in recent years. However, existing LIC\nmethods are redundant in latent representation due to limitations in capturing\nanisotropic frequency components and preserving directional details. To\novercome these challenges, we propose a novel frequency-aware transformer (FAT)\nblock that for the first time achieves multiscale directional ananlysis for\nLIC. The FAT block comprises frequency-decomposition window attention (FDWA)\nmodules to capture multiscale and directional frequency components of natural\nimages. Additionally, we introduce frequency-modulation feed-forward network\n(FMFFN) to adaptively modulate different frequency components, improving\nrate-distortion performance. Furthermore, we present a transformer-based\nchannel-wise autoregressive (T-CA) model that effectively exploits channel\ndependencies. Experiments show that our method achieves state-of-the-art\nrate-distortion performance compared to existing LIC methods, and evidently\noutperforms latest standardized codec VTM-12.1 by 14.5%, 15.1%, 13.0% in\nBD-rate on the Kodak, Tecnick, and CLIC datasets.\n","authors":["Han Li","Shaohui Li","Wenrui Dai","Chenglin Li","Junni Zou","Hongkai Xiong"],"pdf_url":"https://arxiv.org/pdf/2310.16387v4.pdf","comment":"ICLR2024 poster"},{"id":"http://arxiv.org/abs/2312.03289v3","updated":"2024-12-16T06:09:01Z","published":"2023-12-06T04:38:02Z","title":"Enhancing Robustness in Incremental Learning with Adversarial Training","summary":"  Adversarial training is one of the most effective approaches against\nadversarial attacks. However, adversarial training has primarily been studied\nin scenarios where data for all classes is provided, with limited research\nconducted in the context of incremental learning where knowledge is introduced\nsequentially. In this study, we investigate Adversarially Robust Class\nIncremental Learning (ARCIL), which deals with adversarial robustness in\nincremental learning. We first explore a series of baselines that integrate\nincremental learning with existing adversarial training methods, finding that\nthey lead to conflicts between acquiring new knowledge and retaining past\nknowledge. Furthermore, we discover that training new knowledge causes the\ndisappearance of a key characteristic in robust models: a flat loss landscape\nin input space. To address such issues, we propose a novel and robust baseline\nfor ARCIL, named \\textbf{FL}atness-preserving \\textbf{A}dversarial\n\\textbf{I}ncremental learning for \\textbf{R}obustness (\\textbf{FLAIR}).\nExperimental results demonstrate that FLAIR significantly outperforms other\nbaselines. To the best of our knowledge, we are the first to comprehensively\ninvestigate the baselines, challenges, and solutions for ARCIL, which we\nbelieve represents a significant advance toward achieving real-world\nrobustness. Codes are available at \\url{https://github.com/HongsinLee/FLAIR}.\n","authors":["Seungju Cho","Hongsin Lee","Changick Kim"],"pdf_url":"https://arxiv.org/pdf/2312.03289v3.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2412.11468v1","updated":"2024-12-16T06:03:56Z","published":"2024-12-16T06:03:56Z","title":"Block-Based Multi-Scale Image Rescaling","summary":"  Image rescaling (IR) seeks to determine the optimal low-resolution (LR)\nrepresentation of a high-resolution (HR) image to reconstruct a high-quality\nsuper-resolution (SR) image. Typically, HR images with resolutions exceeding 2K\npossess rich information that is unevenly distributed across the image.\nTraditional image rescaling methods often fall short because they focus solely\non the overall scaling rate, ignoring the varying amounts of information in\ndifferent parts of the image. To address this limitation, we propose a\nBlock-Based Multi-Scale Image Rescaling Framework (BBMR), tailored for IR tasks\ninvolving HR images of 2K resolution and higher. BBMR consists of two main\ncomponents: the Downscaling Module and the Upscaling Module. In the Downscaling\nModule, the HR image is segmented into sub-blocks of equal size, with each\nsub-block receiving a dynamically allocated scaling rate while maintaining a\nconstant overall scaling rate. For the Upscaling Module, we introduce the Joint\nSuper-Resolution method (JointSR), which performs SR on these sub-blocks with\nvarying scaling rates and effectively eliminates blocking artifacts.\nExperimental results demonstrate that BBMR significantly enhances the SR image\nquality on the of 2K and 4K test dataset compared to initial network image\nrescaling methods.\n","authors":["Jian Li","Siwang Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.11468v1.pdf","comment":"This paper has been accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.06499v2","updated":"2024-12-16T06:00:52Z","published":"2024-12-09T13:58:00Z","title":"HYATT-Net is Grand: A Hybrid Attention Network for Performant Anatomical\n  Landmark Detection","summary":"  Anatomical landmark detection (ALD) from a medical image is crucial for a\nwide array of clinical applications. While existing methods achieve quite some\nsuccess in ALD, they often struggle to balance global context with\ncomputational efficiency, particularly with high-resolution images, thereby\nleading to the rise of a natural question: where is the performance limit of\nALD? In this paper, we aim to forge performant ALD by proposing a {\\bf HY}brid\n{\\bf ATT}ention {\\bf Net}work (HYATT-Net) with the following designs: (i) A\nnovel hybrid architecture that integrates CNNs and Transformers. Its core is\nthe BiFormer module, utilizing Bi-Level Routing Attention for efficient\nattention to relevant image regions. This, combined with Attention Residual\nModule(ARM), enables precise local feature refinement guided by the global\ncontext. (ii) A Feature Fusion Correction Module that aggregates multi-scale\nfeatures and thus mitigates a resolution loss. Deep supervision with a\nmean-square error loss on multi-resolution heatmaps optimizes the model.\nExperiments on five diverse datasets demonstrate state-of-the-art performance,\nsurpassing existing methods in accuracy, robustness, and efficiency. The\nHYATT-Net provides a promising solution for accurate and efficient ALD in\ncomplex medical images. Our codes and data are already released at:\n\\url{https://github.com/ECNUACRush/HYATT-Net}.\n","authors":["Xiaoqian Zhou","Zhen Huang","Heqin Zhu","Qingsong Yao","S. Kevin Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.06499v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16252v3","updated":"2024-12-16T05:53:28Z","published":"2024-07-23T07:40:41Z","title":"LawLuo: A Multi-Agent Collaborative Framework for Multi-Round Chinese\n  Legal Consultation","summary":"  Legal Large Language Models (LLMs) have shown promise in providing legal\nconsultations to non-experts. However, most existing Chinese legal consultation\nmodels are based on single-agent systems, which differ from real-world legal\nconsultations, where multiple professionals collaborate to offer more tailored\nresponses. To better simulate real consultations, we propose LawLuo, a\nmulti-agent framework for multi-turn Chinese legal consultations. LawLuo\nincludes four agents: the receptionist agent, which assesses user intent and\nselects a lawyer agent; the lawyer agent, which interacts with the user; the\nsecretary agent, which organizes conversation records and generates\nconsultation reports; and the boss agent, which evaluates the performance of\nthe lawyer and secretary agents to ensure optimal results. These agents'\ninteractions mimic the operations of real law firms. To train them to follow\ndifferent legal instructions, we developed distinct fine-tuning datasets. We\nalso introduce a case graph-based RAG to help the lawyer agent address vague\nuser inputs. Experimental results show that LawLuo outperforms baselines in\ngenerating more personalized and professional responses, handling ambiguous\nqueries, and following legal instructions in multi-turn conversations. Our full\ncode and constructed datasets will be open-sourced upon paper acceptance.\n","authors":["Jingyun Sun","Chengxiao Dai","Zhongze Luo","Yangbo Chang","Yang Li"],"pdf_url":"https://arxiv.org/pdf/2407.16252v3.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2412.09177v2","updated":"2024-12-16T05:51:27Z","published":"2024-12-12T11:09:56Z","title":"Weighted Poisson-disk Resampling on Large-Scale Point Clouds","summary":"  For large-scale point cloud processing, resampling takes the important role\nof controlling the point number and density while keeping the geometric\nconsistency. % in related tasks. However, current methods cannot balance such\ndifferent requirements. Particularly with large-scale point clouds, classical\nmethods often struggle with decreased efficiency and accuracy. To address such\nissues, we propose a weighted Poisson-disk (WPD) resampling method to improve\nthe usability and efficiency for the processing. We first design an initial\nPoisson resampling with a voxel-based estimation strategy. It is able to\nestimate a more accurate radius of the Poisson-disk while maintaining high\nefficiency. Then, we design a weighted tangent smoothing step to further\noptimize the Voronoi diagram for each point. At the same time, sharp features\nare detected and kept in the optimized results with isotropic property.\nFinally, we achieve a resampling copy from the original point cloud with the\nspecified point number, uniform density, and high-quality geometric\nconsistency. Experiments show that our method significantly improves the\nperformance of large-scale point cloud resampling for different applications,\nand provides a highly practical solution.\n","authors":["Xianhe Jiao","Chenlei Lv","Junli Zhao","Ran Yi","Yu-Hui Wen","Zhenkuan Pan","Zhongke Wu","Yong-jin Liu"],"pdf_url":"https://arxiv.org/pdf/2412.09177v2.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2412.11467v1","updated":"2024-12-16T05:48:44Z","published":"2024-12-16T05:48:44Z","title":"Exploring Temporal Event Cues for Dense Video Captioning in Cyclic\n  Co-learning","summary":"  Dense video captioning aims to detect and describe all events in untrimmed\nvideos. This paper presents a dense video captioning network called\nMulti-Concept Cyclic Learning (MCCL), which aims to: (1) detect multiple\nconcepts at the frame level, using these concepts to enhance video features and\nprovide temporal event cues; and (2) design cyclic co-learning between the\ngenerator and the localizer within the captioning network to promote semantic\nperception and event localization. Specifically, we perform weakly supervised\nconcept detection for each frame, and the detected concept embeddings are\nintegrated into the video features to provide event cues. Additionally,\nvideo-level concept contrastive learning is introduced to obtain more\ndiscriminative concept embeddings. In the captioning network, we establish a\ncyclic co-learning strategy where the generator guides the localizer for event\nlocalization through semantic matching, while the localizer enhances the\ngenerator's event semantic perception through location matching, making\nsemantic perception and event localization mutually beneficial. MCCL achieves\nstate-of-the-art performance on the ActivityNet Captions and YouCook2 datasets.\nExtensive experiments demonstrate its effectiveness and interpretability.\n","authors":["Zhuyang Xie","Yan Yang","Yankai Yu","Jie Wang","Yongquan Jiang","Xiao Wu"],"pdf_url":"https://arxiv.org/pdf/2412.11467v1.pdf","comment":"Accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2412.11464v1","updated":"2024-12-16T05:44:45Z","published":"2024-12-16T05:44:45Z","title":"MaskCLIP++: A Mask-Based CLIP Fine-tuning Framework for Open-Vocabulary\n  Image Segmentation","summary":"  Open-vocabulary image segmentation has been advanced through the synergy\nbetween mask generators and vision-language models like Contrastive\nLanguage-Image Pre-training (CLIP). Previous approaches focus on generating\nmasks while aligning mask features with text embeddings during training. In\nthis paper, we observe that relying on generated low-quality masks can weaken\nthe alignment of vision and language in regional representations. This\nmotivates us to present a new fine-tuning framework, named MaskCLIP++, which\nuses ground-truth masks instead of generated masks to enhance the mask\nclassification capability of CLIP. Due to the limited diversity of image\nsegmentation datasets with mask annotations, we propose incorporating a\nconsistency alignment constraint during fine-tuning, which alleviates\ncategorical bias toward the fine-tuning dataset. After low-cost fine-tuning,\ncombining with the mask generator in previous state-of-the-art mask-based open\nvocabulary segmentation methods, we achieve performance improvements of +1.7,\n+2.3, +2.1, +3.1, and +0.3 mIoU on the A-847, PC-459, A-150, PC-59, and PAS-20\ndatasets, respectively.\n","authors":["Quan-Sheng Zeng","Yunheng Li","Daquan Zhou","Guanbin Li","Qibin Hou","Ming-Ming Cheng"],"pdf_url":"https://arxiv.org/pdf/2412.11464v1.pdf","comment":"20 pages, 8 figures"},{"id":"http://arxiv.org/abs/2412.11463v1","updated":"2024-12-16T05:43:14Z","published":"2024-12-16T05:43:14Z","title":"FedCAR: Cross-client Adaptive Re-weighting for Generative Models in\n  Federated Learning","summary":"  Generative models trained on multi-institutional datasets can provide an\nenriched understanding through diverse data distributions. However, training\nthe models on medical images is often challenging due to hospitals' reluctance\nto share data for privacy reasons. Federated learning(FL) has emerged as a\nprivacy-preserving solution for training distributed datasets across data\ncenters by aggregating model weights from multiple clients instead of sharing\nraw data. Previous research has explored the adaptation of FL to generative\nmodels, yet effective aggregation algorithms specifically tailored for\ngenerative models remain unexplored. We hereby propose a novel algorithm aimed\nat improving the performance of generative models within FL. Our approach\nadaptively re-weights the contribution of each client, resulting in\nwell-trained shared parameters. In each round, the server side measures the\ndistribution distance between fake images generated by clients instead of\ndirectly comparing the Fr\\'echet Inception Distance per client, thereby\nenhancing efficiency of the learning. Experimental results on three public\nchest X-ray datasets show superior performance in medical image generation,\noutperforming both centralized learning and conventional FL algorithms. Our\ncode is available at https://github.com/danny0628/FedCAR.\n","authors":["Minjun Kim","Minjee Kim","Jinhoon Jeong"],"pdf_url":"https://arxiv.org/pdf/2412.11463v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11458v1","updated":"2024-12-16T05:32:28Z","published":"2024-12-16T05:32:28Z","title":"HResFormer: Hybrid Residual Transformer for Volumetric Medical Image\n  Segmentation","summary":"  Vision Transformer shows great superiority in medical image segmentation due\nto the ability in learning long-range dependency. For medical image\nsegmentation from 3D data, such as computed tomography (CT), existing methods\ncan be broadly classified into 2D-based and 3D-based methods. One key\nlimitation in 2D-based methods is that the intra-slice information is ignored,\nwhile the limitation in 3D-based methods is the high computation cost and\nmemory consumption, resulting in a limited feature representation for\ninner-slice information. During the clinical examination, radiologists\nprimarily use the axial plane and then routinely review both axial and coronal\nplanes to form a 3D understanding of anatomy. Motivated by this fact, our key\ninsight is to design a hybrid model which can first learn fine-grained\ninner-slice information and then generate a 3D understanding of anatomy by\nincorporating 3D information. We present a novel \\textbf{H}ybrid\n\\textbf{Res}idual trans\\textbf{Former} \\textbf{(HResFormer)} for 3D medical\nimage segmentation. Building upon standard 2D and 3D Transformer backbones,\nHResFormer involves two novel key designs: \\textbf{(1)} a \\textbf{H}ybrid\n\\textbf{L}ocal-\\textbf{G}lobal fusion \\textbf{M}odule \\textbf{(HLGM)} to\neffectively and adaptively fuse inner-slice information from 2D Transformer and\nintra-slice information from 3D volumes for 3D Transformer with local\nfine-grained and global long-range representation. \\textbf{(2)} a residual\nlearning of the hybrid model, which can effectively leverage the inner-slice\nand intra-slice information for better 3D understanding of anatomy. Experiments\nshow that our HResFormer outperforms prior art on widely-used medical image\nsegmentation benchmarks. This paper sheds light on an important but neglected\nway to design Transformers for 3D medical image segmentation.\n","authors":["Sucheng Ren","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2412.11458v1.pdf","comment":"Accepted by TNNLS"},{"id":"http://arxiv.org/abs/2412.11457v1","updated":"2024-12-16T05:23:45Z","published":"2024-12-16T05:23:45Z","title":"MOVIS: Enhancing Multi-Object Novel View Synthesis for Indoor Scenes","summary":"  Repurposing pre-trained diffusion models has been proven to be effective for\nNVS. However, these methods are mostly limited to a single object; directly\napplying such methods to compositional multi-object scenarios yields inferior\nresults, especially incorrect object placement and inconsistent shape and\nappearance under novel views. How to enhance and systematically evaluate the\ncross-view consistency of such models remains under-explored. To address this\nissue, we propose MOVIS to enhance the structural awareness of the\nview-conditioned diffusion model for multi-object NVS in terms of model inputs,\nauxiliary tasks, and training strategy. First, we inject structure-aware\nfeatures, including depth and object mask, into the denoising U-Net to enhance\nthe model's comprehension of object instances and their spatial relationships.\nSecond, we introduce an auxiliary task requiring the model to simultaneously\npredict novel view object masks, further improving the model's capability in\ndifferentiating and placing objects. Finally, we conduct an in-depth analysis\nof the diffusion sampling process and carefully devise a structure-guided\ntimestep sampling scheduler during training, which balances the learning of\nglobal object placement and fine-grained detail recovery. To systematically\nevaluate the plausibility of synthesized images, we propose to assess\ncross-view consistency and novel view object placement alongside existing\nimage-level NVS metrics. Extensive experiments on challenging synthetic and\nrealistic datasets demonstrate that our method exhibits strong generalization\ncapabilities and produces consistent novel view synthesis, highlighting its\npotential to guide future 3D-aware multi-object NVS tasks.\n","authors":["Ruijie Lu","Yixin Chen","Junfeng Ni","Baoxiong Jia","Yu Liu","Diwen Wan","Gang Zeng","Siyuan Huang"],"pdf_url":"https://arxiv.org/pdf/2412.11457v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11452v1","updated":"2024-12-16T05:14:08Z","published":"2024-12-16T05:14:08Z","title":"Multilabel Classification for Lung Disease Detection: Integrating Deep\n  Learning and Natural Language Processing","summary":"  Classifying chest radiographs is a time-consuming and challenging task, even\nfor experienced radiologists. This provides an area for improvement due to the\ndifficulty in precisely distinguishing between conditions such as pleural\neffusion, pneumothorax, and pneumonia. We propose a novel transfer learning\nmodel for multi-label lung disease classification, utilizing the CheXpert\ndataset with over 12,617 images of frontal radiographs being analyzed. By\nintegrating RadGraph parsing for efficient annotation extraction, we enhance\nthe model's ability to accurately classify multiple lung diseases from complex\nmedical images. The proposed model achieved an F1 score of 0.69 and an AUROC of\n0.86, demonstrating its potential for clinical applications. Also explored was\nthe use of Natural Language Processing (NLP) to parse report metadata and\naddress uncertainties in disease classification. By comparing uncertain reports\nwith more certain cases, the NLP-enhanced model improves its ability to\nconclusively classify conditions. This research highlights the connection\nbetween deep learning and NLP, underscoring their potential to enhance\nradiological diagnostics and aid in the efficient analysis of chest\nradiographs.\n","authors":["Maria Efimovich","Jayden Lim","Vedant Mehta","Ethan Poon"],"pdf_url":"https://arxiv.org/pdf/2412.11452v1.pdf","comment":"All authors contributed equally"},{"id":"http://arxiv.org/abs/2412.11450v1","updated":"2024-12-16T05:08:15Z","published":"2024-12-16T05:08:15Z","title":"GroupFace: Imbalanced Age Estimation Based on Multi-hop Attention Graph\n  Convolutional Network and Group-aware Margin Optimization","summary":"  With the recent advances in computer vision, age estimation has significantly\nimproved in overall accuracy. However, owing to the most common methods do not\ntake into account the class imbalance problem in age estimation datasets, they\nsuffer from a large bias in recognizing long-tailed groups. To achieve\nhigh-quality imbalanced learning in long-tailed groups, the dominant solution\nlies in that the feature extractor learns the discriminative features of\ndifferent groups and the classifier is able to provide appropriate and unbiased\nmargins for different groups by the discriminative features. Therefore, in this\nnovel, we propose an innovative collaborative learning framework (GroupFace)\nthat integrates a multi-hop attention graph convolutional network and a dynamic\ngroup-aware margin strategy based on reinforcement learning. Specifically, to\nextract the discriminative features of different groups, we design an enhanced\nmulti-hop attention graph convolutional network. This network is capable of\ncapturing the interactions of neighboring nodes at different distances, fusing\nlocal and global information to model facial deep aging, and exploring diverse\nrepresentations of different groups. In addition, to further address the class\nimbalance problem, we design a dynamic group-aware margin strategy based on\nreinforcement learning to provide appropriate and unbiased margins for\ndifferent groups. The strategy divides the sample into four age groups and\nconsiders identifying the optimum margins for various age groups by employing a\nMarkov decision process. Under the guidance of the agent, the feature\nrepresentation bias and the classification margin deviation between different\ngroups can be reduced simultaneously, balancing inter-class separability and\nintra-class proximity. After joint optimization, our architecture achieves\nexcellent performance on several age estimation benchmark datasets.\n","authors":["Yiping Zhang","Yuntao Shou","Wei Ai","Tao Meng","Keqin Li"],"pdf_url":"https://arxiv.org/pdf/2412.11450v1.pdf","comment":"15 pages, 10 figures"},{"id":"http://arxiv.org/abs/2412.11443v1","updated":"2024-12-16T04:55:13Z","published":"2024-12-16T04:55:13Z","title":"Universal Domain Adaptive Object Detection via Dual Probabilistic\n  Alignment","summary":"  Domain Adaptive Object Detection (DAOD) transfers knowledge from a labeled\nsource domain to an unannotated target domain under closed-set assumption.\nUniversal DAOD (UniDAOD) extends DAOD to handle open-set, partial-set, and\nclosed-set domain adaptation. In this paper, we first unveil two issues:\ndomain-private category alignment is crucial for global-level features, and the\ndomain probability heterogeneity of features across different levels. To\naddress these issues, we propose a novel Dual Probabilistic Alignment (DPA)\nframework to model domain probability as Gaussian distribution, enabling the\nheterogeneity domain distribution sampling and measurement. The DPA consists of\nthree tailored modules: the Global-level Domain Private Alignment (GDPA), the\nInstance-level Domain Shared Alignment (IDSA), and the Private Class Constraint\n(PCC). GDPA utilizes the global-level sampling to mine domain-private category\nsamples and calculate alignment weight through a cumulative distribution\nfunction to address the global-level private category alignment. IDSA utilizes\ninstance-level sampling to mine domain-shared category samples and calculates\nalignment weight through Gaussian distribution to conduct the domain-shared\ncategory domain alignment to address the feature heterogeneity. The PCC\naggregates domain-private category centroids between feature and probability\nspaces to mitigate negative transfer. Extensive experiments demonstrate that\nour DPA outperforms state-of-the-art UniDAOD and DAOD methods across various\ndatasets and scenarios, including open, partial, and closed sets. Codes are\navailable at \\url{https://github.com/zyfone/DPA}.\n","authors":["Yuanfan Zheng","Jinlin Wu","Wuyang Li","Zhen Chen"],"pdf_url":"https://arxiv.org/pdf/2412.11443v1.pdf","comment":"This work is accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2405.13082v4","updated":"2024-12-16T04:44:47Z","published":"2024-05-21T06:44:40Z","title":"A Survey of Artificial Intelligence in Gait-Based Neurodegenerative\n  Disease Diagnosis","summary":"  Recent years have witnessed an increasing global population affected by\nneurodegenerative diseases (NDs), which traditionally require extensive\nhealthcare resources and human effort for medical diagnosis and monitoring. As\na crucial disease-related motor symptom, human gait can be exploited to\ncharacterize different NDs. The current advances in artificial intelligence\n(AI) models enable automatic gait analysis for NDs identification and\nclassification, opening a new avenue to facilitate faster and more\ncost-effective diagnosis of NDs. In this paper, we provide a comprehensive\nsurvey on recent progress of machine learning and deep learning based AI\ntechniques applied to diagnosis of five typical NDs through gait. We provide an\noverview of the process of AI-assisted NDs diagnosis, and present a systematic\ntaxonomy of existing gait data and AI models. Meanwhile, a novel quality\nevaluation criterion is proposed to quantitatively assess the quality of\nexisting studies. Through an extensive review and analysis of 169 studies, we\npresent recent technical advancements, discuss existing challenges, potential\nsolutions, and future directions in this field. Finally, we envision the\nprospective utilization of 3D skeleton data for human gait representation and\nthe development of more efficient AI models for NDs diagnosis.\n","authors":["Haocong Rao","Minlin Zeng","Xuejiao Zhao","Chunyan Miao"],"pdf_url":"https://arxiv.org/pdf/2405.13082v4.pdf","comment":"Article: 57 pages, citing 290 papers. Appendix: 30 pages. A\n  up-to-date resource (papers, data, etc.) of this survey (AI4NDD) is provided\n  at https://github.com/minlinzeng/AI4NDD-Survey"},{"id":"http://arxiv.org/abs/2412.11435v1","updated":"2024-12-16T04:23:33Z","published":"2024-12-16T04:23:33Z","title":"Learning Implicit Features with Flow Infused Attention for Realistic\n  Virtual Try-On","summary":"  Image-based virtual try-on is challenging since the generated image should\nfit the garment to model images in various poses and keep the characteristics\nand details of the garment simultaneously. A popular research stream warps the\ngarment image firstly to reduce the burden of the generation stage, which\nrelies highly on the performance of the warping module. Other methods without\nexplicit warping often lack sufficient guidance to fit the garment to the model\nimages. In this paper, we propose FIA-VTON, which leverages the implicit warp\nfeature by adopting a Flow Infused Attention module on virtual try-on. The\ndense warp flow map is projected as indirect guidance attention to enhance the\nfeature map warping in the generation process implicitly, which is less\nsensitive to the warping estimation accuracy than an explicit warp of the\ngarment image. To further enhance implicit warp guidance, we incorporate\nhigh-level spatial attention to complement the dense warp. Experimental results\non the VTON-HD and DressCode dataset significantly outperform state-of-the-art\nmethods, demonstrating that FIA-VTON is effective and robust for virtual\ntry-on.\n","authors":["Delong Zhang","Qiwei Huang","Yuanliu Liu","Yang Sun","Wei-Shi Zheng","Pengfei Xiong","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.11435v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.03887v3","updated":"2024-12-16T04:21:55Z","published":"2024-12-05T05:40:40Z","title":"MOANA: Multi-Radar Dataset for Maritime Odometry and Autonomous\n  Navigation Application","summary":"  Maritime environmental sensing requires overcoming challenges from complex\nconditions such as harsh weather, platform perturbations, large dynamic\nobjects, and the requirement for long detection ranges. While cameras and LiDAR\nare commonly used in ground vehicle navigation, their applicability in maritime\nsettings is limited by range constraints and hardware maintenance issues. Radar\nsensors, however, offer robust long-range detection capabilities and resilience\nto physical contamination from weather and saline conditions, making it a\npowerful sensor for maritime navigation. Among various radar types, X-band\nradar (e.g., marine radar) is widely employed for maritime vessel navigation,\nproviding effective long-range detection essential for situational awareness\nand collision avoidance. Nevertheless, it exhibits limitations during berthing\noperations where close-range object detection is critical. To address this\nshortcoming, we incorporate W-band radar (e.g., Navtech imaging radar), which\nexcels in detecting nearby objects with a higher update rate. We present a\ncomprehensive maritime sensor dataset featuring multi-range detection\ncapabilities. This dataset integrates short-range LiDAR data, medium-range\nW-band radar data, and long-range X-band radar data into a unified framework.\nAdditionally, it includes object labels for oceanic object detection usage,\nderived from radar and stereo camera images. The dataset comprises seven\nsequences collected from diverse regions with varying levels of estimation\ndifficulty, ranging from easy to challenging, and includes common locations\nsuitable for global localization tasks. This dataset serves as a valuable\nresource for advancing research in place recognition, odometry estimation,\nSLAM, object detection, and dynamic object elimination within maritime\nenvironments. Dataset can be found in following link:\nhttps://sites.google.com/view/rpmmoana\n","authors":["Hyesu Jang","Wooseong Yang","Hanguen Kim","Dongje Lee","Yongjin Kim","Jinbum Park","Minsoo Jeon","Jaeseong Koh","Yejin Kang","Minwoo Jung","Sangwoo Jung","Chng Zhen Hao","Wong Yu Hin","Chew Yihang","Ayoung Kim"],"pdf_url":"https://arxiv.org/pdf/2412.03887v3.pdf","comment":"9 pages, 8 figures, 3 tables"},{"id":"http://arxiv.org/abs/2406.16464v5","updated":"2024-12-16T04:13:38Z","published":"2024-06-24T09:13:42Z","title":"InterCLIP-MEP: Interactive CLIP and Memory-Enhanced Predictor for\n  Multi-modal Sarcasm Detection","summary":"  Sarcasm in social media, often expressed through text-image combinations,\nposes challenges for sentiment analysis and intention mining. Current\nmulti-modal sarcasm detection methods have been demonstrated to overly rely on\nspurious cues within the textual modality, revealing a limited ability to\ngenuinely identify sarcasm through nuanced text-image interactions. To solve\nthis problem, we propose InterCLIP-MEP, which introduces Interactive CLIP\n(InterCLIP) with an efficient training strategy to extract enriched text-image\nrepresentations by embedding cross-modal information directly into each\nencoder. Additionally, we design a Memory-Enhanced Predictor (MEP) with a\ndynamic dual-channel memory that stores valuable test sample knowledge during\ninference, acting as a non-parametric classifier for robust sarcasm\nrecognition. Experiments on two benchmarks demonstrate that InterCLIP-MEP\nachieves state-of-the-art performance, with significant accuracy and F1 score\nimprovements on MMSD and MMSD2.0. Our code is available at\nhttps://github.com/CoderChen01/InterCLIP-MEP.\n","authors":["Junjie Chen","Hang Yu","Subin Huang","Sanmin Liu","Linfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.16464v5.pdf","comment":"15 pages, 7 figures, 11 tables; Code and data are available at\n  https://github.com/CoderChen01/InterCLIP-MEP"},{"id":"http://arxiv.org/abs/2412.09645v2","updated":"2024-12-16T04:05:05Z","published":"2024-12-10T18:52:39Z","title":"Evaluation Agent: Efficient and Promptable Evaluation Framework for\n  Visual Generative Models","summary":"  Recent advancements in visual generative models have enabled high-quality\nimage and video generation, opening diverse applications. However, evaluating\nthese models often demands sampling hundreds or thousands of images or videos,\nmaking the process computationally expensive, especially for diffusion-based\nmodels with inherently slow sampling. Moreover, existing evaluation methods\nrely on rigid pipelines that overlook specific user needs and provide numerical\nresults without clear explanations. In contrast, humans can quickly form\nimpressions of a model's capabilities by observing only a few samples. To mimic\nthis, we propose the Evaluation Agent framework, which employs human-like\nstrategies for efficient, dynamic, multi-round evaluations using only a few\nsamples per round, while offering detailed, user-tailored analyses. It offers\nfour key advantages: 1) efficiency, 2) promptable evaluation tailored to\ndiverse user needs, 3) explainability beyond single numerical scores, and 4)\nscalability across various models and tools. Experiments show that Evaluation\nAgent reduces evaluation time to 10% of traditional methods while delivering\ncomparable results. The Evaluation Agent framework is fully open-sourced to\nadvance research in visual generative models and their efficient evaluation.\n","authors":["Fan Zhang","Shulin Tian","Ziqi Huang","Yu Qiao","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2412.09645v2.pdf","comment":"Equal contributions from first three authors. Project page:\n  https://vchitect.github.io/Evaluation-Agent-project Code:\n  https://github.com/Vchitect/Evaluation-Agent"},{"id":"http://arxiv.org/abs/2412.11428v1","updated":"2024-12-16T03:54:08Z","published":"2024-12-16T03:54:08Z","title":"View Transformation Robustness for Multi-View 3D Object Reconstruction\n  with Reconstruction Error-Guided View Selection","summary":"  View transformation robustness (VTR) is critical for deep-learning-based\nmulti-view 3D object reconstruction models, which indicates the methods'\nstability under inputs with various view transformations. However, existing\nresearch seldom focused on view transformation robustness in multi-view 3D\nobject reconstruction. One direct way to improve the models' VTR is to produce\ndata with more view transformations and add them to model training. Recent\nprogress on large vision models, particularly Stable Diffusion models, has\nprovided great potential for generating 3D models or synthesizing novel view\nimages with only a single image input. Directly deploying these models at\ninference consumes heavy computation resources and their robustness to view\ntransformations is not guaranteed either. To fully utilize the power of Stable\nDiffusion models without extra inference computation burdens, we propose to\ngenerate novel views with Stable Diffusion models for better view\ntransformation robustness. Instead of synthesizing random views, we propose a\nreconstruction error-guided view selection method, which considers the\nreconstruction errors' spatial distribution of the 3D predictions and chooses\nthe views that could cover the reconstruction errors as much as possible. The\nmethods are trained and tested on sets with large view transformations to\nvalidate the 3D reconstruction models' robustness to view transformations.\nExtensive experiments demonstrate that the proposed method can outperform\nstate-of-the-art 3D reconstruction methods and other view transformation\nrobustness comparison methods.\n","authors":["Qi Zhang","Zhouhang Luo","Tao Yu","Hui Huang"],"pdf_url":"https://arxiv.org/pdf/2412.11428v1.pdf","comment":"Accepted to AAAI 25"},{"id":"http://arxiv.org/abs/2412.11423v1","updated":"2024-12-16T03:46:45Z","published":"2024-12-16T03:46:45Z","title":"Nearly Zero-Cost Protection Against Mimicry by Personalized Diffusion\n  Models","summary":"  Recent advancements in diffusion models revolutionize image generation but\npose risks of misuse, such as replicating artworks or generating deepfakes.\nExisting image protection methods, though effective, struggle to balance\nprotection efficacy, invisibility, and latency, thus limiting practical use. We\nintroduce perturbation pre-training to reduce latency and propose a\nmixture-of-perturbations approach that dynamically adapts to input images to\nminimize performance degradation. Our novel training strategy computes\nprotection loss across multiple VAE feature spaces, while adaptive targeted\nprotection at inference enhances robustness and invisibility. Experiments show\ncomparable protection performance with improved invisibility and drastically\nreduced inference time. The code and demo are available at\n\\url{https://webtoon.github.io/impasto}\n","authors":["Namhyuk Ahn","KiYoon Yoo","Wonhyuk Ahn","Daesik Kim","Seung-Hun Nam"],"pdf_url":"https://arxiv.org/pdf/2412.11423v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11420v1","updated":"2024-12-16T03:39:33Z","published":"2024-12-16T03:39:33Z","title":"Category Level 6D Object Pose Estimation from a Single RGB Image using\n  Diffusion","summary":"  Estimating the 6D pose and 3D size of an object from an image is a\nfundamental task in computer vision. Most current approaches are restricted to\nspecific instances with known models or require ground truth depth information\nor point cloud captures from LIDAR. We tackle the harder problem of pose\nestimation for category-level objects from a single RGB image. We propose a\nnovel solution that eliminates the need for specific object models or depth\ninformation. Our method utilises score-based diffusion models to generate\nobject pose hypotheses to model the distribution of possible poses for the\nobject. Unlike previous methods that rely on costly trained likelihood\nestimators to remove outliers before pose aggregation using mean pooling, we\nintroduce a simpler approach using Mean Shift to estimate the mode of the\ndistribution as the final pose estimate. Our approach outperforms the current\nstate-of-the-art on the REAL275 dataset by a significant margin.\n","authors":["Adam Bethell","Ravi Garg","Ian Reid"],"pdf_url":"https://arxiv.org/pdf/2412.11420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10275v2","updated":"2024-12-16T03:32:09Z","published":"2024-12-13T16:52:13Z","title":"TIV-Diffusion: Towards Object-Centric Movement for Text-driven Image to\n  Video Generation","summary":"  Text-driven Image to Video Generation (TI2V) aims to generate controllable\nvideo given the first frame and corresponding textual description. The primary\nchallenges of this task lie in two parts: (i) how to identify the target\nobjects and ensure the consistency between the movement trajectory and the\ntextual description. (ii) how to improve the subjective quality of generated\nvideos. To tackle the above challenges, we propose a new diffusion-based TI2V\nframework, termed TIV-Diffusion, via object-centric textual-visual alignment,\nintending to achieve precise control and high-quality video generation based on\ntextual-described motion for different objects. Concretely, we enable our\nTIV-Diffuion model to perceive the textual-described objects and their motion\ntrajectory by incorporating the fused textual and visual knowledge through\nscale-offset modulation. Moreover, to mitigate the problems of object\ndisappearance and misaligned objects and motion, we introduce an object-centric\ntextual-visual alignment module, which reduces the risk of misaligned\nobjects/motion by decoupling the objects in the reference image and aligning\ntextual features with each object individually. Based on the above innovations,\nour TIV-Diffusion achieves state-of-the-art high-quality video generation\ncompared with existing TI2V methods.\n","authors":["Xingrui Wang","Xin Li","Yaosi Hu","Hanxin Zhu","Chen Hou","Cuiling Lan","Zhibo Chen"],"pdf_url":"https://arxiv.org/pdf/2412.10275v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.11412v1","updated":"2024-12-16T03:28:00Z","published":"2024-12-16T03:28:00Z","title":"V-MIND: Building Versatile Monocular Indoor 3D Detector with Diverse 2D\n  Annotations","summary":"  The field of indoor monocular 3D object detection is gaining significant\nattention, fueled by the increasing demand in VR/AR and robotic applications.\nHowever, its advancement is impeded by the limited availability and diversity\nof 3D training data, owing to the labor-intensive nature of 3D data collection\nand annotation processes. In this paper, we present V-MIND (Versatile Monocular\nINdoor Detector), which enhances the performance of indoor 3D detectors across\na diverse set of object classes by harnessing publicly available large-scale 2D\ndatasets. By leveraging well-established monocular depth estimation techniques\nand camera intrinsic predictors, we can generate 3D training data by converting\nlarge-scale 2D images into 3D point clouds and subsequently deriving pseudo 3D\nbounding boxes. To mitigate distance errors inherent in the converted point\nclouds, we introduce a novel 3D self-calibration loss for refining the pseudo\n3D bounding boxes during training. Additionally, we propose a novel ambiguity\nloss to address the ambiguity that arises when introducing new classes from 2D\ndatasets. Finally, through joint training with existing 3D datasets and pseudo\n3D bounding boxes derived from 2D datasets, V-MIND achieves state-of-the-art\nobject detection performance across a wide range of classes on the Omni3D\nindoor dataset.\n","authors":["Jin-Cheng Jhang","Tao Tu","Fu-En Wang","Ke Zhang","Min Sun","Cheng-Hao Kuo"],"pdf_url":"https://arxiv.org/pdf/2412.11412v1.pdf","comment":"WACV 2025"},{"id":"http://arxiv.org/abs/2412.11409v1","updated":"2024-12-16T03:25:23Z","published":"2024-12-16T03:25:23Z","title":"Multi-modal and Multi-scale Spatial Environment Understanding for\n  Immersive Visual Text-to-Speech","summary":"  Visual Text-to-Speech (VTTS) aims to take the environmental image as the\nprompt to synthesize the reverberant speech for the spoken content. The\nchallenge of this task lies in understanding the spatial environment from the\nimage. Many attempts have been made to extract global spatial visual\ninformation from the RGB space of an spatial image. However, local and depth\nimage information are crucial for understanding the spatial environment, which\nprevious works have ignored. To address the issues, we propose a novel\nmulti-modal and multi-scale spatial environment understanding scheme to achieve\nimmersive VTTS, termed M2SE-VTTS. The multi-modal aims to take both the RGB and\nDepth spaces of the spatial image to learn more comprehensive spatial\ninformation, and the multi-scale seeks to model the local and global spatial\nknowledge simultaneously. Specifically, we first split the RGB and Depth images\ninto patches and adopt the Gemini-generated environment captions to guide the\nlocal spatial understanding. After that, the multi-modal and multi-scale\nfeatures are integrated by the local-aware global spatial understanding. In\nthis way, M2SE-VTTS effectively models the interactions between local and\nglobal spatial contexts in the multi-modal spatial environment. Objective and\nsubjective evaluations suggest that our model outperforms the advanced\nbaselines in environmental speech generation. The code and audio samples are\navailable at: https://github.com/AI-S2-Lab/M2SE-VTTS.\n","authors":["Rui Liu","Shuwei He","Yifan Hu","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2412.11409v1.pdf","comment":"9 pages,2 figures, Accepted by AAAI'2025"},{"id":"http://arxiv.org/abs/2412.11407v1","updated":"2024-12-16T03:21:20Z","published":"2024-12-16T03:21:20Z","title":"An Enhanced Classification Method Based on Adaptive Multi-Scale Fusion\n  for Long-tailed Multispectral Point Clouds","summary":"  Multispectral point cloud (MPC) captures 3D spatial-spectral information from\nthe observed scene, which can be used for scene understanding and has a wide\nrange of applications. However, most of the existing classification methods\nwere extensively tested on indoor datasets, and when applied to outdoor\ndatasets they still face problems including sparse labeled targets, differences\nin land-covers scales, and long-tailed distributions. To address the above\nissues, an enhanced classification method based on adaptive multi-scale fusion\nfor MPCs with long-tailed distributions is proposed. In the training set\ngeneration stage, a grid-balanced sampling strategy is designed to reliably\ngenerate training samples from sparse labeled datasets. In the feature learning\nstage, a multi-scale feature fusion module is proposed to fuse shallow features\nof land-covers at different scales, addressing the issue of losing fine\nfeatures due to scale variations in land-covers. In the classification stage,\nan adaptive hybrid loss module is devised to utilize multi-classification heads\nwith adaptive weights to balance the learning ability of different classes,\nimproving the classification performance of small classes due to various-scales\nand long-tailed distributions in land-covers. Experimental results on three MPC\ndatasets demonstrate the effectiveness of the proposed method compared with the\nstate-of-the-art methods.\n","authors":["TianZhu Liu","BangYan Hu","YanFeng Gu","Xian Li","Aleksandra Pižurica"],"pdf_url":"https://arxiv.org/pdf/2412.11407v1.pdf","comment":"16 pages, 9 figures, 5 tables"},{"id":"http://arxiv.org/abs/2409.08056v2","updated":"2024-12-16T03:19:27Z","published":"2024-09-12T14:05:13Z","title":"Expansive Supervision for Neural Radiance Field","summary":"  Neural Radiance Field (NeRF) has achieved remarkable success in creating\nimmersive media representations through its exceptional reconstruction\ncapabilities. However, the computational demands of dense forward passes and\nvolume rendering during training continue to challenge its real-world\napplications. In this paper, we introduce Expansive Supervision to reduce time\nand memory costs during NeRF training from the perspective of partial ray\nselection for supervision. Specifically, we observe that training errors\nexhibit a long-tail distribution correlated with image content. Based on this\nobservation, our method selectively renders a small but crucial subset of\npixels and expands their values to estimate errors across the entire area for\neach iteration. Compared to conventional supervision, our approach effectively\nbypasses redundant rendering processes, resulting in substantial reductions in\nboth time and memory consumption. Experimental results demonstrate that\nintegrating Expansive Supervision within existing state-of-the-art acceleration\nframeworks achieves 52% memory savings and 16% time savings while maintaining\ncomparable visual quality.\n","authors":["Weixiang Zhang","Shuzhao Xie","Shijia Ge","Wei Yao","Chen Tang","Zhi Wang"],"pdf_url":"https://arxiv.org/pdf/2409.08056v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08879v2","updated":"2024-12-16T03:16:59Z","published":"2024-12-12T02:27:46Z","title":"Video Repurposing from User Generated Content: A Large-scale Dataset and\n  Benchmark","summary":"  The demand for producing short-form videos for sharing on social media\nplatforms has experienced significant growth in recent times. Despite notable\nadvancements in the fields of video summarization and highlight detection,\nwhich can create partially usable short films from raw videos, these approaches\nare often domain-specific and require an in-depth understanding of real-world\nvideo content. To tackle this predicament, we propose Repurpose-10K, an\nextensive dataset comprising over 10,000 videos with more than 120,000\nannotated clips aimed at resolving the video long-to-short task. Recognizing\nthe inherent constraints posed by untrained human annotators, which can result\nin inaccurate annotations for repurposed videos, we propose a two-stage\nsolution to obtain annotations from real-world user-generated content.\nFurthermore, we offer a baseline model to address this challenging task by\nintegrating audio, visual, and caption aspects through a cross-modal fusion and\nalignment framework. We aspire for our work to ignite groundbreaking research\nin the lesser-explored realms of video repurposing.\n","authors":["Yongliang Wu","Wenbo Zhu","Jiawang Cao","Yi Lu","Bozheng Li","Weiheng Chi","Zihan Qiu","Lirian Su","Haolin Zheng","Jay Wu","Xu Yang"],"pdf_url":"https://arxiv.org/pdf/2412.08879v2.pdf","comment":"AAAI2025"},{"id":"http://arxiv.org/abs/2406.10126v3","updated":"2024-12-16T03:13:09Z","published":"2024-06-14T15:33:00Z","title":"Training-free Camera Control for Video Generation","summary":"  We propose a training-free and robust solution to offer camera movement\ncontrol for off-the-shelf video diffusion models. Unlike previous work, our\nmethod does not require any supervised finetuning on camera-annotated datasets\nor self-supervised training via data augmentation. Instead, it can be plugged\nand played with most pretrained video diffusion models and generate camera\ncontrollable videos with a single image or text prompt as input. The\ninspiration of our work comes from the layout prior that intermediate latents\nhold towards generated results, thus rearranging noisy pixels in them will make\noutput content reallocated as well. As camera move could also be seen as a kind\nof pixel rearrangement caused by perspective change, videos could be\nreorganized following specific camera motion if their noisy latents change\naccordingly. Established on this, we propose our method CamTrol, which enables\nrobust camera control for video diffusion models. It is achieved by a two-stage\nprocess. First, we model image layout rearrangement through explicit camera\nmovement in 3D point cloud space. Second, we generate videos with camera motion\nusing layout prior of noisy latents formed by a series of rearranged images.\nExtensive experiments have demonstrated the robustness our method holds in\ncontrolling camera motion of generated videos. Furthermore, we show that our\nmethod can produce impressive results in generating 3D rotation videos with\ndynamic content. Project page at https://lifedecoder.github.io/CamTrol/.\n","authors":["Chen Hou","Guoqiang Wei","Yan Zeng","Zhibo Chen"],"pdf_url":"https://arxiv.org/pdf/2406.10126v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09907v2","updated":"2024-12-16T03:04:33Z","published":"2024-12-13T06:52:02Z","title":"IQViC: In-context, Question Adaptive Vision Compressor for Long-term\n  Video Understanding LMMs","summary":"  With the increasing complexity of video data and the need for more efficient\nlong-term temporal understanding, existing long-term video understanding\nmethods often fail to accurately capture and analyze extended video sequences.\nThese methods typically struggle to maintain performance over longer durations\nand to handle the intricate dependencies within the video content. To address\nthese limitations, we propose a simple yet effective large multi-modal model\nframework for long-term video understanding that incorporates a novel visual\ncompressor, the In-context, Question Adaptive Visual Compressor (IQViC). The\nkey idea, inspired by humans' selective attention and in-context memory\nmechanisms, is to introduce a novel visual compressor and incorporate efficient\nmemory management techniques to enhance long-term video question answering. Our\nframework utilizes IQViC, a transformer-based visual compressor, enabling\nquestion-conditioned in-context compression, unlike existing methods that rely\non full video visual features. This selectively extracts relevant information,\nsignificantly reducing memory token requirements. Through extensive experiments\non a new dataset based on InfiniBench for long-term video understanding, and\nstandard benchmarks used for existing methods' evaluation, we demonstrate the\neffectiveness of our proposed IQViC framework and its superiority over\nstate-of-the-art methods in terms of video understanding accuracy and memory\nefficiency.\n","authors":["Sosuke Yamao","Natsuki Miyahara","Yuki Harazono","Shun Takeuchi"],"pdf_url":"https://arxiv.org/pdf/2412.09907v2.pdf","comment":"The first and second authors contributed equally to this work"},{"id":"http://arxiv.org/abs/2412.11396v1","updated":"2024-12-16T02:52:19Z","published":"2024-12-16T02:52:19Z","title":"Leveraging Retrieval-Augmented Tags for Large Vision-Language\n  Understanding in Complex Scenes","summary":"  Object-aware reasoning in vision-language tasks poses significant challenges\nfor current models, particularly in handling unseen objects, reducing\nhallucinations, and capturing fine-grained relationships in complex visual\nscenes. To address these limitations, we propose the Vision-Aware\nRetrieval-Augmented Prompting (VRAP) framework, a generative approach that\nenhances Large Vision-Language Models (LVLMs) by integrating\nretrieval-augmented object tags into their prompts. VRAP introduces a novel\npipeline where structured tags, including objects, attributes, and\nrelationships, are extracted using pretrained visual encoders and scene graph\nparsers. These tags are enriched with external knowledge and incorporated into\nthe LLM's input, enabling detailed and accurate reasoning. We evaluate VRAP\nacross multiple vision-language benchmarks, including VQAv2, GQA, VizWiz, and\nCOCO, achieving state-of-the-art performance in fine-grained reasoning and\nmultimodal understanding. Additionally, our ablation studies highlight the\nimportance of retrieval-augmented tags and contrastive learning, while human\nevaluations confirm VRAP's ability to generate accurate, detailed, and\ncontextually relevant responses. Notably, VRAP achieves a 40% reduction in\ninference latency by eliminating runtime retrieval. These results demonstrate\nthat VRAP is a robust and efficient framework for advancing object-aware\nmultimodal reasoning.\n","authors":["Antonio Carlos Rivera","Anthony Moore","Steven Robinson"],"pdf_url":"https://arxiv.org/pdf/2412.11396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11395v1","updated":"2024-12-16T02:48:55Z","published":"2024-12-16T02:48:55Z","title":"Depth-Centric Dehazing and Depth-Estimation from Real-World Hazy Driving\n  Video","summary":"  In this paper, we study the challenging problem of simultaneously removing\nhaze and estimating depth from real monocular hazy videos. These tasks are\ninherently complementary: enhanced depth estimation improves dehazing via the\natmospheric scattering model (ASM), while superior dehazing contributes to more\naccurate depth estimation through the brightness consistency constraint (BCC).\nTo tackle these intertwined tasks, we propose a novel depth-centric learning\nframework that integrates the ASM model with the BCC constraint. Our key idea\nis that both ASM and BCC rely on a shared depth estimation network. This\nnetwork simultaneously exploits adjacent dehazed frames to enhance depth\nestimation via BCC and uses the refined depth cues to more effectively remove\nhaze through ASM. Additionally, we leverage a non-aligned clear video and its\nestimated depth to independently regularize the dehazing and depth estimation\nnetworks. This is achieved by designing two discriminator networks: $D_{MFIR}$\nenhances high-frequency details in dehazed videos, and $D_{MDR}$ reduces the\noccurrence of black holes in low-texture regions. Extensive experiments\ndemonstrate that the proposed method outperforms current state-of-the-art\ntechniques in both video dehazing and depth estimation tasks, especially in\nreal-world hazy scenes. Project page:\nhttps://fanjunkai1.github.io/projectpage/DCL/index.html.\n","authors":["Junkai Fan","Kun Wang","Zhiqiang Yan","Xiang Chen","Shangbing Gao","Jun Li","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2412.11395v1.pdf","comment":"Accepted by AAAI 20205, Project page:\n  https://fanjunkai1.github.io/projectpage/DCL/index.html"},{"id":"http://arxiv.org/abs/2412.03937v3","updated":"2024-12-16T02:39:18Z","published":"2024-12-05T07:35:19Z","title":"AIpparel: A Large Multimodal Generative Model for Digital Garments","summary":"  Apparel is essential to human life, offering protection, mirroring cultural\nidentities, and showcasing personal style. Yet, the creation of garments\nremains a time-consuming process, largely due to the manual work involved in\ndesigning them. To simplify this process, we introduce AIpparel, a large\nmultimodal model for generating and editing sewing patterns. Our model\nfine-tunes state-of-the-art large multimodal models (LMMs) on a custom-curated\nlarge-scale dataset of over 120,000 unique garments, each with multimodal\nannotations including text, images, and sewing patterns. Additionally, we\npropose a novel tokenization scheme that concisely encodes these complex sewing\npatterns so that LLMs can learn to predict them efficiently. AIpparel achieves\nstate-of-the-art performance in single-modal tasks, including text-to-garment\nand image-to-garment prediction, and enables novel multimodal garment\ngeneration applications such as interactive garment editing. The project\nwebsite is at georgenakayama.github.io/AIpparel/.\n","authors":["Kiyohiro Nakayama","Jan Ackermann","Timur Levent Kesdogan","Yang Zheng","Maria Korosteleva","Olga Sorkine-Hornung","Leonidas J. Guibas","Guandao Yang","Gordon Wetzstein"],"pdf_url":"https://arxiv.org/pdf/2412.03937v3.pdf","comment":"The project website is at georgenakayama.github.io/AIpparel/"},{"id":"http://arxiv.org/abs/2412.11391v1","updated":"2024-12-16T02:37:58Z","published":"2024-12-16T02:37:58Z","title":"Temporal Contrastive Learning for Video Temporal Reasoning in Large\n  Vision-Language Models","summary":"  Temporal reasoning is a critical challenge in video-language understanding,\nas it requires models to align semantic concepts consistently across time.\nWhile existing large vision-language models (LVLMs) and large language models\n(LLMs) excel at static tasks, they struggle to capture dynamic interactions and\ntemporal dependencies in video sequences. In this work, we propose Temporal\nSemantic Alignment via Dynamic Prompting (TSADP), a novel framework that\nenhances temporal reasoning capabilities through dynamic task-specific prompts\nand temporal contrastive learning. TSADP leverages a Dynamic Prompt Generator\n(DPG) to encode fine-grained temporal relationships and a Temporal Contrastive\nLoss (TCL) to align visual and textual embeddings across time. We evaluate our\nmethod on the VidSitu dataset, augmented with enriched temporal annotations,\nand demonstrate significant improvements over state-of-the-art models in tasks\nsuch as Intra-Video Entity Association, Temporal Relationship Understanding,\nand Chronology Prediction. Human evaluations further confirm TSADP's ability to\ngenerate coherent and semantically accurate descriptions. Our analysis\nhighlights the robustness, efficiency, and practical utility of TSADP, making\nit a step forward in the field of video-language understanding.\n","authors":["Rafael Souza","Jia-Hao Lim","Alexander Davis"],"pdf_url":"https://arxiv.org/pdf/2412.11391v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10958v2","updated":"2024-12-16T02:37:33Z","published":"2024-09-17T07:52:09Z","title":"Towards Effective User Attribution for Latent Diffusion Models via\n  Watermark-Informed Blending","summary":"  Rapid advancements in multimodal large language models have enabled the\ncreation of hyper-realistic images from textual descriptions. However, these\nadvancements also raise significant concerns about unauthorized use, which\nhinders their broader distribution. Traditional watermarking methods often\nrequire complex integration or degrade image quality. To address these\nchallenges, we introduce a novel framework Towards Effective user Attribution\nfor latent diffusion models via Watermark-Informed Blending (TEAWIB). TEAWIB\nincorporates a unique ready-to-use configuration approach that allows seamless\nintegration of user-specific watermarks into generative models. This approach\nensures that each user can directly apply a pre-configured set of parameters to\nthe model without altering the original model parameters or compromising image\nquality. Additionally, noise and augmentation operations are embedded at the\npixel level to further secure and stabilize watermarked images. Extensive\nexperiments validate the effectiveness of TEAWIB, showcasing the\nstate-of-the-art performance in perceptual quality and attribution accuracy.\n","authors":["Yongyang Pan","Xiaohong Liu","Siqi Luo","Yi Xin","Xiao Guo","Xiaoming Liu","Xiongkuo Min","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2409.10958v2.pdf","comment":"9 pages, 7 figures"},{"id":"http://arxiv.org/abs/2412.10040v2","updated":"2024-12-16T02:31:03Z","published":"2024-12-13T11:00:57Z","title":"RemDet: Rethinking Efficient Model Design for UAV Object Detection","summary":"  Object detection in Unmanned Aerial Vehicle (UAV) images has emerged as a\nfocal area of research, which presents two significant challenges: i) objects\nare typically small and dense within vast images; ii) computational resource\nconstraints render most models unsuitable for real-time deployment. Current\nreal-time object detectors are not optimized for UAV images, and complex\nmethods designed for small object detection often lack real-time capabilities.\nTo address these challenges, we propose a novel detector, RemDet (Reparameter\nefficient multiplication Detector). Our contributions are as follows: 1)\nRethinking the challenges of existing detectors for small and dense UAV images,\nand proposing information loss as a design guideline for efficient models. 2)\nWe introduce the ChannelC2f module to enhance small object detection\nperformance, demonstrating that high-dimensional representations can\neffectively mitigate information loss. 3) We design the GatedFFN module to\nprovide not only strong performance but also low latency, effectively\naddressing the challenges of real-time detection. Our research reveals that\nGatedFFN, through the use of multiplication, is more cost-effective than\nfeed-forward networks for high-dimensional representation. 4) We propose the\nCED module, which combines the advantages of ViT and CNN downsampling to\neffectively reduce information loss. It specifically enhances context\ninformation for small and dense objects. Extensive experiments on large UAV\ndatasets, Visdrone and UAVDT, validate the real-time efficiency and superior\nperformance of our methods. On the challenging UAV dataset VisDrone, our\nmethods not only provided state-of-the-art results, improving detection by more\nthan 3.4%, but also achieve 110 FPS on a single 4090.\n","authors":["Chen Li","Rui Zhao","Zeyu Wang","Huiying Xu","Xinzhong Zhu"],"pdf_url":"https://arxiv.org/pdf/2412.10040v2.pdf","comment":"Accepted to AAAI25"},{"id":"http://arxiv.org/abs/2410.10084v2","updated":"2024-12-16T02:15:53Z","published":"2024-10-14T01:57:06Z","title":"PointNet with KAN versus PointNet with MLP for 3D Classification and\n  Segmentation of Point Sets","summary":"  Kolmogorov-Arnold Networks (KANs) have recently gained attention as an\nalternative to traditional Multilayer Perceptrons (MLPs) in deep learning\nframeworks. KANs have been integrated into various deep learning architectures\nsuch as convolutional neural networks, graph neural networks, and transformers,\nwith their performance evaluated. However, their effectiveness within\npoint-cloud-based neural networks remains unexplored. To address this gap, we\nincorporate KANs into PointNet for the first time to evaluate their performance\non 3D point cloud classification and segmentation tasks. Specifically, we\nintroduce PointNet-KAN, built upon two key components. First, it employs KANs\ninstead of traditional MLPs. Second, it retains the core principle of PointNet\nby using shared KAN layers and applying symmetric functions for global feature\nextraction, ensuring permutation invariance with respect to the input features.\nIn traditional MLPs, the goal is to train the weights and biases with fixed\nactivation functions; however, in KANs, the goal is to train the activation\nfunctions themselves. We use Jacobi polynomials to construct the KAN layers. We\nextensively and systematically evaluate PointNet-KAN across various polynomial\ndegrees and special types such as the Lagrange, Chebyshev, and Gegenbauer\npolynomials. Our results show that PointNet-KAN achieves competitive\nperformance compared to PointNet with MLPs on benchmark datasets for 3D object\nclassification and segmentation, despite employing a shallower and simpler\nnetwork architecture. We hope this work serves as a foundation and provides\nguidance for integrating KANs, as an alternative to MLPs, into more advanced\npoint cloud processing architectures.\n","authors":["Ali Kashefi"],"pdf_url":"https://arxiv.org/pdf/2410.10084v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11381v1","updated":"2024-12-16T02:11:19Z","published":"2024-12-16T02:11:19Z","title":"Adapting Segment Anything Model (SAM) to Experimental Datasets via\n  Fine-Tuning on GAN-based Simulation: A Case Study in Additive Manufacturing","summary":"  Industrial X-ray computed tomography (XCT) is a powerful tool for\nnon-destructive characterization of materials and manufactured components. XCT\ncommonly accompanied by advanced image analysis and computer vision algorithms\nto extract relevant information from the images. Traditional computer vision\nmodels often struggle due to noise, resolution variability, and complex\ninternal structures, particularly in scientific imaging applications.\nState-of-the-art foundational models, like the Segment Anything Model\n(SAM)-designed for general-purpose image segmentation-have revolutionized image\nsegmentation across various domains, yet their application in specialized\nfields like materials science remains under-explored. In this work, we explore\nthe application and limitations of SAM for industrial X-ray CT inspection of\nadditive manufacturing components. We demonstrate that while SAM shows promise,\nit struggles with out-of-distribution data, multiclass segmentation, and\ncomputational efficiency during fine-tuning. To address these issues, we\npropose a fine-tuning strategy utilizing parameter-efficient techniques,\nspecifically Conv-LoRa, to adapt SAM for material-specific datasets.\nAdditionally, we leverage generative adversarial network (GAN)-generated data\nto enhance the training process and improve the model's segmentation\nperformance on complex X-ray CT data. Our experimental results highlight the\nimportance of tailored segmentation models for accurate inspection, showing\nthat fine-tuning SAM on domain-specific scientific imaging data significantly\nimproves performance. However, despite improvements, the model's ability to\ngeneralize across diverse datasets remains limited, highlighting the need for\nfurther research into robust, scalable solutions for domain-specific\nsegmentation tasks.\n","authors":["Anika Tabassum","Amirkoushyar Ziabari"],"pdf_url":"https://arxiv.org/pdf/2412.11381v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11380v1","updated":"2024-12-16T02:11:02Z","published":"2024-12-16T02:11:02Z","title":"Relation-Guided Adversarial Learning for Data-free Knowledge Transfer","summary":"  Data-free knowledge distillation transfers knowledge by recovering training\ndata from a pre-trained model. Despite the recent success of seeking global\ndata diversity, the diversity within each class and the similarity among\ndifferent classes are largely overlooked, resulting in data homogeneity and\nlimited performance. In this paper, we introduce a novel Relation-Guided\nAdversarial Learning method with triplet losses, which solves the homogeneity\nproblem from two aspects. To be specific, our method aims to promote both\nintra-class diversity and inter-class confusion of the generated samples. To\nthis end, we design two phases, an image synthesis phase and a student training\nphase. In the image synthesis phase, we construct an optimization process to\npush away samples with the same labels and pull close samples with different\nlabels, leading to intra-class diversity and inter-class confusion,\nrespectively. Then, in the student training phase, we perform an opposite\noptimization, which adversarially attempts to reduce the distance of samples of\nthe same classes and enlarge the distance of samples of different classes. To\nmitigate the conflict of seeking high global diversity and keeping inter-class\nconfusing, we propose a focal weighted sampling strategy by selecting the\nnegative in the triplets unevenly within a finite range of distance. RGAL shows\nsignificant improvement over previous state-of-the-art methods in accuracy and\ndata efficiency. Besides, RGAL can be inserted into state-of-the-art methods on\nvarious data-free knowledge transfer applications. Experiments on various\nbenchmarks demonstrate the effectiveness and generalizability of our proposed\nmethod on various tasks, specially data-free knowledge distillation, data-free\nquantization, and non-exemplar incremental learning. Our code is available at\nhttps://github.com/Sharpiless/RGAL.\n","authors":["Yingping Liang","Ying Fu"],"pdf_url":"https://arxiv.org/pdf/2412.11380v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11379v1","updated":"2024-12-16T02:09:32Z","published":"2024-12-16T02:09:32Z","title":"Controllable Distortion-Perception Tradeoff Through Latent Diffusion for\n  Neural Image Compression","summary":"  Neural image compression often faces a challenging trade-off among rate,\ndistortion and perception. While most existing methods typically focus on\neither achieving high pixel-level fidelity or optimizing for perceptual\nmetrics, we propose a novel approach that simultaneously addresses both aspects\nfor a fixed neural image codec. Specifically, we introduce a plug-and-play\nmodule at the decoder side that leverages a latent diffusion process to\ntransform the decoded features, enhancing either low distortion or high\nperceptual quality without altering the original image compression codec. Our\napproach facilitates fusion of original and transformed features without\nadditional training, enabling users to flexibly adjust the balance between\ndistortion and perception during inference. Extensive experimental results\ndemonstrate that our method significantly enhances the pretrained codecs with a\nwide, adjustable distortion-perception range while maintaining their original\ncompression capabilities. For instance, we can achieve more than 150%\nimprovement in LPIPS-BDRate without sacrificing more than 1 dB in PSNR.\n","authors":["Chuqin Zhou","Guo Lu","Jiangchuan Li","Xiangyu Chen","Zhengxue Cheng","Li Song","Wenjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.11379v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2408.10605v5","updated":"2024-12-16T02:08:29Z","published":"2024-08-20T07:37:23Z","title":"MUSES: 3D-Controllable Image Generation via Multi-Modal Agent\n  Collaboration","summary":"  Despite recent advancements in text-to-image generation, most existing\nmethods struggle to create images with multiple objects and complex spatial\nrelationships in the 3D world. To tackle this limitation, we introduce a\ngeneric AI system, namely MUSES, for 3D-controllable image generation from user\nqueries. Specifically, our MUSES addresses this challenging task by developing\na progressive workflow with three key components, including (1) Layout Manager\nfor 2D-to-3D layout lifting, (2) Model Engineer for 3D object acquisition and\ncalibration, (3) Image Artist for 3D-to-2D image rendering. By mimicking the\ncollaboration of human professionals, this multi-modal agent pipeline\nfacilitates the effective and automatic creation of images with 3D-controllable\nobjects, through an explainable integration of top-down planning and bottom-up\ngeneration. Additionally, we find that existing benchmarks lack detailed\ndescriptions of complex 3D spatial relationships of multiple objects. To fill\nthis gap, we further construct a new benchmark of T2I-3DisBench (3D image\nscene), which describes diverse 3D image scenes with 50 detailed prompts.\nExtensive experiments show the state-of-the-art performance of MUSES on both\nT2I-CompBench and T2I-3DisBench, outperforming recent strong competitors such\nas DALL-E 3 and Stable Diffusion 3. These results demonstrate a significant\nstep of MUSES forward in bridging natural language, 2D image generation, and 3D\nworld. Our codes are available at the following link:\nhttps://github.com/DINGYANB/MUSES.\n","authors":["Yanbo Ding","Shaobin Zhuang","Kunchang Li","Zhengrong Yue","Yu Qiao","Yali Wang"],"pdf_url":"https://arxiv.org/pdf/2408.10605v5.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2412.11377v1","updated":"2024-12-16T02:05:15Z","published":"2024-12-16T02:05:15Z","title":"Improving Automatic Fetal Biometry Measurement with Swoosh Activation\n  Function","summary":"  The measurement of fetal thalamus diameter (FTD) and fetal head circumference\n(FHC) are crucial in identifying abnormal fetal thalamus development as it may\nlead to certain neuropsychiatric disorders in later life. However, manual\nmeasurements from 2D-US images are laborious, prone to high inter-observer\nvariability, and complicated by the high signal-to-noise ratio nature of the\nimages. Deep learning-based landmark detection approaches have shown promise in\nmeasuring biometrics from US images, but the current state-of-the-art (SOTA)\nalgorithm, BiometryNet, is inadequate for FTD and FHC measurement due to its\ninability to account for the fuzzy edges of these structures and the complex\nshape of the FTD structure. To address these inadequacies, we propose a novel\nSwoosh Activation Function (SAF) designed to enhance the regularization of\nheatmaps produced by landmark detection algorithms. Our SAF serves as a\nregularization term to enforce an optimum mean squared error (MSE) level\nbetween predicted heatmaps, reducing the dispersiveness of hotspots in\npredicted heatmaps. Our experimental results demonstrate that SAF significantly\nimproves the measurement performances of FTD and FHC with higher intraclass\ncorrelation coefficient scores in FTD and lower mean difference scores in FHC\nmeasurement than those of the current SOTA algorithm BiometryNet. Moreover, our\nproposed SAF is highly generalizable and architecture-agnostic. The SAF's\ncoefficients can be configured for different tasks, making it highly\ncustomizable. Our study demonstrates that the SAF activation function is a\nnovel method that can improve measurement accuracy in fetal biometry landmark\ndetection. This improvement has the potential to contribute to better fetal\nmonitoring and improved neonatal outcomes.\n","authors":["Shijia Zhou","Euijoon Ahn","Hao Wang","Ann Quinton","Narelle Kennedy","Pradeeba Sridar","Ralph Nanan","Jinman Kim"],"pdf_url":"https://arxiv.org/pdf/2412.11377v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11375v1","updated":"2024-12-16T02:03:45Z","published":"2024-12-16T02:03:45Z","title":"Text and Image Are Mutually Beneficial: Enhancing Training-Free Few-Shot\n  Classification with CLIP","summary":"  Contrastive Language-Image Pretraining (CLIP) has been widely used in vision\ntasks. Notably, CLIP has demonstrated promising performance in few-shot\nlearning (FSL). However, existing CLIP-based methods in training-free FSL\n(i.e., without the requirement of additional training) mainly learn different\nmodalities independently, leading to two essential issues: 1) severe anomalous\nmatch in image modality; 2) varying quality of generated text prompts. To\naddress these issues, we build a mutual guidance mechanism, that introduces an\nImage-Guided-Text (IGT) component to rectify varying quality of text prompts\nthrough image representations, and a Text-Guided-Image (TGI) component to\nmitigate the anomalous match of image modality through text representations. By\nintegrating IGT and TGI, we adopt a perspective of Text-Image Mutual guidance\nOptimization, proposing TIMO. Extensive experiments show that TIMO\nsignificantly outperforms the state-of-the-art (SOTA) training-free method.\nAdditionally, by exploring the extent of mutual guidance, we propose an\nenhanced variant, TIMO-S, which even surpasses the best training-required\nmethods by 0.33% with approximately 100 times less time cost. Our code is\navailable at https://github.com/lyymuwu/TIMO.\n","authors":["Yayuan Li","Jintao Guo","Lei Qi","Wenbin Li","Yinghuan Shi"],"pdf_url":"https://arxiv.org/pdf/2412.11375v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2402.11989v3","updated":"2024-12-16T01:54:47Z","published":"2024-02-19T09:32:48Z","title":"Privacy-Preserving Low-Rank Adaptation against Membership Inference\n  Attacks for Latent Diffusion Models","summary":"  Low-rank adaptation (LoRA) is an efficient strategy for adapting latent\ndiffusion models (LDMs) on a private dataset to generate specific images by\nminimizing the adaptation loss. However, the LoRA-adapted LDMs are vulnerable\nto membership inference (MI) attacks that can judge whether a particular data\npoint belongs to the private dataset, thus leading to the privacy leakage. To\ndefend against MI attacks, we first propose a straightforward solution:\nMembership-Privacy-preserving LoRA (MP-LoRA). MP-LoRA is formulated as a\nmin-max optimization problem where a proxy attack model is trained by\nmaximizing its MI gain while the LDM is adapted by minimizing the sum of the\nadaptation loss and the MI gain of the proxy attack model. However, we\nempirically find that MP-LoRA has the issue of unstable optimization, and\ntheoretically analyze that the potential reason is the unconstrained local\nsmoothness, which impedes the privacy-preserving adaptation. To mitigate this\nissue, we further propose a Stable Membership-Privacy-preserving LoRA\n(SMP-LoRA) that adapts the LDM by minimizing the ratio of the adaptation loss\nto the MI gain. Besides, we theoretically prove that the local smoothness of\nSMP-LoRA can be constrained by the gradient norm, leading to improved\nconvergence. Our experimental results corroborate that SMP-LoRA can indeed\ndefend against MI attacks and generate high-quality images. Our Code is\navailable at \\url{https://github.com/WilliamLUO0/StablePrivateLoRA}.\n","authors":["Zihao Luo","Xilie Xu","Feng Liu","Yun Sing Koh","Di Wang","Jingfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.11989v3.pdf","comment":"AAAI 2025 Accept"},{"id":"http://arxiv.org/abs/2412.11365v1","updated":"2024-12-16T01:37:51Z","published":"2024-12-16T01:37:51Z","title":"BiM-VFI: directional Motion Field-Guided Frame Interpolation for Video\n  with Non-uniform Motions","summary":"  Existing Video Frame interpolation (VFI) models tend to suffer from\ntime-to-location ambiguity when trained with video of non-uniform motions, such\nas accelerating, decelerating, and changing directions, which often yield\nblurred interpolated frames. In this paper, we propose (i) a novel motion\ndescription map, Bidirectional Motion field (BiM), to effectively describe\nnon-uniform motions; (ii) a BiM-guided Flow Net (BiMFN) with Content-Aware\nUpsampling Network (CAUN) for precise optical flow estimation; and (iii)\nKnowledge Distillation for VFI-centric Flow supervision (KDVCF) to supervise\nthe motion estimation of VFI model with VFI-centric teacher flows. The proposed\nVFI is called a Bidirectional Motion field-guided VFI (BiM-VFI) model.\nExtensive experiments show that our BiM-VFI model significantly surpasses the\nrecent state-of-the-art VFI methods by 26% and 45% improvements in LPIPS and\nSTLPIPS respectively, yielding interpolated frames with much fewer blurs at\narbitrary time instances.\n","authors":["Wonyong Seo","Jihyong Oh","Munchurl Kim"],"pdf_url":"https://arxiv.org/pdf/2412.11365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11362v1","updated":"2024-12-16T01:28:04Z","published":"2024-12-16T01:28:04Z","title":"VRVVC: Variable-Rate NeRF-Based Volumetric Video Compression","summary":"  Neural Radiance Field (NeRF)-based volumetric video has revolutionized visual\nmedia by delivering photorealistic Free-Viewpoint Video (FVV) experiences that\nprovide audiences with unprecedented immersion and interactivity. However, the\nsubstantial data volumes pose significant challenges for storage and\ntransmission. Existing solutions typically optimize NeRF representation and\ncompression independently or focus on a single fixed rate-distortion (RD)\ntradeoff. In this paper, we propose VRVVC, a novel end-to-end joint\noptimization variable-rate framework for volumetric video compression that\nachieves variable bitrates using a single model while maintaining superior RD\nperformance. Specifically, VRVVC introduces a compact tri-plane implicit\nresidual representation for inter-frame modeling of long-duration dynamic\nscenes, effectively reducing temporal redundancy. We further propose a\nvariable-rate residual representation compression scheme that leverages a\nlearnable quantization and a tiny MLP-based entropy model. This approach\nenables variable bitrates through the utilization of predefined Lagrange\nmultipliers to manage the quantization error of all latent representations.\nFinally, we present an end-to-end progressive training strategy combined with a\nmulti-rate-distortion loss function to optimize the entire framework. Extensive\nexperiments demonstrate that VRVVC achieves a wide range of variable bitrates\nwithin a single model and surpasses the RD performance of existing methods\nacross various datasets.\n","authors":["Qiang Hu","Houqiang Zhong","Zihan Zheng","Xiaoyun Zhang","Zhengxue Cheng","Li Song","Guangtao Zhai","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2412.11362v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11360v1","updated":"2024-12-16T01:23:13Z","published":"2024-12-16T01:23:13Z","title":"Visual IRL for Human-Like Robotic Manipulation","summary":"  We present a novel method for collaborative robots (cobots) to learn\nmanipulation tasks and perform them in a human-like manner. Our method falls\nunder the learn-from-observation (LfO) paradigm, where robots learn to perform\ntasks by observing human actions, which facilitates quicker integration into\nindustrial settings compared to programming from scratch. We introduce Visual\nIRL that uses the RGB-D keypoints in each frame of the observed human task\nperformance directly as state features, which are input to inverse\nreinforcement learning (IRL). The inversely learned reward function, which maps\nkeypoints to reward values, is transferred from the human to the cobot using a\nnovel neuro-symbolic dynamics model, which maps human kinematics to the cobot\narm. This model allows similar end-effector positioning while minimizing joint\nadjustments, aiming to preserve the natural dynamics of human motion in robotic\nmanipulation. In contrast with previous techniques that focus on end-effector\nplacement only, our method maps multiple joint angles of the human arm to the\ncorresponding cobot joints. Moreover, it uses an inverse kinematics model to\nthen minimally adjust the joint angles, for accurate end-effector positioning.\nWe evaluate the performance of this approach on two different realistic\nmanipulation tasks. The first task is produce processing, which involves\npicking, inspecting, and placing onions based on whether they are blemished.\nThe second task is liquid pouring, where the robot picks up bottles, pours the\ncontents into designated containers, and disposes of the empty bottles. Our\nresults demonstrate advances in human-like robotic manipulation, leading to\nmore human-robot compatibility in manufacturing applications.\n","authors":["Ehsan Asali","Prashant Doshi"],"pdf_url":"https://arxiv.org/pdf/2412.11360v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15211v2","updated":"2024-12-16T01:20:42Z","published":"2024-07-21T16:27:24Z","title":"Failures to Find Transferable Image Jailbreaks Between Vision-Language\n  Models","summary":"  The integration of new modalities into frontier AI systems offers exciting\ncapabilities, but also increases the possibility such systems can be\nadversarially manipulated in undesirable ways. In this work, we focus on a\npopular class of vision-language models (VLMs) that generate text outputs\nconditioned on visual and textual inputs. We conducted a large-scale empirical\nstudy to assess the transferability of gradient-based universal image\n``jailbreaks\" using a diverse set of over 40 open-parameter VLMs, including 18\nnew VLMs that we publicly release. Overall, we find that transferable\ngradient-based image jailbreaks are extremely difficult to obtain. When an\nimage jailbreak is optimized against a single VLM or against an ensemble of\nVLMs, the jailbreak successfully jailbreaks the attacked VLM(s), but exhibits\nlittle-to-no transfer to any other VLMs; transfer is not affected by whether\nthe attacked and target VLMs possess matching vision backbones or language\nmodels, whether the language model underwent instruction-following and/or\nsafety-alignment training, or many other factors. Only two settings display\npartially successful transfer: between identically-pretrained and\nidentically-initialized VLMs with slightly different VLM training data, and\nbetween different training checkpoints of a single VLM. Leveraging these\nresults, we then demonstrate that transfer can be significantly improved\nagainst a specific target VLM by attacking larger ensembles of\n``highly-similar\" VLMs. These results stand in stark contrast to existing\nevidence of universal and transferable text jailbreaks against language models\nand transferable adversarial attacks against image classifiers, suggesting that\nVLMs may be more robust to gradient-based transfer attacks.\n","authors":["Rylan Schaeffer","Dan Valentine","Luke Bailey","James Chua","Cristóbal Eyzaguirre","Zane Durante","Joe Benton","Brando Miranda","Henry Sleight","John Hughes","Rajashree Agrawal","Mrinank Sharma","Scott Emmons","Sanmi Koyejo","Ethan Perez"],"pdf_url":"https://arxiv.org/pdf/2407.15211v2.pdf","comment":"NeurIPS 2024 Workshops: RBFM (Best Paper), Frontiers in AdvML (Oral),\n  Red Teaming GenAI (Oral), SoLaR (Spotlight), SATA"},{"id":"http://arxiv.org/abs/2409.10719v3","updated":"2024-12-16T00:37:54Z","published":"2024-09-16T20:47:00Z","title":"Benchmarking VLMs' Reasoning About Persuasive Atypical Images","summary":"  Vision language models (VLMs) have shown strong zero-shot generalization\nacross various tasks, especially when integrated with large language models\n(LLMs). However, their ability to comprehend rhetorical and persuasive visual\nmedia, such as advertisements, remains understudied. Ads often employ atypical\nimagery, using surprising object juxtapositions to convey shared properties.\nFor example, Fig. 1 (e) shows a beer with a feather-like texture. This requires\nadvanced reasoning to deduce that this atypical representation signifies the\nbeer's lightness. We introduce three novel tasks, Multi-label Atypicality\nClassification, Atypicality Statement Retrieval, and Aypical Object\nRecognition, to benchmark VLMs' understanding of atypicality in persuasive\nimages. We evaluate how well VLMs use atypicality to infer an ad's message and\ntest their reasoning abilities by employing semantically challenging negatives.\nFinally, we pioneer atypicality-aware verbalization by extracting comprehensive\nimage descriptions sensitive to atypical elements. Our findings reveal that:\n(1) VLMs lack advanced reasoning capabilities compared to LLMs; (2) simple,\neffective strategies can extract atypicality-aware information, leading to\ncomprehensive image verbalization; (3) atypicality aids persuasive\nadvertisement understanding. Code and data will be made available.\n","authors":["Sina Malakouti","Aysan Aghazadeh","Ashmit Khandelwal","Adriana Kovashka"],"pdf_url":"https://arxiv.org/pdf/2409.10719v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12814v2","updated":"2024-12-16T00:21:27Z","published":"2024-06-18T17:32:48Z","title":"Dissecting Adversarial Robustness of Multimodal LM Agents","summary":"  As language models (LMs) are used to build autonomous agents in real\nenvironments, ensuring their adversarial robustness becomes a critical\nchallenge. Unlike chatbots, agents are compound systems with multiple\ncomponents, which existing LM safety evaluations do not adequately address. To\nbridge this gap, we manually create 200 targeted adversarial tasks and\nevaluation functions in a realistic threat model on top of VisualWebArena, a\nreal environment for web-based agents. In order to systematically examine the\nrobustness of various multimodal we agents, we propose the Agent Robustness\nEvaluation (ARE) framework. ARE views the agent as a graph showing the flow of\nintermediate outputs between components and decomposes robustness as the flow\nof adversarial information on the graph. First, we find that we can\nsuccessfully break a range of the latest agents that use black-box frontier\nLLMs, including those that perform reflection and tree-search. With\nimperceptible perturbations to a single product image (less than 5% of total\nweb page pixels), an attacker can hijack these agents to execute targeted\nadversarial goals with success rates up to 67%. We also use ARE to rigorously\nevaluate how the robustness changes as new components are added. We find that\nnew components that typically improve benign performance can open up new\nvulnerabilities and harm robustness. An attacker can compromise the evaluator\nused by the reflexion agent and the value function of the tree search agent,\nwhich increases the attack success relatively by 15% and 20%. Our data and code\nfor attacks, defenses, and evaluation are available at\nhttps://github.com/ChenWu98/agent-attack\n","authors":["Chen Henry Wu","Rishi Shah","Jing Yu Koh","Ruslan Salakhutdinov","Daniel Fried","Aditi Raghunathan"],"pdf_url":"https://arxiv.org/pdf/2406.12814v2.pdf","comment":"Oral presentation at NeurIPS 2024 Open-World Agents Workshop"},{"id":"http://arxiv.org/abs/2404.02885v3","updated":"2024-12-16T00:10:24Z","published":"2024-04-03T17:38:15Z","title":"PoCo: Point Context Cluster for RGBD Indoor Place Recognition","summary":"  We present a novel end-to-end algorithm (PoCo) for the indoor RGB-D place\nrecognition task, aimed at identifying the most likely match for a given query\nframe within a reference database. The task presents inherent challenges\nattributed to the constrained field of view and limited range of perception\nsensors. We propose a new network architecture, which generalizes the recent\nContext of Clusters (CoCs) to extract global descriptors directly from the\nnoisy point clouds through end-to-end learning. Moreover, we develop the\narchitecture by integrating both color and geometric modalities into the point\nfeatures to enhance the global descriptor representation. We conducted\nevaluations on public datasets ScanNet-PR and ARKit with 807 and 5047\nscenarios, respectively. PoCo achieves SOTA performance: on ScanNet-PR, we\nachieve R@1 of 64.63%, a 5.7% improvement from the best-published result CGis\n(61.12%); on Arkit, we achieve R@1 of 45.12%, a 13.3% improvement from the\nbest-published result CGis (39.82%). In addition, PoCo shows higher efficiency\nthan CGis in inference time (1.75X-faster), and we demonstrate the\neffectiveness of PoCo in recognizing places within a real-world laboratory\nenvironment.\n","authors":["Jing Liang","Zhuo Deng","Zheming Zhou","Omid Ghasemalizadeh","Dinesh Manocha","Min Sun","Cheng-Hao Kuo","Arnie Sen"],"pdf_url":"https://arxiv.org/pdf/2404.02885v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.02703v3","updated":"2024-12-16T23:42:56Z","published":"2022-02-06T04:18:45Z","title":"Multi-modal Sensor Fusion for Auto Driving Perception: A Survey","summary":"  Multi-modal fusion is a fundamental task for the perception of an autonomous\ndriving system, which has recently intrigued many researchers. However,\nachieving a rather good performance is not an easy task due to the noisy raw\ndata, underutilized information, and the misalignment of multi-modal sensors.\nIn this paper, we provide a literature review of the existing multi-modal-based\nmethods for perception tasks in autonomous driving. Generally, we make a\ndetailed analysis including over 50 papers leveraging perception sensors\nincluding LiDAR and camera trying to solve object detection and semantic\nsegmentation tasks. Different from traditional fusion methodology for\ncategorizing fusion models, we propose an innovative way that divides them into\ntwo major classes, four minor classes by a more reasonable taxonomy in the view\nof the fusion stage. Moreover, we dive deep into the current fusion methods,\nfocusing on the remaining problems and open-up discussions on the potential\nresearch opportunities. In conclusion, what we expect to do in this paper is to\npresent a new taxonomy of multi-modal fusion methods for the autonomous driving\nperception tasks and provoke thoughts of the fusion-based techniques in the\nfuture.\n","authors":["Keli Huang","Botian Shi","Xiang Li","Xin Li","Siyuan Huang","Yikang Li"],"pdf_url":"https://arxiv.org/pdf/2202.02703v3.pdf","comment":"14 pages, 8 figures"},{"id":"http://arxiv.org/abs/2306.09301v5","updated":"2024-12-16T23:09:28Z","published":"2023-06-15T17:28:00Z","title":"OpenOOD v1.5: Enhanced Benchmark for Out-of-Distribution Detection","summary":"  Out-of-Distribution (OOD) detection is critical for the reliable operation of\nopen-world intelligent systems. Despite the emergence of an increasing number\nof OOD detection methods, the evaluation inconsistencies present challenges for\ntracking the progress in this field. OpenOOD v1 initiated the unification of\nthe OOD detection evaluation but faced limitations in scalability and scope. In\nresponse, this paper presents OpenOOD v1.5, a significant improvement from its\npredecessor that ensures accurate and standardized evaluation of OOD detection\nmethodologies at large scale. Notably, OpenOOD v1.5 extends its evaluation\ncapabilities to large-scale data sets (ImageNet) and foundation models (e.g.,\nCLIP and DINOv2), and expands its scope to investigate full-spectrum OOD\ndetection which considers semantic and covariate distribution shifts at the\nsame time. This work also contributes in-depth analysis and insights derived\nfrom comprehensive experimental results, thereby enriching the knowledge pool\nof OOD detection methodologies. With these enhancements, OpenOOD v1.5 aims to\ndrive advancements and offer a more robust and comprehensive evaluation\nbenchmark for OOD detection research.\n","authors":["Jingyang Zhang","Jingkang Yang","Pengyun Wang","Haoqi Wang","Yueqian Lin","Haoran Zhang","Yiyou Sun","Xuefeng Du","Yixuan Li","Ziwei Liu","Yiran Chen","Hai Li"],"pdf_url":"https://arxiv.org/pdf/2306.09301v5.pdf","comment":"Accepted by DMLR. See code at https://github.com/Jingkang50/OpenOOD/\n  and leaderboard at https://zjysteven.github.io/OpenOOD/"},{"id":"http://arxiv.org/abs/2412.12392v1","updated":"2024-12-16T23:00:05Z","published":"2024-12-16T23:00:05Z","title":"MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors","summary":"  We present a real-time monocular dense SLAM system designed bottom-up from\nMASt3R, a two-view 3D reconstruction and matching prior. Equipped with this\nstrong prior, our system is robust on in-the-wild video sequences despite\nmaking no assumption on a fixed or parametric camera model beyond a unique\ncamera centre. We introduce efficient methods for pointmap matching, camera\ntracking and local fusion, graph construction and loop closure, and\nsecond-order global optimisation. With known calibration, a simple modification\nto the system achieves state-of-the-art performance across various benchmarks.\nAltogether, we propose a plug-and-play monocular SLAM system capable of\nproducing globally-consistent poses and dense geometry while operating at 15\nFPS.\n","authors":["Riku Murai","Eric Dexheimer","Andrew J. Davison"],"pdf_url":"https://arxiv.org/pdf/2412.12392v1.pdf","comment":"The first two authors contributed equally to this work. Project Page:\n  https://edexheim.github.io/mast3r-slam/"},{"id":"http://arxiv.org/abs/2412.12391v1","updated":"2024-12-16T22:59:26Z","published":"2024-12-16T22:59:26Z","title":"Efficient Scaling of Diffusion Transformers for Text-to-Image Generation","summary":"  We empirically study the scaling properties of various Diffusion Transformers\n(DiTs) for text-to-image generation by performing extensive and rigorous\nablations, including training scaled DiTs ranging from 0.3B upto 8B parameters\non datasets up to 600M images. We find that U-ViT, a pure self-attention based\nDiT model provides a simpler design and scales more effectively in comparison\nwith cross-attention based DiT variants, which allows straightforward expansion\nfor extra conditions and other modalities. We identify a 2.3B U-ViT model can\nget better performance than SDXL UNet and other DiT variants in controlled\nsetting. On the data scaling side, we investigate how increasing dataset size\nand enhanced long caption improve the text-image alignment performance and the\nlearning efficiency.\n","authors":["Hao Li","Shamit Lal","Zhiheng Li","Yusheng Xie","Ying Wang","Yang Zou","Orchid Majumder","R. Manmatha","Zhuowen Tu","Stefano Ermon","Stefano Soatto","Ashwin Swaminathan"],"pdf_url":"https://arxiv.org/pdf/2412.12391v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09624v2","updated":"2024-12-16T22:17:55Z","published":"2024-12-12T18:59:57Z","title":"GenEx: Generating an Explorable World","summary":"  Understanding, navigating, and exploring the 3D physical real world has long\nbeen a central challenge in the development of artificial intelligence. In this\nwork, we take a step toward this goal by introducing GenEx, a system capable of\nplanning complex embodied world exploration, guided by its generative\nimagination that forms priors (expectations) about the surrounding\nenvironments. GenEx generates an entire 3D-consistent imaginative environment\nfrom as little as a single RGB image, bringing it to life through panoramic\nvideo streams. Leveraging scalable 3D world data curated from Unreal Engine,\nour generative model is rounded in the physical world. It captures a continuous\n360-degree environment with little effort, offering a boundless landscape for\nAI agents to explore and interact with. GenEx achieves high-quality world\ngeneration, robust loop consistency over long trajectories, and demonstrates\nstrong 3D capabilities such as consistency and active 3D mapping. Powered by\ngenerative imagination of the world, GPT-assisted agents are equipped to\nperform complex embodied tasks, including both goal-agnostic exploration and\ngoal-driven navigation. These agents utilize predictive expectation regarding\nunseen parts of the physical world to refine their beliefs, simulate different\noutcomes based on potential decisions, and make more informed choices. In\nsummary, we demonstrate that GenEx provides a transformative platform for\nadvancing embodied AI in imaginative spaces and brings potential for extending\nthese capabilities to real-world exploration.\n","authors":["Taiming Lu","Tianmin Shu","Junfei Xiao","Luoxin Ye","Jiahao Wang","Cheng Peng","Chen Wei","Daniel Khashabi","Rama Chellappa","Alan Yuille","Jieneng Chen"],"pdf_url":"https://arxiv.org/pdf/2412.09624v2.pdf","comment":"Website: GenEx.world"},{"id":"http://arxiv.org/abs/2311.07184v3","updated":"2024-12-16T21:43:18Z","published":"2023-11-13T09:19:14Z","title":"Cross-Axis Transformer with 3D Rotary Positional Embeddings","summary":"  Despite lagging behind their modal cousins in many respects, Vision\nTransformers have provided an interesting opportunity to bridge the gap between\nsequence modeling and image modeling. Up until now however, vision transformers\nhave largely been held back, due to both computational inefficiency, and lack\nof proper handling of spatial dimensions. In this paper, we introduce the\nCross-Axis Transformer. CAT is a model inspired by both Axial Transformers, and\nMicrosoft's recent Retentive Network, that drastically reduces the required\nnumber of floating point operations required to process an image, while\nsimultaneously converging faster and more accurately than the Vision\nTransformers it replaces.\n","authors":["Lily Erickson"],"pdf_url":"https://arxiv.org/pdf/2311.07184v3.pdf","comment":"7 pages, 8 figures"},{"id":"http://arxiv.org/abs/2408.05617v3","updated":"2024-12-16T21:35:54Z","published":"2024-08-10T19:31:21Z","title":"Residual-INR: Communication Efficient On-Device Learning Using Implicit\n  Neural Representation","summary":"  Edge computing is a distributed computing paradigm that collects and\nprocesses data at or near the source of data generation. The on-device learning\nat edge relies on device-to-device wireless communication to facilitate\nreal-time data sharing and collaborative decision-making among multiple\ndevices. This significantly improves the adaptability of the edge computing\nsystem to the changing environments. However, as the scale of the edge\ncomputing system is getting larger, communication among devices is becoming the\nbottleneck because of the limited bandwidth of wireless communication leads to\nlarge data transfer latency. To reduce the amount of device-to-device data\ntransmission and accelerate on-device learning, in this paper, we propose\nResidual-INR, a fog computing-based communication-efficient on-device learning\nframework by utilizing implicit neural representation (INR) to compress\nimages/videos into neural network weights. Residual-INR enhances data transfer\nefficiency by collecting JPEG images from edge devices, compressing them into\nINR format at the fog node, and redistributing them for on-device learning. By\nusing a smaller INR for full image encoding and a separate object INR for\nhigh-quality object region reconstruction through residual encoding, our\ntechnique can reduce the encoding redundancy while maintaining the object\nquality. Residual-INR is a promising solution for edge on-device learning\nbecause it reduces data transmission by up to 5.16 x across a network of 10\nedge devices. It also facilitates CPU-free accelerated on-device learning,\nachieving up to 2.9 x speedup without sacrificing accuracy. Our code is\navailable at: https://github.com/sharc-lab/Residual-INR.\n","authors":["Hanqiu Chen","Xuebin Yao","Pradeep Subedi","Cong Hao"],"pdf_url":"https://arxiv.org/pdf/2408.05617v3.pdf","comment":"This paper has been accepted by ICCAD 2024"},{"id":"http://arxiv.org/abs/2412.12359v1","updated":"2024-12-16T21:14:11Z","published":"2024-12-16T21:14:11Z","title":"Visual Instruction Tuning with 500x Fewer Parameters through Modality\n  Linear Representation-Steering","summary":"  Multimodal Large Language Models (MLLMs) have significantly advanced visual\ntasks by integrating visual representations into large language models (LLMs).\nThe textual modality, inherited from LLMs, equips MLLMs with abilities like\ninstruction following and in-context learning. In contrast, the visual modality\nenhances performance in downstream tasks by leveraging rich semantic content,\nspatial information, and grounding capabilities. These intrinsic modalities\nwork synergistically across various visual tasks. Our research initially\nreveals a persistent imbalance between these modalities, with text often\ndominating output generation during visual instruction tuning. This imbalance\noccurs when using both full fine-tuning and parameter-efficient fine-tuning\n(PEFT) methods. We then found that re-balancing these modalities can\nsignificantly reduce the number of trainable parameters required, inspiring a\ndirection for further optimizing visual instruction tuning. We introduce\nModality Linear Representation-Steering (MoReS) to achieve the goal. MoReS\neffectively re-balances the intrinsic modalities throughout the model, where\nthe key idea is to steer visual representations through linear transformations\nin the visual subspace across each model layer. To validate our solution, we\ncomposed LLaVA Steering, a suite of models integrated with the proposed MoReS\nmethod. Evaluation results show that the composed LLaVA Steering models\nrequire, on average, 500 times fewer trainable parameters than LoRA needs while\nstill achieving comparable performance across three visual benchmarks and eight\nvisual question-answering tasks. Last, we present the LLaVA Steering Factory,\nan in-house developed platform that enables researchers to quickly customize\nvarious MLLMs with component-based architecture for seamlessly integrating\nstate-of-the-art models, and evaluate their intrinsic modality imbalance.\n","authors":["Jinhe Bi","Yujun Wang","Haokun Chen","Xun Xiao","Artur Hecker","Volker Tresp","Yunpu Ma"],"pdf_url":"https://arxiv.org/pdf/2412.12359v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10785v3","updated":"2024-12-16T21:02:15Z","published":"2024-07-15T15:03:01Z","title":"Learning biologically relevant features in a pathology foundation model\n  using sparse autoencoders","summary":"  Pathology plays an important role in disease diagnosis, treatment\ndecision-making and drug development. Previous works on interpretability for\nmachine learning models on pathology images have revolved around methods such\nas attention value visualization and deriving human-interpretable features from\nmodel heatmaps. Mechanistic interpretability is an emerging area of model\ninterpretability that focuses on reverse-engineering neural networks. Sparse\nAutoencoders (SAEs) have emerged as a promising direction in terms of\nextracting monosemantic features from polysemantic model activations. In this\nwork, we trained a Sparse Autoencoder on the embeddings of a pathology\npretrained foundation model. We found that Sparse Autoencoder features\nrepresent interpretable and monosemantic biological concepts. In particular,\nindividual SAE dimensions showed strong correlations with cell type counts such\nas plasma cells and lymphocytes. These biological representations were unique\nto the pathology pretrained model and were not found in a self-supervised model\npretrained on natural images. We demonstrated that such biologically-grounded\nmonosemantic representations evolved across the model's depth, and the\npathology foundation model eventually gained robustness to non-biological\nfactors such as scanner type. The emergence of biologically relevant SAE\nfeatures was generalizable to an out-of-domain dataset. Our work paves the way\nfor further exploration around interpretable feature dimensions and their\nutility for medical and clinical applications.\n","authors":["Nhat Minh Le","Ciyue Shen","Neel Patel","Chintan Shah","Darpan Sanghavi","Blake Martin","Alfred Eng","Daniel Shenker","Harshith Padigela","Raymond Biju","Syed Ashar Javed","Jennifer Hipp","John Abel","Harsha Pokkalla","Sean Grullon","Dinkar Juyal"],"pdf_url":"https://arxiv.org/pdf/2407.10785v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03516v2","updated":"2024-12-16T20:54:56Z","published":"2024-08-07T02:54:43Z","title":"Query3D: LLM-Powered Open-Vocabulary Scene Segmentation with Language\n  Embedded 3D Gaussian","summary":"  This paper introduces a novel method for open-vocabulary 3D scene querying in\nautonomous driving by combining Language Embedded 3D Gaussians with Large\nLanguage Models (LLMs). We propose utilizing LLMs to generate both contextually\ncanonical phrases and helping positive words for enhanced segmentation and\nscene interpretation. Our method leverages GPT-3.5 Turbo as an expert model to\ncreate a high-quality text dataset, which we then use to fine-tune smaller,\nmore efficient LLMs for on-device deployment. Our comprehensive evaluation on\nthe WayveScenes101 dataset demonstrates that LLM-guided segmentation\nsignificantly outperforms traditional approaches based on predefined canonical\nphrases. Notably, our fine-tuned smaller models achieve performance comparable\nto larger expert models while maintaining faster inference times. Through\nablation studies, we discover that the effectiveness of helping positive words\ncorrelates with model scale, with larger models better equipped to leverage\nadditional semantic information. This work represents a significant advancement\ntowards more efficient, context-aware autonomous driving systems, effectively\nbridging 3D scene representation with high-level semantic querying while\nmaintaining practical deployment considerations.\n","authors":["Amirhosein Chahe","Lifeng Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.03516v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12349v1","updated":"2024-12-16T20:42:26Z","published":"2024-12-16T20:42:26Z","title":"Domain Generalization in Autonomous Driving: Evaluating YOLOv8s,\n  RT-DETR, and YOLO-NAS with the ROAD-Almaty Dataset","summary":"  This study investigates the domain generalization capabilities of three\nstate-of-the-art object detection models - YOLOv8s, RT-DETR, and YOLO-NAS -\nwithin the unique driving environment of Kazakhstan. Utilizing the newly\nconstructed ROAD-Almaty dataset, which encompasses diverse weather, lighting,\nand traffic conditions, we evaluated the models' performance without any\nretraining. Quantitative analysis revealed that RT-DETR achieved an average\nF1-score of 0.672 at IoU=0.5, outperforming YOLOv8s (0.458) and YOLO-NAS\n(0.526) by approximately 46% and 27%, respectively. Additionally, all models\nexhibited significant performance declines at higher IoU thresholds (e.g., a\ndrop of approximately 20% when increasing IoU from 0.5 to 0.75) and under\nchallenging environmental conditions, such as heavy snowfall and low-light\nscenarios. These findings underscore the necessity for geographically diverse\ntraining datasets and the implementation of specialized domain adaptation\ntechniques to enhance the reliability of autonomous vehicle detection systems\nglobally. This research contributes to the understanding of domain\ngeneralization challenges in autonomous driving, particularly in\nunderrepresented regions.\n","authors":["Madiyar Alimov","Temirlan Meiramkhanov"],"pdf_url":"https://arxiv.org/pdf/2412.12349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12331v1","updated":"2024-12-16T20:01:35Z","published":"2024-12-16T20:01:35Z","title":"Efficient Object-centric Representation Learning with Pre-trained\n  Geometric Prior","summary":"  This paper addresses key challenges in object-centric representation learning\nof video. While existing approaches struggle with complex scenes, we propose a\nnovel weakly-supervised framework that emphasises geometric understanding and\nleverages pre-trained vision models to enhance object discovery. Our method\nintroduces an efficient slot decoder specifically designed for object-centric\nlearning, enabling effective representation of multi-object scenes without\nrequiring explicit depth information. Results on synthetic video benchmarks\nwith increasing complexity in terms of objects and their movement, object\nocclusion and camera motion demonstrate that our approach achieves comparable\nperformance to supervised methods while maintaining computational efficiency.\nThis advances the field towards more practical applications in complex\nreal-world scenarios.\n","authors":["Phúc H. Le Khac","Graham Healy","Alan F. Smeaton"],"pdf_url":"https://arxiv.org/pdf/2412.12331v1.pdf","comment":"6 pages, 4 Figures, 2 Tables"},{"id":"http://arxiv.org/abs/2311.07449v2","updated":"2024-12-16T19:03:08Z","published":"2023-11-13T16:30:49Z","title":"Semantically Grounded QFormer for Efficient Vision Language\n  Understanding","summary":"  General purpose Vision Language Models (VLMs) have received tremendous\ninterest in recent years, owing to their ability to learn rich vision-language\ncorrelations as well as their broad zero-shot competencies. One immensely\npopular line of work utilizes frozen unimodal models, by bridging vision\nrepresentations to language using a trainable module called the QFormer.\nHowever, this method relies heavily on large-scale multimodal pretraining with\nhuge computational overheads. To that end, we propose a more efficient\nframework for QFormer-based vision-language alignment. Our key idea relies on\nthe observation that QFormer latents correspond more strongly to the frozen\nLLM's intermediate latent space. Consequently, instead of using QFormer latents\nas inputs to the LLM, we alter the framework by using the latents to directly\ncondition the LLM latent space for image-to-text generation. We demonstrate the\neffectiveness of our approach against existing baselines in improving the\nefficiency of vision-language pretraining.\n","authors":["Moulik Choraria","Xinbo Wu","Sourya Basu","Nitesh Sekhar","Yue Wu","Xu Zhang","Prateek Singhal","Lav R. Varshney"],"pdf_url":"https://arxiv.org/pdf/2311.07449v2.pdf","comment":"Preprint Under Review"},{"id":"http://arxiv.org/abs/2403.16034v2","updated":"2024-12-16T19:00:47Z","published":"2024-03-24T06:30:02Z","title":"V2X-Real: a Large-Scale Dataset for Vehicle-to-Everything Cooperative\n  Perception","summary":"  Recent advancements in Vehicle-to-Everything (V2X) technologies have enabled\nautonomous vehicles to share sensing information to see through occlusions,\ngreatly boosting the perception capability. However, there are no real-world\ndatasets to facilitate the real V2X cooperative perception research -- existing\ndatasets either only support Vehicle-to-Infrastructure cooperation or\nVehicle-to-Vehicle cooperation. In this paper, we present V2X-Real, a\nlarge-scale dataset that includes a mixture of multiple vehicles and smart\ninfrastructure to facilitate the V2X cooperative perception development with\nmulti-modality sensing data. Our V2X-Real is collected using two connected\nautomated vehicles and two smart infrastructure, which are all equipped with\nmulti-modal sensors including LiDAR sensors and multi-view cameras. The whole\ndataset contains 33K LiDAR frames and 171K camera data with over 1.2M annotated\nbounding boxes of 10 categories in very challenging urban scenarios. According\nto the collaboration mode and ego perspective, we derive four types of datasets\nfor Vehicle-Centric, Infrastructure-Centric, Vehicle-to-Vehicle, and\nInfrastructure-to-Infrastructure cooperative perception. Comprehensive\nmulti-class multi-agent benchmarks of SOTA cooperative perception methods are\nprovided. The V2X-Real dataset and codebase are available at\nhttps://mobility-lab.seas.ucla.edu/v2x-real.\n","authors":["Hao Xiang","Zhaoliang Zheng","Xin Xia","Runsheng Xu","Letian Gao","Zewei Zhou","Xu Han","Xinkai Ji","Mingxi Li","Zonglin Meng","Li Jin","Mingyue Lei","Zhaoyang Ma","Zihang He","Haoxuan Ma","Yunshuang Yuan","Yingqian Zhao","Jiaqi Ma"],"pdf_url":"https://arxiv.org/pdf/2403.16034v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12278v1","updated":"2024-12-16T19:00:19Z","published":"2024-12-16T19:00:19Z","title":"Towards a Universal Synthetic Video Detector: From Face or Background\n  Manipulations to Fully AI-Generated Content","summary":"  Existing DeepFake detection techniques primarily focus on facial\nmanipulations, such as face-swapping or lip-syncing. However, advancements in\ntext-to-video (T2V) and image-to-video (I2V) generative models now allow fully\nAI-generated synthetic content and seamless background alterations, challenging\nface-centric detection methods and demanding more versatile approaches.\n  To address this, we introduce the \\underline{U}niversal \\underline{N}etwork\nfor \\underline{I}dentifying \\underline{T}ampered and synth\\underline{E}tic\nvideos (\\texttt{UNITE}) model, which, unlike traditional detectors, captures\nfull-frame manipulations. \\texttt{UNITE} extends detection capabilities to\nscenarios without faces, non-human subjects, and complex background\nmodifications. It leverages a transformer-based architecture that processes\ndomain-agnostic features extracted from videos via the SigLIP-So400M foundation\nmodel. Given limited datasets encompassing both facial/background alterations\nand T2V/I2V content, we integrate task-irrelevant data alongside standard\nDeepFake datasets in training. We further mitigate the model's tendency to\nover-focus on faces by incorporating an attention-diversity (AD) loss, which\npromotes diverse spatial attention across video frames. Combining AD loss with\ncross-entropy improves detection performance across varied contexts.\nComparative evaluations demonstrate that \\texttt{UNITE} outperforms\nstate-of-the-art detectors on datasets (in cross-data settings) featuring\nface/background manipulations and fully synthetic T2V/I2V videos, showcasing\nits adaptability and generalizable detection capabilities.\n","authors":["Rohit Kundu","Hao Xiong","Vishal Mohanty","Athula Balachandran","Amit K. Roy-Chowdhury"],"pdf_url":"https://arxiv.org/pdf/2412.12278v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12242v1","updated":"2024-12-16T18:59:52Z","published":"2024-12-16T18:59:52Z","title":"OmniPrism: Learning Disentangled Visual Concept for Image Generation","summary":"  Creative visual concept generation often draws inspiration from specific\nconcepts in a reference image to produce relevant outcomes. However, existing\nmethods are typically constrained to single-aspect concept generation or are\neasily disrupted by irrelevant concepts in multi-aspect concept scenarios,\nleading to concept confusion and hindering creative generation. To address\nthis, we propose OmniPrism, a visual concept disentangling approach for\ncreative image generation. Our method learns disentangled concept\nrepresentations guided by natural language and trains a diffusion model to\nincorporate these concepts. We utilize the rich semantic space of a multimodal\nextractor to achieve concept disentanglement from given images and concept\nguidance. To disentangle concepts with different semantics, we construct a\npaired concept disentangled dataset (PCD-200K), where each pair shares the same\nconcept such as content, style, and composition. We learn disentangled concept\nrepresentations through our contrastive orthogonal disentangled (COD) training\npipeline, which are then injected into additional diffusion cross-attention\nlayers for generation. A set of block embeddings is designed to adapt each\nblock's concept domain in the diffusion models. Extensive experiments\ndemonstrate that our method can generate high-quality, concept-disentangled\nresults with high fidelity to text prompts and desired concepts.\n","authors":["Yangyang Li","Daqing Liu","Wu Liu","Allen He","Xinchen Liu","Yongdong Zhang","Guoqing Jin"],"pdf_url":"https://arxiv.org/pdf/2412.12242v1.pdf","comment":"WebPage available at https://tale17.github.io/omni/"},{"id":"http://arxiv.org/abs/2412.12232v1","updated":"2024-12-16T14:46:57Z","published":"2024-12-16T14:46:57Z","title":"You Only Submit One Image to Find the Most Suitable Generative Model","summary":"  Deep generative models have achieved promising results in image generation,\nand various generative model hubs, e.g., Hugging Face and Civitai, have been\ndeveloped that enable model developers to upload models and users to download\nmodels. However, these model hubs lack advanced model management and\nidentification mechanisms, resulting in users only searching for models through\ntext matching, download sorting, etc., making it difficult to efficiently find\nthe model that best meets user requirements. In this paper, we propose a novel\nsetting called Generative Model Identification (GMI), which aims to enable the\nuser to identify the most appropriate generative model(s) for the user's\nrequirements from a large number of candidate models efficiently. To our best\nknowledge, it has not been studied yet. In this paper, we introduce a\ncomprehensive solution consisting of three pivotal modules: a weighted Reduced\nKernel Mean Embedding (RKME) framework for capturing the generated image\ndistribution and the relationship between images and prompts, a pre-trained\nvision-language model aimed at addressing dimensionality challenges, and an\nimage interrogator designed to tackle cross-modality issues. Extensive\nempirical results demonstrate the proposal is both efficient and effective. For\nexample, users only need to submit a single example image to describe their\nrequirements, and the model platform can achieve an average top-4\nidentification accuracy of more than 80%.\n","authors":["Zhi Zhou","Lan-Zhe Guo","Peng-Xiao Song","Yu-Feng Li"],"pdf_url":"https://arxiv.org/pdf/2412.12232v1.pdf","comment":"Accepted by NeurIPS 2023 Workshop on Diffusion Models"},{"id":"http://arxiv.org/abs/2412.11673v1","updated":"2024-12-16T11:26:46Z","published":"2024-12-16T11:26:46Z","title":"DINO-Foresight Looking into the Future with DINO","summary":"  Predicting future dynamics is crucial for applications like autonomous\ndriving and robotics, where understanding the environment is key. Existing\npixel-level methods are computationally expensive and often focus on irrelevant\ndetails. To address these challenges, we introduce $\\texttt{DINO-Foresight}$, a\nnovel framework that operates in the semantic feature space of pretrained\nVision Foundation Models (VFMs). Our approach trains a masked feature\ntransformer in a self-supervised manner to predict the evolution of VFM\nfeatures over time. By forecasting these features, we can apply off-the-shelf,\ntask-specific heads for various scene understanding tasks. In this framework,\nVFM features are treated as a latent space, to which different heads attach to\nperform specific tasks for future-frame analysis. Extensive experiments show\nthat our framework outperforms existing methods, demonstrating its robustness\nand scalability. Additionally, we highlight how intermediate transformer\nrepresentations in $\\texttt{DINO-Foresight}$ improve downstream task\nperformance, offering a promising path for the self-supervised enhancement of\nVFM features. We provide the implementation code at\nhttps://github.com/Sta8is/DINO-Foresight .\n","authors":["Efstathios Karypidis","Ioannis Kakogeorgiou","Spyros Gidaris","Nikos Komodakis"],"pdf_url":"https://arxiv.org/pdf/2412.11673v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12223v1","updated":"2024-12-16T09:02:24Z","published":"2024-12-16T09:02:24Z","title":"Can video generation replace cinematographers? Research on the cinematic\n  language of generated video","summary":"  Recent advancements in text-to-video (T2V) generation have leveraged\ndiffusion models to enhance the visual coherence of videos generated from\ntextual descriptions. However, most research has primarily focused on object\nmotion, with limited attention given to cinematic language in videos, which is\ncrucial for cinematographers to convey emotion and narrative pacing. To address\nthis limitation, we propose a threefold approach to enhance the ability of T2V\nmodels to generate controllable cinematic language. Specifically, we introduce\na cinematic language dataset that encompasses shot framing, angle, and camera\nmovement, enabling models to learn diverse cinematic styles. Building on this,\nto facilitate robust cinematic alignment evaluation, we present CameraCLIP, a\nmodel fine-tuned on the proposed dataset that excels in understanding complex\ncinematic language in generated videos and can further provide valuable\nguidance in the multi-shot composition process. Finally, we propose CLIPLoRA, a\ncost-guided dynamic LoRA composition method that facilitates smooth transitions\nand realistic blending of cinematic language by dynamically fusing multiple\npre-trained cinematic LoRAs within a single video. Our experiments demonstrate\nthat CameraCLIP outperforms existing models in assessing the alignment between\ncinematic language and video, achieving an R@1 score of 0.81. Additionally,\nCLIPLoRA improves the ability for multi-shot composition, potentially bridging\nthe gap between automatically generated videos and those shot by professional\ncinematographers.\n","authors":["Xiaozhe Li","Kai WU","Siyi Yang","YiZhan Qu","Guohua. Zhang","Zhiyu Chen","Jiayao Li","Jiangchuan Mu","Xiaobin Hu","Wen Fang","Mingliang Xiong","Hao Deng","Qingwen Liu","Gang Li","Bin He"],"pdf_url":"https://arxiv.org/pdf/2412.12223v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2412.12222v1","updated":"2024-12-16T07:44:27Z","published":"2024-12-16T07:44:27Z","title":"Endangered Alert: A Field-Validated Self-Training Scheme for Detecting\n  and Protecting Threatened Wildlife on Roads and Roadsides","summary":"  Traffic accidents are a global safety concern, resulting in numerous\nfatalities each year. A considerable number of these deaths are caused by\nanimal-vehicle collisions (AVCs), which not only endanger human lives but also\npresent serious risks to animal populations. This paper presents an innovative\nself-training methodology aimed at detecting rare animals, such as the\ncassowary in Australia, whose survival is threatened by road accidents. The\nproposed method addresses critical real-world challenges, including acquiring\nand labelling sensor data for rare animal species in resource-limited\nenvironments. It achieves this by leveraging cloud and edge computing, and\nautomatic data labelling to improve the detection performance of the\nfield-deployed model iteratively. Our approach introduces Label-Augmentation\nNon-Maximum Suppression (LA-NMS), which incorporates a vision-language model\n(VLM) to enable automated data labelling. During a five-month deployment, we\nconfirmed the method's robustness and effectiveness, resulting in improved\nobject detection accuracy and increased prediction confidence. The source code\nis available: https://github.com/acfr/CassDetect\n","authors":["Kunming Li","Mao Shan","Stephany Berrio Perez","Katie Luo","Stewart Worrall"],"pdf_url":"https://arxiv.org/pdf/2412.12222v1.pdf","comment":"8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2412.12220v1","updated":"2024-12-16T04:04:41Z","published":"2024-12-16T04:04:41Z","title":"Relieving Universal Label Noise for Unsupervised Visible-Infrared Person\n  Re-Identification by Inferring from Neighbors","summary":"  Unsupervised visible-infrared person re-identification (USL-VI-ReID) is of\ngreat research and practical significance yet remains challenging due to the\nabsence of annotations. Existing approaches aim to learn modality-invariant\nrepresentations in an unsupervised setting. However, these methods often\nencounter label noise within and across modalities due to suboptimal clustering\nresults and considerable modality discrepancies, which impedes effective\ntraining. To address these challenges, we propose a straightforward yet\neffective solution for USL-VI-ReID by mitigating universal label noise using\nneighbor information. Specifically, we introduce the Neighbor-guided Universal\nLabel Calibration (N-ULC) module, which replaces explicit hard pseudo labels in\nboth homogeneous and heterogeneous spaces with soft labels derived from\nneighboring samples to reduce label noise. Additionally, we present the\nNeighbor-guided Dynamic Weighting (N-DW) module to enhance training stability\nby minimizing the influence of unreliable samples. Extensive experiments on the\nRegDB and SYSU-MM01 datasets demonstrate that our method outperforms existing\nUSL-VI-ReID approaches, despite its simplicity. The source code is available\nat: https://github.com/tengxiao14/Neighbor-guided-USL-VI-ReID.\n","authors":["Xiao Teng","Long Lan","Dingyao Chen","Kele Xu","Nan Yin"],"pdf_url":"https://arxiv.org/pdf/2412.12220v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12216v1","updated":"2024-12-16T00:40:49Z","published":"2024-12-16T00:40:49Z","title":"SitPose: Real-Time Detection of Sitting Posture and Sedentary Behavior\n  Using Ensemble Learning With Depth Sensor","summary":"  Poor sitting posture can lead to various work-related musculoskeletal\ndisorders (WMSDs). Office employees spend approximately 81.8% of their working\ntime seated, and sedentary behavior can result in chronic diseases such as\ncervical spondylosis and cardiovascular diseases. To address these health\nconcerns, we present SitPose, a sitting posture and sedentary detection system\nutilizing the latest Kinect depth camera. The system tracks 3D coordinates of\nbone joint points in real-time and calculates the angle values of related\njoints. We established a dataset containing six different sitting postures and\none standing posture, totaling 33,409 data points, by recruiting 36\nparticipants. We applied several state-of-the-art machine learning algorithms\nto the dataset and compared their performance in recognizing the sitting poses.\nOur results show that the ensemble learning model based on the soft voting\nmechanism achieves the highest F1 score of 98.1%. Finally, we deployed the\nSitPose system based on this ensemble model to encourage better sitting posture\nand to reduce sedentary habits.\n","authors":["Hang Jin","Xin He","Lingyun Wang","Yujun Zhu","Weiwei Jiang","Xiaobo Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.12216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.11250v2","updated":"2024-12-16T15:58:51Z","published":"2022-06-22T17:56:09Z","title":"Leveraging RGB-D Data with Cross-Modal Context Mining for Glass Surface\n  Detection","summary":"  Glass surfaces are becoming increasingly ubiquitous as modern buildings tend\nto use a lot of glass panels. This, however, poses substantial challenges to\nthe operations of autonomous systems such as robots, self-driving cars, and\ndrones, as these glass panels can become transparent obstacles to navigation.\nExisting works attempt to exploit various cues, including glass boundary\ncontext or reflections, as priors. However, they are all based on input RGB\nimages. We observe that the transmission of 3D depth sensor light through glass\nsurfaces often produces blank regions in the depth maps, which can offer\nadditional insights to complement the RGB image features for glass surface\ndetection. In this work, we first propose a large-scale RGB-D glass surface\ndetection dataset, \\textit{RGB-D GSD}, for rigorous experiments and future\nresearch. It contains 3,009 images, paired with precise annotations, offering a\nwide range of real-world RGB-D glass surface categories. We then propose a\nnovel glass surface detection framework combining RGB and depth information,\nwith two novel modules: a cross-modal context mining (CCM) module to adaptively\nlearn individual and mutual context features from RGB and depth information,\nand a depth-missing aware attention (DAA) module to explicitly exploit spatial\nlocations where missing depths occur to help detect the presence of glass\nsurfaces. Experimental results show that our proposed model outperforms\nstate-of-the-art methods.\n","authors":["Jiaying Lin","Yuen-Hei Yeung","Shuquan Ye","Rynson W. H. Lau"],"pdf_url":"https://arxiv.org/pdf/2206.11250v2.pdf","comment":"Accepted to AAAI 2025. Project Page:\n  https://jiaying.link/AAAI25-RGBDGlass/"}],"Robotics":[{"id":"http://arxiv.org/abs/2412.12098v1","updated":"2024-12-16T18:59:53Z","published":"2024-12-16T18:59:53Z","title":"MaxInfoRL: Boosting exploration in reinforcement learning through\n  information gain maximization","summary":"  Reinforcement learning (RL) algorithms aim to balance exploiting the current\nbest strategy with exploring new options that could lead to higher rewards.\nMost common RL algorithms use undirected exploration, i.e., select random\nsequences of actions. Exploration can also be directed using intrinsic rewards,\nsuch as curiosity or model epistemic uncertainty. However, effectively\nbalancing task and intrinsic rewards is challenging and often task-dependent.\nIn this work, we introduce a framework, MaxInfoRL, for balancing intrinsic and\nextrinsic exploration. MaxInfoRL steers exploration towards informative\ntransitions, by maximizing intrinsic rewards such as the information gain about\nthe underlying task. When combined with Boltzmann exploration, this approach\nnaturally trades off maximization of the value function with that of the\nentropy over states, rewards, and actions. We show that our approach achieves\nsublinear regret in the simplified setting of multi-armed bandits. We then\napply this general formulation to a variety of off-policy model-free RL methods\nfor continuous state-action spaces, yielding novel algorithms that achieve\nsuperior performance across hard exploration problems and complex scenarios\nsuch as visual control tasks.\n","authors":["Bhavya Sukhija","Stelian Coros","Andreas Krause","Pieter Abbeel","Carmelo Sferrazza"],"pdf_url":"https://arxiv.org/pdf/2412.12098v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12089v1","updated":"2024-12-16T18:56:24Z","published":"2024-12-16T18:56:24Z","title":"Stabilizing Reinforcement Learning in Differentiable Multiphysics\n  Simulation","summary":"  Recent advances in GPU-based parallel simulation have enabled practitioners\nto collect large amounts of data and train complex control policies using deep\nreinforcement learning (RL), on commodity GPUs. However, such successes for RL\nin robotics have been limited to tasks sufficiently simulated by fast\nrigid-body dynamics. Simulation techniques for soft bodies are comparatively\nseveral orders of magnitude slower, thereby limiting the use of RL due to\nsample complexity requirements. To address this challenge, this paper presents\nboth a novel RL algorithm and a simulation platform to enable scaling RL on\ntasks involving rigid bodies and deformables. We introduce Soft Analytic Policy\nOptimization (SAPO), a maximum entropy first-order model-based actor-critic RL\nalgorithm, which uses first-order analytic gradients from differentiable\nsimulation to train a stochastic actor to maximize expected return and entropy.\nAlongside our approach, we develop Rewarped, a parallel differentiable\nmultiphysics simulation platform that supports simulating various materials\nbeyond rigid bodies. We re-implement challenging manipulation and locomotion\ntasks in Rewarped, and show that SAPO outperforms baselines over a range of\ntasks that involve interaction between rigid bodies, articulations, and\ndeformables.\n","authors":["Eliot Xing","Vernon Luk","Jean Oh"],"pdf_url":"https://arxiv.org/pdf/2412.12089v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04215v2","updated":"2024-12-16T18:39:49Z","published":"2024-08-08T04:49:24Z","title":"Comp-LTL: Temporal Logic Planning via Zero-Shot Policy Composition","summary":"  This work develops a zero-shot mechanism, Comp-LTL, for an agent to satisfy a\nLinear Temporal Logic (LTL) specification given existing task primitives\ntrained via reinforcement learning (RL). Autonomous robots often need to\nsatisfy spatial and temporal goals that are unknown until run time. Prior work\nfocuses on learning policies for executing a task specified using LTL, but they\nincorporate the specification into the learning process. Any change to the\nspecification requires retraining the policy, either via fine-tuning or from\nscratch. We present a more flexible approach -- to learn a set of composable\ntask primitive policies that can be used to satisfy arbitrary LTL\nspecifications without retraining or fine-tuning. Task primitives can be\nlearned offline using RL and combined using Boolean composition at deployment.\nThis work focuses on creating and pruning a transition system (TS)\nrepresentation of the environment in order to solve for deterministic,\nnon-ambiguous, and feasible solutions to LTL specifications given an\nenvironment and a set of task primitive policies. We show that our pruned TS is\ndeterministic, contains no unrealizable transitions, and is sound. We verify\nour approach via simulation and compare it to other state of the art\napproaches, showing that Comp-LTL is safer and more adaptable.\n","authors":["Taylor Bergeron","Zachary Serlin","Kevin Leahy"],"pdf_url":"https://arxiv.org/pdf/2408.04215v2.pdf","comment":"16 pages, 11 figures. Updated to reflect additional results"},{"id":"http://arxiv.org/abs/2410.12172v2","updated":"2024-12-16T18:25:35Z","published":"2024-10-16T02:31:31Z","title":"The State of Robot Motion Generation","summary":"  This paper reviews the large spectrum of methods for generating robot motion\nproposed over the 50 years of robotics research culminating in recent\ndevelopments. It crosses the boundaries of methodologies, typically not\nsurveyed together, from those that operate over explicit models to those that\nlearn implicit ones. The paper discusses the current state-of-the-art as well\nas properties of varying methodologies, highlighting opportunities for\nintegration.\n","authors":["Kostas E. Bekris","Joe Doerr","Patrick Meng","Sumanth Tangirala"],"pdf_url":"https://arxiv.org/pdf/2410.12172v2.pdf","comment":"Presented at the International Symposium of Robotics Research (ISRR),\n  2024. Website:\n  https://pracsys.cs.rutgers.edu/papers/the-state-of-robot-motion-generation/"},{"id":"http://arxiv.org/abs/2412.12036v1","updated":"2024-12-16T18:03:23Z","published":"2024-12-16T18:03:23Z","title":"LeARN: Learnable and Adaptive Representations for Nonlinear Dynamics in\n  System Identification","summary":"  System identification, the process of deriving mathematical models of\ndynamical systems from observed input-output data, has undergone a paradigm\nshift with the advent of learning-based methods. Addressing the intricate\nchallenges of data-driven discovery in nonlinear dynamical systems, these\nmethods have garnered significant attention. Among them, Sparse Identification\nof Nonlinear Dynamics (SINDy) has emerged as a transformative approach,\ndistilling complex dynamical behaviors into interpretable linear combinations\nof basis functions. However, SINDy relies on domain-specific expertise to\nconstruct its foundational \"library\" of basis functions, which limits its\nadaptability and universality. In this work, we introduce a nonlinear system\nidentification framework called LeARN that transcends the need for prior domain\nknowledge by learning the library of basis functions directly from data. To\nenhance adaptability to evolving system dynamics under varying noise\nconditions, we employ a novel meta-learning-based system identification\napproach that uses a lightweight deep neural network (DNN) to dynamically\nrefine these basis functions. This not only captures intricate system behaviors\nbut also adapts seamlessly to new dynamical regimes. We validate our framework\non the Neural Fly dataset, showcasing its robust adaptation and generalization\ncapabilities. Despite its simplicity, our LeARN achieves competitive dynamical\nerror performance compared to SINDy. This work presents a step toward the\nautonomous discovery of dynamical systems, paving the way for a future where\nmachine learning uncovers the governing principles of complex systems without\nrequiring extensive domain-specific interventions.\n","authors":["Arunabh Singh","Joyjit Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2412.12036v1.pdf","comment":"This work has been submitted to the 7th Annual Learning for Dynamics\n  & Control Conference for review"},{"id":"http://arxiv.org/abs/2412.12035v1","updated":"2024-12-16T18:03:22Z","published":"2024-12-16T18:03:22Z","title":"Backstepping Control of Tendon-Driven Continuum Robots in Large\n  Deflections Using the Cosserat Rod Model","summary":"  This paper presents a study on the backstepping control of tendon-driven\ncontinuum robots for large deflections using the Cosserat rod model. Continuum\nrobots are known for their flexibility and adaptability, making them suitable\nfor various applications. However, modeling and controlling them pose\nchallenges due to their nonlinear dynamics. To model their dynamics, the\nCosserat rod method is employed to account for significant deflections, and a\nnumerical solution method is developed to solve the resulting partial\ndifferential equations. Previous studies on controlling tendon-driven continuum\nrobots using Cosserat rod theory focused on sliding mode control and were not\ntested for large deflections, lacking experimental validation. In this paper,\nbackstepping control is proposed as an alternative to sliding mode control for\nachieving a significant bending. The numerical results are validated through\nexperiments in this study, demonstrating that the proposed backstepping control\napproach is a promising solution for achieving large deflections with smoother\ntrajectories, reduced settling time, and lower overshoot. Furthermore, two\nscenarios involving external forces and disturbances were introduced to further\nhighlight the robustness of the backstepping control approach.\n","authors":["Rana Danesh","Farrokh Janabi-Sharifi"],"pdf_url":"https://arxiv.org/pdf/2412.12035v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12024v1","updated":"2024-12-16T17:51:09Z","published":"2024-12-16T17:51:09Z","title":"Learning to Navigate in Mazes with Novel Layouts using Abstract Top-down\n  Maps","summary":"  Learning navigation capabilities in different environments has long been one\nof the major challenges in decision-making. In this work, we focus on zero-shot\nnavigation ability using given abstract $2$-D top-down maps. Like human\nnavigation by reading a paper map, the agent reads the map as an image when\nnavigating in a novel layout, after learning to navigate on a set of training\nmaps. We propose a model-based reinforcement learning approach for this\nmulti-task learning problem, where it jointly learns a hypermodel that takes\ntop-down maps as input and predicts the weights of the transition network. We\nuse the DeepMind Lab environment and customize layouts using generated maps.\nOur method can adapt better to novel environments in zero-shot and is more\nrobust to noise.\n","authors":["Linfeng Zhao","Lawson L. S. Wong"],"pdf_url":"https://arxiv.org/pdf/2412.12024v1.pdf","comment":"Published at Reinforcement Learning Conference (RLC) 2024. Website:\n  http://lfzhao.com/map-nav/"},{"id":"http://arxiv.org/abs/2409.02920v2","updated":"2024-12-16T17:09:58Z","published":"2024-09-04T17:59:52Z","title":"RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early\n  version)","summary":"  In the rapidly advancing field of robotics, dual-arm coordination and complex\nobject manipulation are essential capabilities for developing advanced\nautonomous systems. However, the scarcity of diverse, high-quality\ndemonstration data and real-world-aligned evaluation benchmarks severely limits\nsuch development. To address this, we introduce RoboTwin, a generative digital\ntwin framework that uses 3D generative foundation models and large language\nmodels to produce diverse expert datasets and provide a real-world-aligned\nevaluation platform for dual-arm robotic tasks. Specifically, RoboTwin creates\nvaried digital twins of objects from single 2D images, generating realistic and\ninteractive scenarios. It also introduces a spatial relation-aware code\ngeneration framework that combines object annotations with large language\nmodels to break down tasks, determine spatial constraints, and generate precise\nrobotic movement code. Our framework offers a comprehensive benchmark with both\nsimulated and real-world data, enabling standardized evaluation and better\nalignment between simulated training and real-world performance. We validated\nour approach using the open-source COBOT Magic Robot platform. Policies\npre-trained on RoboTwin-generated data and fine-tuned with limited real-world\nsamples improve the success rate of over 70% for single-arm tasks and over 40%\nfor dual-arm tasks compared to models trained solely on real-world data. This\nsignificant improvement demonstrates RoboTwin's potential to enhance the\ndevelopment and evaluation of dual-arm robotic manipulation systems. Project\nPage: https://robotwin-benchmark.github.io/early-version/.\n","authors":["Yao Mu","Tianxing Chen","Shijia Peng","Zanxin Chen","Zeyu Gao","Yude Zou","Lunkai Lin","Zhiqiang Xie","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2409.02920v2.pdf","comment":"Project page: https://robotwin-benchmark.github.io/early-version/"},{"id":"http://arxiv.org/abs/2412.11974v1","updated":"2024-12-16T16:58:28Z","published":"2024-12-16T16:58:28Z","title":"Emma-X: An Embodied Multimodal Action Model with Grounded Chain of\n  Thought and Look-ahead Spatial Reasoning","summary":"  Traditional reinforcement learning-based robotic control methods are often\ntask-specific and fail to generalize across diverse environments or unseen\nobjects and instructions. Visual Language Models (VLMs) demonstrate strong\nscene understanding and planning capabilities but lack the ability to generate\nactionable policies tailored to specific robotic embodiments. To address this,\nVisual-Language-Action (VLA) models have emerged, yet they face challenges in\nlong-horizon spatial reasoning and grounded task planning. In this work, we\npropose the Embodied Multimodal Action Model with Grounded Chain of Thought and\nLook-ahead Spatial Reasoning, Emma-X. Emma-X leverages our constructed\nhierarchical embodiment dataset based on BridgeV2, containing 60,000 robot\nmanipulation trajectories auto-annotated with grounded task reasoning and\nspatial guidance. Additionally, we introduce a trajectory segmentation strategy\nbased on gripper states and motion trajectories, which can help mitigate\nhallucination in grounding subtask reasoning generation. Experimental results\ndemonstrate that Emma-X achieves superior performance over competitive\nbaselines, particularly in real-world robotic tasks requiring spatial\nreasoning.\n","authors":["Qi Sun","Pengfei Hong","Tej Deep Pala","Vernon Toh","U-Xuan Tan","Deepanway Ghosal","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2412.11974v1.pdf","comment":"https://github.com/declare-lab/Emma-X,\n  https://huggingface.co/declare-lab/Emma-X"},{"id":"http://arxiv.org/abs/2412.11916v1","updated":"2024-12-16T16:00:55Z","published":"2024-12-16T16:00:55Z","title":"Lightweight Decentralized Neural Network-Based Strategies for\n  Multi-Robot Patrolling","summary":"  The problem of decentralized multi-robot patrol has previously been\napproached primarily with hand-designed strategies for minimization of\n'idlenes' over the vertices of a graph-structured environment. Here we present\ntwo lightweight neural network-based strategies to tackle this problem, and\nshow that they significantly outperform existing strategies in both idleness\nminimization and against an intelligent intruder model, as well as presenting\nan examination of robustness to communication failure. Our results also\nindicate important considerations for future strategy design.\n","authors":["James C. Ward","Ryan McConville","Edmund R. Hunt"],"pdf_url":"https://arxiv.org/pdf/2412.11916v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11913v1","updated":"2024-12-16T15:56:04Z","published":"2024-12-16T15:56:04Z","title":"Learning Human-Aware Robot Policies for Adaptive Assistance","summary":"  Developing robots that can assist humans efficiently, safely, and adaptively\nis crucial for real-world applications such as healthcare. While previous work\noften assumes a centralized system for co-optimizing human-robot interactions,\nwe argue that real-world scenarios are much more complicated, as humans have\nindividual preferences regarding how tasks are performed. Robots typically lack\ndirect access to these implicit preferences. However, to provide effective\nassistance, robots must still be able to recognize and adapt to the individual\nneeds and preferences of different users. To address these challenges, we\npropose a novel framework in which robots infer human intentions and reason\nabout human utilities through interaction. Our approach features two critical\nmodules: the anticipation module is a motion predictor that captures the\nspatial-temporal relationship between the robot agent and user agent, which\ncontributes to predicting human behavior; the utility module infers the\nunderlying human utility functions through progressive task demonstration\nsampling. Extensive experiments across various robot types and assistive tasks\ndemonstrate that the proposed framework not only enhances task success and\nefficiency but also significantly improves user satisfaction, paving the way\nfor more personalized and adaptive assistive robotic systems. Code and demos\nare available at https://asonin.github.io/Human-Aware-Assistance/.\n","authors":["Jason Qin","Shikun Ban","Wentao Zhu","Yizhou Wang","Dimitris Samaras"],"pdf_url":"https://arxiv.org/pdf/2412.11913v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11882v1","updated":"2024-12-16T15:31:15Z","published":"2024-12-16T15:31:15Z","title":"Hardware-in-the-loop Simulation Testbed for Geomagnetic Navigation","summary":"  Geomagnetic navigation leverages the ubiquitous Earth's magnetic signals to\nnavigate missions, without dependence on GPS services or pre-stored geographic\nmaps. It has drawn increasing attention and is promising particularly for\nlong-range navigation into unexplored areas. Current geomagnetic navigation\nstudies are still in the early stages with simulations and computational\nvalidations, without concrete efforts to develop cost-friendly test platforms\nthat can empower deployment and experimental analysis of the developed\napproaches. This paper presents a hardware-in-the-loop simulation testbed to\nsupport geomagnetic navigation experimentation. Our testbed is dedicated to\nsynthesizing geomagnetic field environment for the navigation. We develop the\nsoftware in the testbed to simulate the dynamics of the navigation environment,\nand we build the hardware to generate the physical magnetic field, which\nfollows and aligns with the simulated environment. The testbed aims to provide\ncontrollable magnetic field that can be used to experiment with geomagnetic\nnavigation in labs, thus avoiding real and expensive navigation experiments,\ne.g., in the ocean, for validating navigation prototypes. We build the testbed\nwith off-the-shelf hardware in an unshielded environment to reduce cost. We\nalso develop the field generation control and hardware parameter optimization\nfor quality magnetic field generation. We conduct a detailed performance\nanalysis to show the quality of the field generation by the testbed, and we\nreport the experimental results on performance indicators, including accuracy,\nuniformity, stability, and convergence of the generated field towards the\ntarget geomagnetic environment.\n","authors":["Songnan Yang","Shiliang Zhang","Qianyun Zhang","Xiaohui Zhang","Xuehui Ma"],"pdf_url":"https://arxiv.org/pdf/2412.11882v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11840v1","updated":"2024-12-16T15:03:08Z","published":"2024-12-16T15:03:08Z","title":"Sonar-based Deep Learning in Underwater Robotics: Overview, Robustness\n  and Challenges","summary":"  With the growing interest in underwater exploration and monitoring,\nAutonomous Underwater Vehicles (AUVs) have become essential. The recent\ninterest in onboard Deep Learning (DL) has advanced real-time environmental\ninteraction capabilities relying on efficient and accurate vision-based DL\nmodels. However, the predominant use of sonar in underwater environments,\ncharacterized by limited training data and inherent noise, poses challenges to\nmodel robustness. This autonomy improvement raises safety concerns for\ndeploying such models during underwater operations, potentially leading to\nhazardous situations. This paper aims to provide the first comprehensive\noverview of sonar-based DL under the scope of robustness. It studies\nsonar-based DL perception task models, such as classification, object\ndetection, segmentation, and SLAM. Furthermore, the paper systematizes\nsonar-based state-of-the-art datasets, simulators, and robustness methods such\nas neural network verification, out-of-distribution, and adversarial attacks.\nThis paper highlights the lack of robustness in sonar-based DL research and\nsuggests future research pathways, notably establishing a baseline sonar-based\ndataset and bridging the simulation-to-reality gap.\n","authors":["Martin Aubard","Ana Madureira","Luís Teixeira","José Pinto"],"pdf_url":"https://arxiv.org/pdf/2412.11840v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11829v1","updated":"2024-12-16T14:52:51Z","published":"2024-12-16T14:52:51Z","title":"Robust Contact-rich Manipulation through Implicit Motor Adaptation","summary":"  Contact-rich manipulation plays a vital role in daily human activities, yet\nuncertain physical parameters pose significant challenges for both model-based\nand model-free planning and control. A promising approach to address this\nchallenge is to develop policies robust to a wide range of parameters. Domain\nadaptation and domain randomization are commonly used to achieve such policies\nbut often compromise generalization to new instances or perform conservatively\ndue to neglecting instance-specific information. \\textit{Explicit motor\nadaptation} addresses these issues by estimating system parameters online and\nthen retrieving the parameter-conditioned policy from a parameter-augmented\nbase policy. However, it typically relies on precise system identification or\nadditional high-quality policy retraining, presenting substantial challenges\nfor contact-rich tasks with diverse physical parameters. In this work, we\npropose \\textit{implicit motor adaptation}, which leverages tensor\nfactorization as an implicit representation of the base policy. Given a roughly\nestimated parameter distribution, the parameter-conditioned policy can be\nefficiently derived by exploiting the separable structure of tensor cores from\nthe base policy. This framework eliminates the need for precise system\nestimation and policy retraining while preserving optimal behavior and strong\ngeneralization. We provide a theoretical analysis validating this method,\nsupported by numerical evaluations on three contact-rich manipulation\nprimitives. Both simulation and real-world experiments demonstrate its ability\nto generate robust policies for diverse instances.\n","authors":["Teng Xue","Amirreza Razmjoo","Suhan Shetty","Sylvain Calinon"],"pdf_url":"https://arxiv.org/pdf/2412.11829v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11804v1","updated":"2024-12-16T14:15:14Z","published":"2024-12-16T14:15:14Z","title":"A Long-Duration Autonomy Approach to Connected and Automated Vehicles","summary":"  In this article, we present a long-duration autonomy approach for the control\nof connected and automated vehicles (CAVs) operating in a transportation\nnetwork. In particular, we focus on the performance of CAVs at traffic\nbottlenecks, including roundabouts, merging roadways, and intersections. We\ntake a principled approach based on optimal control, and derive a reactive\ncontroller with guarantees on safety, performance, and energy efficiency. We\nguarantee safety through high order control barrier functions (HOCBFs), which\nwe ``lift'' to first order CBFs using time-optimal motion primitives. We\ndemonstrate the performance of our approach in simulation and compare it to an\noptimal control-based approach.\n","authors":["Logan E. Beaver"],"pdf_url":"https://arxiv.org/pdf/2412.11804v1.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2412.11764v1","updated":"2024-12-16T13:31:26Z","published":"2024-12-16T13:31:26Z","title":"What Matters in Learning A Zero-Shot Sim-to-Real RL Policy for Quadrotor\n  Control? A Comprehensive Study","summary":"  Executing precise and agile flight maneuvers is critical for quadrotors in\nvarious applications. Traditional quadrotor control approaches are limited by\ntheir reliance on flat trajectories or time-consuming optimization, which\nrestricts their flexibility. Recently, RL-based policy has emerged as a\npromising alternative due to its ability to directly map observations to\nactions, reducing the need for detailed system knowledge and actuation\nconstraints. However, a significant challenge remains in bridging the\nsim-to-real gap, where RL-based policies often experience instability when\ndeployed in real world. In this paper, we investigate key factors for learning\nrobust RL-based control policies that are capable of zero-shot deployment in\nreal-world quadrotors. We identify five critical factors and we develop a\nPPO-based training framework named SimpleFlight, which integrates these five\ntechniques. We validate the efficacy of SimpleFlight on Crazyflie quadrotor,\ndemonstrating that it achieves more than a 50% reduction in trajectory tracking\nerror compared to state-of-the-art RL baselines, and achieves 70% improvement\nover the traditional MPC. The policy derived by SimpleFlight consistently\nexcels across both smooth polynominal trajectories and challenging infeasible\nzigzag trajectories on small thrust-to-weight quadrotors. In contrast, baseline\nmethods struggle with high-speed or infeasible trajectories. To support further\nresearch and reproducibility, we integrate SimpleFlight into a GPU-based\nsimulator Omnidrones and provide open-source access to the code and model\ncheckpoints. We hope SimpleFlight will offer valuable insights for advancing\nRL-based quadrotor control. For more details, visit our project website at\nhttps://sites.google.com/view/simpleflight/.\n","authors":["Jiayu Chen","Chao Yu","Yuqing Xie","Feng Gao","Yinuo Chen","Shu'ang Yu","Wenhao Tang","Shilong Ji","Mo Mu","Yi Wu","Huazhong Yang","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2412.11764v1.pdf","comment":"The first two authors contribute equally"},{"id":"http://arxiv.org/abs/2412.11760v1","updated":"2024-12-16T13:24:24Z","published":"2024-12-16T13:24:24Z","title":"Efficient LiDAR Bundle Adjustment for Multi-Scan Alignment Utilizing\n  Continuous-Time Trajectories","summary":"  Constructing precise global maps is a key task in robotics and is required\nfor localization, surveying, monitoring, or constructing digital twins. To\nbuild accurate maps, data from mobile 3D LiDAR sensors is often used. Mapping\nrequires correctly aligning the individual point clouds to each other to obtain\na globally consistent map. In this paper, we investigate the problem of\nmulti-scan alignment to obtain globally consistent point cloud maps. We propose\na 3D LiDAR bundle adjustment approach to solve the global alignment problem and\njointly optimize the available data. Utilizing a continuous-time trajectory\nallows us to consider the ego-motion of the LiDAR scanner while recording a\nsingle scan directly in the least squares adjustment. Furthermore, pruning the\nsearch space of correspondences and utilizing out-of-core circular buffer\nenables our approach to align thousands of point clouds efficiently. We\nsuccessfully align point clouds recorded with a handheld LiDAR, as well as ones\nmounted on a vehicle, and are able to perform multi-session alignment.\n","authors":["Louis Wiesmann","Elias Marks","Saurabh Gupta","Tiziano Guadagnino","Jens Behley","Cyrill Stachniss"],"pdf_url":"https://arxiv.org/pdf/2412.11760v1.pdf","comment":"Submitted to ICRA 2025"},{"id":"http://arxiv.org/abs/2412.11717v1","updated":"2024-12-16T12:39:02Z","published":"2024-12-16T12:39:02Z","title":"Learning UAV-based path planning for efficient localization of objects\n  using prior knowledge","summary":"  UAV's are becoming popular for various object search applications in\nagriculture, however they usually use time-consuming row-by-row flight paths.\nThis paper presents a deep-reinforcement-learning method for path planning to\nefficiently localize objects of interest using UAVs with a minimal flight-path\nlength. The method uses some global prior knowledge with uncertain object\nlocations and limited resolution in combination with a local object map created\nusing the output of an object detection network. The search policy could be\nlearned using deep Q-learning. We trained the agent in simulation, allowing\nthorough evaluation of the object distribution, typical errors in the\nperception system and prior knowledge, and different stopping criteria. When\nobjects were non-uniformly distributed over the field, the agent found the\nobjects quicker than a row-by-row flight path, showing that it learns to\nexploit the distribution of objects. Detection errors and quality of prior\nknowledge had only minor effect on the performance, indicating that the learned\nsearch policy was robust to errors in the perception system and did not need\ndetailed prior knowledge. Without prior knowledge, the learned policy was still\ncomparable in performance to a row-by-row flight path. Finally, we demonstrated\nthat it is possible to learn the appropriate moment to end the search task. The\napplicability of the approach for object search on a real drone was\ncomprehensively discussed and evaluated. Overall, we conclude that the learned\nsearch policy increased the efficiency of finding objects using a UAV, and can\nbe applied in real-world conditions when the specified assumptions are met.\n","authors":["Rick van Essen","Eldert van Henten","Gert Kootstra"],"pdf_url":"https://arxiv.org/pdf/2412.11717v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11682v1","updated":"2024-12-16T11:49:12Z","published":"2024-12-16T11:49:12Z","title":"NEST: A Neuromodulated Small-world Hypergraph Trajectory Prediction\n  Model for Autonomous Driving","summary":"  Accurate trajectory prediction is essential for the safety and efficiency of\nautonomous driving. Traditional models often struggle with real-time\nprocessing, capturing non-linearity and uncertainty in traffic environments,\nefficiency in dense traffic, and modeling temporal dynamics of interactions. We\nintroduce NEST (Neuromodulated Small-world Hypergraph Trajectory Prediction), a\nnovel framework that integrates Small-world Networks and hypergraphs for\nsuperior interaction modeling and prediction accuracy. This integration enables\nthe capture of both local and extended vehicle interactions, while the\nNeuromodulator component adapts dynamically to changing traffic conditions. We\nvalidate the NEST model on several real-world datasets, including nuScenes,\nMoCAD, and HighD. The results consistently demonstrate that NEST outperforms\nexisting methods in various traffic scenarios, showcasing its exceptional\ngeneralization capability, efficiency, and temporal foresight. Our\ncomprehensive evaluation illustrates that NEST significantly improves the\nreliability and operational efficiency of autonomous driving systems, making it\na robust solution for trajectory prediction in complex traffic environments.\n","authors":["Chengyue Wang","Haicheng Liao","Bonan Wang","Yanchen Guan","Bin Rao","Ziyuan Pu","Zhiyong Cui","Chengzhong Xu","Zhenning Li"],"pdf_url":"https://arxiv.org/pdf/2412.11682v1.pdf","comment":"Accepted by AAAI-25"},{"id":"http://arxiv.org/abs/2405.13705v4","updated":"2024-12-16T10:28:34Z","published":"2024-05-22T14:50:05Z","title":"Low Fidelity Digital Twin for Automated Driving Systems: Use Cases and\n  Automatic Generation","summary":"  Automated driving systems are an integral part of the automotive industry.\nTools such as Robot Operating System and simulators support their development.\nHowever, in the end, the developers must test their algorithms on a real\nvehicle. To better observe the difference between reality and simulation--the\nreality gap--digital twin technology offers real-time communication between the\nreal vehicle and its model. We present low fidelity digital twin generator and\ndescribe situations where automatic generation is preferable to high fidelity\nsimulation. We validated our approach of generating a virtual environment with\na vehicle model by replaying the data recorded from the real vehicle.\n","authors":["Jiri Vlasak","Jaroslav Klapálek","Adam Kollarčík","Michal Sojka","Zdeněk Hanzálek"],"pdf_url":"https://arxiv.org/pdf/2405.13705v4.pdf","comment":"8 pages, 3 figures, published in the proceedings for ICITT2024"},{"id":"http://arxiv.org/abs/2412.11632v1","updated":"2024-12-16T10:20:46Z","published":"2024-12-16T10:20:46Z","title":"Multi-Scale Incremental Modeling for Enhanced Human Motion Prediction in\n  Human-Robot Collaboration","summary":"  Accurate human motion prediction is crucial for safe human-robot\ncollaboration but remains challenging due to the complexity of modeling\nintricate and variable human movements. This paper presents Parallel\nMulti-scale Incremental Prediction (PMS), a novel framework that explicitly\nmodels incremental motion across multiple spatio-temporal scales to capture\nsubtle joint evolutions and global trajectory shifts. PMS encodes these\nmulti-scale increments using parallel sequence branches, enabling iterative\nrefinement of predictions. A multi-stage training procedure with a\nfull-timeline loss integrates temporal context. Extensive experiments on four\ndatasets demonstrate substantial improvements in continuity, biomechanical\nconsistency, and long-term forecast stability by modeling inter-frame\nincrements. PMS achieves state-of-the-art performance, increasing prediction\naccuracy by 16.3%-64.2% over previous methods. The proposed multi-scale\nincremental approach provides a powerful technique for advancing human motion\nprediction capabilities critical for seamless human-robot interaction.\n","authors":["Juncheng Zou"],"pdf_url":"https://arxiv.org/pdf/2412.11632v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11590v1","updated":"2024-12-16T09:24:04Z","published":"2024-12-16T09:24:04Z","title":"A Real-Time System for Scheduling and Managing UAV Delivery in Urban","summary":"  As urban logistics demand continues to grow, UAV delivery has become a key\nsolution to improve delivery efficiency, reduce traffic congestion, and lower\nlogistics costs. However, to fully leverage the potential of UAV delivery\nnetworks, efficient swarm scheduling and management are crucial. In this paper,\nwe propose a real-time scheduling and management system based on the\n``Airport-Unloading Station\" model, aiming to bridge the gap between high-level\nscheduling algorithms and low-level execution systems. This system, acting as\nmiddleware, accurately translates the requirements from the scheduling layer\ninto specific execution instructions, ensuring that the scheduling algorithms\nperform effectively in real-world environments. Additionally, we implement\nthree collaborative scheduling schemes involving autonomous ground vehicles\n(AGVs), unmanned aerial vehicles (UAVs), and ground staff to further optimize\noverall delivery efficiency. Through extensive experiments, this study\ndemonstrates the rationality and feasibility of the proposed management system,\nproviding practical solution for the commercial application of UAVs delivery in\nurban.\n  Code: https://github.com/chengji253/UAVDeliverySystem\n","authors":["Han Liu","Tian Liu","Kai Huang"],"pdf_url":"https://arxiv.org/pdf/2412.11590v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11552v1","updated":"2024-12-16T08:34:28Z","published":"2024-12-16T08:34:28Z","title":"Efficient Avoidance of Ellipsoidal Obstacles with Model Predictive\n  Control for Mobile Robots and Vehicles","summary":"  In real-world applications of mobile robots, collision avoidance is of\ncritical importance. Typically, global motion planning in constrained\nenvironments is addressed through high-level control schemes. However,\nadditionally integrating local collision avoidance into robot motion control\noffers significant advantages. For instance, it reduces the reliance on\nheuristics and conservatism that can arise from a two-stage approach separating\nlocal collision avoidance and control. Moreover, using model predictive control\n(MPC), a robot's full potential can be harnessed by considering jointly local\ncollision avoidance, the robot's dynamics, and actuation constraints. In this\ncontext, the present paper focuses on obstacle avoidance for wheeled mobile\nrobots, where both the robot's and obstacles' occupied volumes are modeled as\nellipsoids. To this end, a computationally efficient overlap test, that works\nfor arbitrary ellipsoids, is conducted and novelly integrated into the MPC\nframework. We propose a particularly efficient implementation tailored to\nrobots moving in the plane. The functionality of the proposed obstacle-avoiding\nMPC is demonstrated for two exemplary types of kinematics by means of\nsimulations. A hardware experiment using a real-world wheeled mobile robot\nshows transferability to reality and real-time applicability. The general\ncomputational approach to ellipsoidal obstacle avoidance can also be applied to\nother robotic systems and vehicles as well as three-dimensional scenarios.\n","authors":["Mario Rosenfelder","Hendrik Carius","Markus Herrmann-Wicklmayr","Peter Eberhard","Kathrin Flaßkamp","Henrik Ebel"],"pdf_url":"https://arxiv.org/pdf/2412.11552v1.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2412.11523v1","updated":"2024-12-16T07:59:23Z","published":"2024-12-16T07:59:23Z","title":"ON as ALC: Active Loop Closing Object Goal Navigation","summary":"  In simultaneous localization and mapping, active loop closing (ALC) is an\nactive vision problem that aims to visually guide a robot to maximize the\nchances of revisiting previously visited points, thereby resetting the drift\nerrors accumulated in the incrementally built map during travel. However,\ncurrent mainstream navigation strategies that leverage such incomplete maps as\nworkspace prior knowledge often fail in modern long-term autonomy long-distance\ntravel scenarios where map accumulation errors become significant. To address\nthese limitations of map-based navigation, this paper is the first to explore\nmapless navigation in the embodied AI field, in particular, to utilize\nobject-goal navigation (commonly abbreviated as ON, ObjNav, or OGN) techniques\nthat efficiently explore target objects without using such a prior map.\nSpecifically, in this work, we start from an off-the-shelf mapless ON planner,\nextend it to utilize a prior map, and further show that the performance in\nlong-distance ALC (LD-ALC) can be maximized by minimizing ``ALC loss\" and ``ON\nloss\". This study highlights a simple and effective approach, called ALC-ON\n(ALCON), to accelerate the progress of challenging long-distance ALC technology\nby leveraging the growing frontier-guided, data-driven, and LLM-guided ON\ntechnologies.\n","authors":["Daiki Iwata","Kanji Tanaka","Shoya Miyazaki","Kouki Terashima"],"pdf_url":"https://arxiv.org/pdf/2412.11523v1.pdf","comment":"Draft version of a conference paper with 7 pages, 5 figures, and 1\n  table"},{"id":"http://arxiv.org/abs/2412.11503v1","updated":"2024-12-16T07:25:40Z","published":"2024-12-16T07:25:40Z","title":"Visual-Based Forklift Learning System Enabling Zero-Shot Sim2Real\n  Without Real-World Data","summary":"  Forklifts are used extensively in various industrial settings and are in high\ndemand for automation. In particular, counterbalance forklifts are highly\nversatile and employed in diverse scenarios. However, efforts to automate these\nprocesses are lacking, primarily owing to the absence of a safe and\nperformance-verifiable development environment. This study proposes a learning\nsystem that combines a photorealistic digital learning environment with a\n1/14-scale robotic forklift environment to address this challenge. Inspired by\nthe training-based learning approach adopted by forklift operators, we employ\nan end-to-end vision-based deep reinforcement learning approach. The learning\nis conducted in a digitalized environment created from CAD data, making it safe\nand eliminating the need for real-world data. In addition, we safely validate\nthe method in a physical setting utilizing a 1/14-scale robotic forklift with a\nconfiguration similar to that of a real forklift. We achieved a 60% success\nrate in pallet loading tasks in real experiments using a robotic forklift. Our\napproach demonstrates zero-shot sim2real with a simple method that does not\nrequire heuristic additions. This learning-based approach is considered a first\nstep towards the automation of counterbalance forklifts.\n","authors":["Koshi Oishi","Teruki Kato","Hiroya Makino","Seigo Ito"],"pdf_url":"https://arxiv.org/pdf/2412.11503v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11499v1","updated":"2024-12-16T07:18:02Z","published":"2024-12-16T07:18:02Z","title":"Embodied CoT Distillation From LLM To Off-the-shelf Agents","summary":"  We address the challenge of utilizing large language models (LLMs) for\ncomplex embodied tasks, in the environment where decision-making systems\noperate timely on capacity-limited, off-the-shelf devices. We present DeDer, a\nframework for decomposing and distilling the embodied reasoning capabilities\nfrom LLMs to efficient, small language model (sLM)-based policies. In DeDer,\nthe decision-making process of LLM-based strategies is restructured into a\nhierarchy with a reasoning-policy and planning-policy. The reasoning-policy is\ndistilled from the data that is generated through the embodied in-context\nlearning and self-verification of an LLM, so it can produce effective\nrationales. The planning-policy, guided by the rationales, can render optimized\nplans efficiently. In turn, DeDer allows for adopting sLMs for both policies,\ndeployed on off-the-shelf devices. Furthermore, to enhance the quality of\nintermediate rationales, specific to embodied tasks, we devise the embodied\nknowledge graph, and to generate multiple rationales timely through a single\ninference, we also use the contrastively prompted attention model. Our\nexperiments with the ALFRED benchmark demonstrate that DeDer surpasses leading\nlanguage planning and distillation approaches, indicating the applicability and\nefficiency of sLM-based embodied policies derived through DeDer.\n","authors":["Wonje Choi","Woo Kyung Kim","Minjong Yoo","Honguk Woo"],"pdf_url":"https://arxiv.org/pdf/2412.11499v1.pdf","comment":"Accepted at ICML 2024"},{"id":"http://arxiv.org/abs/2412.11489v1","updated":"2024-12-16T07:06:17Z","published":"2024-12-16T07:06:17Z","title":"HGSFusion: Radar-Camera Fusion with Hybrid Generation and\n  Synchronization for 3D Object Detection","summary":"  Millimeter-wave radar plays a vital role in 3D object detection for\nautonomous driving due to its all-weather and all-lighting-condition\ncapabilities for perception. However, radar point clouds suffer from pronounced\nsparsity and unavoidable angle estimation errors. To address these limitations,\nincorporating a camera may partially help mitigate the shortcomings.\nNevertheless, the direct fusion of radar and camera data can lead to negative\nor even opposite effects due to the lack of depth information in images and\nlow-quality image features under adverse lighting conditions. Hence, in this\npaper, we present the radar-camera fusion network with Hybrid Generation and\nSynchronization (HGSFusion), designed to better fuse radar potentials and image\nfeatures for 3D object detection. Specifically, we propose the Radar Hybrid\nGeneration Module (RHGM), which fully considers the Direction-Of-Arrival (DOA)\nestimation errors in radar signal processing. This module generates denser\nradar points through different Probability Density Functions (PDFs) with the\nassistance of semantic information. Meanwhile, we introduce the Dual Sync\nModule (DSM), comprising spatial sync and modality sync, to enhance image\nfeatures with radar positional information and facilitate the fusion of\ndistinct characteristics in different modalities. Extensive experiments\ndemonstrate the effectiveness of our approach, outperforming the\nstate-of-the-art methods in the VoD and TJ4DRadSet datasets by $6.53\\%$ and\n$2.03\\%$ in RoI AP and BEV AP, respectively. The code is available at\nhttps://github.com/garfield-cpp/HGSFusion.\n","authors":["Zijian Gu","Jianwei Ma","Yan Huang","Honghao Wei","Zhanye Chen","Hui Zhang","Wei Hong"],"pdf_url":"https://arxiv.org/pdf/2412.11489v1.pdf","comment":"12 pages, 8 figures, 7 tables. Accepted by AAAI 2025 , the 39th\n  Annual AAAI Conference on Artificial Intelligence"},{"id":"http://arxiv.org/abs/2412.11484v1","updated":"2024-12-16T06:53:00Z","published":"2024-12-16T06:53:00Z","title":"Efficient Policy Adaptation with Contrastive Prompt Ensemble for\n  Embodied Agents","summary":"  For embodied reinforcement learning (RL) agents interacting with the\nenvironment, it is desirable to have rapid policy adaptation to unseen visual\nobservations, but achieving zero-shot adaptation capability is considered as a\nchallenging problem in the RL context. To address the problem, we present a\nnovel contrastive prompt ensemble (ConPE) framework which utilizes a pretrained\nvision-language model and a set of visual prompts, thus enabling efficient\npolicy learning and adaptation upon a wide range of environmental and physical\nchanges encountered by embodied agents. Specifically, we devise a\nguided-attention-based ensemble approach with multiple visual prompts on the\nvision-language model to construct robust state representations. Each prompt is\ncontrastively learned in terms of an individual domain factor that\nsignificantly affects the agent's egocentric perception and observation. For a\ngiven task, the attention-based ensemble and policy are jointly learned so that\nthe resulting state representations not only generalize to various domains but\nare also optimized for learning the task. Through experiments, we show that\nConPE outperforms other state-of-the-art algorithms for several embodied agent\ntasks including navigation in AI2THOR, manipulation in egocentric-Metaworld,\nand autonomous driving in CARLA, while also improving the sample efficiency of\npolicy learning and adaptation.\n","authors":["Wonje Choi","Woo Kyung Kim","SeungHyun Kim","Honguk Woo"],"pdf_url":"https://arxiv.org/pdf/2412.11484v1.pdf","comment":"Accepted at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2412.09265v3","updated":"2024-12-16T05:43:20Z","published":"2024-12-12T13:22:02Z","title":"Score and Distribution Matching Policy: Advanced Accelerated Visuomotor\n  Policies via Matched Distillation","summary":"  Visual-motor policy learning has advanced with architectures like\ndiffusion-based policies, known for modeling complex robotic trajectories.\nHowever, their prolonged inference times hinder high-frequency control tasks\nrequiring real-time feedback. While consistency distillation (CD) accelerates\ninference, it introduces errors that compromise action quality. To address\nthese limitations, we propose the Score and Distribution Matching Policy (SDM\nPolicy), which transforms diffusion-based policies into single-step generators\nthrough a two-stage optimization process: score matching ensures alignment with\ntrue action distributions, and distribution matching minimizes KL divergence\nfor consistency. A dual-teacher mechanism integrates a frozen teacher for\nstability and an unfrozen teacher for adversarial training, enhancing\nrobustness and alignment with target distributions. Evaluated on a 57-task\nsimulation benchmark, SDM Policy achieves a 6x inference speedup while having\nstate-of-the-art action quality, providing an efficient and reliable framework\nfor high-frequency robotic tasks.\n","authors":["Bofang Jia","Pengxiang Ding","Can Cui","Mingyang Sun","Pengfang Qian","Siteng Huang","Zhaoxin Fan","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2412.09265v3.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2412.03887v3","updated":"2024-12-16T04:21:55Z","published":"2024-12-05T05:40:40Z","title":"MOANA: Multi-Radar Dataset for Maritime Odometry and Autonomous\n  Navigation Application","summary":"  Maritime environmental sensing requires overcoming challenges from complex\nconditions such as harsh weather, platform perturbations, large dynamic\nobjects, and the requirement for long detection ranges. While cameras and LiDAR\nare commonly used in ground vehicle navigation, their applicability in maritime\nsettings is limited by range constraints and hardware maintenance issues. Radar\nsensors, however, offer robust long-range detection capabilities and resilience\nto physical contamination from weather and saline conditions, making it a\npowerful sensor for maritime navigation. Among various radar types, X-band\nradar (e.g., marine radar) is widely employed for maritime vessel navigation,\nproviding effective long-range detection essential for situational awareness\nand collision avoidance. Nevertheless, it exhibits limitations during berthing\noperations where close-range object detection is critical. To address this\nshortcoming, we incorporate W-band radar (e.g., Navtech imaging radar), which\nexcels in detecting nearby objects with a higher update rate. We present a\ncomprehensive maritime sensor dataset featuring multi-range detection\ncapabilities. This dataset integrates short-range LiDAR data, medium-range\nW-band radar data, and long-range X-band radar data into a unified framework.\nAdditionally, it includes object labels for oceanic object detection usage,\nderived from radar and stereo camera images. The dataset comprises seven\nsequences collected from diverse regions with varying levels of estimation\ndifficulty, ranging from easy to challenging, and includes common locations\nsuitable for global localization tasks. This dataset serves as a valuable\nresource for advancing research in place recognition, odometry estimation,\nSLAM, object detection, and dynamic object elimination within maritime\nenvironments. Dataset can be found in following link:\nhttps://sites.google.com/view/rpmmoana\n","authors":["Hyesu Jang","Wooseong Yang","Hanguen Kim","Dongje Lee","Yongjin Kim","Jinbum Park","Minsoo Jeon","Jaeseong Koh","Yejin Kang","Minwoo Jung","Sangwoo Jung","Chng Zhen Hao","Wong Yu Hin","Chew Yihang","Ayoung Kim"],"pdf_url":"https://arxiv.org/pdf/2412.03887v3.pdf","comment":"9 pages, 8 figures, 3 tables"},{"id":"http://arxiv.org/abs/2412.11387v1","updated":"2024-12-16T02:28:34Z","published":"2024-12-16T02:28:34Z","title":"How Can LLMs and Knowledge Graphs Contribute to Robot Safety? A Few-Shot\n  Learning Approach","summary":"  Large Language Models (LLMs) are transforming the robotics domain by enabling\nrobots to comprehend and execute natural language instructions. The cornerstone\nbenefits of LLM include processing textual data from technical manuals,\ninstructions, academic papers, and user queries based on the knowledge\nprovided. However, deploying LLM-generated code in robotic systems without\nsafety verification poses significant risks. This paper outlines a safety layer\nthat verifies the code generated by ChatGPT before executing it to control a\ndrone in a simulated environment. The safety layer consists of a fine-tuned\nGPT-4o model using Few-Shot learning, supported by knowledge graph prompting\n(KGP). Our approach improves the safety and compliance of robotic actions,\nensuring that they adhere to the regulations of drone operations.\n","authors":["Abdulrahman Althobaiti","Angel Ayala","JingYing Gao","Ali Almutairi","Mohammad Deghat","Imran Razzak","Francisco Cruz"],"pdf_url":"https://arxiv.org/pdf/2412.11387v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11360v1","updated":"2024-12-16T01:23:13Z","published":"2024-12-16T01:23:13Z","title":"Visual IRL for Human-Like Robotic Manipulation","summary":"  We present a novel method for collaborative robots (cobots) to learn\nmanipulation tasks and perform them in a human-like manner. Our method falls\nunder the learn-from-observation (LfO) paradigm, where robots learn to perform\ntasks by observing human actions, which facilitates quicker integration into\nindustrial settings compared to programming from scratch. We introduce Visual\nIRL that uses the RGB-D keypoints in each frame of the observed human task\nperformance directly as state features, which are input to inverse\nreinforcement learning (IRL). The inversely learned reward function, which maps\nkeypoints to reward values, is transferred from the human to the cobot using a\nnovel neuro-symbolic dynamics model, which maps human kinematics to the cobot\narm. This model allows similar end-effector positioning while minimizing joint\nadjustments, aiming to preserve the natural dynamics of human motion in robotic\nmanipulation. In contrast with previous techniques that focus on end-effector\nplacement only, our method maps multiple joint angles of the human arm to the\ncorresponding cobot joints. Moreover, it uses an inverse kinematics model to\nthen minimally adjust the joint angles, for accurate end-effector positioning.\nWe evaluate the performance of this approach on two different realistic\nmanipulation tasks. The first task is produce processing, which involves\npicking, inspecting, and placing onions based on whether they are blemished.\nThe second task is liquid pouring, where the robot picks up bottles, pours the\ncontents into designated containers, and disposes of the empty bottles. Our\nresults demonstrate advances in human-like robotic manipulation, leading to\nmore human-robot compatibility in manufacturing applications.\n","authors":["Ehsan Asali","Prashant Doshi"],"pdf_url":"https://arxiv.org/pdf/2412.11360v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12406v1","updated":"2024-12-16T23:17:40Z","published":"2024-12-16T23:17:40Z","title":"Global SLAM in Visual-Inertial Systems with 5G Time-of-Arrival\n  Integration","summary":"  This paper presents a novel approach to improve global localization and\nmapping in indoor drone navigation by integrating 5G Time of Arrival (ToA)\nmeasurements into ORB-SLAM3, a Simultaneous Localization and Mapping (SLAM)\nsystem. By incorporating ToA data from 5G base stations, we align the SLAM's\nlocal reference frame with a global coordinate system, enabling accurate and\nconsistent global localization. We extend ORB-SLAM3's optimization pipeline to\nintegrate ToA measurements alongside bias estimation, transforming the\ninherently local estimation into a globally consistent one. This integration\neffectively resolves scale ambiguity in monocular SLAM systems and enhances\nrobustness, particularly in challenging scenarios where standard SLAM may fail.\nOur method is evaluated using five real-world indoor datasets collected with\nRGB-D cameras and inertial measurement units (IMUs), augmented with simulated\n5G ToA measurements at 28 GHz and 78 GHz frequencies using MATLAB and QuaDRiGa.\nWe tested four SLAM configurations: RGB-D, RGB-D-Inertial, Monocular, and\nMonocular-Inertial. The results demonstrate that while local estimation\naccuracy remains comparable due to the high precision of RGB-D-based ORB-SLAM3\ncompared to ToA measurements, the inclusion of ToA measurements facilitates\nrobust global positioning. In scenarios where standard mono-inertial ORB-SLAM3\nloses tracking, our approach maintains accurate localization throughout the\ntrajectory.\n","authors":["Meisam Kabiri","Holger Voos"],"pdf_url":"https://arxiv.org/pdf/2412.12406v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12392v1","updated":"2024-12-16T23:00:05Z","published":"2024-12-16T23:00:05Z","title":"MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors","summary":"  We present a real-time monocular dense SLAM system designed bottom-up from\nMASt3R, a two-view 3D reconstruction and matching prior. Equipped with this\nstrong prior, our system is robust on in-the-wild video sequences despite\nmaking no assumption on a fixed or parametric camera model beyond a unique\ncamera centre. We introduce efficient methods for pointmap matching, camera\ntracking and local fusion, graph construction and loop closure, and\nsecond-order global optimisation. With known calibration, a simple modification\nto the system achieves state-of-the-art performance across various benchmarks.\nAltogether, we propose a plug-and-play monocular SLAM system capable of\nproducing globally-consistent poses and dense geometry while operating at 15\nFPS.\n","authors":["Riku Murai","Eric Dexheimer","Andrew J. Davison"],"pdf_url":"https://arxiv.org/pdf/2412.12392v1.pdf","comment":"The first two authors contributed equally to this work. Project Page:\n  https://edexheim.github.io/mast3r-slam/"},{"id":"http://arxiv.org/abs/2412.09624v2","updated":"2024-12-16T22:17:55Z","published":"2024-12-12T18:59:57Z","title":"GenEx: Generating an Explorable World","summary":"  Understanding, navigating, and exploring the 3D physical real world has long\nbeen a central challenge in the development of artificial intelligence. In this\nwork, we take a step toward this goal by introducing GenEx, a system capable of\nplanning complex embodied world exploration, guided by its generative\nimagination that forms priors (expectations) about the surrounding\nenvironments. GenEx generates an entire 3D-consistent imaginative environment\nfrom as little as a single RGB image, bringing it to life through panoramic\nvideo streams. Leveraging scalable 3D world data curated from Unreal Engine,\nour generative model is rounded in the physical world. It captures a continuous\n360-degree environment with little effort, offering a boundless landscape for\nAI agents to explore and interact with. GenEx achieves high-quality world\ngeneration, robust loop consistency over long trajectories, and demonstrates\nstrong 3D capabilities such as consistency and active 3D mapping. Powered by\ngenerative imagination of the world, GPT-assisted agents are equipped to\nperform complex embodied tasks, including both goal-agnostic exploration and\ngoal-driven navigation. These agents utilize predictive expectation regarding\nunseen parts of the physical world to refine their beliefs, simulate different\noutcomes based on potential decisions, and make more informed choices. In\nsummary, we demonstrate that GenEx provides a transformative platform for\nadvancing embodied AI in imaginative spaces and brings potential for extending\nthese capabilities to real-world exploration.\n","authors":["Taiming Lu","Tianmin Shu","Junfei Xiao","Luoxin Ye","Jiahao Wang","Cheng Peng","Chen Wei","Daniel Khashabi","Rama Chellappa","Alan Yuille","Jieneng Chen"],"pdf_url":"https://arxiv.org/pdf/2412.09624v2.pdf","comment":"Website: GenEx.world"},{"id":"http://arxiv.org/abs/2409.00303v3","updated":"2024-12-16T21:52:48Z","published":"2024-08-31T00:00:39Z","title":"Rapid and Robust Trajectory Optimization for Humanoids","summary":"  Performing trajectory design for humanoid robots with high degrees of freedom\nis computationally challenging. The trajectory design process also often\ninvolves carefully selecting various hyperparameters and requires a good\ninitial guess which can further complicate the development process. This work\nintroduces a generalized gait optimization framework that directly generates\nsmooth and physically feasible trajectories. The proposed method demonstrates\nfaster and more robust convergence than existing techniques and explicitly\nincorporates closed-loop kinematic constraints that appear in many modern\nhumanoids. The method is implemented as an open-source C++ codebase which can\nbe found at https://roahmlab.github.io/RAPTOR/.\n","authors":["Bohao Zhang","Ram Vasudevan"],"pdf_url":"https://arxiv.org/pdf/2409.00303v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03516v2","updated":"2024-12-16T20:54:56Z","published":"2024-08-07T02:54:43Z","title":"Query3D: LLM-Powered Open-Vocabulary Scene Segmentation with Language\n  Embedded 3D Gaussian","summary":"  This paper introduces a novel method for open-vocabulary 3D scene querying in\nautonomous driving by combining Language Embedded 3D Gaussians with Large\nLanguage Models (LLMs). We propose utilizing LLMs to generate both contextually\ncanonical phrases and helping positive words for enhanced segmentation and\nscene interpretation. Our method leverages GPT-3.5 Turbo as an expert model to\ncreate a high-quality text dataset, which we then use to fine-tune smaller,\nmore efficient LLMs for on-device deployment. Our comprehensive evaluation on\nthe WayveScenes101 dataset demonstrates that LLM-guided segmentation\nsignificantly outperforms traditional approaches based on predefined canonical\nphrases. Notably, our fine-tuned smaller models achieve performance comparable\nto larger expert models while maintaining faster inference times. Through\nablation studies, we discover that the effectiveness of helping positive words\ncorrelates with model scale, with larger models better equipped to leverage\nadditional semantic information. This work represents a significant advancement\ntowards more efficient, context-aware autonomous driving systems, effectively\nbridging 3D scene representation with high-level semantic querying while\nmaintaining practical deployment considerations.\n","authors":["Amirhosein Chahe","Lifeng Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.03516v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07880v3","updated":"2024-12-16T19:47:25Z","published":"2024-04-11T16:10:52Z","title":"Multi-Robot Target Tracking with Sensing and Communication Danger Zones","summary":"  Multi-robot target tracking finds extensive applications in different\nscenarios, such as environmental surveillance and wildfire management, which\nrequire the robustness of the practical deployment of multi-robot systems in\nuncertain and dangerous environments. Traditional approaches often focus on the\nperformance of tracking accuracy with no modeling and assumption of the\nenvironments, neglecting potential environmental hazards which result in system\nfailures in real-world deployments. To address this challenge, we investigate\nmulti-robot target tracking in the adversarial environment considering sensing\nand communication attacks with uncertainty. We design specific strategies to\navoid different danger zones and proposed a multi-agent tracking framework\nunder the perilous environment. We approximate the probabilistic constraints\nand formulate practical optimization strategies to address computational\nchallenges efficiently. We evaluate the performance of our proposed methods in\nsimulations to demonstrate the ability of robots to adjust their risk-aware\nbehaviors under different levels of environmental uncertainty and risk\nconfidence. The proposed method is further validated via real-world robot\nexperiments where a team of drones successfully track dynamic ground robots\nwhile being risk-aware of the sensing and/or communication danger zones.\n","authors":["Jiazhen Liu","Peihan Li","Yuwei Wu","Gaurav S. Sukhatme","Vijay Kumar","Lifeng Zhou"],"pdf_url":"https://arxiv.org/pdf/2404.07880v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12314v1","updated":"2024-12-16T19:34:18Z","published":"2024-12-16T19:34:18Z","title":"A Feasible Workflow for Retinal Vein Cannulation in Ex Vivo Porcine Eyes\n  with Robotic Assistance","summary":"  A potential Retinal Vein Occlusion (RVO) treatment involves Retinal Vein\nCannulation (RVC), which requires the surgeon to insert a microneedle into the\naffected retinal vein and administer a clot-dissolving drug. This procedure\npresents significant challenges due to human physiological limitations, such as\nhand tremors, prolonged tool-holding periods, and constraints in depth\nperception using a microscope. This study proposes a robot-assisted workflow\nfor RVC to overcome these limitations. The test robot is operated through a\nkeyboard. An intraoperative Optical Coherence Tomography (iOCT) system is used\nto verify successful venous puncture before infusion. The workflow is validated\nusing 12 ex vivo porcine eyes. These early results demonstrate a successful\nrate of 10 out of 12 cannulations (83.33%), affirming the feasibility of the\nproposed workflow.\n","authors":["Peiyao Zhang","Peter Gehlbach","Marin Kobilarov","Iulian Iordachita"],"pdf_url":"https://arxiv.org/pdf/2412.12314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12238v1","updated":"2024-12-16T18:07:47Z","published":"2024-12-16T18:07:47Z","title":"Expanded Comprehensive Robotic Cholecystectomy Dataset (CRCD)","summary":"  In recent years, the application of machine learning to minimally invasive\nsurgery (MIS) has attracted considerable interest. Datasets are critical to the\nuse of such techniques. This paper presents a unique dataset recorded during ex\nvivo pseudo-cholecystectomy procedures on pig livers using the da Vinci\nResearch Kit (dVRK). Unlike existing datasets, it addresses a critical gap by\nproviding comprehensive kinematic data, recordings of all pedal inputs, and\noffers a time-stamped record of the endoscope's movements. This expanded\nversion also includes segmentation and keypoint annotations of images,\nenhancing its utility for computer vision applications.\n  Contributed by seven surgeons with varied backgrounds and experience levels\nthat are provided as a part of this expanded version, the dataset is an\nimportant new resource for surgical robotics research. It enables the\ndevelopment of advanced methods for evaluating surgeon skills, tools for\nproviding better context awareness, and automation of surgical tasks. Our work\novercomes the limitations of incomplete recordings and imprecise kinematic data\nfound in other datasets. To demonstrate the potential of the dataset for\nadvancing automation in surgical robotics, we introduce two models that predict\nclutch usage and camera activation, a 3D scene reconstruction example, and the\nresults from our keypoint and segmentation models.\n","authors":["Ki-Hwan Oh","Leonardo Borgioli","Alberto Mangano","Valentina Valle","Marco Di Pangrazio","Francesco Toti","Gioia Pozza","Luciano Ambrosini","Alvaro Ducas","Miloš Žefran","Liaohai Chen","Pier Cristoforo Giulianotti"],"pdf_url":"https://arxiv.org/pdf/2412.12238v1.pdf","comment":"Preprint of an article accepted in Journal of Medical Robotics\n  Research (2024). The metadata will be updated once it is published"},{"id":"http://arxiv.org/abs/2412.12237v1","updated":"2024-12-16T17:51:14Z","published":"2024-12-16T17:51:14Z","title":"Equivariant Action Sampling for Reinforcement Learning and Planning","summary":"  Reinforcement learning (RL) algorithms for continuous control tasks require\naccurate sampling-based action selection. Many tasks, such as robotic\nmanipulation, contain inherent problem symmetries. However, correctly\nincorporating symmetry into sampling-based approaches remains a challenge. This\nwork addresses the challenge of preserving symmetry in sampling-based planning\nand control, a key component for enhancing decision-making efficiency in RL. We\nintroduce an action sampling approach that enforces the desired symmetry. We\napply our proposed method to a coordinate regression problem and show that the\nsymmetry aware sampling method drastically outperforms the naive sampling\napproach. We furthermore develop a general framework for sampling-based\nmodel-based planning with Model Predictive Path Integral (MPPI). We compare our\nMPPI approach with standard sampling methods on several continuous control\ntasks. Empirical demonstrations across multiple continuous control environments\nvalidate the effectiveness of our approach, showcasing the importance of\nsymmetry preservation in sampling-based action selection.\n","authors":["Linfeng Zhao","Owen Howell","Xupeng Zhu","Jung Yeon Park","Zhewen Zhang","Robin Walters","Lawson L. S. Wong"],"pdf_url":"https://arxiv.org/pdf/2412.12237v1.pdf","comment":"Published at International Workshop on the Algorithmic Foundations of\n  Robotics (WAFR) 2024. Website: http://lfzhao.com/EquivSampling"},{"id":"http://arxiv.org/abs/2412.12231v1","updated":"2024-12-16T14:36:51Z","published":"2024-12-16T14:36:51Z","title":"Demonstrating Data-to-Knowledge Pipelines for Connecting Production\n  Sites in the World Wide Lab","summary":"  The digital transformation of production requires new methods of data\nintegration and storage, as well as decision making and support systems that\nwork vertically and horizontally throughout the development, production, and\nuse cycle. In this paper, we propose Data-to-Knowledge (and Knowledge-to-Data)\npipelines for production as a universal concept building on a network of\nDigital Shadows (a concept augmenting Digital Twins). We show a proof of\nconcept that builds on and bridges existing infrastructure to 1) capture and\nsemantically annotates trajectory data from multiple similar but independent\nrobots in different organisations and use cases in a data lakehouse and 2) an\nindependent process that dynamically queries matching data for training an\ninverse dynamic foundation model for robotic control. The article discusses the\nchallenges and benefits of this approach and how Data-to-Knowledge pipelines\ncontribute efficiency gains and industrial scalability in a World Wide Lab as a\nresearch outlook.\n","authors":["Leon Gorißen","Jan-Niklas Schneider","Mohamed Behery","Philipp Brauner","Moritz Lennartz","David Kötter","Thomas Kaster","Oliver Petrovic","Christian Hinke","Thomas Gries","Gerhard Lakemeyer","Martina Ziefle","Christian Brecher","Constantin Häfner"],"pdf_url":"https://arxiv.org/pdf/2412.12231v1.pdf","comment":"15 pages, 6 figures, submitted to CAiSE 2025"}]},"2024-12-15T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2412.11342v1","updated":"2024-12-15T23:52:35Z","published":"2024-12-15T23:52:35Z","title":"One-Shot Multilingual Font Generation Via ViT","summary":"  Font design poses unique challenges for logographic languages like Chinese,\nJapanese, and Korean (CJK), where thousands of unique characters must be\nindividually crafted. This paper introduces a novel Vision Transformer\n(ViT)-based model for multi-language font generation, effectively addressing\nthe complexities of both logographic and alphabetic scripts. By leveraging ViT\nand pretraining with a strong visual pretext task (Masked Autoencoding, MAE),\nour model eliminates the need for complex design components in prior frameworks\nwhile achieving comprehensive results with enhanced generalizability.\nRemarkably, it can generate high-quality fonts across multiple languages for\nunseen, unknown, and even user-crafted characters. Additionally, we integrate a\nRetrieval-Augmented Guidance (RAG) module to dynamically retrieve and adapt\nstyle references, improving scalability and real-world applicability. We\nevaluated our approach in various font generation tasks, demonstrating its\neffectiveness, adaptability, and scalability.\n","authors":["Zhiheng Wang","Jiarui Liu"],"pdf_url":"https://arxiv.org/pdf/2412.11342v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11337v1","updated":"2024-12-15T23:05:16Z","published":"2024-12-15T23:05:16Z","title":"Modality-Driven Design for Multi-Step Dexterous Manipulation: Insights\n  from Neuroscience","summary":"  Multi-step dexterous manipulation is a fundamental skill in household\nscenarios, yet remains an underexplored area in robotics. This paper proposes a\nmodular approach, where each step of the manipulation process is addressed with\ndedicated policies based on effective modality input, rather than relying on a\nsingle end-to-end model. To demonstrate this, a dexterous robotic hand performs\na manipulation task involving picking up and rotating a box. Guided by insights\nfrom neuroscience, the task is decomposed into three sub-skills, 1)reaching,\n2)grasping and lifting, and 3)in-hand rotation, based on the dominant sensory\nmodalities employed in the human brain. Each sub-skill is addressed using\ndistinct methods from a practical perspective: a classical controller, a\nVision-Language-Action model, and a reinforcement learning policy with force\nfeedback, respectively. We tested the pipeline on a real robot to demonstrate\nthe feasibility of our approach. The key contribution of this study lies in\npresenting a neuroscience-inspired, modality-driven methodology for multi-step\ndexterous manipulation.\n","authors":["Naoki Wake","Atsushi Kanehira","Daichi Saito","Jun Takamatsu","Kazuhiro Sasabuchi","Hideki Koike","Katsushi Ikeuchi"],"pdf_url":"https://arxiv.org/pdf/2412.11337v1.pdf","comment":"8 pages, 5 figures, 2 tables. Last updated on December 14th, 2024"},{"id":"http://arxiv.org/abs/2412.07196v2","updated":"2024-12-15T22:56:40Z","published":"2024-12-10T05:09:52Z","title":"Fine-grained Text to Image Synthesis","summary":"  Fine-grained text to image synthesis involves generating images from texts\nthat belong to different categories. In contrast to general text to image\nsynthesis, in fine-grained synthesis there is high similarity between images of\ndifferent subclasses, and there may be linguistic discrepancy among texts\ndescribing the same image. Recent Generative Adversarial Networks (GAN), such\nas the Recurrent Affine Transformation (RAT) GAN model, are able to synthesize\nclear and realistic images from texts. However, GAN models ignore fine-grained\nlevel information. In this paper we propose an approach that incorporates an\nauxiliary classifier in the discriminator and a contrastive learning method to\nimprove the accuracy of fine-grained details in images synthesized by RAT GAN.\nThe auxiliary classifier helps the discriminator classify the class of images,\nand helps the generator synthesize more accurate fine-grained images. The\ncontrastive learning method minimizes the similarity between images from\ndifferent subclasses and maximizes the similarity between images from the same\nsubclass. We evaluate on several state-of-the-art methods on the commonly used\nCUB-200-2011 bird dataset and Oxford-102 flower dataset, and demonstrated\nsuperior performance.\n","authors":["Xu Ouyang","Ying Chen","Kaiyue Zhu","Gady Agam"],"pdf_url":"https://arxiv.org/pdf/2412.07196v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11325v1","updated":"2024-12-15T22:04:26Z","published":"2024-12-15T22:04:26Z","title":"Sonicmesh: Enhancing 3D Human Mesh Reconstruction in Vision-Impaired\n  Environments With Acoustic Signals","summary":"  3D Human Mesh Reconstruction (HMR) from 2D RGB images faces challenges in\nenvironments with poor lighting, privacy concerns, or occlusions. These\nweaknesses of RGB imaging can be complemented by acoustic signals, which are\nwidely available, easy to deploy, and capable of penetrating obstacles.\nHowever, no existing methods effectively combine acoustic signals with RGB data\nfor robust 3D HMR. The primary challenges include the low-resolution images\ngenerated by acoustic signals and the lack of dedicated processing backbones.\nWe introduce SonicMesh, a novel approach combining acoustic signals with RGB\nimages to reconstruct 3D human mesh. To address the challenges of low\nresolution and the absence of dedicated processing backbones in images\ngenerated by acoustic signals, we modify an existing method, HRNet, for\neffective feature extraction. We also integrate a universal feature embedding\ntechnique to enhance the precision of cross-dimensional feature alignment,\nenabling SonicMesh to achieve high accuracy. Experimental results demonstrate\nthat SonicMesh accurately reconstructs 3D human mesh in challenging\nenvironments such as occlusions, non-line-of-sight scenarios, and poor\nlighting.\n","authors":["Xiaoxuan Liang","Wuyang Zhang","Hong Zhou","Zhaolong Wei","Sicheng Zhu","Yansong Li","Rui Yin","Jiantao Yuan","Jeremy Gummeson"],"pdf_url":"https://arxiv.org/pdf/2412.11325v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11306v1","updated":"2024-12-15T20:49:46Z","published":"2024-12-15T20:49:46Z","title":"Unimodal and Multimodal Static Facial Expression Recognition for Virtual\n  Reality Users with EmoHeVRDB","summary":"  In this study, we explored the potential of utilizing Facial Expression\nActivations (FEAs) captured via the Meta Quest Pro Virtual Reality (VR) headset\nfor Facial Expression Recognition (FER) in VR settings. Leveraging the\nEmojiHeroVR Database (EmoHeVRDB), we compared several unimodal approaches and\nachieved up to 73.02% accuracy for the static FER task with seven emotion\ncategories. Furthermore, we integrated FEA and image data in multimodal\napproaches, observing significant improvements in recognition accuracy. An\nintermediate fusion approach achieved the highest accuracy of 80.42%,\nsignificantly surpassing the baseline evaluation result of 69.84% reported for\nEmoHeVRDB's image data. Our study is the first to utilize EmoHeVRDB's unique\nFEA data for unimodal and multimodal static FER, establishing new benchmarks\nfor FER in VR settings. Our findings highlight the potential of fusing\ncomplementary modalities to enhance FER accuracy in VR settings, where\nconventional image-based methods are severely limited by the occlusion caused\nby Head-Mounted Displays (HMDs).\n","authors":["Thorben Ortmann","Qi Wang","Larissa Putzar"],"pdf_url":"https://arxiv.org/pdf/2412.11306v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13181v2","updated":"2024-12-15T20:35:23Z","published":"2024-06-19T03:25:31Z","title":"The Impact of Auxiliary Patient Data on Automated Chest X-Ray Report\n  Generation and How to Incorporate It","summary":"  This study investigates the integration of diverse patient data sources into\nmultimodal language models for automated chest X-ray (CXR) report generation.\nTraditionally, CXR report generation relies solely on CXR images and limited\nradiology data, overlooking valuable information from patient health records,\nparticularly from emergency departments. Utilising the MIMIC-CXR and\nMIMIC-IV-ED datasets, we incorporate detailed patient information such as vital\nsigns, medicines, and clinical history to enhance diagnostic accuracy. We\nintroduce a novel approach to transform these heterogeneous data sources into\nembeddings that prompt a multimodal language model; this significantly enhances\nthe diagnostic accuracy of generated radiology reports. Our comprehensive\nevaluation demonstrates the benefits of using a broader set of patient data,\nunderscoring the potential for enhanced diagnostic capabilities and better\npatient outcomes through the integration of multimodal data in CXR report\ngeneration.\n","authors":["Aaron Nicolson","Shengyao Zhuang","Jason Dowling","Bevan Koopman"],"pdf_url":"https://arxiv.org/pdf/2406.13181v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05050v4","updated":"2024-12-15T20:29:34Z","published":"2024-03-08T04:53:53Z","title":"DyRoNet: Dynamic Routing and Low-Rank Adapters for Autonomous Driving\n  Streaming Perception","summary":"  The advancement of autonomous driving systems hinges on the ability to\nachieve low-latency and high-accuracy perception. To address this critical\nneed, this paper introduces Dynamic Routing Network (DyRoNet), a low-rank\nenhanced dynamic routing framework designed for streaming perception in\nautonomous driving systems. DyRoNet integrates a suite of pre-trained branch\nnetworks, each meticulously fine-tuned to function under distinct environmental\nconditions. At its core, the framework offers a speed router module, developed\nto assess and route input data to the most suitable branch for processing. This\napproach not only addresses the inherent limitations of conventional models in\nadapting to diverse driving conditions but also ensures the balance between\nperformance and efficiency. Extensive experimental evaluations demonstrate the\nadaptability of DyRoNet to diverse branch selection strategies, resulting in\nsignificant performance enhancements across different scenarios. This work\nestablishes a new benchmark for streaming perception and provides valuable\nengineering insights for future work.\n","authors":["Xiang Huang","Zhi-Qi Cheng","Jun-Yan He","Chenyang Li","Wangmeng Xiang","Baigui Sun"],"pdf_url":"https://arxiv.org/pdf/2403.05050v4.pdf","comment":"Accepted to WACV 2025. 17 pages, 8 figures. Project:\n  https://tastevision.github.io/DyRoNet/"},{"id":"http://arxiv.org/abs/2410.09583v3","updated":"2024-12-15T20:05:55Z","published":"2024-10-12T16:28:40Z","title":"POPoS: Improving Efficient and Robust Facial Landmark Detection with\n  Parallel Optimal Position Search","summary":"  Achieving a balance between accuracy and efficiency is a critical challenge\nin facial landmark detection (FLD). This paper introduces Parallel Optimal\nPosition Search (POPoS), a high-precision encoding-decoding framework designed\nto address the limitations of traditional FLD methods. POPoS employs three key\ncontributions: (1) Pseudo-range multilateration is utilized to correct heatmap\nerrors, improving landmark localization accuracy. By integrating multiple\nanchor points, it reduces the impact of individual heatmap inaccuracies,\nleading to robust overall positioning. (2) To enhance the pseudo-range accuracy\nof selected anchor points, a new loss function, named multilateration anchor\nloss, is proposed. This loss function enhances the accuracy of the distance\nmap, mitigates the risk of local optima, and ensures optimal solutions. (3) A\nsingle-step parallel computation algorithm is introduced, boosting\ncomputational efficiency and reducing processing time. Extensive evaluations\nacross five benchmark datasets demonstrate that POPoS consistently outperforms\nexisting methods, particularly excelling in low-resolution heatmaps scenarios\nwith minimal computational overhead. These advantages make POPoS a highly\nefficient and accurate tool for FLD, with broad applicability in real-world\nscenarios.\n","authors":["Chong-Yang Xiang","Jun-Yan He","Zhi-Qi Cheng","Xiao Wu","Xian-Sheng Hua"],"pdf_url":"https://arxiv.org/pdf/2410.09583v3.pdf","comment":"Accepted to AAAI 2025, 9 pages, 6 figures. Code:\n  https://github.com/teslatasy/POPoS"},{"id":"http://arxiv.org/abs/2411.01455v2","updated":"2024-12-15T19:39:55Z","published":"2024-11-03T06:33:37Z","title":"HiMemFormer: Hierarchical Memory-Aware Transformer for Multi-Agent\n  Action Anticipation","summary":"  Understanding and predicting human actions has been a long-standing challenge\nand is a crucial measure of perception in robotics AI. While significant\nprogress has been made in anticipating the future actions of individual agents,\nprior work has largely overlooked a key aspect of real-world human activity --\ninteractions. To address this gap in human-like forecasting within multi-agent\nenvironments, we present the Hierarchical Memory-Aware Transformer\n(HiMemFormer), a transformer-based model for online multi-agent action\nanticipation. HiMemFormer integrates and distributes global memory that\ncaptures joint historical information across all agents through a transformer\nframework, with a hierarchical local memory decoder that interprets\nagent-specific features based on these global representations using a\ncoarse-to-fine strategy. In contrast to previous approaches, HiMemFormer\nuniquely hierarchically applies the global context with agent-specific\npreferences to avoid noisy or redundant information in multi-agent action\nanticipation. Extensive experiments on various multi-agent scenarios\ndemonstrate the significant performance of HiMemFormer, compared with other\nstate-of-the-art methods.\n","authors":["Zirui Wang","Xinran Zhao","Simon Stepputtis","Woojun Kim","Tongshuang Wu","Katia Sycara","Yaqi Xie"],"pdf_url":"https://arxiv.org/pdf/2411.01455v2.pdf","comment":"Workshop on Video-Language Models at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2412.11286v1","updated":"2024-12-15T19:19:39Z","published":"2024-12-15T19:19:39Z","title":"Detecting Daily Living Gait Amid Huntington's Disease Chorea using a\n  Foundation Deep Learning Model","summary":"  Wearable sensors offer a non-invasive way to collect physical activity (PA)\ndata, with walking as a key component. Existing models often struggle to detect\ngait bouts in individuals with neurodegenerative diseases (NDDs) involving\ninvoluntary movements. We developed J-Net, a deep learning model inspired by\nU-Net, which uses a pre-trained self-supervised foundation model fine-tuned\nwith Huntington`s disease (HD) in-lab data and paired with a segmentation head\nfor gait detection. J-Net processes wrist-worn accelerometer data to detect\ngait during daily living. We evaluated J-Net on in-lab and daily-living data\nfrom HD, Parkinson`s disease (PD), and controls. J-Net achieved a 10-percentage\npoint improvement in ROC-AUC for HD over existing methods, reaching 0.97 for\nin-lab data. In daily-living environments, J-Net estimates showed no\nsignificant differences in median daily walking time between HD and controls (p\n= 0.23), in contrast to other models, which indicated counterintuitive results\n(p < 0.005). Walking time measured by J-Net correlated with the UHDRS-TMS\nclinical severity score (r=-0.52; p=0.02), confirming its clinical relevance.\nFine-tuning J-Net on PD data also improved gait detection over current methods.\nJ-Net`s architecture effectively addresses the challenges of gait detection in\nsevere chorea and offers robust performance in daily living. The dataset and\nJ-Net model are publicly available, providing a resource for further research\ninto NDD-related gait impairments.\n","authors":["Dafna Schwartz","Lori Quinn","Nora E. Fritz","Lisa M. Muratori","Jeffery M. Hausdorff","Ran Gilad Bachrach"],"pdf_url":"https://arxiv.org/pdf/2412.11286v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11284v1","updated":"2024-12-15T19:09:45Z","published":"2024-12-15T19:09:45Z","title":"Learning Normal Flow Directly From Event Neighborhoods","summary":"  Event-based motion field estimation is an important task. However, current\noptical flow methods face challenges: learning-based approaches, often\nframe-based and relying on CNNs, lack cross-domain transferability, while\nmodel-based methods, though more robust, are less accurate. To address the\nlimitations of optical flow estimation, recent works have focused on normal\nflow, which can be more reliably measured in regions with limited texture or\nstrong edges. However, existing normal flow estimators are predominantly\nmodel-based and suffer from high errors.\n  In this paper, we propose a novel supervised point-based method for normal\nflow estimation that overcomes the limitations of existing event learning-based\napproaches. Using a local point cloud encoder, our method directly estimates\nper-event normal flow from raw events, offering multiple unique advantages: 1)\nIt produces temporally and spatially sharp predictions. 2) It supports more\ndiverse data augmentation, such as random rotation, to improve robustness\nacross various domains. 3) It naturally supports uncertainty quantification via\nensemble inference, which benefits downstream tasks. 4) It enables training and\ninference on undistorted data in normalized camera coordinates, improving\ntransferability across cameras. Extensive experiments demonstrate our method\nachieves better and more consistent performance than state-of-the-art methods\nwhen transferred across different datasets. Leveraging this transferability, we\ntrain our model on the union of datasets and release it for public use.\nFinally, we introduce an egomotion solver based on a maximum-margin problem\nthat uses normal flow and IMU to achieve strong performance in challenging\nscenarios.\n","authors":["Dehao Yuan","Levi Burner","Jiayi Wu","Minghui Liu","Jingxi Chen","Yiannis Aloimonos","Cornelia Fermüller"],"pdf_url":"https://arxiv.org/pdf/2412.11284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11279v1","updated":"2024-12-15T18:58:32Z","published":"2024-12-15T18:58:32Z","title":"VividFace: A Diffusion-Based Hybrid Framework for High-Fidelity Video\n  Face Swapping","summary":"  Video face swapping is becoming increasingly popular across various\napplications, yet existing methods primarily focus on static images and\nstruggle with video face swapping because of temporal consistency and complex\nscenarios. In this paper, we present the first diffusion-based framework\nspecifically designed for video face swapping. Our approach introduces a novel\nimage-video hybrid training framework that leverages both abundant static image\ndata and temporal video sequences, addressing the inherent limitations of\nvideo-only training. The framework incorporates a specially designed diffusion\nmodel coupled with a VidFaceVAE that effectively processes both types of data\nto better maintain temporal coherence of the generated videos. To further\ndisentangle identity and pose features, we construct the Attribute-Identity\nDisentanglement Triplet (AIDT) Dataset, where each triplet has three face\nimages, with two images sharing the same pose and two sharing the same\nidentity. Enhanced with a comprehensive occlusion augmentation, this dataset\nalso improves robustness against occlusions. Additionally, we integrate 3D\nreconstruction techniques as input conditioning to our network for handling\nlarge pose variations. Extensive experiments demonstrate that our framework\nachieves superior performance in identity preservation, temporal consistency,\nand visual quality compared to existing methods, while requiring fewer\ninference steps. Our approach effectively mitigates key challenges in video\nface swapping, including temporal flickering, identity preservation, and\nrobustness to occlusions and pose variations.\n","authors":["Hao Shao","Shulun Wang","Yang Zhou","Guanglu Song","Dailan He","Shuo Qin","Zhuofan Zong","Bingqi Ma","Yu Liu","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2412.11279v1.pdf","comment":"project page: https://hao-shao.com/projects/vividface.html"},{"id":"http://arxiv.org/abs/2412.11277v1","updated":"2024-12-15T18:49:20Z","published":"2024-12-15T18:49:20Z","title":"Macro2Micro: Cross-modal Magnetic Resonance Imaging Synthesis Leveraging\n  Multi-scale Brain Structures","summary":"  Spanning multiple scales-from macroscopic anatomy down to intricate\nmicroscopic architecture-the human brain exemplifies a complex system that\ndemands integrated approaches to fully understand its complexity. Yet, mapping\nnonlinear relationships between these scales remains challenging due to\ntechnical limitations and the high cost of multimodal Magnetic Resonance\nImaging (MRI) acquisition. Here, we introduce Macro2Micro, a deep learning\nframework that predicts brain microstructure from macrostructure using a\nGenerative Adversarial Network (GAN). Grounded in the scale-free, self-similar\nnature of brain organization-where microscale information can be inferred from\nmacroscale patterns-Macro2Micro explicitly encodes multiscale brain\nrepresentations into distinct processing branches. To further enhance image\nfidelity and suppress artifacts, we propose a simple yet effective auxiliary\ndiscriminator and learning objective. Our results show that Macro2Micro\nfaithfully translates T1-weighted MRIs into corresponding Fractional Anisotropy\n(FA) images, achieving a 6.8% improvement in the Structural Similarity Index\nMeasure (SSIM) compared to previous methods, while preserving the individual\nneurobiological characteristics.\n","authors":["Sooyoung Kim","Joonwoo Kwon","Junbeom Kwon","Sangyoon Bae","Yuewei Lin","Shinjae Yoo","Jiook Cha"],"pdf_url":"https://arxiv.org/pdf/2412.11277v1.pdf","comment":"The code will be made available upon acceptance"},{"id":"http://arxiv.org/abs/2412.11258v1","updated":"2024-12-15T17:44:10Z","published":"2024-12-15T17:44:10Z","title":"GaussianProperty: Integrating Physical Properties to 3D Gaussians with\n  LMMs","summary":"  Estimating physical properties for visual data is a crucial task in computer\nvision, graphics, and robotics, underpinning applications such as augmented\nreality, physical simulation, and robotic grasping. However, this area remains\nunder-explored due to the inherent ambiguities in physical property estimation.\nTo address these challenges, we introduce GaussianProperty, a training-free\nframework that assigns physical properties of materials to 3D Gaussians.\nSpecifically, we integrate the segmentation capability of SAM with the\nrecognition capability of GPT-4V(ision) to formulate a global-local physical\nproperty reasoning module for 2D images. Then we project the physical\nproperties from multi-view 2D images to 3D Gaussians using a voting strategy.\nWe demonstrate that 3D Gaussians with physical property annotations enable\napplications in physics-based dynamic simulation and robotic grasping. For\nphysics-based dynamic simulation, we leverage the Material Point Method (MPM)\nfor realistic dynamic simulation. For robot grasping, we develop a grasping\nforce prediction strategy that estimates a safe force range required for object\ngrasping based on the estimated physical properties. Extensive experiments on\nmaterial segmentation, physics-based dynamic simulation, and robotic grasping\nvalidate the effectiveness of our proposed method, highlighting its crucial\nrole in understanding physical properties from visual data. Online demo, code,\nmore cases and annotated datasets are available on\n\\href{https://Gaussian-Property.github.io}{this https URL}.\n","authors":["Xinli Xu","Wenhang Ge","Dicong Qiu","ZhiFei Chen","Dongyu Yan","Zhuoyun Liu","Haoyu Zhao","Hanfeng Zhao","Shunsi Zhang","Junwei Liang","Ying-Cong Chen"],"pdf_url":"https://arxiv.org/pdf/2412.11258v1.pdf","comment":"17 pages, 17 figures"},{"id":"http://arxiv.org/abs/2412.11248v1","updated":"2024-12-15T16:54:53Z","published":"2024-12-15T16:54:53Z","title":"Multimodal Class-aware Semantic Enhancement Network for Audio-Visual\n  Video Parsing","summary":"  The Audio-Visual Video Parsing task aims to recognize and temporally localize\nall events occurring in either the audio or visual stream, or both. Capturing\naccurate event semantics for each audio/visual segment is vital. Prior works\ndirectly utilize the extracted holistic audio and visual features for intra-\nand cross-modal temporal interactions. However, each segment may contain\nmultiple events, resulting in semantically mixed holistic features that can\nlead to semantic interference during intra- or cross-modal interactions: the\nevent semantics of one segment may incorporate semantics of unrelated events\nfrom other segments. To address this issue, our method begins with a\nClass-Aware Feature Decoupling (CAFD) module, which explicitly decouples the\nsemantically mixed features into distinct class-wise features, including\nmultiple event-specific features and a dedicated background feature. The\ndecoupled class-wise features enable our model to selectively aggregate useful\nsemantics for each segment from clearly matched classes contained in other\nsegments, preventing semantic interference from irrelevant classes.\nSpecifically, we further design a Fine-Grained Semantic Enhancement module for\nencoding intra- and cross-modal relations. It comprises a Segment-wise Event\nCo-occurrence Modeling (SECM) block and a Local-Global Semantic Fusion (LGSF)\nblock. The SECM exploits inter-class dependencies of concurrent events within\nthe same timestamp with the aid of a new event co-occurrence loss. The LGSF\nfurther enhances the event semantics of each segment by incorporating relevant\nsemantics from more informative global video features. Extensive experiments\nvalidate the effectiveness of the proposed modules and loss functions,\nresulting in a new state-of-the-art parsing performance.\n","authors":["Pengcheng Zhao","Jinxing Zhou","Dan Guo","Yang Zhao","Yanxiang Chen"],"pdf_url":"https://arxiv.org/pdf/2412.11248v1.pdf","comment":"Accepted by AAAI-2025"},{"id":"http://arxiv.org/abs/2412.06720v3","updated":"2024-12-15T16:48:59Z","published":"2024-12-09T18:06:39Z","title":"VP-MEL: Visual Prompts Guided Multimodal Entity Linking","summary":"  Multimodal entity linking (MEL), a task aimed at linking mentions within\nmultimodal contexts to their corresponding entities in a knowledge base (KB),\nhas attracted much attention due to its wide applications in recent years.\nHowever, existing MEL methods often rely heavily on mention words as retrieval\ncues, which limits their ability to effectively utilize information from both\nimages and text. This reliance poses significant challenges in scenarios where\nmention words are absent, as current MEL approaches struggle to leverage\nimage-text pairs for accurate entity linking. To solve these issues, we\nintroduce a Visual Prompts guided Multimodal Entity Linking (VP-MEL) task.\nGiven a text-image pair, VP-MEL aims to link a marked region (i.e., visual\nprompt) in an image to its corresponding entities in the knowledge base. To\nfacilitate this task, we present a new dataset, VPWiki, specifically designed\nfor VP-MEL. Furthermore, we propose a framework named FBMEL, which enhances\nvisual feature extraction using visual prompts and leverages the pretrained\nDetective-VLM model to capture latent information. Experimental results on the\nVPWiki dataset demonstrate that FBMEL outperforms baseline methods across\nmultiple benchmarks for the VP-MEL task.\n","authors":["Hongze Mi","Jinyuan Li","Xuying Zhang","Haoran Cheng","Jiahao Wang","Di Sun","Gang Pan"],"pdf_url":"https://arxiv.org/pdf/2412.06720v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11241v1","updated":"2024-12-15T16:46:23Z","published":"2024-12-15T16:46:23Z","title":"Volumetric Mapping with Panoptic Refinement via Kernel Density\n  Estimation for Mobile Robots","summary":"  Reconstructing three-dimensional (3D) scenes with semantic understanding is\nvital in many robotic applications. Robots need to identify which objects,\nalong with their positions and shapes, to manipulate them precisely with given\ntasks. Mobile robots, especially, usually use lightweight networks to segment\nobjects on RGB images and then localize them via depth maps; however, they\noften encounter out-of-distribution scenarios where masks over-cover the\nobjects. In this paper, we address the problem of panoptic segmentation quality\nin 3D scene reconstruction by refining segmentation errors using non-parametric\nstatistical methods. To enhance mask precision, we map the predicted masks into\na depth frame to estimate their distribution via kernel densities. The outliers\nin depth perception are then rejected without the need for additional\nparameters in an adaptive manner to out-of-distribution scenarios, followed by\n3D reconstruction using projective signed distance functions (SDFs). We\nvalidate our method on a synthetic dataset, which shows improvements in both\nquantitative and qualitative results for panoptic mapping. Through real-world\ntesting, the results furthermore show our method's capability to be deployed on\na real-robot system. Our source code is available at:\nhttps://github.com/mkhangg/refined panoptic mapping.\n","authors":["Khang Nguyen","Tuan Dang","Manfred Huber"],"pdf_url":"https://arxiv.org/pdf/2412.11241v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11237v1","updated":"2024-12-15T16:25:30Z","published":"2024-12-15T16:25:30Z","title":"On the Generalizability of Iterative Patch Selection for\n  Memory-Efficient High-Resolution Image Classification","summary":"  Classifying large images with small or tiny regions of interest (ROI) is\nchallenging due to computational and memory constraints. Weakly supervised\nmemory-efficient patch selectors have achieved results comparable with strongly\nsupervised methods. However, low signal-to-noise ratios and low entropy\nattention still cause overfitting. We explore these issues using a novel\ntestbed on a memory-efficient cross-attention transformer with Iterative Patch\nSelection (IPS) as the patch selection module. Our testbed extends the\nmegapixel MNIST benchmark to four smaller O2I (object-to-image) ratios ranging\nfrom 0.01% to 0.14% while keeping the canvas size fixed and introducing a noise\ngeneration component based on B\\'ezier curves. Experimental results generalize\nthe observations made on CNNs to IPS whereby the O2I threshold below which the\nclassifier fails to generalize is affected by the training dataset size. We\nfurther observe that the magnitude of this interaction differs for each task of\nthe Megapixel MNIST. For tasks \"Maj\" and \"Top\", the rate is at its highest,\nfollowed by tasks \"Max\" and \"Multi\" where in the latter, this rate is almost at\n0. Moreover, results show that in a low data setting, tuning the patch size to\nbe smaller relative to the ROI improves generalization, resulting in an\nimprovement of + 15% for the megapixel MNIST and + 5% for the Swedish traffic\nsigns dataset compared to the original object-to-patch ratios in IPS. Further\noutcomes indicate that the similarity between the thickness of the noise\ncomponent and the digits in the megapixel MNIST gradually causes IPS to fail to\ngeneralize, contributing to previous suspicions.\n","authors":["Max Riffi-Aslett","Christina Fell"],"pdf_url":"https://arxiv.org/pdf/2412.11237v1.pdf","comment":"15 pages, submitted to Springer Nature, International Journal of\n  Computer Vision"},{"id":"http://arxiv.org/abs/2404.19227v6","updated":"2024-12-15T16:20:37Z","published":"2024-04-30T03:13:06Z","title":"Espresso: Robust Concept Filtering in Text-to-Image Models","summary":"  Diffusion based text-to-image models are trained on large datasets scraped\nfrom the Internet, potentially containing unacceptable concepts (e.g.,\ncopyright-infringing or unsafe). We need concept removal techniques (CRTs)\nwhich are i) effective in preventing the generation of images with unacceptable\nconcepts, ii) utility-preserving on acceptable concepts, and, iii) robust\nagainst evasion with adversarial prompts. No prior CRT satisfies all these\nrequirements simultaneously. We introduce Espresso, the first robust concept\nfilter based on Contrastive Language-Image Pre-Training (CLIP). We identify\nunacceptable concepts by using the distance between the embedding of a\ngenerated image to the text embeddings of both unacceptable and acceptable\nconcepts. This lets us fine-tune for robustness by separating the text\nembeddings of unacceptable and acceptable concepts while preserving utility. We\npresent a pipeline to evaluate various CRTs to show that Espresso is more\neffective and robust than prior CRTs, while retaining utility.\n","authors":["Anudeep Das","Vasisht Duddu","Rui Zhang","N. Asokan"],"pdf_url":"https://arxiv.org/pdf/2404.19227v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09224v2","updated":"2024-12-15T16:17:04Z","published":"2024-12-12T12:26:08Z","title":"DASK: Distribution Rehearsing via Adaptive Style Kernel Learning for\n  Exemplar-Free Lifelong Person Re-Identification","summary":"  Lifelong person re-identification (LReID) is an important but challenging\ntask that suffers from catastrophic forgetting due to significant domain gaps\nbetween training steps. Existing LReID approaches typically rely on data replay\nand knowledge distillation to mitigate this issue. However, data replay methods\ncompromise data privacy by storing historical exemplars, while knowledge\ndistillation methods suffer from limited performance due to the cumulative\nforgetting of undistilled knowledge. To overcome these challenges, we propose a\nnovel paradigm that models and rehearses the distribution of the old domains to\nenhance knowledge consolidation during the new data learning, possessing a\nstrong anti-forgetting capacity without storing any exemplars. Specifically, we\nintroduce an exemplar-free LReID method called Distribution Rehearsing via\nAdaptive Style Kernel Learning (DASK). DASK includes a Distribution Rehearser\nLearning (DRL) mechanism that learns to transform arbitrary distribution data\ninto the current data style at each learning step. To enhance the style\ntransfer capacity of DRL, an Adaptive Kernel Prediction Network (AKPNet) is\nexplored to achieve an instance-specific distribution adjustment. Additionally,\nwe design a Distribution Rehearsing-driven LReID Training (DRRT) module, which\nrehearses old distribution based on the new data via the old AKPNet model,\nachieving effective new-old knowledge accumulation under a joint knowledge\nconsolidation scheme. Experimental results show our DASK outperforms the\nexisting methods by 3.6%-6.8% and 4.5%-6.5% on anti-forgetting and\ngeneralization capacity, respectively. Our code is available at\nhttps://github.com/zhoujiahuan1991/AAAI2025-DASK\n","authors":["Kunlun Xu","Chenghao Jiang","Peixi Xiong","Yuxin Peng","Jiahuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.09224v2.pdf","comment":"in Proceedings of the 39th AAAI Conference on Artificial Intelligence\n  (AAAI-25)"},{"id":"http://arxiv.org/abs/2409.05274v2","updated":"2024-12-15T16:12:41Z","published":"2024-09-09T01:50:01Z","title":"Rethinking the Atmospheric Scattering-driven Attention via Channel and\n  Gamma Correction Priors for Low-Light Image Enhancement","summary":"  Enhancing low-light images remains a critical challenge in computer vision,\nas does designing lightweight models for edge devices that can handle the\ncomputational demands of deep learning. In this article, we introduce an\nextended version of the Channel-Prior and Gamma-Estimation Network (CPGA-Net),\ntermed CPGA-Net+, which incorporates an attention mechanism driven by a\nreformulated Atmospheric Scattering Model and effectively addresses both global\nand local image processing through Plug-in Attention with gamma correction.\nThese innovations enable CPGA-Net+ to achieve superior performance on image\nenhancement tasks for supervised and unsupervised learning, surpassing\nlightweight state-of-the-art methods with high efficiency. Furthermore, we\nprovide a theoretical analysis showing that our approach inherently decomposes\nthe enhancement process into restoration and lightening stages, aligning with\nthe fundamental image degradation model. To further optimize efficiency, we\nintroduce a block simplification technique that reduces computational costs by\nmore than two-thirds. Experimental results validate the effectiveness of\nCPGA-Net+ and highlight its potential for applications in resource-constrained\nenvironments.\n","authors":["Shyang-En Weng","Cheng-Yen Hsiao","Shaou-Gang Miaou","Ricky Christanto"],"pdf_url":"https://arxiv.org/pdf/2409.05274v2.pdf","comment":"11 pages, 10 figures"},{"id":"http://arxiv.org/abs/2409.13094v2","updated":"2024-12-15T16:11:59Z","published":"2024-09-19T21:32:07Z","title":"DenoMamba: A fused state-space model for low-dose CT denoising","summary":"  Low-dose computed tomography (LDCT) lower potential risks linked to radiation\nexposure while relying on advanced denoising algorithms to maintain diagnostic\nquality in reconstructed images. The reigning paradigm in LDCT denoising is\nbased on neural network models that learn data-driven image priors to separate\nnoise evoked by dose reduction from underlying tissue signals. Naturally, the\nfidelity of these priors depend on the model's ability to capture the broad\nrange of contextual features evident in CT images. Earlier convolutional neural\nnetworks (CNN) are highly adept at efficiently capturing short-range spatial\ncontext, but their limited receptive fields reduce sensitivity to interactions\nover longer distances. Although transformers based on self-attention mechanisms\nhave recently been posed to increase sensitivity to long-range context, they\ncan suffer from suboptimal performance and efficiency due to elevated model\ncomplexity, particularly for high-resolution CT images. For high-quality\nrestoration of LDCT images, here we introduce DenoMamba, a novel denoising\nmethod based on state-space modeling (SSM), that efficiently captures short-\nand long-range context in medical images. Following an hourglass architecture\nwith encoder-decoder stages, DenoMamba employs a spatial SSM module to encode\nspatial context and a novel channel SSM module equipped with a secondary gated\nconvolution network to encode latent features of channel context at each stage.\nFeature maps from the two modules are then consolidated with low-level input\nfeatures via a convolution fusion module (CFM). Comprehensive experiments on\nLDCT datasets with 25\\% and 10\\% dose reduction demonstrate that DenoMamba\noutperforms state-of-the-art denoisers with average improvements of 1.4dB PSNR,\n1.1% SSIM, and 1.6% RMSE in recovered image quality.\n","authors":["Şaban Öztürk","Oğuz Can Duran","Tolga Çukur"],"pdf_url":"https://arxiv.org/pdf/2409.13094v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11228v1","updated":"2024-12-15T15:51:44Z","published":"2024-12-15T15:51:44Z","title":"Uni-AdaFocus: Spatial-temporal Dynamic Computation for Video Recognition","summary":"  This paper presents a comprehensive exploration of the phenomenon of data\nredundancy in video understanding, with the aim to improve computational\nefficiency. Our investigation commences with an examination of spatial\nredundancy, which refers to the observation that the most informative region in\neach video frame usually corresponds to a small image patch, whose shape, size\nand location shift smoothly across frames. Motivated by this phenomenon, we\nformulate the patch localization problem as a dynamic decision task, and\nintroduce a spatially adaptive video recognition approach, termed AdaFocus. In\nspecific, a lightweight encoder is first employed to quickly process the full\nvideo sequence, whose features are then utilized by a policy network to\nidentify the most task-relevant regions. Subsequently, the selected patches are\ninferred by a high-capacity deep network for the final prediction. The full\nmodel can be trained in end-to-end conveniently. Furthermore, AdaFocus can be\nextended by further considering temporal and sample-wise redundancies, i.e.,\nallocating the majority of computation to the most task-relevant frames, and\nminimizing the computation spent on relatively \"easier\" videos. Our resulting\napproach, Uni-AdaFocus, establishes a comprehensive framework that seamlessly\nintegrates spatial, temporal, and sample-wise dynamic computation, while it\npreserves the merits of AdaFocus in terms of efficient end-to-end training and\nhardware friendliness. In addition, Uni-AdaFocus is general and flexible as it\nis compatible with off-the-shelf efficient backbones (e.g., TSM and X3D), which\ncan be readily deployed as our feature extractor, yielding a significantly\nimproved computational efficiency. Empirically, extensive experiments based on\nseven benchmark datasets and three application scenarios substantiate that\nUni-AdaFocus is considerably more efficient than the competitive baselines.\n","authors":["Yulin Wang","Haoji Zhang","Yang Yue","Shiji Song","Chao Deng","Junlan Feng","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2412.11228v1.pdf","comment":"Accepted by IEEE TPAMI. Journal version of arXiv:2105.03245\n  (AdaFocusV1, ICCV 2021 Oral), arXiv:2112.14238 (AdaFocusV2, CVPR 2022), and\n  arXiv:2209.13465 (AdaFocusV3, ECCV 2022). Code and pre-trained models:\n  https://github.com/LeapLabTHU/Uni-AdaFocus"},{"id":"http://arxiv.org/abs/2412.11224v1","updated":"2024-12-15T15:40:40Z","published":"2024-12-15T15:40:40Z","title":"GenLit: Reformulating Single-Image Relighting as Video Generation","summary":"  Manipulating the illumination within a single image represents a fundamental\nchallenge in computer vision and graphics. This problem has been traditionally\naddressed using inverse rendering techniques, which require explicit 3D asset\nreconstruction and costly ray tracing simulations. Meanwhile, recent\nadvancements in visual foundation models suggest that a new paradigm could soon\nbe practical and possible -- one that replaces explicit physical models with\nnetworks that are trained on massive amounts of image and video data. In this\npaper, we explore the potential of exploiting video diffusion models, and in\nparticular Stable Video Diffusion (SVD), in understanding the physical world to\nperform relighting tasks given a single image. Specifically, we introduce\nGenLit, a framework that distills the ability of a graphics engine to perform\nlight manipulation into a video generation model, enabling users to directly\ninsert and manipulate a point light in the 3D world within a given image and\ngenerate the results directly as a video sequence. We find that a model\nfine-tuned on only a small synthetic dataset (270 objects) is able to\ngeneralize to real images, enabling single-image relighting with realistic ray\ntracing effects and cast shadows. These results reveal the ability of video\nfoundation models to capture rich information about lighting, material, and\nshape. Our findings suggest that such models, with minimal training, can be\nused for physically-based rendering without explicit physically asset\nreconstruction and complex ray tracing. This further suggests the potential of\nsuch models for controllable and physically accurate image synthesis tasks.\n","authors":["Shrisha Bharadwaj","Haiwen Feng","Victoria Abrevaya","Michael J. Black"],"pdf_url":"https://arxiv.org/pdf/2412.11224v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03677v5","updated":"2024-12-15T15:37:29Z","published":"2024-08-07T10:36:26Z","title":"L4DR: LiDAR-4DRadar Fusion for Weather-Robust 3D Object Detection","summary":"  LiDAR-based vision systems are integral for 3D object detection, which is\ncrucial for autonomous navigation. However, they suffer from performance\ndegradation in adverse weather conditions due to the quality deterioration of\nLiDAR point clouds. Fusing LiDAR with the weather-robust 4D radar sensor is\nexpected to solve this problem. However, the fusion of LiDAR and 4D radar is\nchallenging because they differ significantly in terms of data quality and the\ndegree of degradation in adverse weather. To address these issues, we introduce\nL4DR, a weather-robust 3D object detection method that effectively achieves\nLiDAR and 4D Radar fusion. Our L4DR includes Multi-Modal Encoding (MME) and\nForeground-Aware Denoising (FAD) technique to reconcile sensor gaps, which is\nthe first exploration of the complementarity of early fusion between LiDAR and\n4D radar. Additionally, we design an Inter-Modal and Intra-Modal ({IM}2 )\nparallel feature extraction backbone coupled with a Multi-Scale Gated Fusion\n(MSGF) module to counteract the varying degrees of sensor degradation under\nadverse weather conditions. Experimental evaluation on a VoD dataset with\nsimulated fog proves that L4DR is more adaptable to changing weather\nconditions. It delivers a significant performance increase under different fog\nlevels, improving the 3D mAP by up to 20.0% over the traditional LiDAR-only\napproach. Moreover, the results on the K-Radar dataset validate the consistent\nperformance improvement of L4DR in real-world adverse weather conditions.\n","authors":["Xun Huang","Ziyu Xu","Hai Wu","Jinlong Wang","Qiming Xia","Yan Xia","Jonathan Li","Kyle Gao","Chenglu Wen","Cheng Wang"],"pdf_url":"https://arxiv.org/pdf/2408.03677v5.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2404.12154v2","updated":"2024-12-15T15:31:56Z","published":"2024-04-18T12:58:55Z","title":"StyleBooth: Image Style Editing with Multimodal Instruction","summary":"  Given an original image, image editing aims to generate an image that align\nwith the provided instruction. The challenges are to accept multimodal inputs\nas instructions and a scarcity of high-quality training data, including crucial\ntriplets of source/target image pairs and multimodal (text and image)\ninstructions. In this paper, we focus on image style editing and present\nStyleBooth, a method that proposes a comprehensive framework for image editing\nand a feasible strategy for building a high-quality style editing dataset. We\nintegrate encoded textual instruction and image exemplar as a unified condition\nfor diffusion model, enabling the editing of original image following\nmultimodal instructions. Furthermore, by iterative style-destyle tuning and\nediting and usability filtering, the StyleBooth dataset provides\ncontent-consistent stylized/plain image pairs in various categories of styles.\nTo show the flexibility of StyleBooth, we conduct experiments on diverse tasks,\nsuch as text-based style editing, exemplar-based style editing and\ncompositional style editing. The results demonstrate that the quality and\nvariety of training data significantly enhance the ability to preserve content\nand improve the overall quality of generated images in editing tasks. Project\npage can be found at https://ali-vilab.github.io/stylebooth-page/.\n","authors":["Zhen Han","Chaojie Mao","Zeyinzi Jiang","Yulin Pan","Jingfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.12154v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.11420v2","updated":"2024-12-15T15:15:40Z","published":"2024-01-21T07:48:39Z","title":"Embedded Hyperspectral Band Selection with Adaptive Optimization for\n  Image Semantic Segmentation","summary":"  The selection of hyperspectral bands plays a pivotal role in remote sensing\nand image analysis, with the aim of identifying the most informative spectral\nbands while minimizing computational overhead. This paper introduces a\npioneering approach for hyperspectral band selection that offers an embedded\nsolution, making it well-suited for resource-constrained or real-time\napplications. Our proposed method, embedded hyperspectral band selection\n(EHBS), excels in selecting the best bands without needing prior processing,\nseamlessly integrating with the downstream task model. This is achieved through\nstochastic band gates along with an approximation of the $l0$ norm on the\nnumber of selected bands as the regularization term and the integration of a\ndynamic optimizer, DoG, which removes the need for the required tuning of the\nlearning rate.\n  We conduct experiments on two distinct semantic-segmentation hyperspectral\nbenchmark datasets, demonstrating their superiority in terms of accuracy and\nease of use compared to many common and state-of-the-art methods. Furthermore,\nour contributions extend beyond hyperspectral band selection. Our approach's\nadaptability to other tasks, especially those involving grouped features, opens\npromising avenues for broader applications within the realm of deep learning,\nsuch as feature selection for feature groups.\n","authors":["Yaniv Zimmer","Oren Glickman"],"pdf_url":"https://arxiv.org/pdf/2401.11420v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11216v1","updated":"2024-12-15T15:13:14Z","published":"2024-12-15T15:13:14Z","title":"Distribution-Consistency-Guided Multi-modal Hashing","summary":"  Multi-modal hashing methods have gained popularity due to their fast speed\nand low storage requirements. Among them, the supervised methods demonstrate\nbetter performance by utilizing labels as supervisory signals compared with\nunsupervised methods. Currently, for almost all supervised multi-modal hashing\nmethods, there is a hidden assumption that training sets have no noisy labels.\nHowever, labels are often annotated incorrectly due to manual labeling in\nreal-world scenarios, which will greatly harm the retrieval performance. To\naddress this issue, we first discover a significant distribution consistency\npattern through experiments, i.e., the 1-0 distribution of the presence or\nabsence of each category in the label is consistent with the high-low\ndistribution of similarity scores of the hash codes relative to category\ncenters. Then, inspired by this pattern, we propose a novel\nDistribution-Consistency-Guided Multi-modal Hashing (DCGMH), which aims to\nfilter and reconstruct noisy labels to enhance retrieval performance.\nSpecifically, the proposed method first randomly initializes several category\ncenters, which are used to compute the high-low distribution of similarity\nscores; Noisy and clean labels are then separately filtered out via the\ndiscovered distribution consistency pattern to mitigate the impact of noisy\nlabels; Subsequently, a correction strategy, which is indirectly designed via\nthe distribution consistency pattern, is applied to the filtered noisy labels,\ncorrecting high-confidence ones while treating low-confidence ones as unlabeled\nfor unsupervised learning, thereby further enhancing the model's performance.\nExtensive experiments on three widely used datasets demonstrate the superiority\nof the proposed method compared to state-of-the-art baselines in multi-modal\nretrieval tasks. The code is available at\nhttps://github.com/LiuJinyu1229/DCGMH.\n","authors":["Jin-Yu Liu","Xian-Ling Mao","Tian-Yi Che","Rong-Cheng Tu"],"pdf_url":"https://arxiv.org/pdf/2412.11216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11214v1","updated":"2024-12-15T15:10:53Z","published":"2024-12-15T15:10:53Z","title":"Image Forgery Localization with State Space Models","summary":"  Pixel dependency modeling from tampered images is pivotal for image forgery\nlocalization. Current approaches predominantly rely on convolutional neural\nnetwork (CNN) or Transformer-based models, which often either lack sufficient\nreceptive fields or entail significant computational overheads. In this paper,\nwe propose LoMa, a novel image forgery localization method that leverages the\nSelective State Space (S6) model for global pixel dependency modeling and\ninverted residual CNN for local pixel dependency modeling. Our method\nintroduces the Mixed-SSM Block, which initially employs atrous selective scan\nto traverse the spatial domain and convert the tampered image into order patch\nsequences, and subsequently applies multidirectional S6 modeling. In addition,\nan auxiliary convolutional branch is introduced to enhance local feature\nextraction. This design facilitates the efficient extraction of global\ndependencies while upholding linear complexity. Upon modeling the pixel\ndependency with the SSM and CNN blocks, the pixel-wise forgery localization\nresults are obtained by a simple MLP decoder. Extensive experimental results\nvalidate the superiority of LoMa over CNN-based and Transformer-based\nstate-of-the-arts.\n","authors":["Zijie Lou","Gang Cao"],"pdf_url":"https://arxiv.org/pdf/2412.11214v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11210v1","updated":"2024-12-15T15:04:27Z","published":"2024-12-15T15:04:27Z","title":"ViPOcc: Leveraging Visual Priors from Vision Foundation Models for\n  Single-View 3D Occupancy Prediction","summary":"  Inferring the 3D structure of a scene from a single image is an ill-posed and\nchallenging problem in the field of vision-centric autonomous driving. Existing\nmethods usually employ neural radiance fields to produce voxelized 3D\noccupancy, lacking instance-level semantic reasoning and temporal photometric\nconsistency. In this paper, we propose ViPOcc, which leverages the visual\npriors from vision foundation models (VFMs) for fine-grained 3D occupancy\nprediction. Unlike previous works that solely employ volume rendering for RGB\nand depth image reconstruction, we introduce a metric depth estimation branch,\nin which an inverse depth alignment module is proposed to bridge the domain gap\nin depth distribution between VFM predictions and the ground truth. The\nrecovered metric depth is then utilized in temporal photometric alignment and\nspatial geometric alignment to ensure accurate and consistent 3D occupancy\nprediction. Additionally, we also propose a semantic-guided non-overlapping\nGaussian mixture sampler for efficient, instance-aware ray sampling, which\naddresses the redundant and imbalanced sampling issue that still exists in\nprevious state-of-the-art methods. Extensive experiments demonstrate the\nsuperior performance of ViPOcc in both 3D occupancy prediction and depth\nestimation tasks on the KITTI-360 and KITTI Raw datasets. Our code is available\nat: \\url{https://mias.group/ViPOcc}.\n","authors":["Yi Feng","Yu Han","Xijing Zhang","Tanghui Li","Yanting Zhang","Rui Fan"],"pdf_url":"https://arxiv.org/pdf/2412.11210v1.pdf","comment":"accepted to AAAI25"},{"id":"http://arxiv.org/abs/2404.12900v2","updated":"2024-12-15T14:53:00Z","published":"2024-04-19T14:13:46Z","title":"Training-and-Prompt-Free General Painterly Harmonization via Zero-Shot\n  Disentenglement on Style and Content References","summary":"  Painterly image harmonization aims at seamlessly blending disparate visual\nelements within a single image. However, previous approaches often struggle due\nto limitations in training data or reliance on additional prompts, leading to\ninharmonious and content-disrupted output. To surmount these hurdles, we design\na Training-and-prompt-Free General Painterly Harmonization method (TF-GPH).\nTF-GPH incorporates a novel ``Similarity Disentangle Mask'', which disentangles\nthe foreground content and background image by redirecting their attention to\ncorresponding reference images, enhancing the attention mechanism for\nmulti-image inputs. Additionally, we propose a ``Similarity Reweighting''\nmechanism to balance harmonization between stylization and content\npreservation. This mechanism minimizes content disruption by prioritizing the\ncontent-similar features within the given background style reference. Finally,\nwe address the deficiencies in existing benchmarks by proposing novel\nrange-based evaluation metrics and a new benchmark to better reflect real-world\napplications. Extensive experiments demonstrate the efficacy of our method in\nall benchmarks. More detailed in https://github.com/BlueDyee/TF-GPH.\n","authors":["Teng-Fang Hsiao","Bo-Kai Ruan","Hong-Han Shuai"],"pdf_url":"https://arxiv.org/pdf/2404.12900v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15595v2","updated":"2024-12-15T14:39:18Z","published":"2024-11-23T16:09:53Z","title":"An adversarial feature learning based semantic communication method for\n  Human 3D Reconstruction","summary":"  With the widespread application of human body 3D reconstruction technology\nacross various fields, the demands for data transmission and processing\nefficiency continue to rise, particularly in scenarios where network bandwidth\nis limited and low latency is required. This paper introduces an Adversarial\nFeature Learning-based Semantic Communication method (AFLSC) for human body 3D\nreconstruction, which focuses on extracting and transmitting semantic\ninformation crucial for the 3D reconstruction task, thereby significantly\noptimizing data flow and alleviating bandwidth pressure. At the sender's end,\nwe propose a multitask learning-based feature extraction method to capture the\nspatial layout, keypoints, posture, and depth information from 2D human images,\nand design a semantic encoding technique based on adversarial feature learning\nto encode these feature information into semantic data. We also develop a\ndynamic compression technique to efficiently transmit this semantic data,\ngreatly enhancing transmission efficiency and reducing latency. At the\nreceiver's end, we design an efficient multi-level semantic feature decoding\nmethod to convert semantic data back into key image features. Finally, an\nimproved ViT-diffusion model is employed for 3D reconstruction, producing human\nbody 3D mesh models. Experimental results validate the advantages of our method\nin terms of data transmission efficiency and reconstruction quality,\ndemonstrating its excellent potential for application in bandwidth-limited\nenvironments.\n","authors":["Shaojiang Liu","Jiajun Zou","Zhendan Liu","Meixia Dong","Zhiping Wan"],"pdf_url":"https://arxiv.org/pdf/2411.15595v2.pdf","comment":"It was published to arXiv without consulting the corresponding\n  author, so the corresponding author requested a withdrawal first"},{"id":"http://arxiv.org/abs/2412.11198v1","updated":"2024-12-15T14:21:19Z","published":"2024-12-15T14:21:19Z","title":"GEM: A Generalizable Ego-Vision Multimodal World Model for Fine-Grained\n  Ego-Motion, Object Dynamics, and Scene Composition Control","summary":"  We present GEM, a Generalizable Ego-vision Multimodal world model that\npredicts future frames using a reference frame, sparse features, human poses,\nand ego-trajectories. Hence, our model has precise control over object\ndynamics, ego-agent motion and human poses. GEM generates paired RGB and depth\noutputs for richer spatial understanding. We introduce autoregressive noise\nschedules to enable stable long-horizon generations. Our dataset is comprised\nof 4000+ hours of multimodal data across domains like autonomous driving,\negocentric human activities, and drone flights. Pseudo-labels are used to get\ndepth maps, ego-trajectories, and human poses. We use a comprehensive\nevaluation framework, including a new Control of Object Manipulation (COM)\nmetric, to assess controllability. Experiments show GEM excels at generating\ndiverse, controllable scenarios and temporal consistency over long generations.\nCode, models, and datasets are fully open-sourced.\n","authors":["Mariam Hassan","Sebastian Stapf","Ahmad Rahimi","Pedro M B Rezende","Yasaman Haghighi","David Brüggemann","Isinsu Katircioglu","Lin Zhang","Xiaoran Chen","Suman Saha","Marco Cannici","Elie Aljalbout","Botao Ye","Xi Wang","Aram Davtyan","Mathieu Salzmann","Davide Scaramuzza","Marc Pollefeys","Paolo Favaro","Alexandre Alahi"],"pdf_url":"https://arxiv.org/pdf/2412.11198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11196v1","updated":"2024-12-15T14:17:14Z","published":"2024-12-15T14:17:14Z","title":"Drawing the Line: Enhancing Trustworthiness of MLLMs Through the Power\n  of Refusal","summary":"  Multimodal large language models (MLLMs) excel at multimodal perception and\nunderstanding, yet their tendency to generate hallucinated or inaccurate\nresponses undermines their trustworthiness. Existing methods have largely\noverlooked the importance of refusal responses as a means of enhancing MLLMs\nreliability. To bridge this gap, we present the Information Boundary-aware\nLearning Framework (InBoL), a novel approach that empowers MLLMs to refuse to\nanswer user queries when encountering insufficient information. To the best of\nour knowledge, InBoL is the first framework that systematically defines the\nconditions under which refusal is appropriate for MLLMs using the concept of\ninformation boundaries proposed in our paper. This framework introduces a\ncomprehensive data generation pipeline and tailored training strategies to\nimprove the model's ability to deliver appropriate refusal responses. To\nevaluate the trustworthiness of MLLMs, we further propose a user-centric\nalignment goal along with corresponding metrics. Experimental results\ndemonstrate a significant improvement in refusal accuracy without noticeably\ncompromising the model's helpfulness, establishing InBoL as a pivotal\nadvancement in building more trustworthy MLLMs.\n","authors":["Yuhao Wang","Zhiyuan Zhu","Heyang Liu","Yusheng Liao","Hongcheng Liu","Yanfeng Wang","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2412.11196v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13472v2","updated":"2024-12-15T13:59:19Z","published":"2024-10-17T12:02:29Z","title":"Day-Night Adaptation: An Innovative Source-free Adaptation Framework for\n  Medical Image Segmentation","summary":"  Distribution shifts widely exist in medical images acquired from different\nmedical centres, hindering the deployment of semantic segmentation models\ntrained on one centre (source domain) to another (target domain). While\nunsupervised domain adaptation has shown significant promise in mitigating\nthese shifts, it poses privacy risks due to sharing data between centres. To\nfacilitate adaptation while preserving data privacy, source-free domain\nadaptation (SFDA) and test-time adaptation (TTA) have emerged as effective\nparadigms, relying solely on target domain data. However, SFDA requires a\npre-collected target domain dataset before deployment. TTA insufficiently\nexploit the potential value of test data, as it processes the test data only\nonce. Considering that most medical centres operate during the day and remain\ninactive at night in clinical practice, we propose a novel adaptation framework\ncalled Day-Night Adaptation (DyNA) with above insights, which performs\nadaptation through day-night cycles without requiring access to source data.\nDuring the day, a low-frequency prompt is trained to adapt the frozen model to\neach test sample. We construct a memory bank for prompt initialization and\ndevelop a warm-up mechanism to enhance prompt training. During the night, we\nreuse test data collected from the day and introduce a global student model to\nbridge the knowledge between teacher and student models, facilitating model\nfine-tuning while ensuring training stability. Extensive experiments\ndemonstrate that our DyNA outperforms existing TTA and SFDA methods on two\nbenchmark medical image segmentation tasks. Code will be available after the\npaper is published.\n","authors":["Ziyang Chen","Yiwen Ye","Yongsheng Pan","Jingfeng Zhang","Yanning Zhang","Yong Xia"],"pdf_url":"https://arxiv.org/pdf/2410.13472v2.pdf","comment":"16 pages, 4 figures, 6 tables"},{"id":"http://arxiv.org/abs/2412.11193v1","updated":"2024-12-15T13:58:37Z","published":"2024-12-15T13:58:37Z","title":"Light-T2M: A Lightweight and Fast Model for Text-to-motion Generation","summary":"  Despite the significant role text-to-motion (T2M) generation plays across\nvarious applications, current methods involve a large number of parameters and\nsuffer from slow inference speeds, leading to high usage costs. To address\nthis, we aim to design a lightweight model to reduce usage costs. First, unlike\nexisting works that focus solely on global information modeling, we recognize\nthe importance of local information modeling in the T2M task by reconsidering\nthe intrinsic properties of human motion, leading us to propose a lightweight\nLocal Information Modeling Module. Second, we introduce Mamba to the T2M task,\nreducing the number of parameters and GPU memory demands, and we have designed\na novel Pseudo-bidirectional Scan to replicate the effects of a bidirectional\nscan without increasing parameter count. Moreover, we propose a novel Adaptive\nTextual Information Injector that more effectively integrates textual\ninformation into the motion during generation. By integrating the\naforementioned designs, we propose a lightweight and fast model named\nLight-T2M. Compared to the state-of-the-art method, MoMask, our Light-T2M model\nfeatures just 10\\% of the parameters (4.48M vs 44.85M) and achieves a 16\\%\nfaster inference time (0.152s vs 0.180s), while surpassing MoMask with an FID\nof \\textbf{0.040} (vs. 0.045) on HumanML3D dataset and 0.161 (vs. 0.228) on\nKIT-ML dataset. The code is available at\nhttps://github.com/qinghuannn/light-t2m.\n","authors":["Ling-An Zeng","Guohong Huang","Gaojie Wu","Wei-Shi Zheng"],"pdf_url":"https://arxiv.org/pdf/2412.11193v1.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2406.04295v2","updated":"2024-12-15T13:53:52Z","published":"2024-06-06T17:39:09Z","title":"Everything to the Synthetic: Diffusion-driven Test-time Adaptation via\n  Synthetic-Domain Alignment","summary":"  Test-time adaptation (TTA) aims to improve the performance of source-domain\npre-trained models on previously unseen, shifted target domains. Traditional\nTTA methods primarily adapt model weights based on target data streams, making\nmodel performance sensitive to the amount and order of target data. The\nrecently proposed diffusion-driven TTA methods mitigate this by adapting model\ninputs instead of weights, where an unconditional diffusion model, trained on\nthe source domain, transforms target-domain data into a synthetic domain that\nis expected to approximate the source domain. However, in this paper, we reveal\nthat although the synthetic data in diffusion-driven TTA seems\nindistinguishable from the source data, it is unaligned with, or even markedly\ndifferent from the latter for deep networks. To address this issue, we propose\na \\textbf{S}ynthetic-\\textbf{D}omain \\textbf{A}lignment (SDA) framework. Our\nkey insight is to fine-tune the source model with synthetic data to ensure\nbetter alignment. Specifically, we first employ a conditional diffusion model\nto generate labeled samples, creating a synthetic dataset. Subsequently, we use\nthe aforementioned unconditional diffusion model to add noise to and denoise\neach sample before fine-tuning. This Mix of Diffusion (MoD) process mitigates\nthe potential domain misalignment between the conditional and unconditional\nmodels. Extensive experiments across classifiers, segmenters, and multimodal\nlarge language models (MLLMs, \\eg, LLaVA) demonstrate that SDA achieves\nsuperior domain alignment and consistently outperforms existing\ndiffusion-driven TTA methods. Our code is available at\nhttps://github.com/SHI-Labs/Diffusion-Driven-Test-Time-Adaptation-via-Synthetic-Domain-Alignment.\n","authors":["Jiayi Guo","Junhao Zhao","Chaoqun Du","Yulin Wang","Chunjiang Ge","Zanlin Ni","Shiji Song","Humphrey Shi","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2406.04295v2.pdf","comment":"GitHub:\n  https://github.com/SHI-Labs/Diffusion-Driven-Test-Time-Adaptation-via-Synthetic-Domain-Alignment"},{"id":"http://arxiv.org/abs/2412.11186v1","updated":"2024-12-15T13:35:07Z","published":"2024-12-15T13:35:07Z","title":"Efficient Quantization-Aware Training on Segment Anything Model in\n  Medical Images and Its Deployment","summary":"  Medical image segmentation is a critical component of clinical practice, and\nthe state-of-the-art MedSAM model has significantly advanced this field.\nNevertheless, critiques highlight that MedSAM demands substantial computational\nresources during inference. To address this issue, the CVPR 2024 MedSAM on\nLaptop Challenge was established to find an optimal balance between accuracy\nand processing speed. In this paper, we introduce a quantization-aware training\npipeline designed to efficiently quantize the Segment Anything Model for\nmedical images and deploy it using the OpenVINO inference engine. This pipeline\noptimizes both training time and disk storage. Our experimental results confirm\nthat this approach considerably enhances processing speed over the baseline,\nwhile still achieving an acceptable accuracy level. The training script,\ninference script, and quantized model are publicly accessible at\nhttps://github.com/AVC2-UESTC/QMedSAM.\n","authors":["Haisheng Lu","Yujie Fu","Fan Zhang","Le Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.11186v1.pdf","comment":"14 pages, 3 figures, to be published in LNCS"},{"id":"http://arxiv.org/abs/2412.11183v1","updated":"2024-12-15T13:26:51Z","published":"2024-12-15T13:26:51Z","title":"OccScene: Semantic Occupancy-based Cross-task Mutual Learning for 3D\n  Scene Generation","summary":"  Recent diffusion models have demonstrated remarkable performance in both 3D\nscene generation and perception tasks. Nevertheless, existing methods typically\nseparate these two processes, acting as a data augmenter to generate synthetic\ndata for downstream perception tasks. In this work, we propose OccScene, a\nnovel mutual learning paradigm that integrates fine-grained 3D perception and\nhigh-quality generation in a unified framework, achieving a cross-task win-win\neffect. OccScene generates new and consistent 3D realistic scenes only\ndepending on text prompts, guided with semantic occupancy in a joint-training\ndiffusion framework. To align the occupancy with the diffusion latent, a\nMamba-based Dual Alignment module is introduced to incorporate fine-grained\nsemantics and geometry as perception priors. Within OccScene, the perception\nmodule can be effectively improved with customized and diverse generated\nscenes, while the perception priors in return enhance the generation\nperformance for mutual benefits. Extensive experiments show that OccScene\nachieves realistic 3D scene generation in broad indoor and outdoor scenarios,\nwhile concurrently boosting the perception models to achieve substantial\nperformance improvements in the 3D perception task of semantic occupancy\nprediction.\n","authors":["Bohan Li","Xin Jin","Jianan Wang","Yukai Shi","Yasheng Sun","Xiaofeng Wang","Zhuang Ma","Baao Xie","Chao Ma","Xiaokang Yang","Wenjun Zeng"],"pdf_url":"https://arxiv.org/pdf/2412.11183v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11170v1","updated":"2024-12-15T12:41:44Z","published":"2024-12-15T12:41:44Z","title":"Benchmarking and Learning Multi-Dimensional Quality Evaluator for\n  Text-to-3D Generation","summary":"  Text-to-3D generation has achieved remarkable progress in recent years, yet\nevaluating these methods remains challenging for two reasons: i) Existing\nbenchmarks lack fine-grained evaluation on different prompt categories and\nevaluation dimensions. ii) Previous evaluation metrics only focus on a single\naspect (e.g., text-3D alignment) and fail to perform multi-dimensional quality\nassessment. To address these problems, we first propose a comprehensive\nbenchmark named MATE-3D. The benchmark contains eight well-designed prompt\ncategories that cover single and multiple object generation, resulting in 1,280\ngenerated textured meshes. We have conducted a large-scale subjective\nexperiment from four different evaluation dimensions and collected 107,520\nannotations, followed by detailed analyses of the results. Based on MATE-3D, we\npropose a novel quality evaluator named HyperScore. Utilizing hypernetwork to\ngenerate specified mapping functions for each evaluation dimension, our metric\ncan effectively perform multi-dimensional quality assessment. HyperScore\npresents superior performance over existing metrics on MATE-3D, making it a\npromising metric for assessing and improving text-to-3D generation. The project\nis available at https://mate-3d.github.io/.\n","authors":["Yujie Zhang","Bingyang Cui","Qi Yang","Zhu Li","Yiling Xu"],"pdf_url":"https://arxiv.org/pdf/2412.11170v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11165v1","updated":"2024-12-15T12:28:57Z","published":"2024-12-15T12:28:57Z","title":"OTLRM: Orthogonal Learning-based Low-Rank Metric for Multi-Dimensional\n  Inverse Problems","summary":"  In real-world scenarios, complex data such as multispectral images and\nmulti-frame videos inherently exhibit robust low-rank property. This property\nis vital for multi-dimensional inverse problems, such as tensor completion,\nspectral imaging reconstruction, and multispectral image denoising. Existing\ntensor singular value decomposition (t-SVD) definitions rely on hand-designed\nor pre-given transforms, which lack flexibility for defining tensor nuclear\nnorm (TNN). The TNN-regularized optimization problem is solved by the singular\nvalue thresholding (SVT) operator, which leverages the t-SVD framework to\nobtain the low-rank tensor. However, it is quite complicated to introduce SVT\ninto deep neural networks due to the numerical instability problem in solving\nthe derivatives of the eigenvectors. In this paper, we introduce a novel\ndata-driven generative low-rank t-SVD model based on the learnable orthogonal\ntransform, which can be naturally solved under its representation. Prompted by\nthe linear algebra theorem of the Householder transformation, our learnable\northogonal transform is achieved by constructing an endogenously orthogonal\nmatrix adaptable to neural networks, optimizing it as arbitrary orthogonal\nmatrices. Additionally, we propose a low-rank solver as a generalization of\nSVT, which utilizes an efficient representation of generative networks to\nobtain low-rank structures. Extensive experiments highlight its significant\nrestoration enhancements.\n","authors":["Xiangming Wang","Haijin Zeng","Jiaoyang Chen","Sheng Liu","Yongyong Chen","Guoqing Chao"],"pdf_url":"https://arxiv.org/pdf/2412.11165v1.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2407.00603v2","updated":"2024-12-15T12:04:12Z","published":"2024-06-30T06:08:12Z","title":"Hierarchical Memory for Long Video QA","summary":"  This paper describes our champion solution to the LOVEU Challenge @ CVPR'24,\nTrack 1 (Long Video VQA). Processing long sequences of visual tokens is\ncomputationally expensive and memory-intensive, making long video\nquestion-answering a challenging task. The key is to compress visual tokens\neffectively, reducing memory footprint and decoding latency, while preserving\nthe essential information for accurate question-answering. We adopt a\nhierarchical memory mechanism named STAR Memory, proposed in Flash-VStream,\nthat is capable of processing long videos with limited GPU memory (VRAM). We\nfurther utilize the video and audio data of MovieChat-1K training set to\nfine-tune the pretrained weight released by Flash-VStream, achieving 1st place\nin the challenge. Code is available at project homepage\nhttps://invinciblewyq.github.io/vstream-page .\n","authors":["Yiqin Wang","Haoji Zhang","Yansong Tang","Yong Liu","Jiashi Feng","Jifeng Dai","Xiaojie Jin"],"pdf_url":"https://arxiv.org/pdf/2407.00603v2.pdf","comment":"Accepted to CVPR 2024 Workshop"},{"id":"http://arxiv.org/abs/2412.11161v1","updated":"2024-12-15T11:59:23Z","published":"2024-12-15T11:59:23Z","title":"Why and How: Knowledge-Guided Learning for Cross-Spectral Image Patch\n  Matching","summary":"  Recently, cross-spectral image patch matching based on feature relation\nlearning has attracted extensive attention. However, performance bottleneck\nproblems have gradually emerged in existing methods. To address this challenge,\nwe make the first attempt to explore a stable and efficient bridge between\ndescriptor learning and metric learning, and construct a knowledge-guided\nlearning network (KGL-Net), which achieves amazing performance improvements\nwhile abandoning complex network structures. Specifically, we find that there\nis feature extraction consistency between metric learning based on feature\ndifference learning and descriptor learning based on Euclidean distance. This\nprovides the foundation for bridge building. To ensure the stability and\nefficiency of the constructed bridge, on the one hand, we conduct an in-depth\nexploration of 20 combined network architectures. On the other hand, a\nfeature-guided loss is constructed to achieve mutual guidance of features. In\naddition, unlike existing methods, we consider that the feature mapping ability\nof the metric branch should receive more attention. Therefore, a hard negative\nsample mining for metric learning (HNSM-M) strategy is constructed. To the best\nof our knowledge, this is the first time that hard negative sample mining for\nmetric networks has been implemented and brings significant performance gains.\nExtensive experimental results show that our KGL-Net achieves SOTA performance\nin three different cross-spectral image patch matching scenarios. Our code are\navailable at https://github.com/YuChuang1205/KGL-Net.\n","authors":["Chuang Yu","Yunpeng Liu","Jinmiao Zhao","Xiangyu Yue"],"pdf_url":"https://arxiv.org/pdf/2412.11161v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07660v2","updated":"2024-12-15T11:52:45Z","published":"2024-11-12T09:22:00Z","title":"HMIL: Hierarchical Multi-Instance Learning for Fine-Grained Whole Slide\n  Image Classification","summary":"  Fine-grained classification of whole slide images (WSIs) is essential in\nprecision oncology, enabling precise cancer diagnosis and personalized\ntreatment strategies. The core of this task involves distinguishing subtle\nmorphological variations within the same broad category of gigapixel-resolution\nimages, which presents a significant challenge. While the multi-instance\nlearning (MIL) paradigm alleviates the computational burden of WSIs, existing\nMIL methods often overlook hierarchical label correlations, treating\nfine-grained classification as a flat multi-class classification task. To\novercome these limitations, we introduce a novel hierarchical multi-instance\nlearning (HMIL) framework. By facilitating on the hierarchical alignment of\ninherent relationships between different hierarchy of labels at instance and\nbag level, our approach provides a more structured and informative learning\nprocess. Specifically, HMIL incorporates a class-wise attention mechanism that\naligns hierarchical information at both the instance and bag levels.\nFurthermore, we introduce supervised contrastive learning to enhance the\ndiscriminative capability for fine-grained classification and a\ncurriculum-based dynamic weighting module to adaptively balance the\nhierarchical feature during training. Extensive experiments on our large-scale\ncytology cervical cancer (CCC) dataset and two public histology datasets, BRACS\nand PANDA, demonstrate the state-of-the-art class-wise and overall performance\nof our HMIL framework. Our source code is available at\nhttps://github.com/ChengJin-git/HMIL.\n","authors":["Cheng Jin","Luyang Luo","Huangjing Lin","Jun Hou","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2411.07660v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2412.11154v1","updated":"2024-12-15T11:08:49Z","published":"2024-12-15T11:08:49Z","title":"From Easy to Hard: Progressive Active Learning Framework for Infrared\n  Small Target Detection with Single Point Supervision","summary":"  Recently, single-frame infrared small target (SIRST) detection with single\npoint supervision has drawn wide-spread attention. However, the latest label\nevolution with single point supervision (LESPS) framework suffers from\ninstability, excessive label evolution, and difficulty in exerting embedded\nnetwork performance. Therefore, we construct a Progressive Active Learning\n(PAL) framework. Specifically, inspired by organisms gradually adapting to\ntheir environment and continuously accumulating knowledge, we propose an\ninnovative progressive active learning idea, which emphasizes that the network\nprogressively and actively recognizes and learns more hard samples to achieve\ncontinuous performance enhancement. Based on this, on the one hand, we propose\na model pre-start concept, which focuses on selecting a portion of easy samples\nand can help models have basic task-specific learning capabilities. On the\nother hand, we propose a refined dual-update strategy, which can promote\nreasonable learning of harder samples and continuous refinement of\npseudo-labels. In addition, to alleviate the risk of excessive label evolution,\na decay factor is reasonably introduced, which helps to achieve a dynamic\nbalance between the expansion and contraction of target annotations. Extensive\nexperiments show that convolutional neural networks (CNNs) equipped with our\nPAL framework have achieved state-of-the-art (SOTA) results on multiple public\ndatasets. Furthermore, our PAL framework can build a efficient and stable\nbridge between full supervision and point supervision tasks. Our code are\navailable at https://github.com/YuChuang1205/PAL.\n","authors":["Chuang Yu","Jinmiao Zhao","Yunpeng Liu","Sicheng Zhao","Xiangyu Yue"],"pdf_url":"https://arxiv.org/pdf/2412.11154v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11152v1","updated":"2024-12-15T11:04:06Z","published":"2024-12-15T11:04:06Z","title":"Dual-Schedule Inversion: Training- and Tuning-Free Inversion for Real\n  Image Editing","summary":"  Text-conditional image editing is a practical AIGC task that has recently\nemerged with great commercial and academic value. For real image editing, most\ndiffusion model-based methods use DDIM Inversion as the first stage before\nediting. However, DDIM Inversion often results in reconstruction failure,\nleading to unsatisfactory performance for downstream editing. To address this\nproblem, we first analyze why the reconstruction via DDIM Inversion fails. We\nthen propose a new inversion and sampling method named Dual-Schedule Inversion.\nWe also design a classifier to adaptively combine Dual-Schedule Inversion with\ndifferent editing methods for user-friendly image editing. Our work can achieve\nsuperior reconstruction and editing performance with the following advantages:\n1) It can reconstruct real images perfectly without fine-tuning, and its\nreversibility is guaranteed mathematically. 2) The edited object/scene conforms\nto the semantics of the text prompt. 3) The unedited parts of the object/scene\nretain the original identity.\n","authors":["Jiancheng Huang","Yi Huang","Jianzhuang Liu","Donghao Zhou","Yifan Liu","Shifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2412.11152v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03540v2","updated":"2024-12-15T10:49:36Z","published":"2024-08-07T04:38:03Z","title":"PoseMamba: Monocular 3D Human Pose Estimation with Bidirectional\n  Global-Local Spatio-Temporal State Space Model","summary":"  Transformers have significantly advanced the field of 3D human pose\nestimation (HPE). However, existing transformer-based methods primarily use\nself-attention mechanisms for spatio-temporal modeling, leading to a quadratic\ncomplexity, unidirectional modeling of spatio-temporal relationships, and\ninsufficient learning of spatial-temporal correlations. Recently, the Mamba\narchitecture, utilizing the state space model (SSM), has exhibited superior\nlong-range modeling capabilities in a variety of vision tasks with linear\ncomplexity. In this paper, we propose PoseMamba, a novel purely SSM-based\napproach with linear complexity for 3D human pose estimation in monocular\nvideo. Specifically, we propose a bidirectional global-local spatio-temporal\nSSM block that comprehensively models human joint relations within individual\nframes as well as temporal correlations across frames. Within this\nbidirectional global-local spatio-temporal SSM block, we introduce a reordering\nstrategy to enhance the local modeling capability of the SSM. This strategy\nprovides a more logical geometric scanning order and integrates it with the\nglobal SSM, resulting in a combined global-local spatial scan. We have\nquantitatively and qualitatively evaluated our approach using two benchmark\ndatasets: Human3.6M and MPI-INF-3DHP. Extensive experiments demonstrate that\nPoseMamba achieves state-of-the-art performance on both datasets while\nmaintaining a smaller model size and reducing computational costs. The code and\nmodels will be released.\n","authors":["Yunlong Huang","Junshuo Liu","Ke Xian","Robert Caiming Qiu"],"pdf_url":"https://arxiv.org/pdf/2408.03540v2.pdf","comment":"Accpeted by the 39th Annual AAAI Conference on Artificial\n  Intelligence (AAAI 2025)"},{"id":"http://arxiv.org/abs/2412.11149v1","updated":"2024-12-15T10:47:26Z","published":"2024-12-15T10:47:26Z","title":"A Comprehensive Survey of Action Quality Assessment: Method and\n  Benchmark","summary":"  Action Quality Assessment (AQA) quantitatively evaluates the quality of human\nactions, providing automated assessments that reduce biases in human judgment.\nIts applications span domains such as sports analysis, skill assessment, and\nmedical care. Recent advances in AQA have introduced innovative methodologies,\nbut similar methods often intertwine across different domains, highlighting the\nfragmented nature that hinders systematic reviews. In addition, the lack of a\nunified benchmark and limited computational comparisons hinder consistent\nevaluation and fair assessment of AQA approaches. In this work, we address\nthese gaps by systematically analyzing over 150 AQA-related papers to develop a\nhierarchical taxonomy, construct a unified benchmark, and provide an in-depth\nanalysis of current trends, challenges, and future directions. Our hierarchical\ntaxonomy categorizes AQA methods based on input modalities (video, skeleton,\nmulti-modal) and their specific characteristics, highlighting the evolution and\ninterrelations across various approaches. To promote standardization, we\npresent a unified benchmark, integrating diverse datasets to evaluate the\nassessment precision and computational efficiency. Finally, we review emerging\ntask-specific applications and identify under-explored challenges in AQA,\nproviding actionable insights into future research directions. This survey aims\nto deepen understanding of AQA progress, facilitate method comparison, and\nguide future innovations. The project web page can be found at\nhttps://ZhouKanglei.github.io/AQA-Survey.\n","authors":["Kanglei Zhou","Ruizhi Cai","Liyuan Wang","Hubert P. H. Shum","Xiaohui Liang"],"pdf_url":"https://arxiv.org/pdf/2412.11149v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11148v1","updated":"2024-12-15T10:47:09Z","published":"2024-12-15T10:47:09Z","title":"Redefining Normal: A Novel Object-Level Approach for Multi-Object\n  Novelty Detection","summary":"  In the realm of novelty detection, accurately identifying outliers in data\nwithout specific class information poses a significant challenge. While current\nmethods excel in single-object scenarios, they struggle with multi-object\nsituations due to their focus on individual objects. Our paper suggests a novel\napproach: redefining `normal' at the object level in training datasets. Rather\nthan the usual image-level view, we consider the most dominant object in a\ndataset as the norm, offering a perspective that is more effective for\nreal-world scenarios. Adapting to our object-level definition of `normal', we\nmodify knowledge distillation frameworks, where a student network learns from a\npre-trained teacher network. Our first contribution, DeFeND(Dense Feature\nFine-tuning on Normal Data), integrates dense feature fine-tuning into the\ndistillation process, allowing the teacher network to focus on object-level\nfeatures with a self-supervised loss. The second is masked knowledge\ndistillation, where the student network works with partially hidden inputs,\nhoning its ability to deduce and generalize from incomplete data. This approach\nnot only fares well in single-object novelty detection but also considerably\nsurpasses existing methods in multi-object contexts. The implementation is\navailable at: https://github.com/SMSD75/Redefining_Normal_ACCV24/tree/main\n","authors":["Mohammadreza Salehi","Nikolaos Apostolikas","Efstratios Gavves","Cees G. M. Snoek","Yuki M. Asano"],"pdf_url":"https://arxiv.org/pdf/2412.11148v1.pdf","comment":"Accepted at ACCV24(Oral)"},{"id":"http://arxiv.org/abs/2406.13123v3","updated":"2024-12-15T10:24:12Z","published":"2024-06-19T00:38:19Z","title":"ViLCo-Bench: VIdeo Language COntinual learning Benchmark","summary":"  Video language continual learning involves continuously adapting to\ninformation from video and text inputs, enhancing a model's ability to handle\nnew tasks while retaining prior knowledge. This field is a relatively\nunder-explored area, and establishing appropriate datasets is crucial for\nfacilitating communication and research in this field. In this study, we\npresent the first dedicated benchmark, ViLCo-Bench, designed to evaluate\ncontinual learning models across a range of video-text tasks. The dataset\ncomprises ten-minute-long videos and corresponding language queries collected\nfrom publicly available datasets. Additionally, we introduce a novel\nmemory-efficient framework that incorporates self-supervised learning and\nmimics long-term and short-term memory effects. This framework addresses\nchallenges including memory complexity from long video clips, natural language\ncomplexity from open queries, and text-video misalignment. We posit that\nViLCo-Bench, with greater complexity compared to existing continual learning\nbenchmarks, would serve as a critical tool for exploring the video-language\ndomain, extending beyond conventional class-incremental tasks, and addressing\ncomplex and limited annotation issues. The curated data, evaluations, and our\nnovel method are available at https://github.com/cruiseresearchgroup/ViLCo.\n","authors":["Tianqi Tang","Shohreh Deldari","Hao Xue","Celso De Melo","Flora D. Salim"],"pdf_url":"https://arxiv.org/pdf/2406.13123v3.pdf","comment":"14 pages, 4 figures, 8 tables, Accepted at NeurIPS Dataset and\n  Benchmark Track 2024"},{"id":"http://arxiv.org/abs/2412.04301v3","updated":"2024-12-15T10:10:39Z","published":"2024-12-05T16:23:11Z","title":"SwiftEdit: Lightning Fast Text-Guided Image Editing via One-Step\n  Diffusion","summary":"  Recent advances in text-guided image editing enable users to perform image\nedits through simple text inputs, leveraging the extensive priors of multi-step\ndiffusion-based text-to-image models. However, these methods often fall short\nof the speed demands required for real-world and on-device applications due to\nthe costly multi-step inversion and sampling process involved. In response to\nthis, we introduce SwiftEdit, a simple yet highly efficient editing tool that\nachieve instant text-guided image editing (in 0.23s). The advancement of\nSwiftEdit lies in its two novel contributions: a one-step inversion framework\nthat enables one-step image reconstruction via inversion and a mask-guided\nediting technique with our proposed attention rescaling mechanism to perform\nlocalized image editing. Extensive experiments are provided to demonstrate the\neffectiveness and efficiency of SwiftEdit. In particular, SwiftEdit enables\ninstant text-guided image editing, which is extremely faster than previous\nmulti-step methods (at least 50 times faster) while maintain a competitive\nperformance in editing results. Our project page is at:\nhttps://swift-edit.github.io/\n","authors":["Trong-Tung Nguyen","Quang Nguyen","Khoi Nguyen","Anh Tran","Cuong Pham"],"pdf_url":"https://arxiv.org/pdf/2412.04301v3.pdf","comment":"16 pages, 15 figures"},{"id":"http://arxiv.org/abs/2312.03032v3","updated":"2024-12-15T09:59:44Z","published":"2023-12-05T11:33:16Z","title":"ZeroReg: Zero-Shot Point Cloud Registration with Foundation Models","summary":"  State-of-the-art 3D point cloud registration methods rely on labeled 3D\ndatasets for training, which limits their practical applications in real-world\nscenarios and often hinders generalization to unseen scenes. Leveraging the\nzero-shot capabilities of foundation models offers a promising solution to\nthese challenges. In this paper, we introduce ZeroReg, a zero-shot registration\napproach that utilizes 2D foundation models to predict 3D correspondences.\nSpecifically, ZeroReg adopts an object-to-point matching strategy, starting\nwith object localization and semantic feature extraction from multi-view images\nusing foundation models. In the object matching stage, semantic features help\nidentify correspondences between objects across views. However, relying solely\non semantic features can lead to ambiguity, especially in scenes with multiple\ninstances of the same category. To address this, we construct scene graphs to\ncapture spatial relationships among objects and apply a graph matching\nalgorithm to these graphs to accurately identify matched objects. Finally,\ncomputing fine-grained point-level correspondences within matched object\nregions using algorithms like SuperGlue and LoFTR achieves robust point cloud\nregistration. Evaluations on benchmarks such as 3DMatch, 3DLoMatch, and ScanNet\ndemonstrate ZeroReg's competitive performance, highlighting its potential to\nadvance point-cloud registration by integrating semantic features from\nfoundation models.\n","authors":["Weijie Wang","Wenqi Ren","Guofeng Mei","Bin Ren","Xiaoshui Huang","Fabio Poiesi","Nicu Sebe","Bruno Lepri"],"pdf_url":"https://arxiv.org/pdf/2312.03032v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10335v4","updated":"2024-12-15T09:46:46Z","published":"2024-04-16T07:19:52Z","title":"Efficient Generation of Targeted and Transferable Adversarial Examples\n  for Vision-Language Models Via Diffusion Models","summary":"  Adversarial attacks, particularly \\textbf{targeted} transfer-based attacks,\ncan be used to assess the adversarial robustness of large visual-language\nmodels (VLMs), allowing for a more thorough examination of potential security\nflaws before deployment. However, previous transfer-based adversarial attacks\nincur high costs due to high iteration counts and complex method structure.\nFurthermore, due to the unnaturalness of adversarial semantics, the generated\nadversarial examples have low transferability. These issues limit the utility\nof existing methods for assessing robustness. To address these issues, we\npropose AdvDiffVLM, which uses diffusion models to generate natural,\nunrestricted and targeted adversarial examples via score matching.\nSpecifically, AdvDiffVLM uses Adaptive Ensemble Gradient Estimation to modify\nthe score during the diffusion model's reverse generation process, ensuring\nthat the produced adversarial examples have natural adversarial targeted\nsemantics, which improves their transferability. Simultaneously, to improve the\nquality of adversarial examples, we use the GradCAM-guided Mask method to\ndisperse adversarial semantics throughout the image rather than concentrating\nthem in a single area. Finally, AdvDiffVLM embeds more target semantics into\nadversarial examples after multiple iterations. Experimental results show that\nour method generates adversarial examples 5x to 10x faster than\nstate-of-the-art transfer-based adversarial attacks while maintaining higher\nquality adversarial examples. Furthermore, compared to previous transfer-based\nadversarial attacks, the adversarial examples generated by our method have\nbetter transferability. Notably, AdvDiffVLM can successfully attack a variety\nof commercial VLMs in a black-box environment, including GPT-4V.\n","authors":["Qi Guo","Shanmin Pang","Xiaojun Jia","Yang Liu","Qing Guo"],"pdf_url":"https://arxiv.org/pdf/2404.10335v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10573v2","updated":"2024-12-15T09:41:09Z","published":"2024-10-14T14:49:34Z","title":"Queryable Prototype Multiple Instance Learning with Vision-Language\n  Models for Incremental Whole Slide Image Classification","summary":"  Whole Slide Image (WSI) classification has very significant applications in\nclinical pathology, e.g., tumor identification and cancer diagnosis. Currently,\nmost research attention is focused on Multiple Instance Learning (MIL) using\nstatic datasets. One of the most obvious weaknesses of these methods is that\nthey cannot efficiently preserve and utilize previously learned knowledge. With\nany new data arriving, classification models are required to be re-trained on\nboth previous and current new data. To overcome this shortcoming and break\nthrough traditional vision modality, this paper proposes the first\nVision-Language-based framework with Queryable Prototype Multiple Instance\nLearning (QPMIL-VL) specially designed for incremental WSI classification. This\nframework mainly consists of two information processing branches: one is for\ngenerating bag-level features by prototype-guided aggregation of instance\nfeatures, while the other is for enhancing class features through a combination\nof class ensemble, tunable vector and class similarity loss. The experiments on\nfour public WSI datasets demonstrate that our QPMIL-VL framework is effective\nfor incremental WSI classification and often significantly outperforms other\ncompared methods, achieving state-of-the-art (SOTA) performance. Our source\ncode is publicly available at https://github.com/can-can-ya/QPMIL-VL.\n","authors":["Jiaxiang Gou","Luping Ji","Pei Liu","Mao Ye"],"pdf_url":"https://arxiv.org/pdf/2410.10573v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2410.07679v3","updated":"2024-12-15T09:31:35Z","published":"2024-10-10T07:40:51Z","title":"Relational Diffusion Distillation for Efficient Image Generation","summary":"  Although the diffusion model has achieved remarkable performance in the field\nof image generation, its high inference delay hinders its wide application in\nedge devices with scarce computing resources. Therefore, many training-free\nsampling methods have been proposed to reduce the number of sampling steps\nrequired for diffusion models. However, they perform poorly under a very small\nnumber of sampling steps. Thanks to the emergence of knowledge distillation\ntechnology, the existing training scheme methods have achieved excellent\nresults at very low step numbers. However, the current methods mainly focus on\ndesigning novel diffusion model sampling methods with knowledge distillation.\nHow to transfer better diffusion knowledge from teacher models is a more\nvaluable problem but rarely studied. Therefore, we propose Relational Diffusion\nDistillation (RDD), a novel distillation method tailored specifically for\ndistilling diffusion models. Unlike existing methods that simply align teacher\nand student models at pixel level or feature distributions, our method\nintroduces cross-sample relationship interaction during the distillation\nprocess and alleviates the memory constraints induced by multiple sample\ninteractions. Our RDD significantly enhances the effectiveness of the\nprogressive distillation framework within the diffusion model. Extensive\nexperiments on several datasets (e.g., CIFAR-10 and ImageNet) demonstrate that\nour proposed RDD leads to 1.47 FID decrease under 1 sampling step compared to\nstate-of-the-art diffusion distillation methods and achieving 256x speed-up\ncompared to DDIM strategy. Code is available at\nhttps://github.com/cantbebetter2/RDD.\n","authors":["Weilun Feng","Chuanguang Yang","Zhulin An","Libo Huang","Boyu Diao","Fei Wang","Yongjun Xu"],"pdf_url":"https://arxiv.org/pdf/2410.07679v3.pdf","comment":"Accepted by ACM MM 2024 Oral"},{"id":"http://arxiv.org/abs/2412.11124v1","updated":"2024-12-15T09:10:46Z","published":"2024-12-15T09:10:46Z","title":"Combating Multimodal LLM Hallucination via Bottom-up Holistic Reasoning","summary":"  Recent advancements in multimodal large language models (MLLMs) have shown\nunprecedented capabilities in advancing various vision-language tasks. However,\nMLLMs face significant challenges with hallucinations, and misleading outputs\nthat do not align with the input data. While existing efforts are paid to\ncombat MLLM hallucinations, several pivotal challenges are still unsolved.\nFirst, while current approaches aggressively focus on addressing errors at the\nperception level, another important type at the cognition level requiring\nfactual commonsense can be overlooked. In addition, existing methods might fall\nshort in finding a more effective way to represent visual input, which is yet a\nkey bottleneck that triggers visual hallucinations. Moreover, MLLMs can\nfrequently be misled by faulty textual inputs and cause hallucinations, while\nunfortunately, this type of issue has long been overlooked by existing studies.\nInspired by human intuition in handling hallucinations, this paper introduces a\nnovel bottom-up reasoning framework. Our framework systematically addresses\npotential issues in both visual and textual inputs by verifying and integrating\nperception-level information with cognition-level commonsense knowledge,\nensuring more reliable outputs. Extensive experiments demonstrate significant\nimprovements in multiple hallucination benchmarks after integrating MLLMs with\nthe proposed framework. In-depth analyses reveal the great potential of our\nmethods in addressing perception- and cognition-level hallucinations.\n","authors":["Shengqiong Wu","Hao Fei","Liangming Pan","William Yang Wang","Shuicheng Yan","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2412.11124v1.pdf","comment":"16 pages, 10 figures, accepted by AAAI 25"},{"id":"http://arxiv.org/abs/2412.11119v1","updated":"2024-12-15T08:41:37Z","published":"2024-12-15T08:41:37Z","title":"Impact of Adversarial Attacks on Deep Learning Model Explainability","summary":"  In this paper, we investigate the impact of adversarial attacks on the\nexplainability of deep learning models, which are commonly criticized for their\nblack-box nature despite their capacity for autonomous feature extraction. This\nblack-box nature can affect the perceived trustworthiness of these models. To\naddress this, explainability techniques such as GradCAM, SmoothGrad, and LIME\nhave been developed to clarify model decision-making processes. Our research\nfocuses on the robustness of these explanations when models are subjected to\nadversarial attacks, specifically those involving subtle image perturbations\nthat are imperceptible to humans but can significantly mislead models. For\nthis, we utilize attack methods like the Fast Gradient Sign Method (FGSM) and\nthe Basic Iterative Method (BIM) and observe their effects on model accuracy\nand explanations. The results reveal a substantial decline in model accuracy,\nwith accuracies dropping from 89.94% to 58.73% and 45.50% under FGSM and BIM\nattacks, respectively. Despite these declines in accuracy, the explanation of\nthe models measured by metrics such as Intersection over Union (IoU) and Root\nMean Square Error (RMSE) shows negligible changes, suggesting that these\nmetrics may not be sensitive enough to detect the presence of adversarial\nperturbations.\n","authors":["Gazi Nazia Nur","Mohammad Ahnaf Sadat"],"pdf_url":"https://arxiv.org/pdf/2412.11119v1.pdf","comment":"29 pages with reference included, submitted to a journal"},{"id":"http://arxiv.org/abs/2410.19452v3","updated":"2024-12-15T08:24:41Z","published":"2024-10-25T10:28:26Z","title":"NeuroClips: Towards High-fidelity and Smooth fMRI-to-Video\n  Reconstruction","summary":"  Reconstruction of static visual stimuli from non-invasion brain activity fMRI\nachieves great success, owning to advanced deep learning models such as CLIP\nand Stable Diffusion. However, the research on fMRI-to-video reconstruction\nremains limited since decoding the spatiotemporal perception of continuous\nvisual experiences is formidably challenging. We contend that the key to\naddressing these challenges lies in accurately decoding both high-level\nsemantics and low-level perception flows, as perceived by the brain in response\nto video stimuli. To the end, we propose NeuroClips, an innovative framework to\ndecode high-fidelity and smooth video from fMRI. NeuroClips utilizes a\nsemantics reconstructor to reconstruct video keyframes, guiding semantic\naccuracy and consistency, and employs a perception reconstructor to capture\nlow-level perceptual details, ensuring video smoothness. During inference, it\nadopts a pre-trained T2V diffusion model injected with both keyframes and\nlow-level perception flows for video reconstruction. Evaluated on a publicly\navailable fMRI-video dataset, NeuroClips achieves smooth high-fidelity video\nreconstruction of up to 6s at 8FPS, gaining significant improvements over\nstate-of-the-art models in various metrics, e.g., a 128% improvement in SSIM\nand an 81% improvement in spatiotemporal metrics. Our project is available at\nhttps://github.com/gongzix/NeuroClips.\n","authors":["Zixuan Gong","Guangyin Bao","Qi Zhang","Zhongwei Wan","Duoqian Miao","Shoujin Wang","Lei Zhu","Changwei Wang","Rongtao Xu","Liang Hu","Ke Liu","Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.19452v3.pdf","comment":"NeurIPS 2024 Oral"},{"id":"http://arxiv.org/abs/2412.11108v1","updated":"2024-12-15T08:10:39Z","published":"2024-12-15T08:10:39Z","title":"Plug-and-Play Priors as a Score-Based Method","summary":"  Plug-and-play (PnP) methods are extensively used for solving imaging inverse\nproblems by integrating physical measurement models with pre-trained deep\ndenoisers as priors. Score-based diffusion models (SBMs) have recently emerged\nas a powerful framework for image generation by training deep denoisers to\nrepresent the score of the image prior. While both PnP and SBMs use deep\ndenoisers, the score-based nature of PnP is unexplored in the literature due to\nits distinct origins rooted in proximal optimization. This letter introduces a\nnovel view of PnP as a score-based method, a perspective that enables the\nre-use of powerful SBMs within classical PnP algorithms without retraining. We\npresent a set of mathematical relationships for adapting popular SBMs as priors\nwithin PnP. We show that this approach enables a direct comparison between PnP\nand SBM-based reconstruction methods using the same neural network as the\nprior. Code is available at https://github.com/wustl-cig/score_pnp.\n","authors":["Chicago Y. Park","Yuyang Hu","Michael T. McCann","Cristina Garcia-Cardona","Brendt Wohlberg","Ulugbek S. Kamilov"],"pdf_url":"https://arxiv.org/pdf/2412.11108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11106v1","updated":"2024-12-15T08:09:56Z","published":"2024-12-15T08:09:56Z","title":"Unpaired Multi-Domain Histopathology Virtual Staining using Dual Path\n  Prompted Inversion","summary":"  Virtual staining leverages computer-aided techniques to transfer the style of\nhistochemically stained tissue samples to other staining types. In virtual\nstaining of pathological images, maintaining strict structural consistency is\ncrucial, as these images emphasize structural integrity more than natural\nimages. Even slight structural alterations can lead to deviations in diagnostic\nsemantic information. Furthermore, the unpaired characteristic of virtual\nstaining data may compromise the preservation of pathological diagnostic\ncontent. To address these challenges, we propose a dual-path inversion virtual\nstaining method using prompt learning, which optimizes visual prompts to\ncontrol content and style, while preserving complete pathological diagnostic\ncontent. Our proposed inversion technique comprises two key components: (1)\nDual Path Prompted Strategy, we utilize a feature adapter function to generate\nreference images for inversion, providing style templates for input image\ninversion, called Style Target Path. We utilize the inversion of the input\nimage as the Structural Target path, employing visual prompt images to maintain\nstructural consistency in this path while preserving style information from the\nstyle Target path. During the deterministic sampling process, we achieve\ncomplete content-style disentanglement through a plug-and-play embedding visual\nprompt approach. (2) StainPrompt Optimization, where we only optimize the null\nvisual prompt as ``operator'' for dual path inversion, rather than fine-tune\npre-trained model. We optimize null visual prompt for structual and style\ntrajectory around pivotal noise on each timestep, ensuring accurate dual-path\ninversion reconstruction. Extensive evaluations on publicly available\nmulti-domain unpaired staining datasets demonstrate high structural consistency\nand accurate style transfer results.\n","authors":["Bing Xiong","Yue Peng","RanRan Zhang","Fuqiang Chen","JiaYe He","Wenjian Qin"],"pdf_url":"https://arxiv.org/pdf/2412.11106v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11102v1","updated":"2024-12-15T07:49:31Z","published":"2024-12-15T07:49:31Z","title":"Empowering LLMs to Understand and Generate Complex Vector Graphics","summary":"  The unprecedented advancements in Large Language Models (LLMs) have\nprofoundly impacted natural language processing but have yet to fully embrace\nthe realm of scalable vector graphics (SVG) generation. While LLMs encode\npartial knowledge of SVG data from web pages during training, recent findings\nsuggest that semantically ambiguous and tokenized representations within LLMs\nmay result in hallucinations in vector primitive predictions. Additionally, LLM\ntraining typically lacks modeling and understanding of the rendering sequence\nof vector paths, which can lead to occlusion between output vector primitives.\nIn this paper, we present LLM4SVG, an initial yet substantial step toward\nbridging this gap by enabling LLMs to better understand and generate vector\ngraphics. LLM4SVG facilitates a deeper understanding of SVG components through\nlearnable semantic tokens, which precisely encode these tokens and their\ncorresponding properties to generate semantically aligned SVG outputs. Using a\nseries of learnable semantic tokens, a structured dataset for instruction\nfollowing is developed to support comprehension and generation across two\nprimary tasks. Our method introduces a modular architecture to existing large\nlanguage models, integrating semantic tags, vector instruction encoders,\nfine-tuned commands, and powerful LLMs to tightly combine geometric,\nappearance, and language information. To overcome the scarcity of SVG-text\ninstruction data, we developed an automated data generation pipeline that\ncollected a massive dataset of more than 250k SVG data and 580k SVG-text\ninstructions, which facilitated the adoption of the two-stage training strategy\npopular in LLM development. By exploring various training strategies, we\ndeveloped LLM4SVG, which significantly moves beyond optimized rendering-based\napproaches and language-model-based baselines to achieve remarkable results in\nhuman evaluation tasks.\n","authors":["Ximing Xing","Juncheng Hu","Guotao Liang","Jing Zhang","Dong Xu","Qian Yu"],"pdf_url":"https://arxiv.org/pdf/2412.11102v1.pdf","comment":"Project Page: https://ximinng.github.io/LLM4SVGProject/"},{"id":"http://arxiv.org/abs/2412.11100v1","updated":"2024-12-15T07:42:26Z","published":"2024-12-15T07:42:26Z","title":"DynamicScaler: Seamless and Scalable Video Generation for Panoramic\n  Scenes","summary":"  The increasing demand for immersive AR/VR applications and spatial\nintelligence has heightened the need to generate high-quality scene-level and\n360{\\deg} panoramic video. However, most video diffusion models are constrained\nby limited resolution and aspect ratio, which restricts their applicability to\nscene-level dynamic content synthesis. In this work, we propose the\nDynamicScaler, addressing these challenges by enabling spatially scalable and\npanoramic dynamic scene synthesis that preserves coherence across panoramic\nscenes of arbitrary size. Specifically, we introduce a Offset Shifting\nDenoiser, facilitating efficient, synchronous, and coherent denoising panoramic\ndynamic scenes via a diffusion model with fixed resolution through a seamless\nrotating Window, which ensures seamless boundary transitions and consistency\nacross the entire panoramic space, accommodating varying resolutions and aspect\nratios. Additionally, we employ a Global Motion Guidance mechanism to ensure\nboth local detail fidelity and global motion continuity. Extensive experiments\ndemonstrate our method achieves superior content and motion quality in\npanoramic scene-level video generation, offering a training-free, efficient,\nand scalable solution for immersive dynamic scene creation with constant VRAM\nconsumption regardless of the output video resolution. Our project page is\navailable at \\url{https://dynamic-scaler.pages.dev/}.\n","authors":["Jinxiu Liu","Shaoheng Lin","Yinxiao Li","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2412.11100v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11088v1","updated":"2024-12-15T07:15:19Z","published":"2024-12-15T07:15:19Z","title":"Seeing the Forest and the Trees: Solving Visual Graph and Tree Based\n  Data Structure Problems using Large Multimodal Models","summary":"  Recent advancements in generative AI systems have raised concerns about\nacademic integrity among educators. Beyond excelling at solving programming\nproblems and text-based multiple-choice questions, recent research has also\nfound that large multimodal models (LMMs) can solve Parsons problems based only\non an image. However, such problems are still inherently text-based and rely on\nthe capabilities of the models to convert the images of code blocks to their\ncorresponding text. In this paper, we further investigate the capabilities of\nLMMs to solve graph and tree data structure problems based only on images. To\nachieve this, we computationally construct and evaluate a novel benchmark\ndataset comprising 9,072 samples of diverse graph and tree data structure tasks\nto assess the performance of the GPT-4o, GPT-4v, Gemini 1.5 Pro, Gemini 1.5\nFlash, Gemini 1.0 Pro Vision, and Claude 3 model families. GPT-4o and Gemini\n1.5 Flash performed best on trees and graphs respectively. GPT-4o achieved\n87.6% accuracy on tree samples, while Gemini 1.5 Flash, achieved 56.2% accuracy\non graph samples. Our findings highlight the influence of structural and visual\nvariations on model performance. This research not only introduces an LMM\nbenchmark to facilitate replication and further exploration but also\nunderscores the potential of LMMs in solving complex computing problems, with\nimportant implications for pedagogy and assessment practices.\n","authors":["Sebastian Gutierrez","Irene Hou","Jihye Lee","Kenneth Angelikas","Owen Man","Sophia Mettille","James Prather","Paul Denny","Stephen MacNeil"],"pdf_url":"https://arxiv.org/pdf/2412.11088v1.pdf","comment":"14 pages, 4 figures, to be published in ACE 2025"},{"id":"http://arxiv.org/abs/2412.11080v1","updated":"2024-12-15T06:40:22Z","published":"2024-12-15T06:40:22Z","title":"Deep Spectral Clustering via Joint Spectral Embedding and Kmeans","summary":"  Spectral clustering is a popular clustering method. It first maps data into\nthe spectral embedding space and then uses Kmeans to find clusters. However,\nthe two decoupled steps prohibit joint optimization for the optimal solution.\nIn addition, it needs to construct the similarity graph for samples, which\nsuffers from the curse of dimensionality when the data are high-dimensional. To\naddress these two challenges, we introduce \\textbf{D}eep \\textbf{S}pectral\n\\textbf{C}lustering (\\textbf{DSC}), which consists of two main modules: the\nspectral embedding module and the greedy Kmeans module. The former module\nlearns to efficiently embed raw samples into the spectral embedding space using\ndeep neural networks and power iteration. The latter module improves the\ncluster structures of Kmeans on the learned spectral embeddings by a greedy\noptimization strategy, which iteratively reveals the direction of the worst\ncluster structures and optimizes embeddings in this direction. To jointly\noptimize spectral embeddings and clustering, we seamlessly integrate the two\nmodules and optimize them in an end-to-end manner. Experimental results on\nseven real-world datasets demonstrate that DSC achieves state-of-the-art\nclustering performance.\n","authors":["Wengang Guo","Wei Ye"],"pdf_url":"https://arxiv.org/pdf/2412.11080v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11077v1","updated":"2024-12-15T06:22:20Z","published":"2024-12-15T06:22:20Z","title":"Reason-before-Retrieve: One-Stage Reflective Chain-of-Thoughts for\n  Training-Free Zero-Shot Composed Image Retrieval","summary":"  Composed Image Retrieval (CIR) aims to retrieve target images that closely\nresemble a reference image while integrating user-specified textual\nmodifications, thereby capturing user intent more precisely. Existing\ntraining-free zero-shot CIR (ZS-CIR) methods often employ a two-stage process:\nthey first generate a caption for the reference image and then use Large\nLanguage Models for reasoning to obtain a target description. However, these\nmethods suffer from missing critical visual details and limited reasoning\ncapabilities, leading to suboptimal retrieval performance. To address these\nchallenges, we propose a novel, training-free one-stage method, One-Stage\nReflective Chain-of-Thought Reasoning for ZS-CIR (OSrCIR), which employs\nMultimodal Large Language Models to retain essential visual information in a\nsingle-stage reasoning process, eliminating the information loss seen in\ntwo-stage methods. Our Reflective Chain-of-Thought framework further improves\ninterpretative accuracy by aligning manipulation intent with contextual cues\nfrom reference images. OSrCIR achieves performance gains of 1.80% to 6.44% over\nexisting training-free methods across multiple tasks, setting new\nstate-of-the-art results in ZS-CIR and enhancing its utility in vision-language\napplications. Our code will be available at\nhttps://github.com/Pter61/osrcir2024/.\n","authors":["Yuanmin Tang","Xiaoting Qin","Jue Zhang","Jing Yu","Gaopeng Gou","Gang Xiong","Qingwei Ling","Saravan Rajmohan","Dongmei Zhang","Qi Wu"],"pdf_url":"https://arxiv.org/pdf/2412.11077v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11076v1","updated":"2024-12-15T06:20:41Z","published":"2024-12-15T06:20:41Z","title":"MoRe: Class Patch Attention Needs Regularization for Weakly Supervised\n  Semantic Segmentation","summary":"  Weakly Supervised Semantic Segmentation (WSSS) with image-level labels\ntypically uses Class Activation Maps (CAM) to achieve dense predictions.\nRecently, Vision Transformer (ViT) has provided an alternative to generate\nlocalization maps from class-patch attention. However, due to insufficient\nconstraints on modeling such attention, we observe that the Localization\nAttention Maps (LAM) often struggle with the artifact issue, i.e., patch\nregions with minimal semantic relevance are falsely activated by class tokens.\nIn this work, we propose MoRe to address this issue and further explore the\npotential of LAM. Our findings suggest that imposing additional regularization\non class-patch attention is necessary. To this end, we first view the attention\nas a novel directed graph and propose the Graph Category Representation module\nto implicitly regularize the interaction among class-patch entities. It ensures\nthat class tokens dynamically condense the related patch information and\nsuppress unrelated artifacts at a graph level. Second, motivated by the\nobservation that CAM from classification weights maintains smooth localization\nof objects, we devise the Localization-informed Regularization module to\nexplicitly regularize the class-patch attention. It directly mines the token\nrelations from CAM and further supervises the consistency between class and\npatch tokens in a learnable manner. Extensive experiments are conducted on\nPASCAL VOC and MS COCO, validating that MoRe effectively addresses the artifact\nissue and achieves state-of-the-art performance, surpassing recent single-stage\nand even multi-stage methods. Code is available at\nhttps://github.com/zwyang6/MoRe.\n","authors":["Zhiwei Yang","Yucong Meng","Kexue Fu","Shuo Wang","Zhijian Song"],"pdf_url":"https://arxiv.org/pdf/2412.11076v1.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2408.10593v2","updated":"2024-12-15T06:18:53Z","published":"2024-08-20T07:10:40Z","title":"An Efficient Sign Language Translation Using Spatial Configuration and\n  Motion Dynamics with LLMs","summary":"  Gloss-free Sign Language Translation (SLT) converts sign videos directly into\nspoken language sentences without relying on glosses. Recently, Large Language\nModels (LLMs) have shown remarkable translation performance in gloss-free\nmethods by harnessing their powerful natural language generation capabilities.\nHowever, these methods often rely on domain-specific fine-tuning of visual\nencoders to achieve optimal results. By contrast, this paper emphasizes the\nimportance of capturing the spatial configurations and motion dynamics inherent\nin sign language. With this in mind, we introduce Spatial and Motion-based Sign\nLanguage Translation (SpaMo), a novel LLM-based SLT framework. The core idea of\nSpaMo is simple yet effective. We first extract spatial and motion features\nusing off-the-shelf visual encoders and then input these features into an LLM\nwith a language prompt. Additionally, we employ a visual-text alignment process\nas a warm-up before the SLT supervision. Our experiments demonstrate that SpaMo\nachieves state-of-the-art performance on two popular datasets, PHOENIX14T and\nHow2Sign.\n","authors":["Eui Jun Hwang","Sukmin Cho","Junmyeong Lee","Jong C. Park"],"pdf_url":"https://arxiv.org/pdf/2408.10593v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2412.11074v1","updated":"2024-12-15T06:14:55Z","published":"2024-12-15T06:14:55Z","title":"Adapter-Enhanced Semantic Prompting for Continual Learning","summary":"  Continual learning (CL) enables models to adapt to evolving data streams. A\nmajor challenge of CL is catastrophic forgetting, where new knowledge will\noverwrite previously acquired knowledge. Traditional methods usually retain the\npast data for replay or add additional branches in the model to learn new\nknowledge, which has high memory requirements. In this paper, we propose a\nnovel lightweight CL framework, Adapter-Enhanced Semantic Prompting (AESP),\nwhich integrates prompt tuning and adapter techniques. Specifically, we design\nsemantic-guided prompts to enhance the generalization ability of visual\nfeatures and utilize adapters to efficiently fuse the semantic information,\naiming to learn more adaptive features for the continual learning task.\nFurthermore, to choose the right task prompt for feature adaptation, we have\ndeveloped a novel matching mechanism for prompt selection. Extensive\nexperiments on three CL datasets demonstrate that our approach achieves\nfavorable performance across multiple metrics, showing its potential for\nadvancing CL.\n","authors":["Baocai Yin","Ji Zhao","Huajie Jiang","Ningning Hou","Yongli Hu","Amin Beheshti","Ming-Hsuan Yang","Yuankai Qi"],"pdf_url":"https://arxiv.org/pdf/2412.11074v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17735v3","updated":"2024-12-15T06:10:41Z","published":"2024-11-23T09:57:43Z","title":"3D-Mem: 3D Scene Memory for Embodied Exploration and Reasoning","summary":"  Constructing compact and informative 3D scene representations is essential\nfor effective embodied exploration and reasoning, especially in complex\nenvironments over extended periods. Existing representations, such as\nobject-centric 3D scene graphs, oversimplify spatial relationships by modeling\nscenes as isolated objects with restrictive textual relationships, making it\ndifficult to address queries requiring nuanced spatial understanding. Moreover,\nthese representations lack natural mechanisms for active exploration and memory\nmanagement, hindering their application to lifelong autonomy. In this work, we\npropose 3D-Mem, a novel 3D scene memory framework for embodied agents. 3D-Mem\nemploys informative multi-view images, termed Memory Snapshots, to represent\nthe scene and capture rich visual information of explored regions. It further\nintegrates frontier-based exploration by introducing Frontier\nSnapshots-glimpses of unexplored areas-enabling agents to make informed\ndecisions by considering both known and potential new information. To support\nlifelong memory in active exploration settings, we present an incremental\nconstruction pipeline for 3D-Mem, as well as a memory retrieval technique for\nmemory management. Experimental results on three benchmarks demonstrate that\n3D-Mem significantly enhances agents' exploration and reasoning capabilities in\n3D environments, highlighting its potential for advancing applications in\nembodied AI.\n","authors":["Yuncong Yang","Han Yang","Jiachen Zhou","Peihao Chen","Hongxin Zhang","Yilun Du","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2411.17735v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11070v1","updated":"2024-12-15T06:04:16Z","published":"2024-12-15T06:04:16Z","title":"HC-LLM: Historical-Constrained Large Language Models for Radiology\n  Report Generation","summary":"  Radiology report generation (RRG) models typically focus on individual exams,\noften overlooking the integration of historical visual or textual data, which\nis crucial for patient follow-ups. Traditional methods usually struggle with\nlong sequence dependencies when incorporating historical information, but large\nlanguage models (LLMs) excel at in-context learning, making them well-suited\nfor analyzing longitudinal medical data. In light of this, we propose a novel\nHistorical-Constrained Large Language Models (HC-LLM) framework for RRG,\nempowering LLMs with longitudinal report generation capabilities by\nconstraining the consistency and differences between longitudinal images and\ntheir corresponding reports. Specifically, our approach extracts both\ntime-shared and time-specific features from longitudinal chest X-rays and\ndiagnostic reports to capture disease progression. Then, we ensure consistent\nrepresentation by applying intra-modality similarity constraints and aligning\nvarious features across modalities with multimodal contrastive and structural\nconstraints. These combined constraints effectively guide the LLMs in\ngenerating diagnostic reports that accurately reflect the progression of the\ndisease, achieving state-of-the-art results on the Longitudinal-MIMIC dataset.\nNotably, our approach performs well even without historical data during testing\nand can be easily adapted to other multimodal large models, enhancing its\nversatility.\n","authors":["Tengfei Liu","Jiapu Wang","Yongli Hu","Mingjie Li","Junfei Yi","Xiaojun Chang","Junbin Gao","Baocai Yin"],"pdf_url":"https://arxiv.org/pdf/2412.11070v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.11067v1","updated":"2024-12-15T05:57:36Z","published":"2024-12-15T05:57:36Z","title":"CFSynthesis: Controllable and Free-view 3D Human Video Synthesis","summary":"  Human video synthesis aims to create lifelike characters in various\nenvironments, with wide applications in VR, storytelling, and content creation.\nWhile 2D diffusion-based methods have made significant progress, they struggle\nto generalize to complex 3D poses and varying scene backgrounds. To address\nthese limitations, we introduce CFSynthesis, a novel framework for generating\nhigh-quality human videos with customizable attributes, including identity,\nmotion, and scene configurations. Our method leverages a texture-SMPL-based\nrepresentation to ensure consistent and stable character appearances across\nfree viewpoints. Additionally, we introduce a novel foreground-background\nseparation strategy that effectively decomposes the scene as foreground and\nbackground, enabling seamless integration of user-defined backgrounds.\nExperimental results on multiple datasets show that CFSynthesis not only\nachieves state-of-the-art performance in complex human animations but also\nadapts effectively to 3D motions in free-view and user-specified scenarios.\n","authors":["Cui Liyuan","Xu Xiaogang","Dong Wenqi","Yang Zesong","Bao Hujun","Cui Zhaopeng"],"pdf_url":"https://arxiv.org/pdf/2412.11067v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17874v2","updated":"2024-12-15T05:38:25Z","published":"2023-11-29T18:20:16Z","title":"FisherRF: Active View Selection and Uncertainty Quantification for\n  Radiance Fields using Fisher Information","summary":"  This study addresses the challenging problem of active view selection and\nuncertainty quantification within the domain of Radiance Fields. Neural\nRadiance Fields (NeRF) have greatly advanced image rendering and\nreconstruction, but the cost of acquiring images poses the need to select the\nmost informative viewpoints efficiently. Existing approaches depend on\nmodifying the model architecture or hypothetical perturbation field to\nindirectly approximate the model uncertainty. However, selecting views from\nindirect approximation does not guarantee optimal information gain for the\nmodel. By leveraging Fisher Information, we directly quantify observed\ninformation on the parameters of Radiance Fields and select candidate views by\nmaximizing the Expected Information Gain(EIG). Our method achieves\nstate-of-the-art results on multiple tasks, including view selection, active\nmapping, and uncertainty quantification, demonstrating its potential to advance\nthe field of Radiance Fields.\n","authors":["Wen Jiang","Boshu Lei","Kostas Daniilidis"],"pdf_url":"https://arxiv.org/pdf/2311.17874v2.pdf","comment":"Project page: https://jiangwenpl.github.io/FisherRF/"},{"id":"http://arxiv.org/abs/2412.11061v1","updated":"2024-12-15T05:33:10Z","published":"2024-12-15T05:33:10Z","title":"Classification Drives Geographic Bias in Street Scene Segmentation","summary":"  Previous studies showed that image datasets lacking geographic diversity can\nlead to biased performance in models trained on them. While earlier work\nstudied general-purpose image datasets (e.g., ImageNet) and simple tasks like\nimage recognition, we investigated geo-biases in real-world driving datasets on\na more complex task: instance segmentation. We examined if instance\nsegmentation models trained on European driving scenes (Eurocentric models) are\ngeo-biased. Consistent with previous work, we found that Eurocentric models\nwere geo-biased. Interestingly, we found that geo-biases came from\nclassification errors rather than localization errors, with classification\nerrors alone contributing 10-90% of the geo-biases in segmentation and 19-88%\nof the geo-biases in detection. This showed that while classification is\ngeo-biased, localization (including detection and segmentation) is\ngeographically robust. Our findings show that in region-specific models (e.g.,\nEurocentric models), geo-biases from classification errors can be significantly\nmitigated by using coarser classes (e.g., grouping car, bus, and truck as\n4-wheeler).\n","authors":["Rahul Nair","Gabriel Tseng","Esther Rolf","Bhanu Tokas","Hannah Kerner"],"pdf_url":"https://arxiv.org/pdf/2412.11061v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11060v1","updated":"2024-12-15T05:32:54Z","published":"2024-12-15T05:32:54Z","title":"Making Bias Amplification in Balanced Datasets Directional and\n  Interpretable","summary":"  Most of the ML datasets we use today are biased. When we train models on\nthese biased datasets, they often not only learn dataset biases but can also\namplify them -- a phenomenon known as bias amplification. Several\nco-occurrence-based metrics have been proposed to measure bias amplification\nbetween a protected attribute A (e.g., gender) and a task T (e.g., cooking).\nHowever, these metrics fail to measure biases when A is balanced with T. To\nmeasure bias amplification in balanced datasets, recent work proposed a\npredictability-based metric called leakage amplification. However, leakage\namplification cannot identify the direction in which biases are amplified. In\nthis work, we propose a new predictability-based metric called directional\npredictability amplification (DPA). DPA measures directional bias\namplification, even for balanced datasets. Unlike leakage amplification, DPA is\neasier to interpret and less sensitive to attacker models (a hyperparameter in\npredictability-based metrics). Our experiments on tabular and image datasets\nshow that DPA is an effective metric for measuring directional bias\namplification. The code will be available soon.\n","authors":["Bhanu Tokas","Rahul Nair","Hannah Kerner"],"pdf_url":"https://arxiv.org/pdf/2412.11060v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11058v1","updated":"2024-12-15T05:29:07Z","published":"2024-12-15T05:29:07Z","title":"SHMT: Self-supervised Hierarchical Makeup Transfer via Latent Diffusion\n  Models","summary":"  This paper studies the challenging task of makeup transfer, which aims to\napply diverse makeup styles precisely and naturally to a given facial image.\nDue to the absence of paired data, current methods typically synthesize\nsub-optimal pseudo ground truths to guide the model training, resulting in low\nmakeup fidelity. Additionally, different makeup styles generally have varying\neffects on the person face, but existing methods struggle to deal with this\ndiversity. To address these issues, we propose a novel Self-supervised\nHierarchical Makeup Transfer (SHMT) method via latent diffusion models.\nFollowing a \"decoupling-and-reconstruction\" paradigm, SHMT works in a\nself-supervised manner, freeing itself from the misguidance of imprecise\npseudo-paired data. Furthermore, to accommodate a variety of makeup styles,\nhierarchical texture details are decomposed via a Laplacian pyramid and\nselectively introduced to the content representation. Finally, we design a\nnovel Iterative Dual Alignment (IDA) module that dynamically adjusts the\ninjection condition of the diffusion model, allowing the alignment errors\ncaused by the domain gap between content and makeup representations to be\ncorrected. Extensive quantitative and qualitative analyses demonstrate the\neffectiveness of our method. Our code is available at\n\\url{https://github.com/Snowfallingplum/SHMT}.\n","authors":["Zhaoyang Sun","Shengwu Xiong","Yaxiong Chen","Fei Du","Weihua Chen","Fan Wang","Yi Rong"],"pdf_url":"https://arxiv.org/pdf/2412.11058v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2409.13983v2","updated":"2024-12-15T05:18:36Z","published":"2024-09-21T02:23:01Z","title":"Multilateral Cascading Network for Semantic Segmentation of Large-Scale\n  Outdoor Point Clouds","summary":"  Semantic segmentation of large-scale outdoor point clouds is of significant\nimportance in environment perception and scene understanding. However, this\ntask continues to present a significant research challenge, due to the inherent\ncomplexity of outdoor objects and their diverse distributions in real-world\nenvironments. In this study, we propose the Multilateral Cascading Network\n(MCNet) designed to address this challenge. The model comprises two key\ncomponents: a Multilateral Cascading Attention Enhancement (MCAE) module, which\nfacilitates the learning of complex local features through multilateral\ncascading operations; and a Point Cross Stage Partial (P-CSP) module, which\nfuses global and local features, thereby optimizing the integration of valuable\nfeature information across multiple scales. Our proposed method demonstrates\nsuperior performance relative to state-of-the-art approaches across two widely\nrecognized benchmark datasets: Toronto3D and SensatUrban. Especially on the\ncity-scale SensatUrban dataset, our results surpassed the current best result\nby 2.1\\% in overall mIoU and yielded an improvement of 15.9\\% on average for\nsmall-sample object categories comprising less than 2\\% of the total samples,\nin comparison to the baseline method.\n","authors":["Haoran Gong","Haodong Wang","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2409.13983v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11056v1","updated":"2024-12-15T05:18:01Z","published":"2024-12-15T05:18:01Z","title":"Overview of TREC 2024 Medical Video Question Answering (MedVidQA) Track","summary":"  One of the key goals of artificial intelligence (AI) is the development of a\nmultimodal system that facilitates communication with the visual world (image\nand video) using a natural language query. Earlier works on medical question\nanswering primarily focused on textual and visual (image) modalities, which may\nbe inefficient in answering questions requiring demonstration. In recent years,\nsignificant progress has been achieved due to the introduction of large-scale\nlanguage-vision datasets and the development of efficient deep neural\ntechniques that bridge the gap between language and visual understanding.\nImprovements have been made in numerous vision-and-language tasks, such as\nvisual captioning visual question answering, and natural language video\nlocalization. Most of the existing work on language vision focused on creating\ndatasets and developing solutions for open-domain applications. We believe\nmedical videos may provide the best possible answers to many first aid, medical\nemergency, and medical education questions. With increasing interest in AI to\nsupport clinical decision-making and improve patient engagement, there is a\nneed to explore such challenges and develop efficient algorithms for medical\nlanguage-video understanding and generation. Toward this, we introduced new\ntasks to foster research toward designing systems that can understand medical\nvideos to provide visual answers to natural language questions, and are\nequipped with multimodal capability to generate instruction steps from the\nmedical video. These tasks have the potential to support the development of\nsophisticated downstream applications that can benefit the public and medical\nprofessionals.\n","authors":["Deepak Gupta","Dina Demner-Fushman"],"pdf_url":"https://arxiv.org/pdf/2412.11056v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13896v3","updated":"2024-12-15T05:13:26Z","published":"2024-08-25T17:33:40Z","title":"HTS-Attack: Heuristic Token Search for Jailbreaking Text-to-Image Models","summary":"  Text-to-Image(T2I) models have achieved remarkable success in image\ngeneration and editing, yet these models still have many potential issues,\nparticularly in generating inappropriate or Not-Safe-For-Work(NSFW) content.\nStrengthening attacks and uncovering such vulnerabilities can advance the\ndevelopment of reliable and practical T2I models. Most of the previous works\ntreat T2I models as white-box systems, using gradient optimization to generate\nadversarial prompts. However, accessing the model's gradient is often\nimpossible in real-world scenarios. Moreover, existing defense methods, those\nusing gradient masking, are designed to prevent attackers from obtaining\naccurate gradient information. While several black-box jailbreak attacks have\nbeen explored, they achieve the limited performance of jailbreaking T2I models\ndue to difficulties associated with optimization in discrete spaces. To address\nthis, we propose HTS-Attack, a heuristic token search attack method. HTS-Attack\nbegins with an initialization that removes sensitive tokens, followed by a\nheuristic search where high-performing candidates are recombined and mutated.\nThis process generates a new pool of candidates, and the optimal adversarial\nprompt is updated based on their effectiveness. By incorporating both optimal\nand suboptimal candidates, HTS-Attack avoids local optima and improves\nrobustness in bypassing defenses. Extensive experiments validate the\neffectiveness of our method in attacking the latest prompt checkers, post-hoc\nimage checkers, securely trained T2I models, and online commercial models.\n","authors":["Sensen Gao","Xiaojun Jia","Yihao Huang","Ranjie Duan","Jindong Gu","Yang Bai","Yang Liu","Qing Guo"],"pdf_url":"https://arxiv.org/pdf/2408.13896v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11050v1","updated":"2024-12-15T04:51:30Z","published":"2024-12-15T04:51:30Z","title":"RAC3: Retrieval-Augmented Corner Case Comprehension for Autonomous\n  Driving with Vision-Language Models","summary":"  Understanding and addressing corner cases is essential for ensuring the\nsafety and reliability of autonomous driving systems. Vision-Language Models\n(VLMs) play a crucial role in enhancing scenario comprehension, yet they face\nsignificant challenges, such as hallucination and insufficient real-world\ngrounding, which compromise their performance in critical driving scenarios. In\nthis work, we propose RAC3, a novel framework designed to improve VLMs' ability\nto handle corner cases effectively. The framework integrates\nRetrieval-Augmented Generation (RAG) to mitigate hallucination by dynamically\nincorporating context-specific external knowledge. A cornerstone of RAC3 is its\ncross-modal alignment fine-tuning, which utilizes contrastive learning to embed\nimage-text pairs into a unified semantic space, enabling robust retrieval of\nsimilar scenarios. We evaluate RAC3 through extensive experiments using a\ncurated dataset of corner case scenarios, demonstrating its ability to enhance\nsemantic alignment, improve hallucination mitigation, and achieve superior\nperformance metrics, such as Cosine Similarity and ROUGE-L scores. For example,\nfor the LLaVA-v1.6-34B VLM, the cosine similarity between the generated text\nand the reference text has increased by 5.22\\%. The F1-score in ROUGE-L has\nincreased by 39.91\\%, the Precision has increased by 55.80\\%, and the Recall\nhas increased by 13.74\\%. This work underscores the potential of\nretrieval-augmented VLMs to advance the robustness and safety of autonomous\ndriving in complex environments.\n","authors":["Yujin Wang","Quanfeng Liu","Jiaqi Fan","Jinlong Hong","Hongqing Chu","Mengjian Tian","Bingzhao Gao","Hong Chen"],"pdf_url":"https://arxiv.org/pdf/2412.11050v1.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2412.11045v1","updated":"2024-12-15T04:06:39Z","published":"2024-12-15T04:06:39Z","title":"Facial Surgery Preview Based on the Orthognathic Treatment Prediction","summary":"  Orthognathic surgery consultation is essential to help patients understand\nthe changes to their facial appearance after surgery. However, current\nvisualization methods are often inefficient and inaccurate due to limited pre-\nand post-treatment data and the complexity of the treatment. To overcome these\nchallenges, this study aims to develop a fully automated pipeline that\ngenerates accurate and efficient 3D previews of postsurgical facial appearances\nfor patients with orthognathic treatment without requiring additional medical\nimages. The study introduces novel aesthetic losses, such as mouth-convexity\nand asymmetry losses, to improve the accuracy of facial surgery prediction.\nAdditionally, it proposes a specialized parametric model for 3D reconstruction\nof the patient, medical-related losses to guide latent code prediction network\noptimization, and a data augmentation scheme to address insufficient data. The\nstudy additionally employs FLAME, a parametric model, to enhance the quality of\nfacial appearance previews by extracting facial latent codes and establishing\ndense correspondences between pre- and post-surgery geometries. Quantitative\ncomparisons showed the algorithm's effectiveness, and qualitative results\nhighlighted accurate facial contour and detail predictions. A user study\nconfirmed that doctors and the public could not distinguish between machine\nlearning predictions and actual postoperative results. This study aims to offer\na practical, effective solution for orthognathic surgery consultations,\nbenefiting doctors and patients.\n","authors":["Huijun Han","Congyi Zhang","Lifeng Zhu","Pradeep Singh","Richard Tai Chiu Hsung","Yiu Yan Leung","Taku Komura","Wenping Wang","Min Gu"],"pdf_url":"https://arxiv.org/pdf/2412.11045v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2310.07707v2","updated":"2024-12-15T03:45:36Z","published":"2023-10-11T17:57:14Z","title":"MatFormer: Nested Transformer for Elastic Inference","summary":"  Foundation models are applied in a broad spectrum of settings with different\ninference constraints, from massive multi-accelerator clusters to\nresource-constrained standalone mobile devices. However, the substantial costs\nassociated with training these models often limit the number of unique model\nsizes that can be offered. Consequently, practitioners are compelled to select\na model that may not be optimally aligned with their specific latency and cost\nrequirements. We present MatFormer, a novel Transformer architecture designed\nto provide elastic inference across diverse deployment constraints. MatFormer\nachieves this by incorporating a nested Feed Forward Network (FFN) block\nstructure within a standard Transformer model. During training, we optimize the\nparameters of multiple nested FFN blocks with varying sizes, enabling the\nextraction of hundreds of accurate smaller models without incurring additional\ncomputational costs. We empirically validate the efficacy of MatFormer across\ndifferent model classes (decoders and encoders) and modalities (language and\nvision), demonstrating its potential for real-world deployment. We show that a\n850M decoder-only MatFormer language model (MatLM) allows us to extract\nmultiple smaller models spanning from 582M to 850M parameters, each exhibiting\nbetter validation loss and one-shot downstream evaluations than independently\ntrained counterparts. Furthermore, we observe that smaller encoders extracted\nfrom a universal MatFormer-based ViT (MatViT) encoder preserve the metric-space\nstructure for adaptive large-scale retrieval. Finally, we showcase that\nspeculative decoding with the accurate and consistent submodels extracted from\nMatFormer can lead to significant reduction in inference latency. Project\nwebsite: https://devvrit.github.io/matformer/\n","authors":[" Devvrit","Sneha Kudugunta","Aditya Kusupati","Tim Dettmers","Kaifeng Chen","Inderjit Dhillon","Yulia Tsvetkov","Hannaneh Hajishirzi","Sham Kakade","Ali Farhadi","Prateek Jain"],"pdf_url":"https://arxiv.org/pdf/2310.07707v2.pdf","comment":"30 pages, 11 figures, first three authors contributed equally.\n  NeurIPS, 2024"},{"id":"http://arxiv.org/abs/2412.11039v1","updated":"2024-12-15T03:35:00Z","published":"2024-12-15T03:35:00Z","title":"A Digitalized Atlas for Pulmonary Airway","summary":"  In this work, we proposed AirwayAtlas, which is an end-to-end pipeline for\nautomatic extraction of airway anatomies with lobar, segmental and subsegmental\nlabeling. A compact representation, AirwaySign, is generated based on diverse\nfeatures of airway branches. Experiments on multi-center datasets validated the\neffectiveness of AirwayAtlas. We also demonstrated that AirwaySign is a\npowerful tool for correlation analysis on pulmonary diseases.\n","authors":["Minghui Zhang","Chenyu Li","Hanxiao Zhang","Yaoyu Liu","Yun Gu"],"pdf_url":"https://arxiv.org/pdf/2412.11039v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2412.08913v2","updated":"2024-12-15T03:23:27Z","published":"2024-12-12T03:51:50Z","title":"Sensing for Space Safety and Sustainability: A Deep Learning Approach\n  with Vision Transformers","summary":"  The rapid increase of space assets represented by small satellites in low\nEarth orbit can enable ubiquitous digital services for everyone. However, due\nto the dynamic space environment, numerous space objects, complex atmospheric\nconditions, and unexpected events can easily introduce adverse conditions\naffecting space safety, operations, and sustainability of the outer space\nenvironment. This challenge calls for responsive, effective satellite object\ndetection (SOD) solutions that allow a small satellite to assess and respond to\ncollision risks, with the consideration of constrained resources on a small\nsatellite platform. This paper discusses the SOD tasks and onboard deep\nlearning (DL) approach to the tasks. Two new DL models are proposed, called\nGELAN-ViT and GELAN-RepViT, which incorporate vision transformer (ViT) into the\nGeneralized Efficient Layer Aggregation Network (GELAN) architecture and\naddress limitations by separating the convolutional neural network and ViT\npaths. These models outperform the state-of-the-art YOLOv9-t in terms of mean\naverage precision (mAP) and computational costs. On the SOD dataset, our\nproposed models can achieve around 95% mAP50 with giga-floating point\noperations (GFLOPs) reduced by over 5.0. On the VOC 2012 dataset, they can\nachieve $\\geq$ 60.7% mAP50 with GFLOPs reduced by over 5.2.\n","authors":["Wenxuan Zhang","Peng Hu"],"pdf_url":"https://arxiv.org/pdf/2412.08913v2.pdf","comment":"To be published in the 12th Annual IEEE International Conference on\n  Wireless for Space and Extreme Environments (WiSEE 2024)"},{"id":"http://arxiv.org/abs/2412.11034v1","updated":"2024-12-15T03:11:41Z","published":"2024-12-15T03:11:41Z","title":"SAM-IF: Leveraging SAM for Incremental Few-Shot Instance Segmentation","summary":"  We propose SAM-IF, a novel method for incremental few-shot instance\nsegmentation leveraging the Segment Anything Model (SAM). SAM-IF addresses the\nchallenges of class-agnostic instance segmentation by introducing a multi-class\nclassifier and fine-tuning SAM to focus on specific target objects. To enhance\nfew-shot learning capabilities, SAM-IF employs a cosine-similarity-based\nclassifier, enabling efficient adaptation to novel classes with minimal data.\nAdditionally, SAM-IF supports incremental learning by updating classifier\nweights without retraining the decoder. Our method achieves competitive but\nmore reasonable results compared to existing approaches, particularly in\nscenarios requiring specific object segmentation with limited labeled data.\n","authors":["Xudong Zhou","Wenhao He"],"pdf_url":"https://arxiv.org/pdf/2412.11034v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11033v1","updated":"2024-12-15T03:06:22Z","published":"2024-12-15T03:06:22Z","title":"AURORA: Automated Unleash of 3D Room Outlines for VR Applications","summary":"  Creating realistic VR experiences is challenging due to the labor-intensive\nprocess of accurately replicating real-world details into virtual scenes,\nhighlighting the need for automated methods that maintain spatial accuracy and\nprovide design flexibility. In this paper, we propose AURORA, a novel method\nthat leverages RGB-D images to automatically generate both purely virtual\nreality (VR) scenes and VR scenes combined with real-world elements. This\napproach can benefit designers by streamlining the process of converting\nreal-world details into virtual scenes. AURORA integrates advanced techniques\nin image processing, segmentation, and 3D reconstruction to efficiently create\nrealistic and detailed interior designs from real-world environments. The\ndesign of this integration ensures optimal performance and precision,\naddressing key challenges in automated indoor design generation by uniquely\ncombining and leveraging the strengths of foundation models. We demonstrate the\neffectiveness of our approach through experiments, both on self-captured data\nand public datasets, showcasing its potential to enhance virtual reality (VR)\napplications by providing interior designs that conform to real-world\npositioning.\n","authors":["Huijun Han","Yongqing Liang","Yuanlong Zhou","Wenping Wang","Edgar J. Rojas-Munoz","Xin Li"],"pdf_url":"https://arxiv.org/pdf/2412.11033v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2411.09502v3","updated":"2024-12-15T02:48:48Z","published":"2024-11-14T15:13:13Z","title":"Golden Noise for Diffusion Models: A Learning Framework","summary":"  Text-to-image diffusion model is a popular paradigm that synthesizes\npersonalized images by providing a text prompt and a random Gaussian noise.\nWhile people observe that some noises are ``golden noises'' that can achieve\nbetter text-image alignment and higher human preference than others, we still\nlack a machine learning framework to obtain those golden noises. To learn\ngolden noises for diffusion sampling, we mainly make three contributions in\nthis paper. First, we identify a new concept termed the \\textit{noise prompt},\nwhich aims at turning a random Gaussian noise into a golden noise by adding a\nsmall desirable perturbation derived from the text prompt. Following the\nconcept, we first formulate the \\textit{noise prompt learning} framework that\nsystematically learns ``prompted'' golden noise associated with a text prompt\nfor diffusion models. Second, we design a noise prompt data collection pipeline\nand collect a large-scale \\textit{noise prompt dataset}~(NPD) that contains\n100k pairs of random noises and golden noises with the associated text prompts.\nWith the prepared NPD as the training dataset, we trained a small \\textit{noise\nprompt network}~(NPNet) that can directly learn to transform a random noise\ninto a golden noise. The learned golden noise perturbation can be considered as\na kind of prompt for noise, as it is rich in semantic information and tailored\nto the given text prompt. Third, our extensive experiments demonstrate the\nimpressive effectiveness and generalization of NPNet on improving the quality\nof synthesized images across various diffusion models, including SDXL,\nDreamShaper-xl-v2-turbo, and Hunyuan-DiT. Moreover, NPNet is a small and\nefficient controller that acts as a plug-and-play module with very limited\nadditional inference and computational costs, as it just provides a golden\nnoise instead of a random noise without accessing the original pipeline.\n","authors":["Zikai Zhou","Shitong Shao","Lichen Bai","Zhiqiang Xu","Bo Han","Zeke Xie"],"pdf_url":"https://arxiv.org/pdf/2411.09502v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11026v1","updated":"2024-12-15T02:41:31Z","published":"2024-12-15T02:41:31Z","title":"SceneLLM: Implicit Language Reasoning in LLM for Dynamic Scene Graph\n  Generation","summary":"  Dynamic scenes contain intricate spatio-temporal information, crucial for\nmobile robots, UAVs, and autonomous driving systems to make informed decisions.\nParsing these scenes into semantic triplets <Subject-Predicate-Object> for\naccurate Scene Graph Generation (SGG) is highly challenging due to the\nfluctuating spatio-temporal complexity. Inspired by the reasoning capabilities\nof Large Language Models (LLMs), we propose SceneLLM, a novel framework that\nleverages LLMs as powerful scene analyzers for dynamic SGG. Our framework\nintroduces a Video-to-Language (V2L) mapping module that transforms video\nframes into linguistic signals (scene tokens), making the input more\ncomprehensible for LLMs. To better encode spatial information, we devise a\nSpatial Information Aggregation (SIA) scheme, inspired by the structure of\nChinese characters, which encodes spatial data into tokens. Using Optimal\nTransport (OT), we generate an implicit language signal from the frame-level\ntoken sequence that captures the video's spatio-temporal information. To\nfurther improve the LLM's ability to process this implicit linguistic input, we\napply Low-Rank Adaptation (LoRA) to fine-tune the model. Finally, we use a\ntransformer-based SGG predictor to decode the LLM's reasoning and predict\nsemantic triplets. Our method achieves state-of-the-art results on the Action\nGenome (AG) benchmark, and extensive experiments show the effectiveness of\nSceneLLM in understanding and generating accurate dynamic scene graphs.\n","authors":["Hang Zhang","Zhuoling Li","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2412.11026v1.pdf","comment":"27 pages, 4 figures"},{"id":"http://arxiv.org/abs/2412.11025v1","updated":"2024-12-15T02:37:20Z","published":"2024-12-15T02:37:20Z","title":"From Simple to Professional: A Combinatorial Controllable Image\n  Captioning Agent","summary":"  The Controllable Image Captioning Agent (CapAgent) is an innovative system\ndesigned to bridge the gap between user simplicity and professional-level\noutputs in image captioning tasks. CapAgent automatically transforms\nuser-provided simple instructions into detailed, professional instructions,\nenabling precise and context-aware caption generation. By leveraging multimodal\nlarge language models (MLLMs) and external tools such as object detection tool\nand search engines, the system ensures that captions adhere to specified\nguidelines, including sentiment, keywords, focus, and formatting. CapAgent\ntransparently controls each step of the captioning process, and showcases its\nreasoning and tool usage at every step, fostering user trust and engagement.\nThe project code is available at https://github.com/xin-ran-w/CapAgent.\n","authors":["Xinran Wang","Muxi Diao","Baoteng Li","Haiwen Zhang","Kongming Liang","Zhanyu Ma"],"pdf_url":"https://arxiv.org/pdf/2412.11025v1.pdf","comment":"A technical report. Project: https://github.com/xin-ran-w/CapAgent"},{"id":"http://arxiv.org/abs/2412.11024v1","updated":"2024-12-15T02:35:31Z","published":"2024-12-15T02:35:31Z","title":"Exploring Diffusion and Flow Matching Under Generator Matching","summary":"  In this paper, we present a comprehensive theoretical comparison of diffusion\nand flow matching under the Generator Matching framework. Despite their\napparent differences, both diffusion and flow matching can be viewed under the\nunified framework of Generator Matching. By recasting both diffusion and flow\nmatching under the same generative Markov framework, we provide theoretical\ninsights into why flow matching models can be more robust empirically and how\nnovel model classes can be constructed by mixing deterministic and stochastic\ncomponents. Our analysis offers a fresh perspective on the relationships\nbetween state-of-the-art generative modeling paradigms.\n","authors":["Zeeshan Patel","James DeLoye","Lance Mathias"],"pdf_url":"https://arxiv.org/pdf/2412.11024v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11023v1","updated":"2024-12-15T02:33:37Z","published":"2024-12-15T02:33:37Z","title":"Exploring Enhanced Contextual Information for Video-Level Object\n  Tracking","summary":"  Contextual information at the video level has become increasingly crucial for\nvisual object tracking. However, existing methods typically use only a few\ntokens to convey this information, which can lead to information loss and limit\ntheir ability to fully capture the context. To address this issue, we propose a\nnew video-level visual object tracking framework called MCITrack. It leverages\nMamba's hidden states to continuously record and transmit extensive contextual\ninformation throughout the video stream, resulting in more robust object\ntracking. The core component of MCITrack is the Contextual Information Fusion\nmodule, which consists of the mamba layer and the cross-attention layer. The\nmamba layer stores historical contextual information, while the cross-attention\nlayer integrates this information into the current visual features of each\nbackbone block. This module enhances the model's ability to capture and utilize\ncontextual information at multiple levels through deep integration with the\nbackbone. Experiments demonstrate that MCITrack achieves competitive\nperformance across numerous benchmarks. For instance, it gets 76.6% AUC on\nLaSOT and 80.0% AO on GOT-10k, establishing a new state-of-the-art performance.\nCode and models are available at https://github.com/kangben258/MCITrack.\n","authors":["Ben Kang","Xin Chen","Simiao Lai","Yang Liu","Yi Liu","Dong Wang"],"pdf_url":"https://arxiv.org/pdf/2412.11023v1.pdf","comment":"This paper was accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.11017v1","updated":"2024-12-15T02:10:18Z","published":"2024-12-15T02:10:18Z","title":"On Distilling the Displacement Knowledge for Few-Shot Class-Incremental\n  Learning","summary":"  Few-shot Class-Incremental Learning (FSCIL) addresses the challenges of\nevolving data distributions and the difficulty of data acquisition in\nreal-world scenarios. To counteract the catastrophic forgetting typically\nencountered in FSCIL, knowledge distillation is employed as a way to maintain\nthe knowledge from learned data distribution. Recognizing the limitations of\ngenerating discriminative feature representations in a few-shot context, our\napproach incorporates structural information between samples into knowledge\ndistillation. This structural information serves as a remedy for the low\nquality of features. Diverging from traditional structured distillation methods\nthat compute sample similarity, we introduce the Displacement Knowledge\nDistillation (DKD) method. DKD utilizes displacement rather than similarity\nbetween samples, incorporating both distance and angular information to\nsignificantly enhance the information density retained through knowledge\ndistillation. Observing performance disparities in feature distribution between\nbase and novel classes, we propose the Dual Distillation Network (DDNet). This\nnetwork applies traditional knowledge distillation to base classes and DKD to\nnovel classes, challenging the conventional integration of novel classes with\nbase classes. Additionally, we implement an instance-aware sample selector\nduring inference to dynamically adjust dual branch weights, thereby leveraging\nthe complementary strengths of each approach. Extensive testing on three\nbenchmarks demonstrates that DDNet achieves state-of-the-art results. Moreover,\nthrough rigorous experimentation and comparison, we establish the robustness\nand general applicability of our proposed DKD method.\n","authors":["Pengfei Fang","Yongchun Qin","Hui Xue"],"pdf_url":"https://arxiv.org/pdf/2412.11017v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08505v3","updated":"2024-12-15T02:07:57Z","published":"2024-03-13T13:12:57Z","title":"CAMSIC: Content-aware Masked Image Modeling Transformer for Stereo Image\n  Compression","summary":"  Existing learning-based stereo image codec adopt sophisticated transformation\nwith simple entropy models derived from single image codecs to encode latent\nrepresentations. However, those entropy models struggle to effectively capture\nthe spatial-disparity characteristics inherent in stereo images, which leads to\nsuboptimal rate-distortion results. In this paper, we propose a stereo image\ncompression framework, named CAMSIC. CAMSIC independently transforms each image\nto latent representation and employs a powerful decoder-free Transformer\nentropy model to capture both spatial and disparity dependencies, by\nintroducing a novel content-aware masked image modeling (MIM) technique. Our\ncontent-aware MIM facilitates efficient bidirectional interaction between prior\ninformation and estimated tokens, which naturally obviates the need for an\nextra Transformer decoder. Experiments show that our stereo image codec\nachieves state-of-the-art rate-distortion performance on two stereo image\ndatasets Cityscapes and InStereo2K with fast encoding and decoding speed. Code\nis available at https://github.com/Xinjie-Q/CAMSIC.\n","authors":["Xinjie Zhang","Shenyuan Gao","Zhening Liu","Jiawei Shao","Xingtong Ge","Dailan He","Tongda Xu","Yan Wang","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.08505v3.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.11008v1","updated":"2024-12-15T01:29:33Z","published":"2024-12-15T01:29:33Z","title":"Towards Context-aware Convolutional Network for Image Restoration","summary":"  Image restoration (IR) is a long-standing task to recover a high-quality\nimage from its corrupted observation. Recently, transformer-based algorithms\nand some attention-based convolutional neural networks (CNNs) have presented\npromising results on several IR tasks. However, existing convolutional residual\nbuilding modules for IR encounter limited ability to map inputs into\nhigh-dimensional and non-linear feature spaces, and their local receptive\nfields have difficulty in capturing long-range context information like\nTransformer. Besides, CNN-based attention modules for IR either face static\nabundant parameters or have limited receptive fields. To address the first\nissue, we propose an efficient residual star module (ERSM) that includes\ncontext-aware \"star operation\" (element-wise multiplication) to contextually\nmap features into exceedingly high-dimensional and non-linear feature spaces,\nwhich greatly enhances representation learning. To further boost the extraction\nof contextual information, as for the second issue, we propose a large dynamic\nintegration module (LDIM) which possesses an extremely large receptive field.\nThus, LDIM can dynamically and efficiently integrate more contextual\ninformation that helps to further significantly improve the reconstruction\nperformance. Integrating ERSM and LDIM into an U-shaped backbone, we propose a\ncontext-aware convolutional network (CCNet) with powerful learning ability for\ncontextual high-dimensional mapping and abundant contextual information.\nExtensive experiments show that our CCNet with low model complexity achieves\nsuperior performance compared to other state-of-the-art IR methods on several\nIR tasks, including image dehazing, image motion deblurring, and image\ndesnowing.\n","authors":["Fangwei Hao","Ji Du","Weiyun Liang","Jing Xu","Xiaoxuan Xu"],"pdf_url":"https://arxiv.org/pdf/2412.11008v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05184v4","updated":"2024-12-15T00:13:20Z","published":"2024-06-07T18:04:21Z","title":"The Unmet Promise of Synthetic Training Images: Using Retrieved Real\n  Images Performs Better","summary":"  Generative text-to-image models enable us to synthesize unlimited amounts of\nimages in a controllable manner, spurring many recent efforts to train vision\nmodels with synthetic data. However, every synthetic image ultimately\noriginates from the upstream data used to train the generator. Does the\nintermediate generator provide additional information over directly training on\nrelevant parts of the upstream data? Grounding this question in the setting of\nimage classification, we compare finetuning on task-relevant, targeted\nsynthetic data generated by Stable Diffusion -- a generative model trained on\nthe LAION-2B dataset -- against finetuning on targeted real images retrieved\ndirectly from LAION-2B. We show that while synthetic data can benefit some\ndownstream tasks, it is universally matched or outperformed by real data from\nthe simple retrieval baseline. Our analysis suggests that this underperformance\nis partially due to generator artifacts and inaccurate task-relevant visual\ndetails in the synthetic images. Overall, we argue that targeted retrieval is a\ncritical baseline to consider when training with synthetic data -- a baseline\nthat current methods do not yet surpass. We release code, data, and models at\nhttps://github.com/scottgeng00/unmet-promise.\n","authors":["Scott Geng","Cheng-Yu Hsieh","Vivek Ramanujan","Matthew Wallingford","Chun-Liang Li","Pang Wei Koh","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2406.05184v4.pdf","comment":"Correspondence to sgeng at cs dot washington dot edu. RK and PWK\n  equally advised the project"},{"id":"http://arxiv.org/abs/2412.12208v1","updated":"2024-12-15T17:40:46Z","published":"2024-12-15T17:40:46Z","title":"AI-Driven Innovations in Volumetric Video Streaming: A Review","summary":"  Recent efforts to enhance immersive and interactive user experiences have\ndriven the development of volumetric video, a form of 3D content that enables 6\nDoF. Unlike traditional 2D content, volumetric content can be represented in\nvarious ways, such as point clouds, meshes, or neural representations. However,\ndue to its complex structure and large amounts of data size, deploying this new\nform of 3D data presents significant challenges in transmission and rendering.\nThese challenges have hindered the widespread adoption of volumetric video in\ndaily applications. In recent years, researchers have proposed various\nAI-driven techniques to address these challenges and improve the efficiency and\nquality of volumetric content streaming. This paper provides a comprehensive\noverview of recent advances in AI-driven approaches to facilitate volumetric\ncontent streaming. Through this review, we aim to offer insights into the\ncurrent state-of-the-art and suggest potential future directions for advancing\nthe deployment of volumetric video streaming in real-world applications.\n","authors":["Erfan Entezami","Hui Guan"],"pdf_url":"https://arxiv.org/pdf/2412.12208v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12206v1","updated":"2024-12-15T16:10:10Z","published":"2024-12-15T16:10:10Z","title":"Provably Secure Robust Image Steganography via Cross-Modal Error\n  Correction","summary":"  The rapid development of image generation models has facilitated the\nwidespread dissemination of generated images on social networks, creating\nfavorable conditions for provably secure image steganography. However, existing\nmethods face issues such as low quality of generated images and lack of\nsemantic control in the generation process. To leverage provably secure\nsteganography with more effective and high-performance image generation models,\nand to ensure that stego images can accurately extract secret messages even\nafter being uploaded to social networks and subjected to lossy processing such\nas JPEG compression, we propose a high-quality, provably secure, and robust\nimage steganography method based on state-of-the-art autoregressive (AR) image\ngeneration models using Vector-Quantized (VQ) tokenizers. Additionally, we\nemploy a cross-modal error-correction framework that generates stego text from\nstego images to aid in restoring lossy images, ultimately enabling the\nextraction of secret messages embedded within the images. Extensive experiments\nhave demonstrated that the proposed method provides advantages in stego\nquality, embedding capacity, and robustness, while ensuring provable\nundetectability.\n","authors":["Yuang Qi","Kejiang Chen","Na Zhao","Zijin Yang","Weiming Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.12206v1.pdf","comment":"7 pages. Accepted by AAAI 2025"}],"Robotics":[{"id":"http://arxiv.org/abs/2412.11337v1","updated":"2024-12-15T23:05:16Z","published":"2024-12-15T23:05:16Z","title":"Modality-Driven Design for Multi-Step Dexterous Manipulation: Insights\n  from Neuroscience","summary":"  Multi-step dexterous manipulation is a fundamental skill in household\nscenarios, yet remains an underexplored area in robotics. This paper proposes a\nmodular approach, where each step of the manipulation process is addressed with\ndedicated policies based on effective modality input, rather than relying on a\nsingle end-to-end model. To demonstrate this, a dexterous robotic hand performs\na manipulation task involving picking up and rotating a box. Guided by insights\nfrom neuroscience, the task is decomposed into three sub-skills, 1)reaching,\n2)grasping and lifting, and 3)in-hand rotation, based on the dominant sensory\nmodalities employed in the human brain. Each sub-skill is addressed using\ndistinct methods from a practical perspective: a classical controller, a\nVision-Language-Action model, and a reinforcement learning policy with force\nfeedback, respectively. We tested the pipeline on a real robot to demonstrate\nthe feasibility of our approach. The key contribution of this study lies in\npresenting a neuroscience-inspired, modality-driven methodology for multi-step\ndexterous manipulation.\n","authors":["Naoki Wake","Atsushi Kanehira","Daichi Saito","Jun Takamatsu","Kazuhiro Sasabuchi","Hideki Koike","Katsushi Ikeuchi"],"pdf_url":"https://arxiv.org/pdf/2412.11337v1.pdf","comment":"8 pages, 5 figures, 2 tables. Last updated on December 14th, 2024"},{"id":"http://arxiv.org/abs/2412.11281v1","updated":"2024-12-15T19:05:26Z","published":"2024-12-15T19:05:26Z","title":"Multi-robot workspace design and motion planning for package sorting","summary":"  Robotic systems are routinely used in the logistics industry to enhance\noperational efficiency, but the design of robot workspaces remains a complex\nand manual task, which limits the system's flexibility to changing demands.\nThis paper aims to automate robot workspace design by proposing a computational\nframework to generate a budget-minimizing layout by selectively placing\nstationary robots on a floor grid, which includes robotic arms and conveyor\nbelts, and plan their cooperative motions to sort packages from given input and\noutput locations. We propose a hierarchical solving strategy that first\noptimizes the layout to minimize the hardware budget with a subgraph\noptimization subject to network flow constraints, followed by task allocation\nand motion planning based on the generated layout. In addition, we demonstrate\nhow to model conveyor belts as manipulators with multiple end effectors to\nintegrate them into our design and planning framework. We evaluated our\nframework on a set of simulated scenarios and showed that it can generate\noptimal layouts and collision-free motion trajectories, adapting to different\navailable robots, cost assignments, and box payloads.\n","authors":["Peiyu Zeng","Yijiang Huang","Simon Huber","Stelian Coros"],"pdf_url":"https://arxiv.org/pdf/2412.11281v1.pdf","comment":"9 pages, submitted to IEEE RA-L"},{"id":"http://arxiv.org/abs/2412.11275v1","updated":"2024-12-15T18:41:34Z","published":"2024-12-15T18:41:34Z","title":"Adaptive Visual Perception for Robotic Construction Process: A\n  Multi-Robot Coordination Framework","summary":"  Construction robots operate in unstructured construction sites, where\neffective visual perception is crucial for ensuring safe and seamless\noperations. However, construction robots often handle large elements and\nperform tasks across expansive areas, resulting in occluded views from onboard\ncameras and necessitating the use of multiple environmental cameras to capture\nthe large task space. This study proposes a multi-robot coordination framework\nin which a team of supervising robots equipped with cameras adaptively adjust\ntheir poses to visually perceive the operation of the primary construction\nrobot and its surrounding environment. A viewpoint selection method is proposed\nto determine each supervising robot's camera viewpoint, optimizing visual\ncoverage and proximity while considering the visibility of the upcoming\nconstruction robot operation. A case study on prefabricated wooden frame\ninstallation demonstrates the system's feasibility, and further experiments are\nconducted to validate the performance and robustness of the proposed viewpoint\nselection method across various settings. This research advances visual\nperception of robotic construction processes and paves the way for integrating\ncomputer vision techniques to enable real-time adaption and responsiveness.\nSuch advancements contribute to the safe and efficient operation of\nconstruction robots in inherently unstructured construction sites.\n","authors":["Jia Xu","Manish Dixit","Xi Wang"],"pdf_url":"https://arxiv.org/pdf/2412.11275v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11270v1","updated":"2024-12-15T18:24:26Z","published":"2024-12-15T18:24:26Z","title":"Monte Carlo Tree Search with Spectral Expansion for Planning with\n  Dynamical Systems","summary":"  The ability of a robot to plan complex behaviors with real-time computation,\nrather than adhering to predesigned or offline-learned routines, alleviates the\nneed for specialized algorithms or training for each problem instance. Monte\nCarlo Tree Search is a powerful planning algorithm that strategically explores\nsimulated future possibilities, but it requires a discrete problem\nrepresentation that is irreconcilable with the continuous dynamics of the\nphysical world. We present Spectral Expansion Tree Search (SETS), a real-time,\ntree-based planner that uses the spectrum of the locally linearized system to\nconstruct a low-complexity and approximately equivalent discrete representation\nof the continuous world. We prove SETS converges to a bound of the globally\noptimal solution for continuous, deterministic and differentiable Markov\nDecision Processes, a broad class of problems that includes underactuated\nnonlinear dynamics, non-convex reward functions, and unstructured environments.\nWe experimentally validate SETS on drone, spacecraft, and ground vehicle robots\nand one numerical experiment, each of which is not directly solvable with\nexisting methods. We successfully show SETS automatically discovers a diverse\nset of optimal behaviors and motion trajectories in real time.\n","authors":["Benjamin Riviere","John Lathrop","Soon-Jo Chung"],"pdf_url":"https://arxiv.org/pdf/2412.11270v1.pdf","comment":"The first two authors contributed equally to this article"},{"id":"http://arxiv.org/abs/2412.11258v1","updated":"2024-12-15T17:44:10Z","published":"2024-12-15T17:44:10Z","title":"GaussianProperty: Integrating Physical Properties to 3D Gaussians with\n  LMMs","summary":"  Estimating physical properties for visual data is a crucial task in computer\nvision, graphics, and robotics, underpinning applications such as augmented\nreality, physical simulation, and robotic grasping. However, this area remains\nunder-explored due to the inherent ambiguities in physical property estimation.\nTo address these challenges, we introduce GaussianProperty, a training-free\nframework that assigns physical properties of materials to 3D Gaussians.\nSpecifically, we integrate the segmentation capability of SAM with the\nrecognition capability of GPT-4V(ision) to formulate a global-local physical\nproperty reasoning module for 2D images. Then we project the physical\nproperties from multi-view 2D images to 3D Gaussians using a voting strategy.\nWe demonstrate that 3D Gaussians with physical property annotations enable\napplications in physics-based dynamic simulation and robotic grasping. For\nphysics-based dynamic simulation, we leverage the Material Point Method (MPM)\nfor realistic dynamic simulation. For robot grasping, we develop a grasping\nforce prediction strategy that estimates a safe force range required for object\ngrasping based on the estimated physical properties. Extensive experiments on\nmaterial segmentation, physics-based dynamic simulation, and robotic grasping\nvalidate the effectiveness of our proposed method, highlighting its crucial\nrole in understanding physical properties from visual data. Online demo, code,\nmore cases and annotated datasets are available on\n\\href{https://Gaussian-Property.github.io}{this https URL}.\n","authors":["Xinli Xu","Wenhang Ge","Dicong Qiu","ZhiFei Chen","Dongyu Yan","Zhuoyun Liu","Haoyu Zhao","Hanfeng Zhao","Shunsi Zhang","Junwei Liang","Ying-Cong Chen"],"pdf_url":"https://arxiv.org/pdf/2412.11258v1.pdf","comment":"17 pages, 17 figures"},{"id":"http://arxiv.org/abs/2412.11241v1","updated":"2024-12-15T16:46:23Z","published":"2024-12-15T16:46:23Z","title":"Volumetric Mapping with Panoptic Refinement via Kernel Density\n  Estimation for Mobile Robots","summary":"  Reconstructing three-dimensional (3D) scenes with semantic understanding is\nvital in many robotic applications. Robots need to identify which objects,\nalong with their positions and shapes, to manipulate them precisely with given\ntasks. Mobile robots, especially, usually use lightweight networks to segment\nobjects on RGB images and then localize them via depth maps; however, they\noften encounter out-of-distribution scenarios where masks over-cover the\nobjects. In this paper, we address the problem of panoptic segmentation quality\nin 3D scene reconstruction by refining segmentation errors using non-parametric\nstatistical methods. To enhance mask precision, we map the predicted masks into\na depth frame to estimate their distribution via kernel densities. The outliers\nin depth perception are then rejected without the need for additional\nparameters in an adaptive manner to out-of-distribution scenarios, followed by\n3D reconstruction using projective signed distance functions (SDFs). We\nvalidate our method on a synthetic dataset, which shows improvements in both\nquantitative and qualitative results for panoptic mapping. Through real-world\ntesting, the results furthermore show our method's capability to be deployed on\na real-robot system. Our source code is available at:\nhttps://github.com/mkhangg/refined panoptic mapping.\n","authors":["Khang Nguyen","Tuan Dang","Manfred Huber"],"pdf_url":"https://arxiv.org/pdf/2412.11241v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.01923v2","updated":"2024-12-15T16:27:09Z","published":"2024-05-03T08:33:58Z","title":"Task-Driven Computational Framework for Simultaneously Optimizing Design\n  and Mounted Pose of Modular Reconfigurable Manipulators","summary":"  Modular reconfigurable manipulators enable quick adaptation and versatility\nto address different application environments and tailor to the specific\nrequirements of the tasks. Task performance significantly depends on the\nmanipulator's mounted pose and morphology design, therefore posing the need of\nmethodologies for selecting suitable modular robot configurations and mounted\npose that can address the specific task requirements and required performance.\nMorphological changes in modular robots can be derived through a discrete\noptimization process involving the selective addition or removal of modules. In\ncontrast, the adjustment of the mounted pose operates within a continuous\nspace, allowing for smooth and precise alterations in both orientation and\nposition. This work introduces a computational framework that simultaneously\noptimizes modular manipulators' mounted pose and morphology. The core of the\nwork is that we design a mapping function that \\textit{implicitly} captures the\nmorphological state of manipulators in the continuous space. This\ntransformation function unifies the optimization of mounted pose and morphology\nwithin a continuous space. Furthermore, our optimization framework incorporates\na array of performance metrics, such as minimum joint effort and maximum\nmanipulability, and considerations for trajectory execution error and physical\nand safety constraints. To highlight our method's benefits, we compare it with\nprevious methods that framed such problem as a combinatorial optimization\nproblem and demonstrate its practicality in selecting the modular robot\nconfiguration for executing a drilling task with the CONCERT modular robotic\nplatform.\n","authors":["Maolin Lei","Edoardo Romiti","Arturo Laurenz","Nikos G. Tsagarakis"],"pdf_url":"https://arxiv.org/pdf/2405.01923v2.pdf","comment":null},{"id":"http://arxiv.org/abs/1912.08718v2","updated":"2024-12-15T13:39:01Z","published":"2019-12-17T10:09:21Z","title":"Poisson Multi-Bernoulli Mixtures for Sets of Trajectories","summary":"  The Poisson Multi-Bernoulli Mixture (PMBM) density is a conjugate\nmulti-target density for the standard point target model with Poisson point\nprocess birth. This means that both the filtering and predicted densities for\nthe set of targets are PMBM. In this paper, we first show that the PMBM density\nis also conjugate for sets of trajectories with the standard point target\nmeasurement model. Second, based on this theoretical foundation, we develop two\ntrajectory PMBM filters that provide recursions to calculate the posterior\ndensity for the set of all trajectories that have ever been present in the\nsurveillance area, and the posterior density of the set of trajectories present\nat the current time step in the surveillance area. These two filters therefore\nprovide complete probabilistic information on the considered trajectories\nenabling optimal trajectory estimation. Third, we establish that the density of\nthe set of trajectories in any time window, given the measurements in a\npossibly different time window, is also a PMBM. Finally, the trajectory PMBM\nfilters are evaluated via simulations, and are shown to yield state-of-the-art\nperformance compared to other multi-target tracking algorithms based on random\nfinite sets and multiple hypothesis tracking.\n","authors":["Karl Granström","Lennart Svensson","Yuxuan Xia","Jason Williams","Ángel F. García-Fernández"],"pdf_url":"https://arxiv.org/pdf/1912.08718v2.pdf","comment":"accepted in IEEE Transactions on Aerospace and Electronic Systems.\n  Matlab code of trajectory PMBM filters can be found at\n  https://github.com/Agarciafernandez and https://github.com/yuhsuansia"},{"id":"http://arxiv.org/abs/2412.11169v1","updated":"2024-12-15T12:40:36Z","published":"2024-12-15T12:40:36Z","title":"Design Challenges for Robots in Industrial Applications","summary":"  Nowadays, electric robots play big role in many fields as they can replace\nhumans and/or decrease the amount of load on humans. There are several types of\nrobots that are present in the daily life, some of them are fully controlled by\nhumans while others are programmed to be self-controlled. In addition there are\nself-control robots with partial human control. Robots can be classified into\nthree major kinds: industry robots, autonomous robots and mobile robots.\nIndustry robots are used in industries and factories to perform mankind tasks\nin the easier and faster way which will help in developing products. Typically\nindustrial robots perform difficult and dangerous tasks, as they lift heavy\nobjects, handle chemicals, paint and assembly work and so on. They are working\nall the time hour after hour, day by day with the same precision and they do\nnot get tired which means that they do not make errors due to fatigue. Indeed,\nthey are ideally suited to complete repetitive tasks.\n","authors":["Nesreen Mufid"],"pdf_url":"https://arxiv.org/pdf/2412.11169v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17735v3","updated":"2024-12-15T06:10:41Z","published":"2024-11-23T09:57:43Z","title":"3D-Mem: 3D Scene Memory for Embodied Exploration and Reasoning","summary":"  Constructing compact and informative 3D scene representations is essential\nfor effective embodied exploration and reasoning, especially in complex\nenvironments over extended periods. Existing representations, such as\nobject-centric 3D scene graphs, oversimplify spatial relationships by modeling\nscenes as isolated objects with restrictive textual relationships, making it\ndifficult to address queries requiring nuanced spatial understanding. Moreover,\nthese representations lack natural mechanisms for active exploration and memory\nmanagement, hindering their application to lifelong autonomy. In this work, we\npropose 3D-Mem, a novel 3D scene memory framework for embodied agents. 3D-Mem\nemploys informative multi-view images, termed Memory Snapshots, to represent\nthe scene and capture rich visual information of explored regions. It further\nintegrates frontier-based exploration by introducing Frontier\nSnapshots-glimpses of unexplored areas-enabling agents to make informed\ndecisions by considering both known and potential new information. To support\nlifelong memory in active exploration settings, we present an incremental\nconstruction pipeline for 3D-Mem, as well as a memory retrieval technique for\nmemory management. Experimental results on three benchmarks demonstrate that\n3D-Mem significantly enhances agents' exploration and reasoning capabilities in\n3D environments, highlighting its potential for advancing applications in\nembodied AI.\n","authors":["Yuncong Yang","Han Yang","Jiachen Zhou","Peihao Chen","Hongxin Zhang","Yilun Du","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2411.17735v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04987v2","updated":"2024-12-15T05:41:54Z","published":"2024-12-06T12:15:24Z","title":"FlowPolicy: Enabling Fast and Robust 3D Flow-based Policy via\n  Consistency Flow Matching for Robot Manipulation","summary":"  Robots can acquire complex manipulation skills by learning policies from\nexpert demonstrations, which is often known as vision-based imitation learning.\nGenerating policies based on diffusion and flow matching models has been shown\nto be effective, particularly in robotic manipulation tasks. However,\nrecursion-based approaches are inference inefficient in working from noise\ndistributions to policy distributions, posing a challenging trade-off between\nefficiency and quality. This motivates us to propose FlowPolicy, a novel\nframework for fast policy generation based on consistency flow matching and 3D\nvision. Our approach refines the flow dynamics by normalizing the\nself-consistency of the velocity field, enabling the model to derive task\nexecution policies in a single inference step. Specifically, FlowPolicy\nconditions on the observed 3D point cloud, where consistency flow matching\ndirectly defines straight-line flows from different time states to the same\naction space, while simultaneously constraining their velocity values, that is,\nwe approximate the trajectories from noise to robot actions by normalizing the\nself-consistency of the velocity field within the action space, thus improving\nthe inference efficiency. We validate the effectiveness of FlowPolicy in Adroit\nand Metaworld, demonstrating a 7$\\times$ increase in inference speed while\nmaintaining competitive average success rates compared to state-of-the-art\nmethods. Code is available at https://github.com/zql-kk/FlowPolicy.\n","authors":["Qinglun Zhang","Zhen Liu","Haoqiang Fan","Guanghui Liu","Bing Zeng","Shuaicheng Liu"],"pdf_url":"https://arxiv.org/pdf/2412.04987v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16439v2","updated":"2024-12-15T03:48:49Z","published":"2024-05-26T05:48:21Z","title":"Multi-Agent Inverse Reinforcement Learning in Real World Unstructured\n  Pedestrian Crowds","summary":"  Social robot navigation in crowded public spaces such as university campuses,\nrestaurants, grocery stores, and hospitals, is an increasingly important area\nof research. One of the core strategies for achieving this goal is to\nunderstand humans' intent--underlying psychological factors that govern their\nmotion--by learning their reward functions, typically via inverse reinforcement\nlearning (IRL). Despite significant progress in IRL, learning reward functions\nof multiple agents simultaneously in dense unstructured pedestrian crowds has\nremained intractable due to the nature of the tightly coupled social\ninteractions that occur in these scenarios \\textit{e.g.} passing,\nintersections, swerving, weaving, etc. In this paper, we present a new\nmulti-agent maximum entropy inverse reinforcement learning algorithm for real\nworld unstructured pedestrian crowds. Key to our approach is a simple, but\neffective, mathematical trick which we name the so-called\ntractability-rationality trade-off trick that achieves tractability at the cost\nof a slight reduction in accuracy. We compare our approach to the classical\nsingle-agent MaxEnt IRL as well as state-of-the-art trajectory prediction\nmethods on several datasets including the ETH, UCY, SCAND, JRDB, and a new\ndataset, called Speedway, collected at a busy intersection on a University\ncampus focusing on dense, complex agent interactions. Our key findings show\nthat, on the dense Speedway dataset, our approach ranks 1st among top 7\nbaselines with >2X improvement over single-agent IRL, and is competitive with\nstate-of-the-art large transformer-based encoder-decoder models on sparser\ndatasets such as ETH/UCY (ranks 3rd among top 7 baselines).\n","authors":["Rohan Chandra","Haresh Karnan","Negar Mehr","Peter Stone","Joydeep Biswas"],"pdf_url":"https://arxiv.org/pdf/2405.16439v2.pdf","comment":null}]},"2024-12-14T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2412.10997v1","updated":"2024-12-14T23:40:53Z","published":"2024-12-14T23:40:53Z","title":"Mask Enhanced Deeply Supervised Prostate Cancer Detection on B-mode\n  Micro-Ultrasound","summary":"  Prostate cancer is a leading cause of cancer-related deaths among men. The\nrecent development of high frequency, micro-ultrasound imaging offers improved\nresolution compared to conventional ultrasound and potentially a better ability\nto differentiate clinically significant cancer from normal tissue. However, the\nfeatures of prostate cancer remain subtle, with ambiguous borders with normal\ntissue and large variations in appearance, making it challenging for both\nmachine learning and humans to localize it on micro-ultrasound images.\n  We propose a novel Mask Enhanced Deeply-supervised Micro-US network, termed\nMedMusNet, to automatically and more accurately segment prostate cancer to be\nused as potential targets for biopsy procedures. MedMusNet leverages predicted\nmasks of prostate cancer to enforce the learned features layer-wisely within\nthe network, reducing the influence of noise and improving overall consistency\nacross frames.\n  MedMusNet successfully detected 76% of clinically significant cancer with a\nDice Similarity Coefficient of 0.365, significantly outperforming the baseline\nSwin-M2F in specificity and accuracy (Wilcoxon test, Bonferroni correction,\np-value<0.05). While the lesion-level and patient-level analyses showed\nimproved performance compared to human experts and different baseline, the\nimprovements did not reach statistical significance, likely on account of the\nsmall cohort.\n  We have presented a novel approach to automatically detect and segment\nclinically significant prostate cancer on B-mode micro-ultrasound images. Our\nMedMusNet model outperformed other models, surpassing even human experts. These\npreliminary results suggest the potential for aiding urologists in prostate\ncancer diagnosis via biopsy and treatment decision-making.\n","authors":["Lichun Zhang","Steve Ran Zhou","Moon Hyung Choi","Jeong Hoon Lee","Shengtian Sang","Adam Kinnaird","Wayne G. Brisbane","Giovanni Lughezzani","Davide Maffei","Vittorio Fasulo","Patrick Albers","Sulaiman Vesal","Wei Shao","Ahmed N. El Kaffas","Richard E. Fan","Geoffrey A. Sonn","Mirabela Rusu"],"pdf_url":"https://arxiv.org/pdf/2412.10997v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10995v1","updated":"2024-12-14T23:39:03Z","published":"2024-12-14T23:39:03Z","title":"RapidNet: Multi-Level Dilated Convolution Based Mobile Backbone","summary":"  Vision transformers (ViTs) have dominated computer vision in recent years.\nHowever, ViTs are computationally expensive and not well suited for mobile\ndevices; this led to the prevalence of convolutional neural network (CNN) and\nViT-based hybrid models for mobile vision applications. Recently, Vision GNN\n(ViG) and CNN hybrid models have also been proposed for mobile vision tasks.\nHowever, all of these methods remain slower compared to pure CNN-based models.\nIn this work, we propose Multi-Level Dilated Convolutions to devise a purely\nCNN-based mobile backbone. Using Multi-Level Dilated Convolutions allows for a\nlarger theoretical receptive field than standard convolutions. Different levels\nof dilation also allow for interactions between the short-range and long-range\nfeatures in an image. Experiments show that our proposed model outperforms\nstate-of-the-art (SOTA) mobile CNN, ViT, ViG, and hybrid architectures in terms\nof accuracy and/or speed on image classification, object detection, instance\nsegmentation, and semantic segmentation. Our fastest model, RapidNet-Ti,\nachieves 76.3\\% top-1 accuracy on ImageNet-1K with 0.9 ms inference latency on\nan iPhone 13 mini NPU, which is faster and more accurate than MobileNetV2x1.4\n(74.7\\% top-1 with 1.0 ms latency). Our work shows that pure CNN architectures\ncan beat SOTA hybrid and ViT models in terms of accuracy and speed when\ndesigned properly.\n","authors":["Mustafa Munir","Md Mostafijur Rahman","Radu Marculescu"],"pdf_url":"https://arxiv.org/pdf/2412.10995v1.pdf","comment":"Accepted in 2025 IEEE/CVF Winter Conference on Applications of\n  Computer Vision (WACV 2025)"},{"id":"http://arxiv.org/abs/2412.10985v1","updated":"2024-12-14T22:29:22Z","published":"2024-12-14T22:29:22Z","title":"MorphiNet: A Graph Subdivision Network for Adaptive Bi-ventricle Surface\n  Reconstruction","summary":"  Cardiac Magnetic Resonance (CMR) imaging is widely used for heart modelling\nand digital twin computational analysis due to its ability to visualize soft\ntissues and capture dynamic functions. However, the anisotropic nature of CMR\nimages, characterized by large inter-slice distances and misalignments from\ncardiac motion, poses significant challenges to accurate model reconstruction.\nThese limitations result in data loss and measurement inaccuracies, hindering\nthe capture of detailed anatomical structures. This study introduces MorphiNet,\na novel network that enhances heart model reconstruction by leveraging\nhigh-resolution Computer Tomography (CT) images, unpaired with CMR images, to\nlearn heart anatomy. MorphiNet encodes anatomical structures as gradient\nfields, transforming template meshes into patient-specific geometries. A\nmulti-layer graph subdivision network refines these geometries while\nmaintaining dense point correspondence. The proposed method achieves high\nanatomy fidelity, demonstrating approximately 40% higher Dice scores, half the\nHausdorff distance, and around 3 mm average surface error compared to\nstate-of-the-art methods. MorphiNet delivers superior results with greater\ninference efficiency. This approach represents a significant advancement in\naddressing the challenges of CMR-based heart model reconstruction, potentially\nimproving digital twin computational analyses of cardiac structure and\nfunctions.\n","authors":["Yu Deng","Yiyang Xu","Linglong Qian","Charlene Mauger","Anastasia Nasopoulou","Steven Williams","Michelle Williams","Steven Niederer","David Newby","Andrew McCulloch","Jeff Omens","Kuberan Pushprajah","Alistair Young"],"pdf_url":"https://arxiv.org/pdf/2412.10985v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12868v2","updated":"2024-12-14T21:53:36Z","published":"2023-10-19T16:18:02Z","title":"DiffBoost: Enhancing Medical Image Segmentation via Text-Guided\n  Diffusion Model","summary":"  Large-scale, big-variant, high-quality data are crucial for developing robust\nand successful deep-learning models for medical applications since they\npotentially enable better generalization performance and avoid overfitting.\nHowever, the scarcity of high-quality labeled data always presents significant\nchallenges. This paper proposes a novel approach to address this challenge by\ndeveloping controllable diffusion models for medical image synthesis, called\nDiffBoost. We leverage recent diffusion probabilistic models to generate\nrealistic and diverse synthetic medical image data that preserve the essential\ncharacteristics of the original medical images by incorporating edge\ninformation of objects to guide the synthesis process. In our approach, we\nensure that the synthesized samples adhere to medically relevant constraints\nand preserve the underlying structure of imaging data. Due to the random\nsampling process by the diffusion model, we can generate an arbitrary number of\nsynthetic images with diverse appearances. To validate the effectiveness of our\nproposed method, we conduct an extensive set of medical image segmentation\nexperiments on multiple datasets, including Ultrasound breast (+13.87%), CT\nspleen (+0.38%), and MRI prostate (+7.78%), achieving significant improvements\nover the baseline segmentation methods. The promising results demonstrate the\neffectiveness of our \\textcolor{black}{DiffBoost} for medical image\nsegmentation tasks and show the feasibility of introducing a first-ever\ntext-guided diffusion model for general medical image segmentation tasks. With\ncarefully designed ablation experiments, we investigate the influence of\nvarious data augmentations, hyper-parameter settings, patch size for generating\nrandom merging mask settings, and combined influence with different network\narchitectures. Source code are available at\nhttps://github.com/NUBagciLab/DiffBoost.\n","authors":["Zheyuan Zhang","Lanhong Yao","Bin Wang","Debesh Jha","Gorkem Durak","Elif Keles","Alpay Medetalibeyoglu","Ulas Bagci"],"pdf_url":"https://arxiv.org/pdf/2310.12868v2.pdf","comment":"Accepted by IEEE TRANSACTIONS ON MEDICAL IMAGING"},{"id":"http://arxiv.org/abs/2412.10977v1","updated":"2024-12-14T21:39:43Z","published":"2024-12-14T21:39:43Z","title":"Point Cloud to Mesh Reconstruction: A Focus on Key Learning-Based\n  Paradigms","summary":"  Reconstructing meshes from point clouds is an important task in fields such\nas robotics, autonomous systems, and medical imaging. This survey examines\nstate-of-the-art learning-based approaches to mesh reconstruction, categorizing\nthem into five paradigms: PointNet family, autoencoder architectures,\ndeformation-based methods, point-move techniques, and primitive-based\napproaches. Each paradigm is explored in depth, detailing the primary\napproaches and their underlying methodologies. By comparing these techniques,\nour study serves as a comprehensive guide, and equips researchers and\npractitioners with the knowledge to navigate the landscape of learning-based\nmesh reconstruction techniques. The findings underscore the transformative\npotential of these methods, which often surpass traditional techniques in\nallowing detailed and efficient reconstructions.\n","authors":["Fatima Zahra Iguenfer","Achraf Hsain","Hiba Amissa","Yousra Chtouki"],"pdf_url":"https://arxiv.org/pdf/2412.10977v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10972v1","updated":"2024-12-14T21:26:44Z","published":"2024-12-14T21:26:44Z","title":"DCSEG: Decoupled 3D Open-Set Segmentation using Gaussian Splatting","summary":"  Open-set 3D segmentation represents a major point of interest for multiple\ndownstream robotics and augmented/virtual reality applications. Recent advances\nintroduce 3D Gaussian Splatting as a computationally efficient representation\nof the underlying scene. They enable the rendering of novel views while\nachieving real-time display rates and matching the quality of computationally\nfar more expensive methods. We present a decoupled 3D segmentation pipeline to\nensure modularity and adaptability to novel 3D representations and semantic\nsegmentation foundation models. The pipeline proposes class-agnostic masks\nbased on a 3D reconstruction of the scene. Given the resulting class-agnostic\nmasks, we use a class-aware 2D foundation model to add class annotations to the\n3D masks. We test this pipeline with 3D Gaussian Splatting and different 2D\nsegmentation models and achieve better performance than more tailored\napproaches while also significantly increasing the modularity.\n","authors":["Luis Wiedmann","Luca Wiehe","David Rozenberszki"],"pdf_url":"https://arxiv.org/pdf/2412.10972v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10967v1","updated":"2024-12-14T20:55:31Z","published":"2024-12-14T20:55:31Z","title":"Biological and Radiological Dictionary of Radiomics Features: Addressing\n  Understandable AI Issues in Personalized Prostate Cancer; Dictionary version\n  PM1.0","summary":"  This study investigates the connection between visual semantic features in\nPI-RADS and associated risk factors, moving beyond abnormal imaging findings by\ncreating a standardized dictionary of biological/radiological radiomics\nfeatures (RFs). Using multiparametric prostate MRI sequences (T2-weighted\nimaging [T2WI], diffusion-weighted imaging [DWI], and apparent diffusion\ncoefficient [ADC]), six interpretable and seven complex classifiers, combined\nwith nine feature selection algorithms (FSAs), were applied to segmented\nlesions to predict UCLA scores. Combining T2WI, DWI, and ADC with FSAs such as\nANOVA F-test, Correlation Coefficient, and Fisher Score, and utilizing logistic\nregression, identified key features: the 90th percentile from T2WI\n(hypo-intensity linked to cancer risk), variance from T2WI (lesion\nheterogeneity), shape metrics like Least Axis Length and Surface Area to Volume\nratio from ADC (lesion compactness), and Run Entropy from ADC (texture\nconsistency). This approach achieved an average accuracy of 0.78, outperforming\nsingle-sequence methods (p < 0.05). The developed dictionary provides a common\nlanguage, fostering collaboration between clinical professionals and AI\ndevelopers to enable trustworthy, interpretable AI for reliable clinical\ndecisions.\n","authors":["Mohammad R. Salmanpour","Sajad Amiri","Sara Gharibi","Ahmad Shariftabrizi","Yixi Xu","William B Weeks","Arman Rahmim","Ilker Hacihaliloglu"],"pdf_url":"https://arxiv.org/pdf/2412.10967v1.pdf","comment":"24 pages, 3 Figures, 2 Tables"},{"id":"http://arxiv.org/abs/2412.10958v1","updated":"2024-12-14T20:29:29Z","published":"2024-12-14T20:29:29Z","title":"SoftVQ-VAE: Efficient 1-Dimensional Continuous Tokenizer","summary":"  Efficient image tokenization with high compression ratios remains a critical\nchallenge for training generative models. We present SoftVQ-VAE, a continuous\nimage tokenizer that leverages soft categorical posteriors to aggregate\nmultiple codewords into each latent token, substantially increasing the\nrepresentation capacity of the latent space. When applied to Transformer-based\narchitectures, our approach compresses 256x256 and 512x512 images using as few\nas 32 or 64 1-dimensional tokens. Not only does SoftVQ-VAE show consistent and\nhigh-quality reconstruction, more importantly, it also achieves\nstate-of-the-art and significantly faster image generation results across\ndifferent denoising-based generative models. Remarkably, SoftVQ-VAE improves\ninference throughput by up to 18x for generating 256x256 images and 55x for\n512x512 images while achieving competitive FID scores of 1.78 and 2.21 for\nSiT-XL. It also improves the training efficiency of the generative models by\nreducing the number of training iterations by 2.3x while maintaining comparable\nperformance. With its fully-differentiable design and semantic-rich latent\nspace, our experiment demonstrates that SoftVQ-VQE achieves efficient\ntokenization without compromising generation quality, paving the way for more\nefficient generative models. Code and model are released.\n","authors":["Hao Chen","Ze Wang","Xiang Li","Ximeng Sun","Fangyi Chen","Jiang Liu","Jindong Wang","Bhiksha Raj","Zicheng Liu","Emad Barsoum"],"pdf_url":"https://arxiv.org/pdf/2412.10958v1.pdf","comment":"Code and model: https://github.com/Hhhhhhao/continuous_tokenizer"},{"id":"http://arxiv.org/abs/2408.13024v2","updated":"2024-12-14T20:23:16Z","published":"2024-08-23T12:27:33Z","title":"Learning 2D Invariant Affordance Knowledge for 3D Affordance Grounding","summary":"  3D Object Affordance Grounding aims to predict the functional regions on a 3D\nobject and has laid the foundation for a wide range of applications in\nrobotics. Recent advances tackle this problem via learning a mapping between 3D\nregions and a single human-object interaction image. However, the geometric\nstructure of the 3D object and the object in the human-object interaction image\nare not always consistent, leading to poor generalization. To address this\nissue, we propose to learn generalizable invariant affordance knowledge from\nmultiple human-object interaction images within the same affordance category.\nSpecifically, we introduce the \\textbf{M}ulti-\\textbf{I}mage Guided\nInvariant-\\textbf{F}eature-Aware 3D \\textbf{A}ffordance \\textbf{G}rounding\n(\\textbf{MIFAG}) framework. It grounds 3D object affordance regions by\nidentifying common interaction patterns across multiple human-object\ninteraction images. First, the Invariant Affordance Knowledge Extraction Module\n(\\textbf{IAM}) utilizes an iterative updating strategy to gradually extract\naligned affordance knowledge from multiple images and integrate it into an\naffordance dictionary. Then, the Affordance Dictionary Adaptive Fusion Module\n(\\textbf{ADM}) learns comprehensive point cloud representations that consider\nall affordance candidates in multiple images. Besides, the Multi-Image and\nPoint Affordance (\\textbf{MIPA}) benchmark is constructed and our method\noutperforms existing state-of-the-art methods on various experimental\ncomparisons. Project page: \\url{https://goxq.github.io/mifag}\n","authors":["Xianqiang Gao","Pingrui Zhang","Delin Qu","Dong Wang","Zhigang Wang","Yan Ding","Bin Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.13024v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.10955v1","updated":"2024-12-14T20:18:16Z","published":"2024-12-14T20:18:16Z","title":"Deep Learning-Based Noninvasive Screening of Type 2 Diabetes with Chest\n  X-ray Images and Electronic Health Records","summary":"  The imperative for early detection of type 2 diabetes mellitus (T2DM) is\nchallenged by its asymptomatic onset and dependence on suboptimal clinical\ndiagnostic tests, contributing to its widespread global prevalence. While\nresearch into noninvasive T2DM screening tools has advanced, conventional\nmachine learning approaches remain limited to unimodal inputs due to extensive\nfeature engineering requirements. In contrast, deep learning models can\nleverage multimodal data for a more holistic understanding of patients' health\nconditions. However, the potential of chest X-ray (CXR) imaging, one of the\nmost commonly performed medical procedures, remains underexplored. This study\nevaluates the integration of CXR images with other noninvasive data sources,\nincluding electronic health records (EHRs) and electrocardiography signals, for\nT2DM detection. Utilising datasets meticulously compiled from the MIMIC-IV\ndatabases, we investigated two deep fusion paradigms: an early fusion-based\nmultimodal transformer and a modular joint fusion ResNet-LSTM architecture. The\nend-to-end trained ResNet-LSTM model achieved an AUROC of 0.86, surpassing the\nCXR-only baseline by 2.3% with just 9863 training samples. These findings\ndemonstrate the diagnostic value of CXRs within multimodal frameworks for\nidentifying at-risk individuals early. Additionally, the dataset preprocessing\npipeline has also been released to support further research in this domain.\n","authors":["Sanjana Gundapaneni","Zhuo Zhi","Miguel Rodrigues"],"pdf_url":"https://arxiv.org/pdf/2412.10955v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.10960v3","updated":"2024-12-14T19:51:15Z","published":"2022-11-20T12:02:07Z","title":"CoCoNet: Coupled Contrastive Learning Network with Multi-level Feature\n  Ensemble for Multi-modality Image Fusion","summary":"  Infrared and visible image fusion targets to provide an informative image by\ncombining complementary information from different sensors. Existing\nlearning-based fusion approaches attempt to construct various loss functions to\npreserve complementary features, while neglecting to discover the\ninter-relationship between the two modalities, leading to redundant or even\ninvalid information on the fusion results. Moreover, most methods focus on\nstrengthening the network with an increase in depth while neglecting the\nimportance of feature transmission, causing vital information degeneration. To\nalleviate these issues, we propose a coupled contrastive learning network,\ndubbed CoCoNet, to realize infrared and visible image fusion in an end-to-end\nmanner. Concretely, to simultaneously retain typical features from both\nmodalities and to avoid artifacts emerging on the fused result, we develop a\ncoupled contrastive constraint in our loss function. In a fused image, its\nforeground target / background detail part is pulled close to the infrared /\nvisible source and pushed far away from the visible / infrared source in the\nrepresentation space. We further exploit image characteristics to provide\ndata-sensitive weights, allowing our loss function to build a more reliable\nrelationship with source images. A multi-level attention module is established\nto learn rich hierarchical feature representation and to comprehensively\ntransfer features in the fusion process. We also apply the proposed CoCoNet on\nmedical image fusion of different types, e.g., magnetic resonance image,\npositron emission tomography image, and single photon emission computed\ntomography image. Extensive experiments demonstrate that our method achieves\nstate-of-the-art (SOTA) performance under both subjective and objective\nevaluation, especially in preserving prominent targets and recovering vital\ntextural details.\n","authors":["Jinyuan Liu","Runjia Lin","Guanyao Wu","Risheng Liu","Zhongxuan Luo","Xin Fan"],"pdf_url":"https://arxiv.org/pdf/2211.10960v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10946v1","updated":"2024-12-14T19:44:25Z","published":"2024-12-14T19:44:25Z","title":"SegHeD+: Segmentation of Heterogeneous Data for Multiple Sclerosis\n  Lesions with Anatomical Constraints and Lesion-aware Augmentation","summary":"  Assessing lesions and tracking their progression over time in brain magnetic\nresonance (MR) images is essential for diagnosing and monitoring multiple\nsclerosis (MS). Machine learning models have shown promise in automating the\nsegmentation of MS lesions. However, training these models typically requires\nlarge, well-annotated datasets. Unfortunately, MS imaging datasets are often\nlimited in size, spread across multiple hospital sites, and exhibit different\nformats (such as cross-sectional or longitudinal) and annotation styles. This\ndata diversity presents a significant obstacle to developing a unified model\nfor MS lesion segmentation. To address this issue, we introduce SegHeD+, a\nnovel segmentation model that can handle multiple datasets and tasks,\naccommodating heterogeneous input data and performing segmentation for all\nlesions, new lesions, and vanishing lesions. We integrate domain knowledge\nabout MS lesions by incorporating longitudinal, anatomical, and volumetric\nconstraints into the segmentation model. Additionally, we perform lesion-level\ndata augmentation to enlarge the training set and further improve segmentation\nperformance. SegHeD+ is evaluated on five MS datasets and demonstrates superior\nperformance in segmenting all, new, and vanishing lesions, surpassing several\nstate-of-the-art methods in the field.\n","authors":["Berke Doga Basaran","Paul M. Matthews","Wenjia Bai"],"pdf_url":"https://arxiv.org/pdf/2412.10946v1.pdf","comment":"20 pages, 6 figures, 6 tables"},{"id":"http://arxiv.org/abs/2412.10943v1","updated":"2024-12-14T19:37:17Z","published":"2024-12-14T19:37:17Z","title":"Unconstrained Salient and Camouflaged Object Detection","summary":"  Visual Salient Object Detection (SOD) and Camouflaged Object Detection (COD)\nare two interrelated yet distinct tasks. Both tasks model the human visual\nsystem's ability to perceive the presence of objects. The traditional SOD\ndatasets and methods are designed for scenes where only salient objects are\npresent, similarly, COD datasets and methods are designed for scenes where only\ncamouflaged objects are present. However, scenes where both salient and\ncamouflaged objects coexist, or where neither is present, are not considered.\nThis simplifies the existing research on SOD and COD. In this paper, to explore\na more generalized approach to SOD and COD, we introduce a benchmark called\nUnconstrained Salient and Camouflaged Object Detection (USCOD), which supports\nthe simultaneous detection of salient and camouflaged objects in unconstrained\nscenes, regardless of their presence. Towards this, we construct a large-scale\ndataset, CS12K, that encompasses a variety of scenes, including four distinct\ntypes: only salient objects, only camouflaged objects, both, and neither. In\nour benchmark experiments, we identify a major challenge in USCOD:\ndistinguishing between salient and camouflaged objects within the same scene.\nTo address this challenge, we propose USCNet, a baseline model for USCOD that\ndecouples the learning of attribute distinction from mask reconstruction. The\nmodel incorporates an APG module, which learns both sample-generic and\nsample-specific features to enhance the attribute differentiation between\nsalient and camouflaged objects. Furthermore, to evaluate models' ability to\ndistinguish between salient and camouflaged objects, we design a metric called\nCamouflage-Saliency Confusion Score (CSCS). The proposed method achieves\nstate-of-the-art performance on the newly introduced USCOD task. The code and\ndataset will be publicly available.\n","authors":["Zhangjun Zhou","Yiping Li","Chunlin Zhong","Jianuo Huang","Jialun Pei","He Tang"],"pdf_url":"https://arxiv.org/pdf/2412.10943v1.pdf","comment":"24 pages, 12 figures"},{"id":"http://arxiv.org/abs/2412.10942v1","updated":"2024-12-14T19:34:32Z","published":"2024-12-14T19:34:32Z","title":"Meta-evaluating stability measures: MAX-Senstivity & AVG-Sensitivity","summary":"  The use of eXplainable Artificial Intelligence (XAI) systems has introduced a\nset of challenges that need resolution. The XAI robustness, or stability, has\nbeen one of the goals of the community from its beginning. Multiple authors\nhave proposed evaluating this feature using objective evaluation measures.\nNonetheless, many questions remain. With this work, we propose a novel approach\nto meta-evaluate these metrics, i.e. analyze the correctness of the evaluators.\nWe propose two new tests that allowed us to evaluate two different stability\nmeasures: AVG-Sensitiviy and MAX-Senstivity. We tested their reliability in the\npresence of perfect and robust explanations, generated with a Decision Tree; as\nwell as completely random explanations and prediction. The metrics results\nshowed their incapacity of identify as erroneous the random explanations,\nhighlighting their overall unreliability.\n","authors":["Miquel Miró-Nicolau","Antoni Jaume-i-Capó","Gabriel Moyà-Alcover"],"pdf_url":"https://arxiv.org/pdf/2412.10942v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10935v1","updated":"2024-12-14T19:06:01Z","published":"2024-12-14T19:06:01Z","title":"Progressive Compression with Universally Quantized Diffusion Models","summary":"  Diffusion probabilistic models have achieved mainstream success in many\ngenerative modeling tasks, from image generation to inverse problem solving. A\ndistinct feature of these models is that they correspond to deep hierarchical\nlatent variable models optimizing a variational evidence lower bound (ELBO) on\nthe data likelihood. Drawing on a basic connection between likelihood modeling\nand compression, we explore the potential of diffusion models for progressive\ncoding, resulting in a sequence of bits that can be incrementally transmitted\nand decoded with progressively improving reconstruction quality. Unlike prior\nwork based on Gaussian diffusion or conditional diffusion models, we propose a\nnew form of diffusion model with uniform noise in the forward process, whose\nnegative ELBO corresponds to the end-to-end compression cost using universal\nquantization. We obtain promising first results on image compression, achieving\ncompetitive rate-distortion and rate-realism results on a wide range of\nbit-rates with a single model, bringing neural codecs a step closer to\npractical deployment.\n","authors":["Yibo Yang","Justus C. Will","Stephan Mandt"],"pdf_url":"https://arxiv.org/pdf/2412.10935v1.pdf","comment":"20 pages, 10 figures, submitted to ICLR 2025"},{"id":"http://arxiv.org/abs/2406.04339v2","updated":"2024-12-14T18:41:03Z","published":"2024-06-06T17:59:47Z","title":"RoboMamba: Efficient Vision-Language-Action Model for Robotic Reasoning\n  and Manipulation","summary":"  A fundamental objective in robot manipulation is to enable models to\ncomprehend visual scenes and execute actions. Although existing\nVision-Language-Action (VLA) models for robots can handle a range of basic\ntasks, they still face challenges in two areas: (1) insufficient reasoning\nability to tackle complex tasks, and (2) high computational costs for VLA model\nfine-tuning and inference. The recently proposed state space model (SSM) known\nas Mamba demonstrates promising capabilities in non-trivial sequence modeling\nwith linear inference complexity. Inspired by this, we introduce RoboMamba, an\nend-to-end robotic VLA model that leverages Mamba to deliver both robotic\nreasoning and action capabilities, while maintaining efficient fine-tuning and\ninference. Specifically, we first integrate the vision encoder with Mamba,\naligning visual tokens with language embedding through co-training, empowering\nour model with visual common sense and robotic-related reasoning. To further\nequip RoboMamba with SE(3) pose prediction abilities, we explore an efficient\nfine-tuning strategy with a simple policy head. We find that once RoboMamba\npossesses sufficient reasoning capability, it can acquire manipulation skills\nwith minimal fine-tuning parameters (0.1\\% of the model) and time. In\nexperiments, RoboMamba demonstrates outstanding reasoning capabilities on\ngeneral and robotic evaluation benchmarks. Meanwhile, our model showcases\nimpressive pose prediction results in both simulation and real-world\nexperiments, achieving inference speeds 3 times faster than existing VLA\nmodels. Our project web page: https://sites.google.com/view/robomamba-web\n","authors":["Jiaming Liu","Mengzhen Liu","Zhenyu Wang","Pengju An","Xiaoqi Li","Kaichen Zhou","Senqiao Yang","Renrui Zhang","Yandong Guo","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.04339v2.pdf","comment":"Accepted by Neurips 2024"},{"id":"http://arxiv.org/abs/2212.12965v2","updated":"2024-12-14T18:40:10Z","published":"2022-12-25T22:27:32Z","title":"BD-KD: Balancing the Divergences for Online Knowledge Distillation","summary":"  We address the challenge of producing trustworthy and accurate compact models\nfor edge devices. While Knowledge Distillation (KD) has improved model\ncompression in terms of achieving high accuracy performance, calibration of\nthese compact models has been overlooked. We introduce BD-KD (Balanced\nDivergence Knowledge Distillation), a framework for logit-based online KD.\nBD-KD enhances both accuracy and model calibration simultaneously, eliminating\nthe need for post-hoc recalibration techniques, which add computational\noverhead to the overall training pipeline and degrade performance. Our method\nencourages student-centered training by adjusting the conventional online\ndistillation loss on both the student and teacher losses, employing sample-wise\nweighting of forward and reverse Kullback-Leibler divergence. This strategy\nbalances student network confidence and boosts performance. Experiments across\nCIFAR10, CIFAR100, TinyImageNet, and ImageNet datasets, and various\narchitectures demonstrate improved calibration and accuracy compared to recent\nonline KD methods.\n","authors":["Ibtihel Amara","Nazanin Sepahvand","Brett H. Meyer","Warren J. Gross","James J. Clark"],"pdf_url":"https://arxiv.org/pdf/2212.12965v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.18623v2","updated":"2024-12-14T18:38:03Z","published":"2024-11-27T18:59:52Z","title":"Lift3D Foundation Policy: Lifting 2D Large-Scale Pretrained Models for\n  Robust 3D Robotic Manipulation","summary":"  3D geometric information is essential for manipulation tasks, as robots need\nto perceive the 3D environment, reason about spatial relationships, and\ninteract with intricate spatial configurations. Recent research has\nincreasingly focused on the explicit extraction of 3D features, while still\nfacing challenges such as the lack of large-scale robotic 3D data and the\npotential loss of spatial geometry. To address these limitations, we propose\nthe Lift3D framework, which progressively enhances 2D foundation models with\nimplicit and explicit 3D robotic representations to construct a robust 3D\nmanipulation policy. Specifically, we first design a task-aware masked\nautoencoder that masks task-relevant affordance patches and reconstructs depth\ninformation, enhancing the 2D foundation model's implicit 3D robotic\nrepresentation. After self-supervised fine-tuning, we introduce a 2D\nmodel-lifting strategy that establishes a positional mapping between the input\n3D points and the positional embeddings of the 2D model. Based on the mapping,\nLift3D utilizes the 2D foundation model to directly encode point cloud data,\nleveraging large-scale pretrained knowledge to construct explicit 3D robotic\nrepresentations while minimizing spatial information loss. In experiments,\nLift3D consistently outperforms previous state-of-the-art methods across\nseveral simulation benchmarks and real-world scenarios.\n","authors":["Yueru Jia","Jiaming Liu","Sixiang Chen","Chenyang Gu","Zhilue Wang","Longzan Luo","Lily Lee","Pengwei Wang","Zhongyuan Wang","Renrui Zhang","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.18623v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10925v1","updated":"2024-12-14T18:33:29Z","published":"2024-12-14T18:33:29Z","title":"Video Representation Learning with Joint-Embedding Predictive\n  Architectures","summary":"  Video representation learning is an increasingly important topic in machine\nlearning research. We present Video JEPA with Variance-Covariance\nRegularization (VJ-VCR): a joint-embedding predictive architecture for\nself-supervised video representation learning that employs variance and\ncovariance regularization to avoid representation collapse. We show that hidden\nrepresentations from our VJ-VCR contain abstract, high-level information about\nthe input data. Specifically, they outperform representations obtained from a\ngenerative baseline on downstream tasks that require understanding of the\nunderlying dynamics of moving objects in the videos. Additionally, we explore\ndifferent ways to incorporate latent variables into the VJ-VCR framework that\ncapture information about uncertainty in the future in non-deterministic\nsettings.\n","authors":["Katrina Drozdov","Ravid Shwartz-Ziv","Yann LeCun"],"pdf_url":"https://arxiv.org/pdf/2412.10925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.01682v3","updated":"2024-12-14T17:46:13Z","published":"2024-12-02T16:29:06Z","title":"Diffusion Models with Anisotropic Gaussian Splatting for Image\n  Inpainting","summary":"  Image inpainting is a fundamental task in computer vision, aiming to restore\nmissing or corrupted regions in images realistically. While recent deep\nlearning approaches have significantly advanced the state-of-the-art,\nchallenges remain in maintaining structural continuity and generating coherent\ntextures, particularly in large missing areas. Diffusion models have shown\npromise in generating high-fidelity images but often lack the structural\nguidance necessary for realistic inpainting. We propose a novel inpainting\nmethod that combines diffusion models with anisotropic Gaussian splatting to\ncapture both local structures and global context effectively. By modeling\nmissing regions using anisotropic Gaussian functions that adapt to local image\ngradients, our approach provides structural guidance to the diffusion-based\ninpainting network. The Gaussian splat maps are integrated into the diffusion\nprocess, enhancing the model's ability to generate high-fidelity and\nstructurally coherent inpainting results. Extensive experiments demonstrate\nthat our method outperforms state-of-the-art techniques, producing visually\nplausible results with enhanced structural integrity and texture realism.\n","authors":["Jacob Fein-Ashley","Benjamin Fein-Ashley"],"pdf_url":"https://arxiv.org/pdf/2412.01682v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10908v1","updated":"2024-12-14T17:35:27Z","published":"2024-12-14T17:35:27Z","title":"Do large language vision models understand 3D shapes?","summary":"  Large vision language models (LVLM) are the leading A.I approach for\nachieving a general visual understanding of the world. Models such as GPT,\nClaude, Gemini, and LLama can use images to understand and analyze complex\nvisual scenes. 3D objects and shapes are the basic building blocks of the\nworld, recognizing them is a fundamental part of human perception. The goal of\nthis work is to test whether LVLMs truly understand 3D shapes by testing the\nmodels ability to identify and match objects of the exact same 3D shapes but\nwith different orientations and materials/textures. Test images were created\nusing CGI with a huge number of highly diverse objects, materials, and scenes.\nThe results of this test show that the ability of such models to match 3D\nshapes is significantly below humans but much higher than random guesses.\nSuggesting that the models have gained some abstract understanding of 3D shapes\nbut still trail far beyond humans in this task. Mainly it seems that the models\ncan easily identify the same object with a different orientation as well as\nmatching identical 3D shapes of the same orientation but with different\nmaterial textures. However, when both the object material and orientation are\nchanged, all models perform poorly relative to humans.\n","authors":["Sagi Eppel"],"pdf_url":"https://arxiv.org/pdf/2412.10908v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10902v1","updated":"2024-12-14T17:20:30Z","published":"2024-12-14T17:20:30Z","title":"Enhancing Road Crack Detection Accuracy with BsS-YOLO: Optimizing\n  Feature Fusion and Attention Mechanisms","summary":"  Effective road crack detection is crucial for road safety, infrastructure\npreservation, and extending road lifespan, offering significant economic\nbenefits. However, existing methods struggle with varied target scales, complex\nbackgrounds, and low adaptability to different environments. This paper\npresents the BsS-YOLO model, which optimizes multi-scale feature fusion through\nan enhanced Path Aggregation Network (PAN) and Bidirectional Feature Pyramid\nNetwork (BiFPN). The incorporation of weighted feature fusion improves feature\nrepresentation, boosting detection accuracy and robustness. Furthermore, a\nSimple and Effective Attention Mechanism (SimAM) within the backbone enhances\nprecision via spatial and channel-wise attention. The detection layer\nintegrates a Shuffle Attention mechanism, which rearranges and mixes features\nacross channels, refining key representations and further improving accuracy.\nExperimental results show that BsS-YOLO achieves a 2.8% increase in mean\naverage precision (mAP) for road crack detection, supporting its applicability\nin diverse scenarios, including urban road maintenance and highway inspections.\n","authors":["Jiaze Tang","Angzehua Feng","Vladimir Korkhov","Yuxi Pu"],"pdf_url":"https://arxiv.org/pdf/2412.10902v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10900v1","updated":"2024-12-14T17:13:30Z","published":"2024-12-14T17:13:30Z","title":"PEARL: Input-Agnostic Prompt Enhancement with Negative Feedback\n  Regulation for Class-Incremental Learning","summary":"  Class-incremental learning (CIL) aims to continuously introduce novel\ncategories into a classification system without forgetting previously learned\nones, thus adapting to evolving data distributions. Researchers are currently\nfocusing on leveraging the rich semantic information of pre-trained models\n(PTMs) in CIL tasks. Prompt learning has been adopted in CIL for its ability to\nadjust data distribution to better align with pre-trained knowledge. This paper\ncritically examines the limitations of existing methods from the perspective of\nprompt learning, which heavily rely on input information. To address this\nissue, we propose a novel PTM-based CIL method called Input-Agnostic Prompt\nEnhancement with Negative Feedback Regulation (PEARL). In PEARL, we implement\nan input-agnostic global prompt coupled with an adaptive momentum update\nstrategy to reduce the model's dependency on data distribution, thereby\neffectively mitigating catastrophic forgetting. Guided by negative feedback\nregulation, this adaptive momentum update addresses the parameter sensitivity\ninherent in fixed-weight momentum updates. Furthermore, it fosters the\ncontinuous enhancement of the prompt for new tasks by harnessing correlations\nbetween different tasks in CIL. Experiments on six benchmarks demonstrate that\nour method achieves state-of-the-art performance. The code is available at:\nhttps://github.com/qinyongchun/PEARL.\n","authors":["Yongchun Qin","Pengfei Fang","Hui Xue"],"pdf_url":"https://arxiv.org/pdf/2412.10900v1.pdf","comment":"Accepted by AAAI-25"},{"id":"http://arxiv.org/abs/2412.10891v1","updated":"2024-12-14T16:42:41Z","published":"2024-12-14T16:42:41Z","title":"Zigzag Diffusion Sampling: The Path to Success Is Zigzag","summary":"  Diffusion models, the most popular generative paradigm so far, can inject\nconditional information into the generation path to guide the latent towards\ndesired directions. However, existing text-to-image diffusion models often fail\nto maintain high image quality and high prompt-image alignment for those\nchallenging prompts. To mitigate this issue and enhance existing pretrained\ndiffusion models, we mainly made three contributions in this paper. First, we\ntheoretically and empirically demonstrate that the conditional guidance gap\nbetween the denoising and inversion processes captures prompt-related semantic\ninformation. Second, motivated by theoretical analysis, we derive Zigzag\nDiffusion Sampling (Z-Sampling), a novel sampling method that leverages the\nguidance gap to accumulate semantic information step-by-step throughout the\nentire generation process, leading to improved sampling results. Moreover, as a\nplug-and-play method, Z-Sampling can be generally applied to various diffusion\nmodels (e.g., accelerated ones and Transformer-based ones) with very limited\ncoding and computational costs. Third, our extensive experiments demonstrate\nthat Z-Sampling can generally and significantly enhance generation quality\nacross various benchmark datasets, diffusion models, and performance evaluation\nmetrics. For example, Z-Sampling can even make DreamShaper achieve the HPSv2\nwinning rate higher than 94% over the original results. Moreover, Z-Sampling\ncan further enhance existing diffusion models combined with other orthogonal\nmethods, including Diffusion-DPO.\n","authors":["Lichen Bai","Shitong Shao","Zikai Zhou","Zipeng Qi","Zhiqiang Xu","Haoyi Xiong","Zeke Xie"],"pdf_url":"https://arxiv.org/pdf/2412.10891v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10882v1","updated":"2024-12-14T16:16:37Z","published":"2024-12-14T16:16:37Z","title":"Integrating Generative and Physics-Based Models for Ptychographic\n  Imaging with Uncertainty Quantification","summary":"  Ptychography is a scanning coherent diffractive imaging technique that\nenables imaging nanometer-scale features in extended samples. One main\nchallenge is that widely used iterative image reconstruction methods often\nrequire significant amount of overlap between adjacent scan locations, leading\nto large data volumes and prolonged acquisition times. To address this key\nlimitation, this paper proposes a Bayesian inversion method for ptychography\nthat performs effectively even with less overlap between neighboring scan\nlocations. Furthermore, the proposed method can quantify the inherent\nuncertainty on the ptychographic object, which is created by the ill-posed\nnature of the ptychographic inverse problem. At a high level, the proposed\nmethod first utilizes a deep generative model to learn the prior distribution\nof the object and then generates samples from the posterior distribution of the\nobject by using a Markov Chain Monte Carlo algorithm. Our results from\nsimulated ptychography experiments show that the proposed framework can\nconsistently outperform a widely used iterative reconstruction algorithm in\ncases of reduced overlap. Moreover, the proposed framework can provide\nuncertainty estimates that closely correlate with the true error, which is not\navailable in practice. The project website is available here.\n","authors":["Canberk Ekmekci","Tekin Bicer","Zichao Wendy Di","Junjing Deng","Mujdat Cetin"],"pdf_url":"https://arxiv.org/pdf/2412.10882v1.pdf","comment":"Machine Learning and the Physical Sciences Workshop at NeurIPS 2024,\n  7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2412.06699v2","updated":"2024-12-14T15:42:05Z","published":"2024-12-09T17:44:56Z","title":"You See it, You Got it: Learning 3D Creation on Pose-Free Videos at\n  Scale","summary":"  Recent 3D generation models typically rely on limited-scale 3D `gold-labels'\nor 2D diffusion priors for 3D content creation. However, their performance is\nupper-bounded by constrained 3D priors due to the lack of scalable learning\nparadigms. In this work, we present See3D, a visual-conditional multi-view\ndiffusion model trained on large-scale Internet videos for open-world 3D\ncreation. The model aims to Get 3D knowledge by solely Seeing the visual\ncontents from the vast and rapidly growing video data -- You See it, You Got\nit. To achieve this, we first scale up the training data using a proposed data\ncuration pipeline that automatically filters out multi-view inconsistencies and\ninsufficient observations from source videos. This results in a high-quality,\nrichly diverse, large-scale dataset of multi-view images, termed WebVi3D,\ncontaining 320M frames from 16M video clips. Nevertheless, learning generic 3D\npriors from videos without explicit 3D geometry or camera pose annotations is\nnontrivial, and annotating poses for web-scale videos is prohibitively\nexpensive. To eliminate the need for pose conditions, we introduce an\ninnovative visual-condition - a purely 2D-inductive visual signal generated by\nadding time-dependent noise to the masked video data. Finally, we introduce a\nnovel visual-conditional 3D generation framework by integrating See3D into a\nwarping-based pipeline for high-fidelity 3D generation. Our numerical and\nvisual comparisons on single and sparse reconstruction benchmarks show that\nSee3D, trained on cost-effective and scalable video data, achieves notable\nzero-shot and open-world generation capabilities, markedly outperforming models\ntrained on costly and constrained 3D datasets. Please refer to our project page\nat: https://vision.baai.ac.cn/see3d\n","authors":["Baorui Ma","Huachen Gao","Haoge Deng","Zhengxiong Luo","Tiejun Huang","Lulu Tang","Xinlong Wang"],"pdf_url":"https://arxiv.org/pdf/2412.06699v2.pdf","comment":"Project Page: https://vision.baai.ac.cn/see3d"},{"id":"http://arxiv.org/abs/2412.10861v1","updated":"2024-12-14T15:17:49Z","published":"2024-12-14T15:17:49Z","title":"Heterogeneous Graph Transformer for Multiple Tiny Object Tracking in\n  RGB-T Videos","summary":"  Tracking multiple tiny objects is highly challenging due to their weak\nappearance and limited features. Existing multi-object tracking algorithms\ngenerally focus on single-modality scenes, and overlook the complementary\ncharacteristics of tiny objects captured by multiple remote sensors. To enhance\ntracking performance by integrating complementary information from multiple\nsources, we propose a novel framework called {HGT-Track (Heterogeneous Graph\nTransformer based Multi-Tiny-Object Tracking)}. Specifically, we first employ a\nTransformer-based encoder to embed images from different modalities.\nSubsequently, we utilize Heterogeneous Graph Transformer to aggregate spatial\nand temporal information from multiple modalities to generate detection and\ntracking features. Additionally, we introduce a target re-detection module\n(ReDet) to ensure tracklet continuity by maintaining consistency across\ndifferent modalities. Furthermore, this paper introduces the first benchmark\nVT-Tiny-MOT (Visible-Thermal Tiny Multi-Object Tracking) for RGB-T fused\nmultiple tiny object tracking. Extensive experiments are conducted on\nVT-Tiny-MOT, and the results have demonstrated the effectiveness of our method.\nCompared to other state-of-the-art methods, our method achieves better\nperformance in terms of MOTA (Multiple-Object Tracking Accuracy) and ID-F1\nscore. The code and dataset will be made available at\nhttps://github.com/xuqingyu26/HGTMT.\n","authors":["Qingyu Xu","Longguang Wang","Weidong Sheng","Yingqian Wang","Chao Xiao","Chao Ma","Wei An"],"pdf_url":"https://arxiv.org/pdf/2412.10861v1.pdf","comment":"N/A"},{"id":"http://arxiv.org/abs/2412.08421v2","updated":"2024-12-14T15:14:24Z","published":"2024-12-11T14:37:21Z","title":"PointCFormer: a Relation-based Progressive Feature Extraction Network\n  for Point Cloud Completion","summary":"  Point cloud completion aims to reconstruct the complete 3D shape from\nincomplete point clouds, and it is crucial for tasks such as 3D object\ndetection and segmentation. Despite the continuous advances in point cloud\nanalysis techniques, feature extraction methods are still confronted with\napparent limitations. The sparse sampling of point clouds, used as inputs in\nmost methods, often results in a certain loss of global structure information.\nMeanwhile, traditional local feature extraction methods usually struggle to\ncapture the intricate geometric details. To overcome these drawbacks, we\nintroduce PointCFormer, a transformer framework optimized for robust global\nretention and precise local detail capture in point cloud completion. This\nframework embraces several key advantages. First, we propose a relation-based\nlocal feature extraction method to perceive local delicate geometry\ncharacteristics. This approach establishes a fine-grained relationship metric\nbetween the target point and its k-nearest neighbors, quantifying each\nneighboring point's contribution to the target point's local features.\nSecondly, we introduce a progressive feature extractor that integrates our\nlocal feature perception method with self-attention. Starting with a denser\nsampling of points as input, it iteratively queries long-distance global\ndependencies and local neighborhood relationships. This extractor maintains\nenhanced global structure and refined local details, without generating\nsubstantial computational overhead. Additionally, we develop a correction\nmodule after generating point proxies in the latent space to reintroduce denser\ninformation from the input points, enhancing the representation capability of\nthe point proxies. PointCFormer demonstrates state-of-the-art performance on\nseveral widely used benchmarks. Our code is available at\nhttps://github.com/Zyyyyy0926/PointCFormer_Plus_Pytorch.\n","authors":["Yi Zhong","Weize Quan","Dong-ming Yan","Jie Jiang","Yingmei Wei"],"pdf_url":"https://arxiv.org/pdf/2412.08421v2.pdf","comment":"9 pages, 8 figures, AAAI 2025, references added"},{"id":"http://arxiv.org/abs/2412.10857v1","updated":"2024-12-14T15:11:42Z","published":"2024-12-14T15:11:42Z","title":"Robust Recognition of Persian Isolated Digits in Speech using Deep\n  Neural Network","summary":"  In recent years, artificial intelligence (AI) has advanced significantly in\nspeech recognition applications. Speech-based interaction with digital systems,\nparticularly AI-driven digit recognition, has emerged as a prominent\napplication. However, existing neural network-based methods often neglect the\nimpact of noise, leading to reduced accuracy in noisy environments. This study\ntackles the challenge of recognizing the isolated spoken Persian numbers (zero\nto nine), particularly distinguishing phonetically similar numbers, in noisy\nenvironments. The proposed method, which is designed for speaker-independent\nrecognition, combines residual convolutional neural network and bidirectional\ngated recurrent unit in a hybrid structure for Persian number recognition. This\nmethod employs word units as input instead of phoneme units. Audio data from 51\nspeakers of FARSDIGIT1 database are utilized after augmentation using various\nnoises, and the Mel-Frequency Cepstral Coefficients (MFCC) technique is\nemployed for feature extraction. The experimental results show the proposed\nmethod efficacy with 98.53%, 96.10%, and 95.9% recognition accuracy for\ntraining, validation, and test, respectively. In the noisy environment, the\nproposed method exhibits an average performance improvement of 26.88% over\nphoneme unit-based LSTM method for Persian numbers. In addition, the accuracy\nof the proposed method is 7.61% better than that of the Mel-scale Two Dimension\nRoot Cepstrum Coefficients (MTDRCC) feature extraction technique along with MLP\nmodel in the test data for the same dataset.\n","authors":["Ali Nasr-Esfahani","Mehdi Bekrani","Roozbeh Rajabi"],"pdf_url":"https://arxiv.org/pdf/2412.10857v1.pdf","comment":"15 pages, submitted to journal"},{"id":"http://arxiv.org/abs/2412.10853v1","updated":"2024-12-14T14:54:44Z","published":"2024-12-14T14:54:44Z","title":"SEW: Self-calibration Enhanced Whole Slide Pathology Image Analysis","summary":"  Pathology images are considered the \"gold standard\" for cancer diagnosis and\ntreatment, with gigapixel images providing extensive tissue and cellular\ninformation. Existing methods fail to simultaneously extract global structural\nand local detail f\n","authors":["Haoming Luo","Xiaotian Yu","Shengxuming Zhang","Jiabin Xia","Yang Jian","Yuning Sun","Liang Xue","Mingli Song","Jing Zhang","Xiuming Zhang","Zunlei Feng"],"pdf_url":"https://arxiv.org/pdf/2412.10853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10846v1","updated":"2024-12-14T14:38:27Z","published":"2024-12-14T14:38:27Z","title":"Detecting Activities of Daily Living in Egocentric Video to\n  Contextualize Hand Use at Home in Outpatient Neurorehabilitation Settings","summary":"  Wearable egocentric cameras and machine learning have the potential to\nprovide clinicians with a more nuanced understanding of patient hand use at\nhome after stroke and spinal cord injury (SCI). However, they require detailed\ncontextual information (i.e., activities and object interactions) to\neffectively interpret metrics and meaningfully guide therapy planning. We\ndemonstrate that an object-centric approach, focusing on what objects patients\ninteract with rather than how they move, can effectively recognize Activities\nof Daily Living (ADL) in real-world rehabilitation settings. We evaluated our\nmodels on a complex dataset collected in the wild comprising 2261 minutes of\negocentric video from 16 participants with impaired hand function. By\nleveraging pre-trained object detection and hand-object interaction models, our\nsystem achieves robust performance across different impairment levels and\nenvironments, with our best model achieving a mean weighted F1-score of 0.78\n+/- 0.12 and maintaining an F1-score > 0.5 for all participants using\nleave-one-subject-out cross validation. Through qualitative analysis, we\nobserve that this approach generates clinically interpretable information about\nfunctional object use while being robust to patient-specific movement\nvariations, making it particularly suitable for rehabilitation contexts with\nprevalent upper limb impairment.\n","authors":["Adesh Kadambi","José Zariffa"],"pdf_url":"https://arxiv.org/pdf/2412.10846v1.pdf","comment":"To be submitted to IEEE Transactions on Neural Systems and\n  Rehabilitation Engineering. 11 pages, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2412.10843v1","updated":"2024-12-14T14:31:36Z","published":"2024-12-14T14:31:36Z","title":"Learning Semantic-Aware Representation in Visual-Language Models for\n  Multi-Label Recognition with Partial Labels","summary":"  Multi-label recognition with partial labels (MLR-PL), in which only some\nlabels are known while others are unknown for each image, is a practical task\nin computer vision, since collecting large-scale and complete multi-label\ndatasets is difficult in real application scenarios. Recently, vision language\nmodels (e.g. CLIP) have demonstrated impressive transferability to downstream\ntasks in data limited or label limited settings. However, current CLIP-based\nmethods suffer from semantic confusion in MLR task due to the lack of\nfine-grained information in the single global visual and textual representation\nfor all categories. In this work, we address this problem by introducing a\nsemantic decoupling module and a category-specific prompt optimization method\nin CLIP-based framework. Specifically, the semantic decoupling module following\nthe visual encoder learns category-specific feature maps by utilizing the\nsemantic-guided spatial attention mechanism. Moreover, the category-specific\nprompt optimization method is introduced to learn text representations aligned\nwith category semantics. Therefore, the prediction of each category is\nindependent, which alleviate the semantic confusion problem. Extensive\nexperiments on Microsoft COCO 2014 and Pascal VOC 2007 datasets demonstrate\nthat the proposed framework significantly outperforms current state-of-art\nmethods with a simpler model structure. Additionally, visual analysis shows\nthat our method effectively separates information from different categories and\nachieves better performance compared to CLIP-based baseline method.\n","authors":["Haoxian Ruan","Zhihua Xu","Zhijing Yang","Yongyi Lu","Jinghui Qin","Tianshui Chen"],"pdf_url":"https://arxiv.org/pdf/2412.10843v1.pdf","comment":"ACM Transactions on Multimedia Computing Communications and\n  Applications"},{"id":"http://arxiv.org/abs/2412.10840v1","updated":"2024-12-14T14:30:05Z","published":"2024-12-14T14:30:05Z","title":"Attention-driven GUI Grounding: Leveraging Pretrained Multimodal Large\n  Language Models without Fine-Tuning","summary":"  Recent advancements in Multimodal Large Language Models (MLLMs) have\ngenerated significant interest in their ability to autonomously interact with\nand interpret Graphical User Interfaces (GUIs). A major challenge in these\nsystems is grounding-accurately identifying critical GUI components such as\ntext or icons based on a GUI image and a corresponding text query.\nTraditionally, this task has relied on fine-tuning MLLMs with specialized\ntraining data to predict component locations directly. However, in this paper,\nwe propose a novel Tuning-free Attention-driven Grounding (TAG) method that\nleverages the inherent attention patterns in pretrained MLLMs to accomplish\nthis task without the need for additional fine-tuning. Our method involves\nidentifying and aggregating attention maps from specific tokens within a\ncarefully constructed query prompt. Applied to MiniCPM-Llama3-V 2.5, a\nstate-of-the-art MLLM, our tuning-free approach achieves performance comparable\nto tuning-based methods, with notable success in text localization.\nAdditionally, we demonstrate that our attention map-based grounding technique\nsignificantly outperforms direct localization predictions from MiniCPM-Llama3-V\n2.5, highlighting the potential of using attention maps from pretrained MLLMs\nand paving the way for future innovations in this domain.\n","authors":["Hai-Ming Xu","Qi Chen","Lei Wang","Lingqiao Liu"],"pdf_url":"https://arxiv.org/pdf/2412.10840v1.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2402.12928v5","updated":"2024-12-14T14:04:28Z","published":"2024-02-20T11:28:50Z","title":"A Literature Review of Literature Reviews in Pattern Analysis and\n  Machine Intelligence","summary":"  The rapid advancements in Pattern Analysis and Machine Intelligence (PAMI)\nhave led to an overwhelming expansion of scientific knowledge, spawning\nnumerous literature reviews aimed at collecting and synthesizing fragmented\ninformation. This paper presents a thorough analysis of these literature\nreviews within the PAMI field, and tries to address three core research\nquestions: (1) What are the prevalent structural and statistical\ncharacteristics of PAMI literature reviews? (2) What strategies can researchers\nemploy to efficiently navigate the growing corpus of reviews? (3) What are the\nadvantages and limitations of AI-generated reviews compared to human-authored\nones? To address the first research question, we begin with a narrative\noverview to highlight common preferences in composing PAMI reviews, followed by\na statistical analysis to quantitatively uncover patterns in these preferences.\nOur findings reveal several key insights. First, fewer than 20% of PAMI reviews\ncurrently comply with PRISMA standards, although this proportion is gradually\nincreasing. Second, there is a moderate positive correlation between the\nquality of references and the scholarly impact of reviews, emphasizing the\nimportance of reference selection. To further assist researchers in efficiently\nmanaging the rapidly growing number of literature reviews, we introduce four\nnovel, real-time, article-level bibliometric indicators that facilitate the\nscreening of numerous reviews. Finally, our comparative analysis reveals that\nAI-generated reviews currently fall short of human-authored ones in accurately\nevaluating the academic significance of newly published articles and\nintegrating rich visual elements, which limits their practical utility.\nOverall, this study provides a deeper understanding of PAMI literature reviews\nby uncovering key trends, evaluating current practices, and highlighting areas\nfor future improvement.\n","authors":["Penghai Zhao","Xin Zhang","Jiayue Cao","Ming-Ming Cheng","Jian Yang","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2402.12928v5.pdf","comment":"V2, V3, and V4 with incremental quality improvements. V5 introduces\n  major updates, featuring 27 pages, 16 figures, and 12 tables"},{"id":"http://arxiv.org/abs/2412.10834v1","updated":"2024-12-14T13:39:56Z","published":"2024-12-14T13:39:56Z","title":"SegACIL: Solving the Stability-Plasticity Dilemma in Class-Incremental\n  Semantic Segmentation","summary":"  While deep learning has made remarkable progress in recent years, models\ncontinue to struggle with catastrophic forgetting when processing continuously\nincoming data. This issue is particularly critical in continual learning, where\nthe balance between retaining prior knowledge and adapting to new\ninformation-known as the stability-plasticity dilemma-remains a significant\nchallenge. In this paper, we propose SegACIL, a novel continual learning method\nfor semantic segmentation based on a linear closed-form solution. Unlike\ntraditional methods that require multiple epochs for training, SegACIL only\nrequires a single epoch, significantly reducing computational costs.\nFurthermore, we provide a theoretical analysis demonstrating that SegACIL\nachieves performance on par with joint learning, effectively retaining\nknowledge from previous data which makes it to keep both stability and\nplasticity at the same time. Extensive experiments on the Pascal VOC2012\ndataset show that SegACIL achieves superior performance in the sequential,\ndisjoint, and overlap settings, offering a robust solution to the challenges of\nclass-incremental semantic segmentation. Code is available at\nhttps://github.com/qwrawq/SegACIL.\n","authors":["Jiaxu Li","Songning Lai","Rui Li","Di Fang","Kejia Fan","Jianheng Tang","Yuhan Zhao","Rongchang Zhao","Dongzhan Zhou","Yutao Yue","Huiping Zhuang"],"pdf_url":"https://arxiv.org/pdf/2412.10834v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10831v1","updated":"2024-12-14T13:28:40Z","published":"2024-12-14T13:28:40Z","title":"Unbiased General Annotated Dataset Generation","summary":"  Pre-training backbone networks on a general annotated dataset (e.g.,\nImageNet) that comprises numerous manually collected images with category\nannotations has proven to be indispensable for enhancing the generalization\ncapacity of downstream visual tasks. However, those manually collected images\noften exhibit bias, which is non-transferable across either categories or\ndomains, thus causing the model's generalization capacity degeneration. To\nmitigate this problem, we present an unbiased general annotated dataset\ngeneration framework (ubGen). Instead of expensive manual collection, we aim at\ndirectly generating unbiased images with category annotations. To achieve this\ngoal, we propose to leverage the advantage of a multimodal foundation model\n(e.g., CLIP), in terms of aligning images in an unbiased semantic space defined\nby language. Specifically, we develop a bi-level semantic alignment loss, which\nnot only forces all generated images to be consistent with the semantic\ndistribution of all categories belonging to the target dataset in an\nadversarial learning manner, but also requires each generated image to match\nthe semantic description of its category name. In addition, we further cast an\nexisting image quality scoring model into a quality assurance loss to preserve\nthe quality of the generated image. By leveraging these two loss functions, we\ncan obtain an unbiased image generation model by simply fine-tuning a\npre-trained diffusion model using only all category names in the target dataset\nas input. Experimental results confirm that, compared with the manually labeled\ndataset or other synthetic datasets, the utilization of our generated unbiased\ndatasets leads to stable generalization capacity enhancement of different\nbackbone networks across various tasks, especially in tasks where the manually\nlabeled samples are scarce.\n","authors":["Dengyang Jiang","Haoyu Wang","Lei Zhang","Wei Wei","Guang Dai","Mengmeng Wang","Jingdong Wang","Yanning Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.10831v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2412.10826v1","updated":"2024-12-14T13:12:09Z","published":"2024-12-14T13:12:09Z","title":"Generative AI: A Pix2pix-GAN-Based Machine Learning Approach for Robust\n  and Efficient Lung Segmentation","summary":"  Chest radiography is climacteric in identifying different pulmonary diseases,\nyet radiologist workload and inefficiency can lead to misdiagnoses. Automatic,\naccurate, and efficient segmentation of lung from X-ray images of chest is\nparamount for early disease detection. This study develops a deep learning\nframework using a Pix2pix Generative Adversarial Network (GAN) to segment\npulmonary abnormalities from CXR images. This framework's image preprocessing\nand augmentation techniques were properly incorporated with a U-Net-inspired\ngenerator-discriminator architecture. Initially, it loaded the CXR images and\nmanual masks from the Montgomery and Shenzhen datasets, after which\npreprocessing and resizing were performed. A U-Net generator is applied to the\nprocessed CXR images that yield segmented masks; then, a Discriminator Network\ndifferentiates between the generated and real masks. Montgomery dataset served\nas the model's training set in the study, and the Shenzhen dataset was used to\ntest its robustness, which was used here for the first time. An adversarial\nloss and an L1 distance were used to optimize the model in training. All\nmetrics, which assess precision, recall, F1 score, and Dice coefficient, prove\nthe effectiveness of this framework in pulmonary abnormality segmentation. It,\ntherefore, sets the basis for future studies to be performed shortly using\ndiverse datasets that could further confirm its clinical applicability in\nmedical imaging.\n","authors":["Sharmin Akter"],"pdf_url":"https://arxiv.org/pdf/2412.10826v1.pdf","comment":"6 pages, 12 figures, 2 tables"},{"id":"http://arxiv.org/abs/2412.10824v1","updated":"2024-12-14T13:05:05Z","published":"2024-12-14T13:05:05Z","title":"Diffusion Model from Scratch","summary":"  Diffusion generative models are currently the most popular generative models.\nHowever, their underlying modeling process is quite complex, and starting\ndirectly with the seminal paper Denoising Diffusion Probability Model (DDPM)\ncan be challenging. This paper aims to assist readers in building a\nfoundational understanding of generative models by tracing the evolution from\nVAEs to DDPM through detailed mathematical derivations and a problem-oriented\nanalytical approach. It also explores the core ideas and improvement strategies\nof current mainstream methodologies, providing guidance for undergraduate and\ngraduate students interested in learning about diffusion models.\n","authors":["Wang Zhen","Dong Yunyun"],"pdf_url":"https://arxiv.org/pdf/2412.10824v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10817v1","updated":"2024-12-14T12:58:15Z","published":"2024-12-14T12:58:15Z","title":"Enhance Vision-Language Alignment with Noise","summary":"  With the advancement of pre-trained vision-language (VL) models, enhancing\nthe alignment between visual and linguistic modalities in downstream tasks has\nemerged as a critical challenge. Different from existing fine-tuning methods\nthat add extra modules to these two modalities, we investigate whether the\nfrozen model can be fine-tuned by customized noise. Our approach is motivated\nby the scientific study of beneficial noise, namely Positive-incentive Noise\n(Pi-noise or $\\pi$-noise) , which quantitatively analyzes the impact of noise.\nIt therefore implies a new scheme to learn beneficial noise distribution that\ncan be employed to fine-tune VL models. Focusing on few-shot classification\ntasks based on CLIP, we reformulate the inference process of CLIP and apply\nvariational inference, demonstrating how to generate $\\pi$-noise towards visual\nand linguistic modalities. Then, we propose Positive-incentive Noise Injector\n(PiNI), which can fine-tune CLIP via injecting noise into both visual and text\nencoders. Since the proposed method can learn the distribution of beneficial\nnoise, we can obtain more diverse embeddings of vision and language to better\nalign these two modalities for specific downstream tasks within limited\ncomputational resources. We evaluate different noise incorporation approaches\nand network architectures of PiNI. The evaluation across 11 datasets\ndemonstrates its effectiveness.\n","authors":["Sida Huang","Hongyuan Zhang","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2412.10817v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10816v1","updated":"2024-12-14T12:54:42Z","published":"2024-12-14T12:54:42Z","title":"Hyper-Fusion Network for Semi-Automatic Segmentation of Skin Lesions","summary":"  Automatic skin lesion segmentation methods based on fully convolutional\nnetworks (FCNs) are regarded as the state-of-the-art for accuracy. When there\nare, however, insufficient training data to cover all the variations in skin\nlesions, where lesions from different patients may have major differences in\nsize/shape/texture, these methods failed to segment the lesions that have image\ncharacteristics, which are less common in the training datasets. FCN-based\nsemi-automatic segmentation methods, which fuse user-inputs with high-level\nsemantic image features derived from FCNs offer an ideal complement to overcome\nlimitations of automatic segmentation methods. These semi-automatic methods\nrely on the automated state-of-the-art FCNs coupled with user-inputs for\nrefinements, and therefore being able to tackle challenging skin lesions.\nHowever, there are a limited number of FCN-based semi-automatic segmentation\nmethods and all these methods focused on early-fusion, where the first few\nconvolutional layers are used to fuse image features and user-inputs and then\nderive fused image features for segmentation. For early-fusion based methods,\nbecause the user-input information can be lost after the first few\nconvolutional layers, consequently, the user-input information will have\nlimited guidance and constraint in segmenting the challenging skin lesions with\ninhomogeneous textures and fuzzy boundaries. Hence, in this work, we introduce\na hyper-fusion network (HFN) to fuse the extracted user-inputs and image\nfeatures over multiple stages. We separately extract complementary features\nwhich then allows for an iterative use of user-inputs along all the fusion\nstages to refine the segmentation. We evaluated our HFN on ISIC 2017, ISIC 2016\nand PH2 datasets, and our results show that the HFN is more accurate and\ngeneralizable than the state-of-the-art methods.\n","authors":["Lei Bi","Michael Fulham","Jinman Kim"],"pdf_url":"https://arxiv.org/pdf/2412.10816v1.pdf","comment":"Accepted by the journal of medical image analysis"},{"id":"http://arxiv.org/abs/2412.10804v1","updated":"2024-12-14T12:09:41Z","published":"2024-12-14T12:09:41Z","title":"Medical Manifestation-Aware De-Identification","summary":"  Face de-identification (DeID) has been widely studied for common scenes, but\nremains under-researched for medical scenes, mostly due to the lack of\nlarge-scale patient face datasets. In this paper, we release MeMa, consisting\nof over 40,000 photo-realistic patient faces. MeMa is re-generated from massive\nreal patient photos. By carefully modulating the generation and data-filtering\nprocedures, MeMa avoids breaching real patient privacy, while ensuring rich and\nplausible medical manifestations. We recruit expert clinicians to annotate MeMa\nwith both coarse- and fine-grained labels, building the first medical-scene\nDeID benchmark. Additionally, we propose a baseline approach for this new\nmedical-aware DeID task, by integrating data-driven medical semantic priors\ninto the DeID procedure. Despite its conciseness and simplicity, our approach\nsubstantially outperforms previous ones. Dataset is available at\nhttps://github.com/tianyuan168326/MeMa-Pytorch.\n","authors":["Yuan Tian","Shuo Wang","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2412.10804v1.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2407.19451v4","updated":"2024-12-14T11:58:51Z","published":"2024-07-28T10:05:11Z","title":"Perm: A Parametric Representation for Multi-Style 3D Hair Modeling","summary":"  We present Perm, a learned parametric representation of human 3D hair\ndesigned to facilitate various hair-related applications. Unlike previous work\nthat jointly models the global hair structure and local curl patterns, we\npropose to disentangle them using a PCA-based strand representation in the\nfrequency domain, thereby allowing more precise editing and output control.\nSpecifically, we leverage our strand representation to fit and decompose hair\ngeometry textures into low- to high-frequency hair structures, termed guide\ntextures and residual textures, respectively. These decomposed textures are\nlater parameterized with different generative models, emulating common stages\nin the hair grooming process. We conduct extensive experiments to validate the\narchitecture design of Perm, and finally deploy the trained model as a generic\nprior to solve task-agnostic problems, further showcasing its flexibility and\nsuperiority in tasks such as single-view hair reconstruction, hairstyle\nediting, and hair-conditioned image generation. More details can be found on\nour project page: https://cs.yale.edu/homes/che/projects/perm/.\n","authors":["Chengan He","Xin Sun","Zhixin Shu","Fujun Luan","Sören Pirk","Jorge Alejandro Amador Herrera","Dominik L. Michels","Tuanfeng Y. Wang","Meng Zhang","Holly Rushmeier","Yi Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.19451v4.pdf","comment":"Project page: https://cs.yale.edu/homes/che/projects/perm/"},{"id":"http://arxiv.org/abs/2409.05650v2","updated":"2024-12-14T11:21:40Z","published":"2024-09-09T14:16:27Z","title":"Replay Consolidation with Label Propagation for Continual Object\n  Detection","summary":"  Continual Learning (CL) aims to learn new data while remembering previously\nacquired knowledge. In contrast to CL for image classification, CL for Object\nDetection faces additional challenges such as the missing annotations problem.\nIn this scenario, images from previous tasks may contain instances of unknown\nclasses that could reappear as labeled in future tasks, leading to task\ninterference in replay-based approaches. Consequently, most approaches in the\nliterature have focused on distillation-based techniques, which are effective\nwhen there is a significant class overlap between tasks. In our work, we\npropose an alternative to distillation-based approaches with a novel approach\ncalled Replay Consolidation with Label Propagation for Object Detection\n(RCLPOD). RCLPOD enhances the replay memory by improving the quality of the\nstored samples through a technique that promotes class balance while also\nimproving the quality of the ground truth associated with these samples through\na technique called label propagation. RCLPOD outperforms existing techniques on\nwell-established benchmarks such as VOC and COC. Moreover, our approach is\ndeveloped to work with modern architectures like YOLOv8, making it suitable for\ndynamic, real-world applications such as autonomous driving and robotics, where\ncontinuous learning and resource efficiency are essential.\n","authors":["Riccardo De Monte","Davide Dalle Pezze","Marina Ceccon","Francesco Pasti","Francesco Paissan","Elisabetta Farella","Gian Antonio Susto","Nicola Bellotto"],"pdf_url":"https://arxiv.org/pdf/2409.05650v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10795v1","updated":"2024-12-14T11:14:05Z","published":"2024-12-14T11:14:05Z","title":"Reliable and superior elliptic Fourier descriptor normalization and its\n  application software ElliShape with efficient image processing","summary":"  Elliptic Fourier analysis (EFA) is a powerful tool for shape analysis, which\nis often employed in geometric morphometrics. However, the normalization of\nelliptic Fourier descriptors has persistently posed challenges in obtaining\nunique results in basic contour transformations, requiring extensive manual\nalignment. Additionally, contemporary contour/outline extraction methods often\nstruggle to handle complex digital images. Here, we reformulated the procedure\nof EFDs calculation to improve computational efficiency and introduced a novel\napproach for EFD normalization, termed true EFD normalization, which remains\ninvariant under all basic contour transformations. These improvements are\ncrucial for processing large sets of contour curves collected from different\nplatforms with varying transformations. Based on these improvements, we\ndeveloped ElliShape, a user-friendly software. Particularly, the improved\ncontour/outline extraction employs an interactive approach that combines\nautomatic contour generation for efficiency with manual correction for\nessential modifications and refinements. We evaluated ElliShape's stability,\nrobustness, and ease of use by comparing it with existing software using\nstandard datasets. ElliShape consistently produced reliable reconstructed\nshapes and normalized EFD values across different contours and transformations,\nand it demonstrated superior performance in visualization and efficient\nprocessing of various digital images for contour analysis.The output annotated\nimages and EFDs could be utilized in deep learning-based data training, thereby\nadvancing artificial intelligence in botany and offering innovative solutions\nfor critical challenges in biodiversity conservation, species classification,\necosystem function assessment, and related critical issues.\n","authors":["Hui Wu","Jia-Jie Yang","Chao-Qun Li","Jin-Hua Ran","Ren-Hua Peng","Xiao-Quan Wang"],"pdf_url":"https://arxiv.org/pdf/2412.10795v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10786v1","updated":"2024-12-14T10:47:52Z","published":"2024-12-14T10:47:52Z","title":"Optimizing Few-Step Sampler for Diffusion Probabilistic Model","summary":"  Diffusion Probabilistic Models (DPMs) have demonstrated exceptional\ncapability of generating high-quality and diverse images, but their practical\napplication is hindered by the intensive computational cost during inference.\nThe DPM generation process requires solving a Probability-Flow Ordinary\nDifferential Equation (PF-ODE), which involves discretizing the integration\ndomain into intervals for numerical approximation. This corresponds to the\nsampling schedule of a diffusion ODE solver, and we notice the solution from a\nfirst-order solver can be expressed as a convex combination of model outputs at\nall scheduled time-steps. We derive an upper bound for the discretization error\nof the sampling schedule, which can be efficiently optimized with Monte-Carlo\nestimation. Building on these theoretical results, we purpose a two-phase\nalternating optimization algorithm. In Phase-1, the sampling schedule is\noptimized for the pre-trained DPM; in Phase-2, the DPM further tuned on the\nselected time-steps. Experiments on a pre-trained DPM for ImageNet64 dataset\ndemonstrate the purposed method consistently improves the baseline across\nvarious number of sampling steps.\n","authors":["Jen-Yuan Huang"],"pdf_url":"https://arxiv.org/pdf/2412.10786v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10785v1","updated":"2024-12-14T10:47:17Z","published":"2024-12-14T10:47:17Z","title":"StyleDiT: A Unified Framework for Diverse Child and Partner Faces\n  Synthesis with Style Latent Diffusion Transformer","summary":"  Kinship face synthesis is a challenging problem due to the scarcity and low\nquality of the available kinship data. Existing methods often struggle to\ngenerate descendants with both high diversity and fidelity while precisely\ncontrolling facial attributes such as age and gender. To address these issues,\nwe propose the Style Latent Diffusion Transformer (StyleDiT), a novel framework\nthat integrates the strengths of StyleGAN with the diffusion model to generate\nhigh-quality and diverse kinship faces. In this framework, the rich facial\npriors of StyleGAN enable fine-grained attribute control, while our conditional\ndiffusion model is used to sample a StyleGAN latent aligned with the kinship\nrelationship of conditioning images by utilizing the advantage of modeling\ncomplex kinship relationship distribution. StyleGAN then handles latent\ndecoding for final face generation. Additionally, we introduce the Relational\nTrait Guidance (RTG) mechanism, enabling independent control of influencing\nconditions, such as each parent's facial image. RTG also enables a fine-grained\nadjustment between the diversity and fidelity in synthesized faces.\nFurthermore, we extend the application to an unexplored domain: predicting a\npartner's facial images using a child's image and one parent's image within the\nsame framework. Extensive experiments demonstrate that our StyleDiT outperforms\nexisting methods by striking an excellent balance between generating diverse\nand high-fidelity kinship faces.\n","authors":["Pin-Yen Chiu","Dai-Jie Wu","Po-Hsun Chu","Chia-Hsuan Hsu","Hsiang-Chen Chiu","Chih-Yu Wang","Jun-Cheng Chen"],"pdf_url":"https://arxiv.org/pdf/2412.10785v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10783v1","updated":"2024-12-14T10:39:55Z","published":"2024-12-14T10:39:55Z","title":"Video Diffusion Transformers are In-Context Learners","summary":"  This paper investigates a solution for enabling in-context capabilities of\nvideo diffusion transformers, with minimal tuning required for activation.\nSpecifically, we propose a simple pipeline to leverage in-context generation:\n($\\textbf{i}$) concatenate videos along spacial or time dimension,\n($\\textbf{ii}$) jointly caption multi-scene video clips from one source, and\n($\\textbf{iii}$) apply task-specific fine-tuning using carefully curated small\ndatasets. Through a series of diverse controllable tasks, we demonstrate\nqualitatively that existing advanced text-to-video models can effectively\nperform in-context generation. Notably, it allows for the creation of\nconsistent multi-scene videos exceeding 30 seconds in duration, without\nadditional computational overhead. Importantly, this method requires no\nmodifications to the original models, results in high-fidelity video outputs\nthat better align with prompt specifications and maintain role consistency. Our\nframework presents a valuable tool for the research community and offers\ncritical insights for advancing product-level controllable video generation\nsystems. The data, code, and model weights are publicly available at:\n\\url{https://github.com/feizc/Video-In-Context}.\n","authors":["Zhengcong Fei","Di Qiu","Changqian Yu","Debang Li","Mingyuan Fan","Xiang Wen"],"pdf_url":"https://arxiv.org/pdf/2412.10783v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07511v2","updated":"2024-12-14T10:36:04Z","published":"2024-12-10T13:48:11Z","title":"Stealthy and Robust Backdoor Attack against 3D Point Clouds through\n  Additional Point Features","summary":"  Recently, 3D backdoor attacks have posed a substantial threat to 3D Deep\nNeural Networks (3D DNNs) designed for 3D point clouds, which are extensively\ndeployed in various security-critical applications. Although the existing 3D\nbackdoor attacks achieved high attack performance, they remain vulnerable to\npreprocessing-based defenses (e.g., outlier removal and rotation augmentation)\nand are prone to detection by human inspection. In pursuit of a more\nchallenging-to-defend and stealthy 3D backdoor attack, this paper introduces\nthe Stealthy and Robust Backdoor Attack (SRBA), which ensures robustness and\nstealthiness through intentional design considerations. The key insight of our\nattack involves applying a uniform shift to the additional point features of\npoint clouds (e.g., reflection intensity) widely utilized as part of inputs for\n3D DNNs as the trigger. Without altering the geometric information of the point\nclouds, our attack ensures visual consistency between poisoned and benign\nsamples, and demonstrate robustness against preprocessing-based defenses. In\naddition, to automate our attack, we employ Bayesian Optimization (BO) to\nidentify the suitable trigger. Extensive experiments suggest that SRBA achieves\nan attack success rate (ASR) exceeding 94% in all cases, and significantly\noutperforms previous SOTA methods when multiple preprocessing operations are\napplied during training.\n","authors":["Xiaoyang Ning","Qing Xie","Jinyu Xu","Wenbo Jiang","Jiachen Li","Yanchun Ma"],"pdf_url":"https://arxiv.org/pdf/2412.07511v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10778v1","updated":"2024-12-14T10:12:22Z","published":"2024-12-14T10:12:22Z","title":"Sample-efficient Unsupervised Policy Cloning from Ensemble\n  Self-supervised Labeled Videos","summary":"  Current advanced policy learning methodologies have demonstrated the ability\nto develop expert-level strategies when provided enough information. However,\ntheir requirements, including task-specific rewards, expert-labeled\ntrajectories, and huge environmental interactions, can be expensive or even\nunavailable in many scenarios. In contrast, humans can efficiently acquire\nskills within a few trials and errors by imitating easily accessible internet\nvideo, in the absence of any other supervision. In this paper, we try to let\nmachines replicate this efficient watching-and-learning process through\nUnsupervised Policy from Ensemble Self-supervised labeled Videos (UPESV), a\nnovel framework to efficiently learn policies from videos without any other\nexpert supervision. UPESV trains a video labeling model to infer the expert\nactions in expert videos, through several organically combined self-supervised\ntasks. Each task performs its own duties, and they together enable the model to\nmake full use of both expert videos and reward-free interactions for advanced\ndynamics understanding and robust prediction. Simultaneously, UPESV clones a\npolicy from the labeled expert videos, in turn collecting environmental\ninteractions for self-supervised tasks. After a sample-efficient and\nunsupervised (i.e., reward-free) training process, an advanced video-imitated\npolicy is obtained. Extensive experiments in sixteen challenging\nprocedurally-generated environments demonstrate that the proposed UPESV\nachieves state-of-the-art few-shot policy learning (outperforming five current\nadvanced baselines on 12/16 tasks) without exposure to any other supervision\nexcept videos. Detailed analysis is also provided, verifying the necessity of\neach self-supervised task employed in UPESV.\n","authors":["Xin Liu","Yaran Chen"],"pdf_url":"https://arxiv.org/pdf/2412.10778v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10776v1","updated":"2024-12-14T10:03:08Z","published":"2024-12-14T10:03:08Z","title":"Boosting ViT-based MRI Reconstruction from the Perspectives of Frequency\n  Modulation, Spatial Purification, and Scale Diversification","summary":"  The accelerated MRI reconstruction process presents a challenging ill-posed\ninverse problem due to the extensive under-sampling in k-space. Recently,\nVision Transformers (ViTs) have become the mainstream for this task,\ndemonstrating substantial performance improvements. However, there are still\nthree significant issues remain unaddressed: (1) ViTs struggle to capture\nhigh-frequency components of images, limiting their ability to detect local\ntextures and edge information, thereby impeding MRI restoration; (2) Previous\nmethods calculate multi-head self-attention (MSA) among both related and\nunrelated tokens in content, introducing noise and significantly increasing\ncomputational burden; (3) The naive feed-forward network in ViTs cannot model\nthe multi-scale information that is important for image restoration. In this\npaper, we propose FPS-Former, a powerful ViT-based framework, to address these\nissues from the perspectives of frequency modulation, spatial purification, and\nscale diversification. Specifically, for issue (1), we introduce a frequency\nmodulation attention module to enhance the self-attention map by adaptively\nre-calibrating the frequency information in a Laplacian pyramid. For issue (2),\nwe customize a spatial purification attention module to capture interactions\namong closely related tokens, thereby reducing redundant or irrelevant feature\nrepresentations. For issue (3), we propose an efficient feed-forward network\nbased on a hybrid-scale fusion strategy. Comprehensive experiments conducted on\nthree public datasets show that our FPS-Former outperforms state-of-the-art\nmethods while requiring lower computational costs.\n","authors":["Yucong Meng","Zhiwei Yang","Yonghong Shi","Zhijian Song"],"pdf_url":"https://arxiv.org/pdf/2412.10776v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08475v2","updated":"2024-12-14T09:51:44Z","published":"2024-06-12T17:57:25Z","title":"Human-3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent\n  Diffusion Models","summary":"  Creating realistic avatars from a single RGB image is an attractive yet\nchallenging problem. Due to its ill-posed nature, recent works leverage\npowerful prior from 2D diffusion models pretrained on large datasets. Although\n2D diffusion models demonstrate strong generalization capability, they cannot\nprovide multi-view shape priors with guaranteed 3D consistency. We propose\nHuman 3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent\nDiffusion. Our key insight is that 2D multi-view diffusion and 3D\nreconstruction models provide complementary information for each other, and by\ncoupling them in a tight manner, we can fully leverage the potential of both\nmodels. We introduce a novel image-conditioned generative 3D Gaussian Splats\nreconstruction model that leverages the priors from 2D multi-view diffusion\nmodels, and provides an explicit 3D representation, which further guides the 2D\nreverse sampling process to have better 3D consistency. Experiments show that\nour proposed framework outperforms state-of-the-art methods and enables the\ncreation of realistic avatars from a single RGB image, achieving high-fidelity\nin both geometry and appearance. Extensive ablations also validate the efficacy\nof our design, (1) multi-view 2D priors conditioning in generative 3D\nreconstruction and (2) consistency refinement of sampling trajectory via the\nexplicit 3D representation. Our code and models will be released on\nhttps://yuxuan-xue.com/human-3diffusion.\n","authors":["Yuxuan Xue","Xianghui Xie","Riccardo Marin","Gerard Pons-Moll"],"pdf_url":"https://arxiv.org/pdf/2406.08475v2.pdf","comment":"Accepted to NeurIPS2024. Project Page:\n  https://yuxuan-xue.com/human-3diffusion"},{"id":"http://arxiv.org/abs/2412.10768v1","updated":"2024-12-14T09:36:10Z","published":"2024-12-14T09:36:10Z","title":"VinTAGe: Joint Video and Text Conditioning for Holistic Audio Generation","summary":"  Recent advances in audio generation have focused on text-to-audio (T2A) and\nvideo-to-audio (V2A) tasks. However, T2A or V2A methods cannot generate\nholistic sounds (onscreen and off-screen). This is because T2A cannot generate\nsounds aligning with onscreen objects, while V2A cannot generate semantically\ncomplete (offscreen sounds missing). In this work, we address the task of\nholistic audio generation: given a video and a text prompt, we aim to generate\nboth onscreen and offscreen sounds that are temporally synchronized with the\nvideo and semantically aligned with text and video. Previous approaches for\njoint text and video-to-audio generation often suffer from modality bias,\nfavoring one modality over the other. To overcome this limitation, we introduce\nVinTAGe, a flow-based transformer model that jointly considers text and video\nto guide audio generation. Our framework comprises two key components: a\nVisual-Text Encoder and a Joint VT-SiT model. To reduce modality bias and\nimprove generation quality, we employ pretrained uni-modal text-to-audio and\nvideo-to-audio generation models for additional guidance. Due to the lack of\nappropriate benchmarks, we also introduce VinTAGe-Bench, a dataset of 636\nvideo-text-audio pairs containing both onscreen and offscreen sounds. Our\ncomprehensive experiments on VinTAGe-Bench demonstrate that joint text and\nvisual interaction is necessary for holistic audio generation. Furthermore,\nVinTAGe achieves state-of-the-art results on the VGGSound benchmark. Our source\ncode and pre-trained models will be released. Demo is available at:\nhttps://www.youtube.com/watch?v=QmqWhUjPkJI.\n","authors":["Saksham Singh Kushwaha","Yapeng Tian"],"pdf_url":"https://arxiv.org/pdf/2412.10768v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10765v1","updated":"2024-12-14T09:29:44Z","published":"2024-12-14T09:29:44Z","title":"Neural Network Meta Classifier: Improving the Reliability of Anomaly\n  Segmentation","summary":"  Deep neural networks (DNNs) are a contemporary solution for semantic\nsegmentation and are usually trained to operate on a predefined closed set of\nclasses. In open-set environments, it is possible to encounter semantically\nunknown objects or anomalies. Road driving is an example of such an environment\nin which, from a safety standpoint, it is important to ensure that a DNN\nindicates it is operating outside of its learned semantic domain. One possible\napproach to anomaly segmentation is entropy maximization, which is paired with\na logistic regression based post-processing step called meta classification,\nwhich is in turn used to improve the reliability of detection of anomalous\npixels. We propose to substitute the logistic regression meta classifier with a\nmore expressive lightweight fully connected neural network. We analyze\nadvantages and drawbacks of the proposed neural network meta classifier and\ndemonstrate its better performance over logistic regression. We also introduce\nthe concept of informative out-of-distribution examples which we show to\nimprove training results when using entropy maximization in practice. Finally,\nwe discuss the loss of interpretability and show that the behavior of logistic\nregression and neural network is strongly correlated.\n","authors":["Jurica Runtas","Tomislav Petkovic"],"pdf_url":"https://arxiv.org/pdf/2412.10765v1.pdf","comment":"Accepted to VISAPP 2025"},{"id":"http://arxiv.org/abs/2412.10761v1","updated":"2024-12-14T09:10:36Z","published":"2024-12-14T09:10:36Z","title":"Rebalanced Vision-Language Retrieval Considering Structure-Aware\n  Distillation","summary":"  Vision-language retrieval aims to search for similar instances in one\nmodality based on queries from another modality. The primary objective is to\nlearn cross-modal matching representations in a latent common space. Actually,\nthe assumption underlying cross-modal matching is modal balance, where each\nmodality contains sufficient information to represent the others. However,\nnoise interference and modality insufficiency often lead to modal imbalance,\nmaking it a common phenomenon in practice. The impact of imbalance on retrieval\nperformance remains an open question. In this paper, we first demonstrate that\nultimate cross-modal matching is generally sub-optimal for cross-modal\nretrieval when imbalanced modalities exist. The structure of instances in the\ncommon space is inherently influenced when facing imbalanced modalities, posing\na challenge to cross-modal similarity measurement. To address this issue, we\nemphasize the importance of meaningful structure-preserved matching.\nAccordingly, we propose a simple yet effective method to rebalance cross-modal\nmatching by learning structure-preserved matching representations.\nSpecifically, we design a novel multi-granularity cross-modal matching that\nincorporates structure-aware distillation alongside the cross-modal matching\nloss. While the cross-modal matching loss constraints instance-level matching,\nthe structure-aware distillation further regularizes the geometric consistency\nbetween learned matching representations and intra-modal representations\nthrough the developed relational matching. Extensive experiments on different\ndatasets affirm the superior cross-modal retrieval performance of our approach,\nsimultaneously enhancing single-modal retrieval capabilities compared to the\nbaseline models.\n","authors":["Yang Yang","Wenjuan Xi","Luping Zhou","Jinhui Tang"],"pdf_url":"https://arxiv.org/pdf/2412.10761v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10758v1","updated":"2024-12-14T09:04:32Z","published":"2024-12-14T09:04:32Z","title":"Optimizing Vision-Language Interactions Through Decoder-Only Models","summary":"  Vision-Language Models (VLMs) have emerged as key enablers for multimodal\ntasks, but their reliance on separate visual encoders introduces challenges in\nefficiency, scalability, and modality alignment. To address these limitations,\nwe propose MUDAIF (Multimodal Unified Decoder with Adaptive Input Fusion), a\ndecoder-only vision-language model that seamlessly integrates visual and\ntextual inputs through a novel Vision-Token Adapter (VTA) and adaptive\nco-attention mechanism. By eliminating the need for a visual encoder, MUDAIF\nachieves enhanced efficiency, flexibility, and cross-modal understanding.\nTrained on a large-scale dataset of 45M image-text pairs, MUDAIF consistently\noutperforms state-of-the-art methods across multiple benchmarks, including VQA,\nimage captioning, and multimodal reasoning tasks. Extensive analyses and human\nevaluations demonstrate MUDAIF's robustness, generalization capabilities, and\npractical usability, establishing it as a new standard in encoder-free\nvision-language models.\n","authors":["Kaito Tanaka","Benjamin Tan","Brian Wong"],"pdf_url":"https://arxiv.org/pdf/2412.10758v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10756v1","updated":"2024-12-14T08:56:22Z","published":"2024-12-14T08:56:22Z","title":"Damage Assessment after Natural Disasters with UAVs: Semantic Feature\n  Extraction using Deep Learning","summary":"  Unmanned aerial vehicle-assisted disaster recovery missions have been\npromoted recently due to their reliability and flexibility. Machine learning\nalgorithms running onboard significantly enhance the utility of UAVs by\nenabling real-time data processing and efficient decision-making, despite being\nin a resource-constrained environment. However, the limited bandwidth and\nintermittent connectivity make transmitting the outputs to ground stations\nchallenging. This paper proposes a novel semantic extractor that can be adopted\ninto any machine learning downstream task for identifying the critical data\nrequired for decision-making. The semantic extractor can be executed onboard\nwhich results in a reduction of data that needs to be transmitted to ground\nstations. We test the proposed architecture together with the semantic\nextractor on two publicly available datasets, FloodNet and RescueNet, for two\ndownstream tasks: visual question answering and disaster damage level\nclassification. Our experimental results demonstrate the proposed method\nmaintains high accuracy across different downstream tasks while significantly\nreducing the volume of transmitted data, highlighting the effectiveness of our\nsemantic extractor in capturing task-specific salient information.\n","authors":["Nethmi S. Hewawiththi","M. Mahesha Viduranga","Vanodhya G. Warnasooriya","Tharindu Fernando","Himal A. Suraweera","Sridha Sridharan","Clinton Fookes"],"pdf_url":"https://arxiv.org/pdf/2412.10756v1.pdf","comment":"11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2412.10749v1","updated":"2024-12-14T08:34:44Z","published":"2024-12-14T08:34:44Z","title":"Patch-level Sounding Object Tracking for Audio-Visual Question Answering","summary":"  Answering questions related to audio-visual scenes, i.e., the AVQA task, is\nbecoming increasingly popular. A critical challenge is accurately identifying\nand tracking sounding objects related to the question along the timeline. In\nthis paper, we present a new Patch-level Sounding Object Tracking (PSOT)\nmethod. It begins with a Motion-driven Key Patch Tracking (M-KPT) module, which\nrelies on visual motion information to identify salient visual patches with\nsignificant movements that are more likely to relate to sounding objects and\nquestions. We measure the patch-wise motion intensity map between neighboring\nvideo frames and utilize it to construct and guide a motion-driven graph\nnetwork. Meanwhile, we design a Sound-driven KPT (S-KPT) module to explicitly\ntrack sounding patches. This module also involves a graph network, with the\nadjacency matrix regularized by the audio-visual correspondence map. The M-KPT\nand S-KPT modules are performed in parallel for each temporal segment, allowing\nbalanced tracking of salient and sounding objects. Based on the tracked\npatches, we further propose a Question-driven KPT (Q-KPT) module to retain\npatches highly relevant to the question, ensuring the model focuses on the most\ninformative clues. The audio-visual-question features are updated during the\nprocessing of these modules, which are then aggregated for final answer\nprediction. Extensive experiments on standard datasets demonstrate the\neffectiveness of our method, achieving competitive performance even compared to\nrecent large-scale pretraining-based approaches.\n","authors":["Zhangbin Li","Jinxing Zhou","Jing Zhang","Shengeng Tang","Kun Li","Dan Guo"],"pdf_url":"https://arxiv.org/pdf/2412.10749v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.10748v1","updated":"2024-12-14T08:31:56Z","published":"2024-12-14T08:31:56Z","title":"A Pioneering Neural Network Method for Efficient and Robust Fuel\n  Sloshing Simulation in Aircraft","summary":"  Simulating fuel sloshing within aircraft tanks during flight is crucial for\naircraft safety research. Traditional methods based on Navier-Stokes equations\nare computationally expensive. In this paper, we treat fluid motion as point\ncloud transformation and propose the first neural network method specifically\ndesigned for simulating fuel sloshing in aircraft. This model is also the deep\nlearning model that is the first to be capable of stably modeling fluid\nparticle dynamics in such complex scenarios. Our triangle feature fusion design\nachieves an optimal balance among fluid dynamics modeling, momentum\nconservation constraints, and global stability control. Additionally, we\nconstructed the Fueltank dataset, the first dataset for aircraft fuel surface\nsloshing. It comprises 320,000 frames across four typical tank types and covers\na wide range of flight maneuvers, including multi-directional rotations. We\nconducted comprehensive experiments on both our dataset and the take-off\nscenario of the aircraft. Compared to existing neural network-based fluid\nsimulation algorithms, we significantly enhanced accuracy while maintaining\nhigh computational speed. Compared to traditional SPH methods, our speed\nimproved approximately 10 times. Furthermore, compared to traditional fluid\nsimulation software such as Flow3D, our computation speed increased by more\nthan 300 times.\n","authors":["Yu Chen","Shuai Zheng","Nianyi Wang","Menglong Jin","Yan Chang"],"pdf_url":"https://arxiv.org/pdf/2412.10748v1.pdf","comment":"This paper has been accepted by AAAI Conference on Artificial\n  Intelligence (AAAI-25)"},{"id":"http://arxiv.org/abs/2412.10741v1","updated":"2024-12-14T08:22:49Z","published":"2024-12-14T08:22:49Z","title":"RegMixMatch: Optimizing Mixup Utilization in Semi-Supervised Learning","summary":"  Consistency regularization and pseudo-labeling have significantly advanced\nsemi-supervised learning (SSL). Prior works have effectively employed Mixup for\nconsistency regularization in SSL. However, our findings indicate that applying\nMixup for consistency regularization may degrade SSL performance by\ncompromising the purity of artificial labels. Moreover, most pseudo-labeling\nbased methods utilize thresholding strategy to exclude low-confidence data,\naiming to mitigate confirmation bias; however, this approach limits the utility\nof unlabeled samples. To address these challenges, we propose RegMixMatch, a\nnovel framework that optimizes the use of Mixup with both high- and\nlow-confidence samples in SSL. First, we introduce semi-supervised RegMixup,\nwhich effectively addresses reduced artificial labels purity by using both\nmixed samples and clean samples for training. Second, we develop a class-aware\nMixup technique that integrates information from the top-2 predicted classes\ninto low-confidence samples and their artificial labels, reducing the\nconfirmation bias associated with these samples and enhancing their effective\nutilization. Experimental results demonstrate that RegMixMatch achieves\nstate-of-the-art performance across various SSL benchmarks.\n","authors":["Haorong Han","Jidong Yuan","Chixuan Wei","Zhongyang Yu"],"pdf_url":"https://arxiv.org/pdf/2412.10741v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10739v1","updated":"2024-12-14T08:19:30Z","published":"2024-12-14T08:19:30Z","title":"DSRC: Learning Density-insensitive and Semantic-aware Collaborative\n  Representation against Corruptions","summary":"  As a potential application of Vehicle-to-Everything (V2X) communication,\nmulti-agent collaborative perception has achieved significant success in 3D\nobject detection. While these methods have demonstrated impressive results on\nstandard benchmarks, the robustness of such approaches in the face of complex\nreal-world environments requires additional verification. To bridge this gap,\nwe introduce the first comprehensive benchmark designed to evaluate the\nrobustness of collaborative perception methods in the presence of natural\ncorruptions typical of real-world environments. Furthermore, we propose DSRC, a\nrobustness-enhanced collaborative perception method aiming to learn\nDensity-insensitive and Semantic-aware collaborative Representation against\nCorruptions. DSRC consists of two key designs: i) a semantic-guided\nsparse-to-dense distillation framework, which constructs multi-view dense\nobjects painted by ground truth bounding boxes to effectively learn\ndensity-insensitive and semantic-aware collaborative representation; ii) a\nfeature-to-point cloud reconstruction approach to better fuse critical\ncollaborative representation across agents. To thoroughly evaluate DSRC, we\nconduct extensive experiments on real-world and simulated datasets. The results\ndemonstrate that our method outperforms SOTA collaborative perception methods\nin both clean and corrupted conditions. Code is available at\nhttps://github.com/Terry9a/DSRC.\n","authors":["Jingyu Zhang","Yilei Wang","Lang Qian","Peng Sun","Zengwen Li","Sudong Jiang","Maolin Liu","Liang Song"],"pdf_url":"https://arxiv.org/pdf/2412.10739v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2312.04106v2","updated":"2024-12-14T08:09:27Z","published":"2023-12-07T07:41:10Z","title":"Do Not DeepFake Me: Privacy-Preserving Neural 3D Head Reconstruction\n  Without Sensitive Images","summary":"  While 3D head reconstruction is widely used for modeling, existing neural\nreconstruction approaches rely on high-resolution multi-view images, posing\nnotable privacy issues. Individuals are particularly sensitive to facial\nfeatures, and facial image leakage can lead to many malicious activities, such\nas unauthorized tracking and deepfake. In contrast, geometric data is less\nsusceptible to misuse due to its complex processing requirements, and absence\nof facial texture features. In this paper, we propose a novel two-stage 3D\nfacial reconstruction method aimed at avoiding exposure to sensitive facial\ninformation while preserving detailed geometric accuracy. Our approach first\nuses non-sensitive rear-head images for initial geometry and then refines this\ngeometry using processed privacy-removed gradient images. Extensive experiments\nshow that the resulting geometry is comparable to methods using full images,\nwhile the process is resistant to DeepFake applications and facial recognition\n(FR) systems, thereby proving its effectiveness in privacy protection.\n","authors":["Jiayi Kong","Xurui Song","Shuo Huai","Baixin Xu","Jun Luo","Ying He"],"pdf_url":"https://arxiv.org/pdf/2312.04106v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10734v1","updated":"2024-12-14T08:08:40Z","published":"2024-12-14T08:08:40Z","title":"OmniHD-Scenes: A Next-Generation Multimodal Dataset for Autonomous\n  Driving","summary":"  The rapid advancement of deep learning has intensified the need for\ncomprehensive data for use by autonomous driving algorithms. High-quality\ndatasets are crucial for the development of effective data-driven autonomous\ndriving solutions. Next-generation autonomous driving datasets must be\nmultimodal, incorporating data from advanced sensors that feature extensive\ndata coverage, detailed annotations, and diverse scene representation. To\naddress this need, we present OmniHD-Scenes, a large-scale multimodal dataset\nthat provides comprehensive omnidirectional high-definition data. The\nOmniHD-Scenes dataset combines data from 128-beam LiDAR, six cameras, and six\n4D imaging radar systems to achieve full environmental perception. The dataset\ncomprises 1501 clips, each approximately 30-s long, totaling more than 450K\nsynchronized frames and more than 5.85 million synchronized sensor data points.\nWe also propose a novel 4D annotation pipeline. To date, we have annotated 200\nclips with more than 514K precise 3D bounding boxes. These clips also include\nsemantic segmentation annotations for static scene elements. Additionally, we\nintroduce a novel automated pipeline for generation of the dense occupancy\nground truth, which effectively leverages information from non-key frames.\nAlongside the proposed dataset, we establish comprehensive evaluation metrics,\nbaseline models, and benchmarks for 3D detection and semantic occupancy\nprediction. These benchmarks utilize surround-view cameras and 4D imaging radar\nto explore cost-effective sensor solutions for autonomous driving applications.\nExtensive experiments demonstrate the effectiveness of our low-cost sensor\nconfiguration and its robustness under adverse conditions. Data will be\nreleased at https://www.2077ai.com/OmniHD-Scenes.\n","authors":["Lianqing Zheng","Long Yang","Qunshu Lin","Wenjin Ai","Minghao Liu","Shouyi Lu","Jianan Liu","Hongze Ren","Jingyue Mo","Xiaokai Bai","Jie Bai","Zhixiong Ma","Xichan Zhu"],"pdf_url":"https://arxiv.org/pdf/2412.10734v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10730v1","updated":"2024-12-14T07:58:24Z","published":"2024-12-14T07:58:24Z","title":"MAL: Cluster-Masked and Multi-Task Pretraining for Enhanced xLSTM Vision\n  Performance","summary":"  The Long Short-Term Memory (LSTM) networks have traditionally faced\nchallenges in scaling and effectively capturing complex dependencies in visual\ntasks. The xLSTM architecture has emerged to address these limitations,\nincorporating exponential gating and a parallel matrix memory structure to\nenhance performance and scalability. Despite these advancements, the potential\nof xLSTM in visual computing has not been fully realized, particularly in\nleveraging autoregressive techniques for improved feature extraction. In this\npaper, we introduce MAL (Cluster-Masked and Multi-Task Pretraining for Enhanced\nxLSTM Vision Performance), a novel framework that enhances xLSTM's capabilities\nthrough innovative pretraining strategies. We propose a cluster-masked masking\nmethod that significantly improves local feature capture and optimizes image\nscanning efficiency. Additionally, our universal encoder-decoder pretraining\napproach integrates multiple tasks, including image autoregression, depth\nestimation, and image segmentation, thereby enhancing the model's adaptability\nand robustness across diverse visual tasks. Our experimental results\ndemonstrate that MAL surpasses traditional supervised models and fully\nleverages the scaling potential of xLSTM, setting a new benchmark in visual\ntask performance.\n","authors":["Wenjun Huang","Jianguo Hu"],"pdf_url":"https://arxiv.org/pdf/2412.10730v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10726v1","updated":"2024-12-14T07:52:24Z","published":"2024-12-14T07:52:24Z","title":"NoisyEQA: Benchmarking Embodied Question Answering Against Noisy Queries","summary":"  The rapid advancement of Vision-Language Models (VLMs) has significantly\nadvanced the development of Embodied Question Answering (EQA), enhancing\nagents' abilities in language understanding and reasoning within complex and\nrealistic scenarios. However, EQA in real-world scenarios remains challenging,\nas human-posed questions often contain noise that can interfere with an agent's\nexploration and response, bringing challenges especially for language beginners\nand non-expert users. To address this, we introduce a NoisyEQA benchmark\ndesigned to evaluate an agent's ability to recognize and correct noisy\nquestions. This benchmark introduces four common types of noise found in\nreal-world applications: Latent Hallucination Noise, Memory Noise, Perception\nNoise, and Semantic Noise generated through an automated dataset creation\nframework. Additionally, we also propose a 'Self-Correction' prompting\nmechanism and a new evaluation metric to enhance and measure both noise\ndetection capability and answer quality. Our comprehensive evaluation reveals\nthat current EQA agents often struggle to detect noise in questions, leading to\nresponses that frequently contain erroneous information. Through our\nSelf-Correct Prompting mechanism, we can effectively improve the accuracy of\nagent answers.\n","authors":["Tao Wu","Chuhao Zhou","Yen Heng Wong","Lin Gu","Jianfei Yang"],"pdf_url":"https://arxiv.org/pdf/2412.10726v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10723v1","updated":"2024-12-14T07:42:56Z","published":"2024-12-14T07:42:56Z","title":"HEP-NAS: Towards Efficient Few-shot Neural Architecture Search via\n  Hierarchical Edge Partitioning","summary":"  One-shot methods have significantly advanced the field of neural architecture\nsearch (NAS) by adopting weight-sharing strategy to reduce search costs.\nHowever, the accuracy of performance estimation can be compromised by\nco-adaptation. Few-shot methods divide the entire supernet into individual\nsub-supernets by splitting edge by edge to alleviate this issue, yet neglect\nrelationships among edges and result in performance degradation on huge search\nspace. In this paper, we introduce HEP-NAS, a hierarchy-wise partition\nalgorithm designed to further enhance accuracy. To begin with, HEP-NAS treats\nedges sharing the same end node as a hierarchy, permuting and splitting edges\nwithin the same hierarchy to directly search for the optimal operation\ncombination for each intermediate node. This approach aligns more closely with\nthe ultimate goal of NAS. Furthermore, HEP-NAS selects the most promising\nsub-supernet after each segmentation, progressively narrowing the search space\nin which the optimal architecture may exist. To improve performance evaluation\nof sub-supernets, HEP-NAS employs search space mutual distillation, stabilizing\nthe training process and accelerating the convergence of each individual\nsub-supernet. Within a given budget, HEP-NAS enables the splitting of all edges\nand gradually searches for architectures with higher accuracy. Experimental\nresults across various datasets and search spaces demonstrate the superiority\nof HEP-NAS compared to state-of-the-art methods.\n","authors":["Jianfeng Li","Jiawen Zhang","Feng Wang","Lianbo Ma"],"pdf_url":"https://arxiv.org/pdf/2412.10723v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19845v2","updated":"2024-12-14T07:36:15Z","published":"2024-11-29T17:04:03Z","title":"A Visual-inertial Localization Algorithm using Opportunistic Visual\n  Beacons and Dead-Reckoning for GNSS-Denied Large-scale Applications","summary":"  With the development of smart cities, the demand for continuous pedestrian\nnavigation in large-scale urban environments has significantly increased. While\nglobal navigation satellite systems (GNSS) provide low-cost and reliable\npositioning services, they are often hindered in complex urban canyon\nenvironments. Thus, exploring opportunistic signals for positioning in urban\nareas has become a key solution. Augmented reality (AR) allows pedestrians to\nacquire real-time visual information. Accordingly, we propose a low-cost\nvisual-inertial positioning solution. This method comprises a lightweight\nmulti-scale group convolution (MSGC)-based visual place recognition (VPR)\nneural network, a pedestrian dead reckoning (PDR) algorithm, and a\nvisual/inertial fusion approach based on a Kalman filter with gross error\nsuppression. The VPR serves as a conditional observation to the Kalman filter,\neffectively correcting the errors accumulated through the PDR method. This\nenables the entire algorithm to ensure the reliability of long-term positioning\nin GNSS-denied areas. Extensive experimental results demonstrate that our\nmethod maintains stable positioning during large-scale movements. Compared to\nthe lightweight MobileNetV3-based VPR method, our proposed VPR solution\nimproves Recall@1 by at least 3\\% on two public datasets while reducing the\nnumber of parameters by 63.37\\%. It also achieves performance that is\ncomparable to the VGG16-based method. The VPR-PDR algorithm improves\nlocalization accuracy by more than 40\\% compared to the original PDR.\n","authors":["Liqiang Zhang","Ye Tian","Dongyan Wei"],"pdf_url":"https://arxiv.org/pdf/2411.19845v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10720v1","updated":"2024-12-14T07:28:38Z","published":"2024-12-14T07:28:38Z","title":"Bridging Vision and Language: Modeling Causality and Temporality in\n  Video Narratives","summary":"  Video captioning is a critical task in the field of multimodal machine\nlearning, aiming to generate descriptive and coherent textual narratives for\nvideo content. While large vision-language models (LVLMs) have shown\nsignificant progress, they often struggle to capture the causal and temporal\ndynamics inherent in complex video sequences. To address this limitation, we\npropose an enhanced framework that integrates a Causal-Temporal Reasoning\nModule (CTRM) into state-of-the-art LVLMs. CTRM comprises two key components:\nthe Causal Dynamics Encoder (CDE) and the Temporal Relational Learner (TRL),\nwhich collectively encode causal dependencies and temporal consistency from\nvideo frames. We further design a multi-stage learning strategy to optimize the\nmodel, combining pre-training on large-scale video-text datasets, fine-tuning\non causally annotated data, and contrastive alignment for better embedding\ncoherence. Experimental results on standard benchmarks such as MSVD and MSR-VTT\ndemonstrate that our method outperforms existing approaches in both automatic\nmetrics (CIDEr, BLEU-4, ROUGE-L) and human evaluations, achieving more fluent,\ncoherent, and relevant captions. These results validate the effectiveness of\nour approach in generating captions with enriched causal-temporal narratives.\n","authors":["Ji-jun Park","Soo-joon Choi"],"pdf_url":"https://arxiv.org/pdf/2412.10720v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10719v1","updated":"2024-12-14T07:23:14Z","published":"2024-12-14T07:23:14Z","title":"Just a Few Glances: Open-Set Visual Perception with Image Prompt\n  Paradigm","summary":"  To break through the limitations of pre-training models on fixed categories,\nOpen-Set Object Detection (OSOD) and Open-Set Segmentation (OSS) have attracted\na surge of interest from researchers. Inspired by large language models,\nmainstream OSOD and OSS methods generally utilize text as a prompt, achieving\nremarkable performance. Following SAM paradigm, some researchers use visual\nprompts, such as points, boxes, and masks that cover detection or segmentation\ntargets. Despite these two prompt paradigms exhibit excellent performance, they\nalso reveal inherent limitations. On the one hand, it is difficult to\naccurately describe characteristics of specialized category using textual\ndescription. On the other hand, existing visual prompt paradigms heavily rely\non multi-round human interaction, which hinders them being applied to fully\nautomated pipeline. To address the above issues, we propose a novel prompt\nparadigm in OSOD and OSS, that is, \\textbf{Image Prompt Paradigm}. This brand\nnew prompt paradigm enables to detect or segment specialized categories without\nmulti-round human intervention. To achieve this goal, the proposed image prompt\nparadigm uses just a few image instances as prompts, and we propose a novel\nframework named \\textbf{MI Grounding} for this new paradigm. In this framework,\nhigh-quality image prompts are automatically encoded, selected and fused,\nachieving the single-stage and non-interactive inference. We conduct extensive\nexperiments on public datasets, showing that MI Grounding achieves competitive\nperformance on OSOD and OSS benchmarks compared to text prompt paradigm methods\nand visual prompt paradigm methods. Moreover, MI Grounding can greatly\noutperform existing method on our constructed specialized ADR50K dataset.\n","authors":["Jinrong Zhang","Penghui Wang","Chunxiao Liu","Wei Liu","Dian Jin","Qiong Zhang","Erli Meng","Zhengnan Hu"],"pdf_url":"https://arxiv.org/pdf/2412.10719v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.10718v1","updated":"2024-12-14T07:22:03Z","published":"2024-12-14T07:22:03Z","title":"GRID: Visual Layout Generation","summary":"  In this paper, we introduce GRID, a novel paradigm that reframes a broad\nrange of visual generation tasks as the problem of arranging grids, akin to\nfilm strips. At its core, GRID transforms temporal sequences into grid layouts,\nenabling image generation models to process visual sequences holistically. To\nachieve both layout consistency and motion coherence, we develop a parallel\nflow-matching training strategy that combines layout matching and temporal\nlosses, guided by a coarse-to-fine schedule that evolves from basic layouts to\nprecise motion control. Our approach demonstrates remarkable efficiency,\nachieving up to 35 faster inference speeds while using 1/1000 of the\ncomputational resources compared to specialized models. Extensive experiments\nshow that GRID exhibits exceptional versatility across diverse visual\ngeneration tasks, from Text-to-Video to 3D Editing, while maintaining its\nfoundational image generation capabilities. This dual strength in both expanded\napplications and preserved core competencies establishes GRID as an efficient\nand versatile omni-solution for visual generation.\n","authors":["Cong Wan","Xiangyang Luo","Zijian Cai","Yiren Song","Yunlong Zhao","Yifan Bai","Yuhang He","Yihong Gong"],"pdf_url":"https://arxiv.org/pdf/2412.10718v1.pdf","comment":"preprint, codes: https://github.com/Should-AI-Lab/GRID"},{"id":"http://arxiv.org/abs/2412.10710v1","updated":"2024-12-14T06:50:10Z","published":"2024-12-14T06:50:10Z","title":"Virtual Trial Room with Computer Vision and Machine Learning","summary":"  Online shopping has revolutionized the retail industry, providing customers\nwith convenience and accessibility. However, customers often hesitate to\npurchase wearable products such as watches, jewelry, glasses, shoes, and\nclothes due to the lack of certainty regarding fit and suitability. This leads\nto significant return rates, causing problems for both customers and vendors.\nTo address this issue, a platform called the Virtual Trial Room with Computer\nVision and Machine Learning is designed which enables customers to easily check\nwhether a product will fit and suit them or not. To achieve this, an\nAI-generated 3D model of the human head was created from a single 2D image\nusing the DECA model. This 3D model was then superimposed with a custom-made 3D\nmodel of glass which is based on real-world measurements and fitted over the\nhuman head. To replicate the real-world look and feel, the model was retouched\nwith textures, lightness, and smoothness. Furthermore, a full-stack application\nwas developed utilizing various fornt-end and back-end technologies. This\napplication enables users to view 3D-generated results on the website,\nproviding an immersive and interactive experience.\n","authors":["Tulashi Prasasd Joshi","Amrendra Kumar Yadav","Arjun Chhetri","Suraj Agrahari","Umesh Kanta Ghimire"],"pdf_url":"https://arxiv.org/pdf/2412.10710v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10707v1","updated":"2024-12-14T06:33:53Z","published":"2024-12-14T06:33:53Z","title":"MambaPro: Multi-Modal Object Re-Identification with Mamba Aggregation\n  and Synergistic Prompt","summary":"  Multi-modal object Re-IDentification (ReID) aims to retrieve specific objects\nby utilizing complementary image information from different modalities.\nRecently, large-scale pre-trained models like CLIP have demonstrated impressive\nperformance in traditional single-modal object ReID tasks. However, they remain\nunexplored for multi-modal object ReID. Furthermore, current multi-modal\naggregation methods have obvious limitations in dealing with long sequences\nfrom different modalities. To address above issues, we introduce a novel\nframework called MambaPro for multi-modal object ReID. To be specific, we first\nemploy a Parallel Feed-Forward Adapter (PFA) for adapting CLIP to multi-modal\nobject ReID. Then, we propose the Synergistic Residual Prompt (SRP) to guide\nthe joint learning of multi-modal features. Finally, leveraging Mamba's\nsuperior scalability for long sequences, we introduce Mamba Aggregation (MA) to\nefficiently model interactions between different modalities. As a result,\nMambaPro could extract more robust features with lower complexity. Extensive\nexperiments on three multi-modal object ReID benchmarks (i.e., RGBNT201,\nRGBNT100 and MSVR310) validate the effectiveness of our proposed methods. The\nsource code is available at https://github.com/924973292/MambaPro.\n","authors":["Yuhao Wang","Xuehu Liu","Tianyu Yan","Yang Liu","Aihua Zheng","Pingping Zhang","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2412.10707v1.pdf","comment":"This work is accepted by AAAI2025. More modifications may be\n  performed"},{"id":"http://arxiv.org/abs/2412.10702v1","updated":"2024-12-14T06:21:24Z","published":"2024-12-14T06:21:24Z","title":"Memory Efficient Matting with Adaptive Token Routing","summary":"  Transformer-based models have recently achieved outstanding performance in\nimage matting. However, their application to high-resolution images remains\nchallenging due to the quadratic complexity of global self-attention. To\naddress this issue, we propose MEMatte, a memory-efficient matting framework\nfor processing high-resolution images. MEMatte incorporates a router before\neach global attention block, directing informative tokens to the global\nattention while routing other tokens to a Lightweight Token Refinement Module\n(LTRM). Specifically, the router employs a local-global strategy to predict the\nrouting probability of each token, and the LTRM utilizes efficient modules to\nsimulate global attention. Additionally, we introduce a Batch-constrained\nAdaptive Token Routing (BATR) mechanism, which allows each router to\ndynamically route tokens based on image content and the stages of attention\nblock in the network. Furthermore, we construct an ultra high-resolution image\nmatting dataset, UHR-395, comprising 35,500 training images and 1,000 test\nimages, with an average resolution of $4872\\times6017$. This dataset is created\nby compositing 395 different alpha mattes across 11 categories onto various\nbackgrounds, all with high-quality manual annotation. Extensive experiments\ndemonstrate that MEMatte outperforms existing methods on both high-resolution\nand real-world datasets, significantly reducing memory usage by approximately\n88% and latency by 50% on the Composition-1K benchmark.\n","authors":["Yiheng Lin","Yihan Hu","Chenyi Zhang","Ting Liu","Xiaochao Qu","Luoqi Liu","Yao Zhao","Yunchao Wei"],"pdf_url":"https://arxiv.org/pdf/2412.10702v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.20592v2","updated":"2024-12-14T06:15:40Z","published":"2024-07-30T06:57:00Z","title":"EgoSonics: Generating Synchronized Audio for Silent Egocentric Videos","summary":"  We introduce EgoSonics, a method to generate semantically meaningful and\nsynchronized audio tracks conditioned on silent egocentric videos. Generating\naudio for silent egocentric videos could open new applications in virtual\nreality, assistive technologies, or for augmenting existing datasets. Existing\nwork has been limited to domains like speech, music, or impact sounds and\ncannot capture the broad range of audio frequencies found in egocentric videos.\nEgoSonics addresses these limitations by building on the strengths of latent\ndiffusion models for conditioned audio synthesis. We first encode and process\npaired audio-video data to make them suitable for generation. The encoded data\nis then used to train a model that can generate an audio track that captures\nthe semantics of the input video. Our proposed SyncroNet builds on top of\nControlNet to provide control signals that enables generation of temporally\nsynchronized audio. Extensive evaluations and a comprehensive user study show\nthat our model outperforms existing work in audio quality, and in our proposed\nsynchronization evaluation method. Furthermore, we demonstrate downstream\napplications of our model in improving video summarization.\n","authors":["Aashish Rai","Srinath Sridhar"],"pdf_url":"https://arxiv.org/pdf/2407.20592v2.pdf","comment":"WACV 2025"},{"id":"http://arxiv.org/abs/2411.01859v2","updated":"2024-12-14T06:06:17Z","published":"2024-11-04T07:21:06Z","title":"A Novel Deep Learning Tractography Fiber Clustering Framework for\n  Functionally Consistent White Matter Parcellation Using Multimodal Diffusion\n  MRI and Functional MRI","summary":"  Tractography fiber clustering using diffusion MRI (dMRI) is a crucial\nstrategy for white matter (WM) parcellation. Current methods primarily use the\ngeometric information of fibers (i.e., the spatial trajectories) to group\nsimilar fibers into clusters, overlooking the important functional signals\npresent along the fiber tracts. There is increasing evidence that neural\nactivity in the WM can be measured using functional MRI (fMRI), offering\npotentially valuable multimodal information for fiber clustering. In this\npaper, we develop a novel deep learning fiber clustering framework, namely Deep\nMulti-view Fiber Clustering (DMVFC), that uses joint dMRI and fMRI data to\nenable functionally consistent WM parcellation. DMVFC can effectively integrate\nthe geometric characteristics of the WM fibers with the fMRI BOLD signals along\nthe fiber tracts. It includes two major components: 1) a multi-view pretraining\nmodule to compute embedding features from fiber geometric information and\nfunctional signals separately, and 2) a collaborative fine-tuning module to\nsimultaneously refine the two kinds of embeddings. In the experiments, we\ncompare DMVFC with two state-of-the-art fiber clustering methods and\ndemonstrate superior performance in achieving functionally meaningful and\nconsistent WM parcellation results.\n","authors":["Jin Wang","Bocheng Guo","Yijie Li","Junyi Wang","Yuqian Chen","Jarrett Rushmore","Nikos Makris","Yogesh Rathi","Lauren J O'Donnell","Fan Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.01859v2.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2412.09220v2","updated":"2024-12-14T05:42:51Z","published":"2024-12-12T12:20:27Z","title":"USDRL: Unified Skeleton-Based Dense Representation Learning with\n  Multi-Grained Feature Decorrelation","summary":"  Contrastive learning has achieved great success in skeleton-based\nrepresentation learning recently. However, the prevailing methods are\npredominantly negative-based, necessitating additional momentum encoder and\nmemory bank to get negative samples, which increases the difficulty of model\ntraining. Furthermore, these methods primarily concentrate on learning a global\nrepresentation for recognition and retrieval tasks, while overlooking the rich\nand detailed local representations that are crucial for dense prediction tasks.\nTo alleviate these issues, we introduce a Unified Skeleton-based Dense\nRepresentation Learning framework based on feature decorrelation, called USDRL,\nwhich employs feature decorrelation across temporal, spatial, and instance\ndomains in a multi-grained manner to reduce redundancy among dimensions of the\nrepresentations to maximize information extraction from features. Additionally,\nwe design a Dense Spatio-Temporal Encoder (DSTE) to capture fine-grained action\nrepresentations effectively, thereby enhancing the performance of dense\nprediction tasks. Comprehensive experiments, conducted on the benchmarks\nNTU-60, NTU-120, PKU-MMD I, and PKU-MMD II, across diverse downstream tasks\nincluding action recognition, action retrieval, and action detection,\nconclusively demonstrate that our approach significantly outperforms the\ncurrent state-of-the-art (SOTA) approaches. Our code and models are available\nat https://github.com/wengwanjiang/USDRL.\n","authors":["Wanjiang Weng","Hongsong Wang","Junbo Wang","Lei He","Guosen Xie"],"pdf_url":"https://arxiv.org/pdf/2412.09220v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2406.11551v4","updated":"2024-12-14T05:28:01Z","published":"2024-06-17T13:49:12Z","title":"ARNet: Self-Supervised FG-SBIR with Unified Sample Feature Alignment and\n  Multi-Scale Token Recycling","summary":"  Fine-Grained Sketch-Based Image Retrieval (FG-SBIR) aims to minimize the\ndistance between sketches and corresponding images in the embedding space.\nHowever, scalability is hindered by the growing complexity of solutions, mainly\ndue to the abstract nature of fine-grained sketches. In this paper, we propose\nan effective approach to narrow the gap between the two domains. It mainly\nfacilitates unified mutual information sharing both intra- and inter-samples,\nrather than treating them as a single feature alignment problem between\nmodalities. Specifically, our approach includes: (i) Employing dual\nweight-sharing networks to optimize alignment within the sketch and image\ndomain, which also effectively mitigates model learning saturation issues. (ii)\nIntroducing an objective optimization function based on contrastive loss to\nenhance the model's ability to align features in both intra- and inter-samples.\n(iii) Presenting a self-supervised Multi-Scale Token Recycling (MSTR) Module\nfeatured by recycling discarded patch tokens in multi-scale features, further\nenhancing representation capability and retrieval performance. Our framework\nachieves excellent results on CNN- and ViT-based backbones. Extensive\nexperiments demonstrate its superiority over existing methods. We also\nintroduce Cloths-V1, the first professional fashion sketch-image dataset,\nutilized to validate our method and will be beneficial for other applications.\n","authors":["Jianan Jiang","Hao Tang","Zhilin Jiang","Weiren Yu","Di Wu"],"pdf_url":"https://arxiv.org/pdf/2406.11551v4.pdf","comment":"Accepted by the 39th Annual AAAI Conference on Artificial\n  Intelligence (AAAI-25)"},{"id":"http://arxiv.org/abs/2412.10687v1","updated":"2024-12-14T05:25:17Z","published":"2024-12-14T05:25:17Z","title":"Linked Adapters: Linking Past and Future to Present for Effective\n  Continual Learning","summary":"  Continual learning allows the system to learn and adapt to new tasks while\nretaining the knowledge acquired from previous tasks. However, deep learning\nmodels suffer from catastrophic forgetting of knowledge learned from earlier\ntasks while learning a new task. Moreover, retraining large models like\ntransformers from scratch for every new task is costly. An effective approach\nto address continual learning is to use a large pre-trained model with\ntask-specific adapters to adapt to the new tasks. Though this approach can\nmitigate catastrophic forgetting, they fail to transfer knowledge across tasks\nas each task is learning adapters separately. To address this, we propose a\nnovel approach Linked Adapters that allows knowledge transfer through a\nweighted attention mechanism to other task-specific adapters. Linked adapters\nuse a multi-layer perceptron (MLP) to model the attention weights, which\novercomes the challenge of backward knowledge transfer in continual learning in\naddition to modeling the forward knowledge transfer. During inference, our\nproposed approach effectively leverages knowledge transfer through MLP-based\nattention weights across all the lateral task adapters. Through numerous\nexperiments conducted on diverse image classification datasets, we effectively\ndemonstrated the improvement in performance on the continual learning tasks\nusing Linked Adapters.\n","authors":["Dupati Srikar Chandra","P. K. Srijith","Dana Rezazadegan","Chris McCarthy"],"pdf_url":"https://arxiv.org/pdf/2412.10687v1.pdf","comment":"13 Pages, 5 Figures"},{"id":"http://arxiv.org/abs/2412.10681v1","updated":"2024-12-14T05:01:46Z","published":"2024-12-14T05:01:46Z","title":"One Pixel is All I Need","summary":"  Vision Transformers (ViTs) have achieved record-breaking performance in\nvarious visual tasks. However, concerns about their robustness against backdoor\nattacks have grown. Backdoor attacks involve associating a specific trigger\nwith a target label, causing the model to predict the attacker-specified label\nwhen the trigger is present, while correctly identifying clean images.We found\nthat ViTs exhibit higher attack success rates for quasi-triggers(patterns\ndifferent from but similar to the original training triggers)compared to CNNs.\nMoreover, some backdoor features in clean samples can suppress the original\ntrigger, making quasi-triggers more effective.To better understand and exploit\nthese vulnerabilities, we developed a tool called the Perturbation Sensitivity\nDistribution Map (PSDM). PSDM computes and sums gradients over many inputs to\nshow how sensitive the model is to small changes in the input. In ViTs, PSDM\nreveals a patch-like pattern where central pixels are more sensitive than\nedges. We use PSDM to guide the creation of quasi-triggers.Based on these\nfindings, we designed \"WorstVIT,\" a simple yet effective data poisoning\nbackdoor for ViT models. This attack requires an extremely low poisoning rate,\ntrains for just one epoch, and modifies a single pixel to successfully attack\nall validation images.\n","authors":["Deng Siqin","Zhou Xiaoyi"],"pdf_url":"https://arxiv.org/pdf/2412.10681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10680v1","updated":"2024-12-14T04:59:38Z","published":"2024-12-14T04:59:38Z","title":"UCDR-Adapter: Exploring Adaptation of Pre-Trained Vision-Language Models\n  for Universal Cross-Domain Retrieval","summary":"  Universal Cross-Domain Retrieval (UCDR) retrieves relevant images from unseen\ndomains and classes without semantic labels, ensuring robust generalization.\nExisting methods commonly employ prompt tuning with pre-trained vision-language\nmodels but are inherently limited by static prompts, reducing adaptability. We\npropose UCDR-Adapter, which enhances pre-trained models with adapters and\ndynamic prompt generation through a two-phase training strategy. First, Source\nAdapter Learning integrates class semantics with domain-specific visual\nknowledge using a Learnable Textual Semantic Template and optimizes Class and\nDomain Prompts via momentum updates and dual loss functions for robust\nalignment. Second, Target Prompt Generation creates dynamic prompts by\nattending to masked source prompts, enabling seamless adaptation to unseen\ndomains and classes. Unlike prior approaches, UCDR-Adapter dynamically adapts\nto evolving data distributions, enhancing both flexibility and generalization.\nDuring inference, only the image branch and generated prompts are used,\neliminating reliance on textual inputs for highly efficient retrieval.\nExtensive benchmark experiments show that UCDR-Adapter consistently outperforms\nProS in most cases and other state-of-the-art methods on UCDR, U(c)CDR, and\nU(d)CDR settings.\n","authors":["Haoyu Jiang","Zhi-Qi Cheng","Gabriel Moreira","Jiawen Zhu","Jingdong Sun","Bukun Ren","Jun-Yan He","Qi Dai","Xian-Sheng Hua"],"pdf_url":"https://arxiv.org/pdf/2412.10680v1.pdf","comment":"Accepted to WACV 2025. Project link:\n  https://github.com/fine68/UCDR2024"},{"id":"http://arxiv.org/abs/2412.10679v1","updated":"2024-12-14T04:51:32Z","published":"2024-12-14T04:51:32Z","title":"U-FaceBP: Uncertainty-aware Bayesian Ensemble Deep Learning for Face\n  Video-based Blood Pressure Measurement","summary":"  Blood pressure (BP) measurement plays an essential role in assessing health\non a daily basis. Remote photoplethysmography (rPPG), which extracts pulse\nwaves from camera-captured face videos, has the potential to easily measure BP\nfor daily health monitoring. However, there are many uncertainties in BP\nestimation using rPPG, resulting in limited estimation performance. In this\npaper, we propose U-FaceBP, an uncertainty-aware Bayesian ensemble deep\nlearning method for face video-based BP measurement. U-FaceBP models three\ntypes of uncertainty, i.e., data, model, and ensemble uncertainties, in face\nvideo-based BP estimation with a Bayesian neural network (BNN). We also design\nU-FaceBP as an ensemble method, with which BP is estimated from rPPG signals,\nPPG signals estimated from face videos, and face images using multiple BNNs. A\nlarge-scale experiment with 786 subjects demonstrates that U-FaceBP outperforms\nstate-of-the-art BP estimation methods. We also show that the uncertainties\nestimated from U-FaceBP are reasonable and useful for prediction confidence.\n","authors":["Yusuke Akamatsu","Terumi Umematsu","Hitoshi Imaoka"],"pdf_url":"https://arxiv.org/pdf/2412.10679v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.02045v2","updated":"2024-12-14T04:19:10Z","published":"2024-09-03T16:47:01Z","title":"AllWeatherNet:Unified Image Enhancement for Autonomous Driving under\n  Adverse Weather and Lowlight-conditions","summary":"  Adverse conditions like snow, rain, nighttime, and fog, pose challenges for\nautonomous driving perception systems. Existing methods have limited\neffectiveness in improving essential computer vision tasks, such as semantic\nsegmentation, and often focus on only one specific condition, such as removing\nrain or translating nighttime images into daytime ones. To address these\nlimitations, we propose a method to improve the visual quality and clarity\ndegraded by such adverse conditions. Our method, AllWeather-Net, utilizes a\nnovel hierarchical architecture to enhance images across all adverse\nconditions. This architecture incorporates information at three semantic\nlevels: scene, object, and texture, by discriminating patches at each level.\nFurthermore, we introduce a Scaled Illumination-aware Attention Mechanism\n(SIAM) that guides the learning towards road elements critical for autonomous\ndriving perception. SIAM exhibits robustness, remaining unaffected by changes\nin weather conditions or environmental scenes. AllWeather-Net effectively\ntransforms images into normal weather and daytime scenes, demonstrating\nsuperior image enhancement results and subsequently enhancing the performance\nof semantic segmentation, with up to a 5.3% improvement in mIoU in the trained\ndomain. We also show our model's generalization ability by applying it to\nunseen domains without re-training, achieving up to 3.9% mIoU improvement. Code\ncan be accessed at: https://github.com/Jumponthemoon/AllWeatherNet.\n","authors":["Chenghao Qian","Mahdi Rezaei","Saeed Anwar","Wenjing Li","Tanveer Hussain","Mohsen Azarmi","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2409.02045v2.pdf","comment":"ICPR 2024, Piero Zamperoni Overall Best Student Paper Award"},{"id":"http://arxiv.org/abs/2406.05835v2","updated":"2024-12-14T03:50:47Z","published":"2024-06-09T15:56:19Z","title":"Mamba YOLO: A Simple Baseline for Object Detection with State Space\n  Model","summary":"  Driven by the rapid development of deep learning technology, the YOLO series\nhas set a new benchmark for real-time object detectors. Additionally,\ntransformer-based structures have emerged as the most powerful solution in the\nfield, greatly extending the model's receptive field and achieving significant\nperformance improvements. However, this improvement comes at a cost as the\nquadratic complexity of the self-attentive mechanism increases the\ncomputational burden of the model. To address this problem, we introduce a\nsimple yet effective baseline approach called Mamba YOLO. Our contributions are\nas follows: 1) We propose that the ODMamba backbone introduce a \\textbf{S}tate\n\\textbf{S}pace \\textbf{M}odel (\\textbf{SSM}) with linear complexity to address\nthe quadratic complexity of self-attention. Unlike the other Transformer-base\nand SSM-base method, ODMamba is simple to train without pretraining. 2) For\nreal-time requirement, we designed the macro structure of ODMamba, determined\nthe optimal stage ratio and scaling size. 3) We design the RG Block that\nemploys a multi-branch structure to model the channel dimensions, which\naddresses the possible limitations of SSM in sequence modeling, such as\ninsufficient receptive fields and weak image localization. This design captures\nlocalized image dependencies more accurately and significantly. Extensive\nexperiments on the publicly available COCO benchmark dataset show that Mamba\nYOLO achieves state-of-the-art performance compared to previous methods.\nSpecifically, a tiny version of Mamba YOLO achieves a \\textbf{7.5}\\%\nimprovement in mAP on a single 4090 GPU with an inference time of \\textbf{1.5}\nms. The pytorch code is available at:\n\\url{https://github.com/HZAI-ZJNU/Mamba-YOLO}\n","authors":["Zeyu Wang","Chen Li","Huiying Xu","Xinzhong Zhu","Hongbo Li"],"pdf_url":"https://arxiv.org/pdf/2406.05835v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10025v2","updated":"2024-12-14T03:38:30Z","published":"2024-06-14T13:36:30Z","title":"ProtoS-ViT: Visual foundation models for sparse self-explainable\n  classifications","summary":"  Prototypical networks aim to build intrinsically explainable models based on\nthe linear summation of concepts. Concepts are coherent entities that we, as\nhumans, can recognize and associate with a certain object or entity. However,\nimportant challenges remain in the fair evaluation of explanation quality\nprovided by these models. This work first proposes an extensive set of\nquantitative and qualitative metrics which allow to identify drawbacks in\ncurrent prototypical networks. It then introduces a novel architecture which\nprovides compact explanations, outperforming current prototypical models in\nterms of explanation quality. Overall, the proposed architecture demonstrates\nhow frozen pre-trained ViT backbones can be effectively turned into\nprototypical models for both general and domain-specific tasks, in our case\nbiomedical image classifiers. Code is available at\n\\url{https://github.com/hturbe/protosvit}.\n","authors":["Hugues Turbé","Mina Bjelogrlic","Gianmarco Mengaldo","Christian Lovis"],"pdf_url":"https://arxiv.org/pdf/2406.10025v2.pdf","comment":"Update publication to match paper presented at the Interpretable AI:\n  Past, Present and Future Workshop at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2412.10663v1","updated":"2024-12-14T03:32:54Z","published":"2024-12-14T03:32:54Z","title":"Memory-Efficient 4-bit Preconditioned Stochastic Optimization","summary":"  Preconditioned stochastic optimization algorithms, exemplified by Shampoo,\nhave demonstrated superior performance over first-order optimizers, providing\nboth theoretical advantages in convergence rates and practical improvements in\nlarge-scale neural network training. However, they incur substantial memory\noverhead due to the storage demands of non-diagonal preconditioning matrices.\nTo address this, we introduce 4-bit quantization for Shampoo's preconditioners.\nWe introduced two key methods: First, we apply Cholesky decomposition followed\nby quantization of the Cholesky factors, reducing memory usage by leveraging\ntheir lower triangular structure while preserving symmetry and positive\ndefiniteness to minimize information loss. To our knowledge, this is the first\nquantization approach applied to Cholesky factors of preconditioners. Second,\nwe incorporate error feedback in the quantization process, efficiently storing\nCholesky factors and error states in the lower and upper triangular parts of\nthe same matrix. Through extensive experiments, we demonstrate that combining\nCholesky quantization with error feedback enhances memory efficiency and\nalgorithm performance in large-scale deep-learning tasks. Theoretically, we\nalso provide convergence proofs for quantized Shampoo under both smooth and\nnon-smooth stochastic optimization settings.\n","authors":["Jingyang Li","Kuangyu Ding","Kim-Chuan Toh","Pan Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.10663v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01579v2","updated":"2024-12-14T03:24:00Z","published":"2024-08-02T21:24:14Z","title":"THOR2: Topological Analysis for 3D Shape and Color-Based Human-Inspired\n  Object Recognition in Unseen Environments","summary":"  Visual object recognition in unseen and cluttered indoor environments is a\nchallenging problem for mobile robots. This study presents a 3D shape and\ncolor-based descriptor, TOPS2, for point clouds generated from RGB-D images and\nan accompanying recognition framework, THOR2. The TOPS2 descriptor embodies\nobject unity, a human cognition mechanism, by retaining the slicing-based\ntopological representation of 3D shape from the TOPS descriptor while capturing\nobject color information through slicing-based color embeddings computed using\na network of coarse color regions. These color regions, analogous to the\nMacAdam ellipses identified in human color perception, are obtained using the\nMapper algorithm, a topological soft-clustering technique. THOR2, trained using\nsynthetic data, demonstrates markedly improved recognition accuracy compared to\nTHOR, its 3D shape-based predecessor, on two benchmark real-world datasets: the\nOCID dataset capturing cluttered scenes from different viewpoints and the UW-IS\nOccluded dataset reflecting different environmental conditions and degrees of\nobject occlusion recorded using commodity hardware. THOR2 also outperforms\nbaseline deep learning networks, and a widely-used Vision Transformer (ViT)\nadapted for RGB-D inputs trained using synthetic and limited real-world data on\nboth the datasets. Therefore, THOR2 is a promising step toward achieving robust\nrecognition in low-cost robots.\n","authors":["Ekta U. Samani","Ashis G. Banerjee"],"pdf_url":"https://arxiv.org/pdf/2408.01579v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10542v3","updated":"2024-12-14T03:18:34Z","published":"2024-09-01T12:09:33Z","title":"SAM4MLLM: Enhance Multi-Modal Large Language Model for Referring\n  Expression Segmentation","summary":"  We introduce SAM4MLLM, an innovative approach which integrates the Segment\nAnything Model (SAM) with Multi-Modal Large Language Models (MLLMs) for\npixel-aware tasks. Our method enables MLLMs to learn pixel-level location\ninformation without requiring excessive modifications to the existing model\narchitecture or adding specialized tokens. We introduce an inquiry-based\napproach that can effectively find prompt points for SAM to perform\nsegmentation based on MLLM. It combines detailed visual information with the\npowerful expressive capabilities of large language models in a unified\nlanguage-based manner without additional computational overhead in learning.\nExperimental results on pubic benchmarks demonstrate the effectiveness of our\napproach.\n","authors":["Yi-Chia Chen","Wei-Hua Li","Cheng Sun","Yu-Chiang Frank Wang","Chu-Song Chen"],"pdf_url":"https://arxiv.org/pdf/2409.10542v3.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2412.10659v1","updated":"2024-12-14T03:09:04Z","published":"2024-12-14T03:09:04Z","title":"MEATRD: Multimodal Anomalous Tissue Region Detection Enhanced with\n  Spatial Transcriptomics","summary":"  The detection of anomalous tissue regions (ATRs) within affected tissues is\ncrucial in clinical diagnosis and pathological studies. Conventional automated\nATR detection methods, primarily based on histology images alone, falter in\ncases where ATRs and normal tissues have subtle visual differences. The recent\nspatial transcriptomics (ST) technology profiles gene expressions across tissue\nregions, offering a molecular perspective for detecting ATRs. However, there is\na dearth of ATR detection methods that effectively harness complementary\ninformation from both histology images and ST. To address this gap, we propose\nMEATRD, a novel ATR detection method that integrates histology image and ST\ndata. MEATRD is trained to reconstruct image patches and gene expression\nprofiles of normal tissue spots (inliers) from their multimodal embeddings,\nfollowed by learning a one-class classification AD model based on latent\nmultimodal reconstruction errors. This strategy harmonizes the strengths of\nreconstruction-based and one-class classification approaches. At the heart of\nMEATRD is an innovative masked graph dual-attention transformer (MGDAT)\nnetwork, which not only facilitates cross-modality and cross-node information\nsharing but also addresses the model over-generalization issue commonly seen in\nreconstruction-based AD methods. Additionally, we demonstrate that\nmodality-specific, task-relevant information is collated and condensed in\nmultimodal bottleneck encoding generated in MGDAT, marking the first\ntheoretical analysis of the informational properties of multimodal bottleneck\nencoding. Extensive evaluations across eight real ST datasets reveal MEATRD's\nsuperior performance in ATR detection, surpassing various state-of-the-art AD\nmethods. Remarkably, MEATRD also proves adept at discerning ATRs that only show\nslight visual deviations from normal tissues.\n","authors":["Kaichen Xu","Qilong Wu","Yan Lu","Yinan Zheng","Wenlin Li","Xingjie Tang","Jun Wang","Xiaobo Sun"],"pdf_url":"https://arxiv.org/pdf/2412.10659v1.pdf","comment":"AAAI 2025. Code: https://github.com/wqlzuel/MEATRD"},{"id":"http://arxiv.org/abs/2412.10651v1","updated":"2024-12-14T02:46:25Z","published":"2024-12-14T02:46:25Z","title":"LAN: Learning to Adapt Noise for Image Denoising","summary":"  Removing noise from images, a.k.a image denoising, can be a very challenging\ntask since the type and amount of noise can greatly vary for each image due to\nmany factors including a camera model and capturing environments. While there\nhave been striking improvements in image denoising with the emergence of\nadvanced deep learning architectures and real-world datasets, recent denoising\nnetworks struggle to maintain performance on images with noise that has not\nbeen seen during training. One typical approach to address the challenge would\nbe to adapt a denoising network to new noise distribution. Instead, in this\nwork, we shift our focus to adapting the input noise itself, rather than\nadapting a network. Thus, we keep a pretrained network frozen, and adapt an\ninput noise to capture the fine-grained deviations. As such, we propose a new\ndenoising algorithm, dubbed Learning-to-Adapt-Noise (LAN), where a learnable\nnoise offset is directly added to a given noisy image to bring a given input\nnoise closer towards the noise distribution a denoising network is trained to\nhandle. Consequently, the proposed framework exhibits performance improvement\non images with unseen noise, displaying the potential of the proposed research\ndirection. The code is available at https://github.com/chjinny/LAN\n","authors":["Changjin Kim","Tae Hyun Kim","Sungyong Baik"],"pdf_url":"https://arxiv.org/pdf/2412.10651v1.pdf","comment":"CVPR2024"},{"id":"http://arxiv.org/abs/2412.10650v1","updated":"2024-12-14T02:36:56Z","published":"2024-12-14T02:36:56Z","title":"DeMo: Decoupled Feature-Based Mixture of Experts for Multi-Modal Object\n  Re-Identification","summary":"  Multi-modal object Re-IDentification (ReID) aims to retrieve specific objects\nby combining complementary information from multiple modalities. Existing\nmulti-modal object ReID methods primarily focus on the fusion of heterogeneous\nfeatures. However, they often overlook the dynamic quality changes in\nmulti-modal imaging. In addition, the shared information between different\nmodalities can weaken modality-specific information. To address these issues,\nwe propose a novel feature learning framework called DeMo for multi-modal\nobject ReID, which adaptively balances decoupled features using a mixture of\nexperts. To be specific, we first deploy a Patch-Integrated Feature Extractor\n(PIFE) to extract multi-granularity and multi-modal features. Then, we\nintroduce a Hierarchical Decoupling Module (HDM) to decouple multi-modal\nfeatures into non-overlapping forms, preserving the modality uniqueness and\nincreasing the feature diversity. Finally, we propose an Attention-Triggered\nMixture of Experts (ATMoE), which replaces traditional gating with dynamic\nattention weights derived from decoupled features. With these modules, our DeMo\ncan generate more robust multi-modal features. Extensive experiments on three\nmulti-modal object ReID benchmarks fully verify the effectiveness of our\nmethods. The source code is available at https://github.com/924973292/DeMo.\n","authors":["Yuhao Wang","Yang Liu","Aihua Zheng","Pingping Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.10650v1.pdf","comment":"This work is accepted by AAAI2025. More motifications may be\n  performed"},{"id":"http://arxiv.org/abs/2412.10647v1","updated":"2024-12-14T02:29:07Z","published":"2024-12-14T02:29:07Z","title":"Enhancement of text recognition for hanja handwritten documents of\n  Ancient Korea","summary":"  We implemented a high-performance optical character recognition model for\nclassical handwritten documents using data augmentation with highly variable\ncropping within the document region. Optical character recognition in\nhandwritten documents, especially classical documents, has been a challenging\ntopic in many countries and research organizations due to its difficulty.\nAlthough many researchers have conducted research on this topic, the quality of\nclassical texts over time and the unique stylistic characteristics of various\nauthors have made it difficult, and it is clear that the recognition of hanja\nhandwritten documents is a meaningful and special challenge, especially since\nhanja, which has been developed by reflecting the vocabulary, semantic, and\nsyntactic features of the Joseon Dynasty, is different from classical Chinese\ncharacters. To study this challenge, we used 1100 cursive documents, which are\nsmall in size, and augmented 100 documents per document by cropping a randomly\nsized region within each document for training, and trained them using a\ntwo-stage object detection model, High resolution neural network (HRNet), and\napplied the resulting model to achieve a high inference recognition rate of 90%\nfor cursive documents. Through this study, we also confirmed that the\nperformance of OCR is affected by the simplified characters, variants, variant\ncharacters, common characters, and alternators of Chinese characters that are\ndifficult to see in other studies, and we propose that the results of this\nstudy can be applied to optical character recognition of modern documents in\nmultiple languages as well as other typefaces in classical documents.\n","authors":["Joonmo Ahna","Taehong Jang","Quan Fengnyu","Hyungil Lee","Jaehyuk Lee","Sojung Lucia Kim"],"pdf_url":"https://arxiv.org/pdf/2412.10647v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.05517v2","updated":"2024-12-14T01:51:33Z","published":"2024-12-07T03:18:07Z","title":"Test-time Cost-and-Quality Controllable Arbitrary-Scale Super-Resolution\n  with Variable Fourier Components","summary":"  Super-resolution (SR) with arbitrary scale factor and cost-and-quality\ncontrollability at test time is essential for various applications. While\nseveral arbitrary-scale SR methods have been proposed, these methods require us\nto modify the model structure and retrain it to control the computational cost\nand SR quality. To address this limitation, we propose a novel SR method using\na Recurrent Neural Network (RNN) with the Fourier representation. In our\nmethod, the RNN sequentially estimates Fourier components, each consisting of\nfrequency and amplitude, and aggregates these components to reconstruct an SR\nimage. Since the RNN can adjust the number of recurrences at test time, we can\ncontrol the computational cost and SR quality in a single model: fewer\nrecurrences (i.e., fewer Fourier components) lead to lower cost but lower\nquality, while more recurrences (i.e., more Fourier components) lead to better\nquality but more cost. Experimental results prove that more Fourier components\nimprove the PSNR score. Furthermore, even with fewer Fourier components, our\nmethod achieves a lower PSNR drop than other state-of-the-art arbitrary-scale\nSR methods.\n","authors":["Kazutoshi Akita","Norimichi Ukita"],"pdf_url":"https://arxiv.org/pdf/2412.05517v2.pdf","comment":"11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2410.15618v3","updated":"2024-12-14T00:50:36Z","published":"2024-10-21T03:40:29Z","title":"Erasing Undesirable Concepts in Diffusion Models with Adversarial\n  Preservation","summary":"  Diffusion models excel at generating visually striking content from text but\ncan inadvertently produce undesirable or harmful content when trained on\nunfiltered internet data. A practical solution is to selectively removing\ntarget concepts from the model, but this may impact the remaining concepts.\nPrior approaches have tried to balance this by introducing a loss term to\npreserve neutral content or a regularization term to minimize changes in the\nmodel parameters, yet resolving this trade-off remains challenging. In this\nwork, we propose to identify and preserving concepts most affected by parameter\nchanges, termed as \\textit{adversarial concepts}. This approach ensures stable\nerasure with minimal impact on the other concepts. We demonstrate the\neffectiveness of our method using the Stable Diffusion model, showing that it\noutperforms state-of-the-art erasure methods in eliminating unwanted content\nwhile maintaining the integrity of other unrelated elements. Our code is\navailable at\n\\url{https://github.com/tuananhbui89/Erasing-Adversarial-Preservation}.\n","authors":["Anh Bui","Long Vuong","Khanh Doan","Trung Le","Paul Montague","Tamas Abraham","Dinh Phung"],"pdf_url":"https://arxiv.org/pdf/2410.15618v3.pdf","comment":"Erasing Concepts, Generative Unlearning, NeurIPS 2024"},{"id":"http://arxiv.org/abs/2408.11402v3","updated":"2024-12-14T00:49:52Z","published":"2024-08-21T08:01:00Z","title":"Video Diffusion Models are Strong Video Inpainter","summary":"  Propagation-based video inpainting using optical flow at the pixel or feature\nlevel has recently garnered significant attention. However, it has limitations\nsuch as the inaccuracy of optical flow prediction and the propagation of noise\nover time. These issues result in non-uniform noise and time consistency\nproblems throughout the video, which are particularly pronounced when the\nremoved area is large and involves substantial movement. To address these\nissues, we propose a novel First Frame Filling Video Diffusion Inpainting model\n(FFF-VDI). We design FFF-VDI inspired by the capabilities of pre-trained\nimage-to-video diffusion models that can transform the first frame image into a\nhighly natural video. To apply this to the video inpainting task, we propagate\nthe noise latent information of future frames to fill the masked areas of the\nfirst frame's noise latent code. Next, we fine-tune the pre-trained\nimage-to-video diffusion model to generate the inpainted video. The proposed\nmodel addresses the limitations of existing methods that rely on optical flow\nquality, producing much more natural and temporally consistent videos. This\nproposed approach is the first to effectively integrate image-to-video\ndiffusion models into video inpainting tasks. Through various comparative\nexperiments, we demonstrate that the proposed model can robustly handle diverse\ninpainting types with high quality.\n","authors":["Minhyeok Lee","Suhwan Cho","Chajin Shin","Jungho Lee","Sunghun Yang","Sangyoun Lee"],"pdf_url":"https://arxiv.org/pdf/2408.11402v3.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2412.01941v2","updated":"2024-12-14T00:43:24Z","published":"2024-12-02T20:05:05Z","title":"Global Average Feature Augmentation for Robust Semantic Segmentation\n  with Transformers","summary":"  Robustness to out-of-distribution data is crucial for deploying modern neural\nnetworks. Recently, Vision Transformers, such as SegFormer for semantic\nsegmentation, have shown impressive robustness to visual corruptions like blur\nor noise affecting the acquisition device. In this paper, we propose Channel\nWise Feature Augmentation (CWFA), a simple yet efficient feature augmentation\ntechnique to improve the robustness of Vision Transformers for semantic\nsegmentation. CWFA applies a globally estimated perturbation per encoder with\nminimal compute overhead during training. Extensive evaluations on Cityscapes\nand ADE20K, with three state-of-the-art Vision Transformer architectures :\nSegFormer, Swin Transformer, and Twins demonstrate that CWFA-enhanced models\nsignificantly improve robustness without affecting clean data performance. For\ninstance, on Cityscapes, a CWFA-augmented SegFormer-B1 model yields up to 27.7%\nmIoU robustness gain on impulse noise compared to the non-augmented\nSegFormer-B1. Furthermore, CWFA-augmented SegFormer-B5 achieves a new\nstate-of-the-art 84.3% retention rate, a 0.7% improvement over the recently\npublished FAN+STL.\n","authors":["Alberto Gonzalo Rodriguez Salgado","Maying Shen","Philipp Harzig","Peter Mayer","Jose M. Alvarez"],"pdf_url":"https://arxiv.org/pdf/2412.01941v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10629v1","updated":"2024-12-14T00:43:11Z","published":"2024-12-14T00:43:11Z","title":"Rapid Reconstruction of Extremely Accelerated Liver 4D MRI via Chained\n  Iterative Refinement","summary":"  Abstract Purpose: High-quality 4D MRI requires an impractically long scanning\ntime for dense k-space signal acquisition covering all respiratory phases.\nAccelerated sparse sampling followed by reconstruction enhancement is desired\nbut often results in degraded image quality and long reconstruction time. We\nhereby propose the chained iterative reconstruction network (CIRNet) for\nefficient sparse-sampling reconstruction while maintaining clinically\ndeployable quality. Methods: CIRNet adopts the denoising diffusion\nprobabilistic framework to condition the image reconstruction through a\nstochastic iterative denoising process. During training, a forward Markovian\ndiffusion process is designed to gradually add Gaussian noise to the densely\nsampled ground truth (GT), while CIRNet is optimized to iteratively reverse the\nMarkovian process from the forward outputs. At the inference stage, CIRNet\nperforms the reverse process solely to recover signals from noise, conditioned\nupon the undersampled input. CIRNet processed the 4D data (3D+t) as temporal\nslices (2D+t). The proposed framework is evaluated on a data cohort consisting\nof 48 patients (12332 temporal slices) who underwent free-breathing liver 4D\nMRI. 3-, 6-, 10-, 20- and 30-times acceleration were examined with a\nretrospective random undersampling scheme. Compressed sensing (CS)\nreconstruction with a spatiotemporal constraint and a recently proposed deep\nnetwork, Re-Con-GAN, are selected as baselines. Results: CIRNet consistently\nachieved superior performance compared to CS and Re-Con-GAN. The inference time\nof CIRNet, CS, and Re-Con-GAN are 11s, 120s, and 0.15s. Conclusion: A novel\nframework, CIRNet, is presented. CIRNet maintains useable image quality for\nacceleration up to 30 times, significantly reducing the burden of 4DMRI.\n","authors":["Di Xu","Xin Miao","Hengjie Liu","Jessica E. Scholey","Wensha Yang","Mary Feng","Michael Ohliger","Hui Lin","Yi Lao","Yang Yang","Ke Sheng"],"pdf_url":"https://arxiv.org/pdf/2412.10629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10624v1","updated":"2024-12-14T00:06:37Z","published":"2024-12-14T00:06:37Z","title":"CATALOG: A Camera Trap Language-guided Contrastive Learning Model","summary":"  Foundation Models (FMs) have been successful in various computer vision tasks\nlike image classification, object detection and image segmentation. However,\nthese tasks remain challenging when these models are tested on datasets with\ndifferent distributions from the training dataset, a problem known as domain\nshift. This is especially problematic for recognizing animal species in\ncamera-trap images where we have variability in factors like lighting,\ncamouflage and occlusions. In this paper, we propose the Camera Trap\nLanguage-guided Contrastive Learning (CATALOG) model to address these issues.\nOur approach combines multiple FMs to extract visual and textual features from\ncamera-trap data and uses a contrastive loss function to train the model. We\nevaluate CATALOG on two benchmark datasets and show that it outperforms\nprevious state-of-the-art methods in camera-trap image recognition, especially\nwhen the training and testing data have different animal species or come from\ndifferent geographical areas. Our approach demonstrates the potential of using\nFMs in combination with multi-modal fusion and contrastive learning for\naddressing domain shifts in camera-trap image recognition. The code of CATALOG\nis publicly available at https://github.com/Julian075/CATALOG.\n","authors":["Julian D. Santamaria","Claudia Isaza","Jhony H. Giraldo"],"pdf_url":"https://arxiv.org/pdf/2412.10624v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01200v2","updated":"2024-12-14T09:51:09Z","published":"2024-11-02T10:09:08Z","title":"GarmentLab: A Unified Simulation and Benchmark for Garment Manipulation","summary":"  Manipulating garments and fabrics has long been a critical endeavor in the\ndevelopment of home-assistant robots. However, due to complex dynamics and\ntopological structures, garment manipulations pose significant challenges.\nRecent successes in reinforcement learning and vision-based methods offer\npromising avenues for learning garment manipulation. Nevertheless, these\napproaches are severely constrained by current benchmarks, which offer limited\ndiversity of tasks and unrealistic simulation behavior. Therefore, we present\nGarmentLab, a content-rich benchmark and realistic simulation designed for\ndeformable object and garment manipulation. Our benchmark encompasses a diverse\nrange of garment types, robotic systems and manipulators. The abundant tasks in\nthe benchmark further explores of the interactions between garments, deformable\nobjects, rigid bodies, fluids, and human body. Moreover, by incorporating\nmultiple simulation methods such as FEM and PBD, along with our proposed\nsim-to-real algorithms and real-world benchmark, we aim to significantly narrow\nthe sim-to-real gap. We evaluate state-of-the-art vision methods, reinforcement\nlearning, and imitation learning approaches on these tasks, highlighting the\nchallenges faced by current algorithms, notably their limited generalization\ncapabilities. Our proposed open-source environments and comprehensive analysis\nshow promising boost to future research in garment manipulation by unlocking\nthe full potential of these methods. We guarantee that we will open-source our\ncode as soon as possible. You can watch the videos in supplementary files to\nlearn more about the details of our work. Our project page is available at:\nhttps://garmentlab.github.io/\n","authors":["Haoran Lu","Ruihai Wu","Yitong Li","Sijie Li","Ziyu Zhu","Chuanruo Ning","Yan Shen","Longzan Luo","Yuanpei Chen","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2411.01200v2.pdf","comment":"NeurIPS 2024"}],"Robotics":[{"id":"http://arxiv.org/abs/2412.10973v1","updated":"2024-12-14T21:32:53Z","published":"2024-12-14T21:32:53Z","title":"Semi-autonomous Teleoperation using Differential Flatness of a Crane\n  Robot for Aircraft In-Wing Inspection","summary":"  Visual inspection of confined spaces such as aircraft wings is ergonomically\nchallenging for human mechanics. This work presents a novel crane robot that\ncan travel the entire span of the aircraft wing, enabling mechanics to perform\ninspection from outside of the confined space. However, teleoperation of the\ncrane robot can still be a challenge due to the need to avoid obstacles in the\nworkspace and potential oscillations of the camera payload. The main\ncontribution of this work is to exploit the differential flatness of the\ncrane-robot dynamics for designing reduced-oscillation, collision-free time\ntrajectories of the camera payload for use in teleoperation. Autonomous\nexperiments verify the efficacy of removing undesired oscillations by 89%.\nFurthermore, teleoperation experiments demonstrate that the controller\neliminated collisions (from 33% to 0%) when 12 participants performed an\ninspection task with the use of proposed trajectory selection when compared to\nthe case without it. Moreover, even discounting the failures due to collisions,\nthe proposed approach improved task efficiency by 18.7% when compared to the\ncase without it.\n","authors":["Wade Marquette","Kyle Schultz","Vamsi Jonnalagadda","Benjamin Wong","Joseph Garbini","Santosh Devasia"],"pdf_url":"https://arxiv.org/pdf/2412.10973v1.pdf","comment":"This is an extended version of an article submitted to IEEE for\n  possible publication. The paper consists of 12 pages and includes 10 figures"},{"id":"http://arxiv.org/abs/2412.10917v1","updated":"2024-12-14T18:04:18Z","published":"2024-12-14T18:04:18Z","title":"Adaptive Reward Design for Reinforcement Learning in Complex Robotic\n  Tasks","summary":"  There is a surge of interest in using formal languages such as Linear\nTemporal Logic (LTL) and finite automata to precisely and succinctly specify\ncomplex tasks and derive reward functions for reinforcement learning (RL) in\nrobotic applications. However, existing methods often assign sparse rewards\n(e.g., giving a reward of 1 only if a task is completed and 0 otherwise),\nnecessitating extensive exploration to converge to a high-quality policy. To\naddress this limitation, we propose a suite of reward functions that\nincentivize an RL agent to make measurable progress on tasks specified by LTL\nformulas and develop an adaptive reward shaping approach that dynamically\nupdates these reward functions during the learning process. Experimental\nresults on a range of RL-based robotic tasks demonstrate that the proposed\napproach is compatible with various RL algorithms and consistently outperforms\nbaselines, achieving earlier convergence to better policies with higher task\nsuccess rates and returns.\n","authors":["Minjae Kwon","Ingy ElSayed-Aly","Lu Feng"],"pdf_url":"https://arxiv.org/pdf/2412.10917v1.pdf","comment":"9 pages, 7 figures. Under review at RA-L"},{"id":"http://arxiv.org/abs/2412.10916v1","updated":"2024-12-14T18:02:59Z","published":"2024-12-14T18:02:59Z","title":"Distributed Shape Learning of Complex Objects Using Gaussian Kernel","summary":"  This paper addresses distributed learning of a complex object for multiple\nnetworked robots based on distributed optimization and kernel-based support\nvector machine. In order to overcome a fundamental limitation of polynomial\nkernels assumed in our antecessor, we employ Gaussian kernel as a kernel\nfunction for classification. The Gaussian kernel prohibits the robots to share\nthe function through a finite number of equality constraints due to its\ninfinite dimensionality of the function space. We thus reformulate the\noptimization problem assuming that the target function space is identified with\nthe space spanned by the bases associated with not the data but a finite number\nof grid points. The above relaxation is shown to allow the robots to share the\nfunction by a finite number of equality constraints. We finally demonstrate the\npresent approach through numerical simulations.\n","authors":["Toshiyuki Oshima","Junya Yamauchi","Tatsuya Ibuki","Michio Seto","Takeshi Hatanaka"],"pdf_url":"https://arxiv.org/pdf/2412.10916v1.pdf","comment":"4 pages, 5 figures, Discussion paper, 22nd IFAC World Congress, pp.\n  2475-2478, 2023"},{"id":"http://arxiv.org/abs/2404.08362v2","updated":"2024-12-14T17:12:06Z","published":"2024-04-12T09:57:28Z","title":"Optimization-Based System Identification and Moving Horizon Estimation\n  Using Low-Cost Sensors for a Miniature Car-Like Robot","summary":"  This paper presents an open-source miniature car-like robot with low-cost\nsensing and a pipeline for optimization-based system identification, state\nestimation, and control. The overall robotics platform comes at a cost of less\nthan \\$\\,700 and thus significantly simplifies the verification of advanced\nalgorithms in a realistic setting. We present a modified bicycle model with\nPacejka tire forces to model the dynamics of the considered all-wheel drive\nvehicle and to prevent singularities of the model at low velocities.\nFurthermore, we provide an optimization-based system identification approach\nand a moving horizon estimation (MHE) scheme. In extensive hardware\nexperiments, we show that the presented system identification approach results\nin a model with high prediction accuracy, while the MHE results in accurate\nstate estimates. Finally, the overall closed-loop system is shown to perform\nwell even in the presence of sensor failure for limited time intervals. All\nhardware, firmware, and control and estimation software is released under a BSD\n2-clause license to promote widespread adoption and collaboration within the\ncommunity.\n","authors":["Sabrina Bodmer","Lukas Vogel","Simon Muntwiler","Alexander Hansson","Tobias Bodewig","Jonas Wahlen","Melanie N. Zeilinger","Andrea Carron"],"pdf_url":"https://arxiv.org/pdf/2404.08362v2.pdf","comment":"This version contains an additional appendix giving an overview of\n  the software, hardware and firmware"},{"id":"http://arxiv.org/abs/2412.10855v1","updated":"2024-12-14T15:03:33Z","published":"2024-12-14T15:03:33Z","title":"Fast and Robust Visuomotor Riemannian Flow Matching Policy","summary":"  Diffusion-based visuomotor policies excel at learning complex robotic tasks\nby effectively combining visual data with high-dimensional, multi-modal action\ndistributions. However, diffusion models often suffer from slow inference due\nto costly denoising processes or require complex sequential training arising\nfrom recent distilling approaches. This paper introduces Riemannian Flow\nMatching Policy (RFMP), a model that inherits the easy training and fast\ninference capabilities of flow matching (FM). Moreover, RFMP inherently\nincorporates geometric constraints commonly found in realistic robotic\napplications, as the robot state resides on a Riemannian manifold. To enhance\nthe robustness of RFMP, we propose Stable RFMP (SRFMP), which leverages\nLaSalle's invariance principle to equip the dynamics of FM with stability to\nthe support of a target Riemannian distribution. Rigorous evaluation on eight\nsimulated and real-world tasks show that RFMP successfully learns and\nsynthesizes complex sensorimotor policies on Euclidean and Riemannian spaces\nwith efficient training and inference phases, outperforming Diffusion Policies\nwhile remaining competitive with Consistency Policies.\n","authors":["Haoran Ding","Noémie Jaquier","Jan Peters","Leonel Rozo"],"pdf_url":"https://arxiv.org/pdf/2412.10855v1.pdf","comment":"14 pages, 10 figures, 9 tables, project website:\n  https://sites.google.com/view/rfmp"},{"id":"http://arxiv.org/abs/2412.10809v1","updated":"2024-12-14T12:24:53Z","published":"2024-12-14T12:24:53Z","title":"Affine EKF: Exploring and Utilizing Sufficient and Necessary Conditions\n  for Observability Maintenance to Improve EKF Consistency","summary":"  Inconsistency issue is one crucial challenge for the performance of extended\nKalman filter (EKF) based methods for state estimation problems, which is\nmainly affected by the discrepancy of observability between the EKF model and\nthe underlying dynamic system. In this work, some sufficient and necessary\nconditions for observability maintenance are first proved. We find that under\ncertain conditions, an EKF can naturally maintain correct observability if the\ncorresponding linearization makes unobservable subspace independent of the\nstate values. Based on this theoretical finding, a novel affine EKF (Aff-EKF)\nframework is proposed to overcome the inconsistency of standard EKF (Std-EKF)\nby affine transformations, which not only naturally satisfies the observability\nconstraint but also has a clear design procedure. The advantages of our Aff-EKF\nframework over some commonly used methods are demonstrated through mathematical\nanalyses. The effectiveness of our proposed method is demonstrated on three\nsimultaneous localization and mapping (SLAM) applications with different types\nof features, typical point features, point features on a horizontal plane and\nplane features. Specifically, following the proposed procedure, the naturally\nconsistent Aff-EKFs can be explicitly derived for these problems. The\nconsistency improvement of these Aff-EKFs are validated by Monte Carlo\nsimulations.\n","authors":["Yang Song","Liang Zhao","Shoudong Huang"],"pdf_url":"https://arxiv.org/pdf/2412.10809v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10773v1","updated":"2024-12-14T09:53:43Z","published":"2024-12-14T09:53:43Z","title":"Omni Differential Drive for Simultaneous Reconfiguration and\n  Omnidirectional Mobility of Wheeled Robots","summary":"  Wheeled robots are highly efficient in human living environments. However,\nconventional wheeled designs, limited by degrees of freedom, struggle to meet\nvarying footprint needs and achieve omnidirectional mobility. This paper\nproposes a novel robot drive model inspired by human movements, termed as the\nOmni Differential Drive (ODD). The ODD model innovatively utilizes a lateral\ndifferential drive to adjust wheel spacing without adding additional actuators\nto the existing omnidirectional drive. This approach enables wheeled robots to\nachieve both simultaneous reconfiguration and omnidirectional mobility.\nAdditionally, a prototype was developed to validate the ODD, followed by\nkinematic analysis. Control systems for self-balancing and motion were designed\nand implemented. Experimental validations confirmed the feasibility of the ODD\nmechanism and the effectiveness of the control strategies. The results\nunderline the potential of this innovative drive system to enhance the mobility\nand adaptability of robotic platforms.\n","authors":["Ziqi Zhao","Peijia Xie","Max Q. -H. Meng"],"pdf_url":"https://arxiv.org/pdf/2412.10773v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10726v1","updated":"2024-12-14T07:52:24Z","published":"2024-12-14T07:52:24Z","title":"NoisyEQA: Benchmarking Embodied Question Answering Against Noisy Queries","summary":"  The rapid advancement of Vision-Language Models (VLMs) has significantly\nadvanced the development of Embodied Question Answering (EQA), enhancing\nagents' abilities in language understanding and reasoning within complex and\nrealistic scenarios. However, EQA in real-world scenarios remains challenging,\nas human-posed questions often contain noise that can interfere with an agent's\nexploration and response, bringing challenges especially for language beginners\nand non-expert users. To address this, we introduce a NoisyEQA benchmark\ndesigned to evaluate an agent's ability to recognize and correct noisy\nquestions. This benchmark introduces four common types of noise found in\nreal-world applications: Latent Hallucination Noise, Memory Noise, Perception\nNoise, and Semantic Noise generated through an automated dataset creation\nframework. Additionally, we also propose a 'Self-Correction' prompting\nmechanism and a new evaluation metric to enhance and measure both noise\ndetection capability and answer quality. Our comprehensive evaluation reveals\nthat current EQA agents often struggle to detect noise in questions, leading to\nresponses that frequently contain erroneous information. Through our\nSelf-Correct Prompting mechanism, we can effectively improve the accuracy of\nagent answers.\n","authors":["Tao Wu","Chuhao Zhou","Yen Heng Wong","Lin Gu","Jianfei Yang"],"pdf_url":"https://arxiv.org/pdf/2412.10726v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10713v1","updated":"2024-12-14T06:56:11Z","published":"2024-12-14T06:56:11Z","title":"RAT: Adversarial Attacks on Deep Reinforcement Agents for Targeted\n  Behaviors","summary":"  Evaluating deep reinforcement learning (DRL) agents against targeted behavior\nattacks is critical for assessing their robustness. These attacks aim to\nmanipulate the victim into specific behaviors that align with the attacker's\nobjectives, often bypassing traditional reward-based defenses. Prior methods\nhave primarily focused on reducing cumulative rewards; however, rewards are\ntypically too generic to capture complex safety requirements effectively. As a\nresult, focusing solely on reward reduction can lead to suboptimal attack\nstrategies, particularly in safety-critical scenarios where more precise\nbehavior manipulation is needed. To address these challenges, we propose RAT, a\nmethod designed for universal, targeted behavior attacks. RAT trains an\nintention policy that is explicitly aligned with human preferences, serving as\na precise behavioral target for the adversary. Concurrently, an adversary\nmanipulates the victim's policy to follow this target behavior. To enhance the\neffectiveness of these attacks, RAT dynamically adjusts the state occupancy\nmeasure within the replay buffer, allowing for more controlled and effective\nbehavior manipulation. Our empirical results on robotic simulation tasks\ndemonstrate that RAT outperforms existing adversarial attack algorithms in\ninducing specific behaviors. Additionally, RAT shows promise in improving agent\nrobustness, leading to more resilient policies. We further validate RAT by\nguiding Decision Transformer agents to adopt behaviors aligned with human\npreferences in various MuJoCo tasks, demonstrating its effectiveness across\ndiverse tasks.\n","authors":["Fengshuo Bai","Runze Liu","Yali Du","Ying Wen","Yaodong Yang"],"pdf_url":"https://arxiv.org/pdf/2412.10713v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.10706v1","updated":"2024-12-14T06:32:20Z","published":"2024-12-14T06:32:20Z","title":"SHIFT Planner: Speedy Hybrid Iterative Field and Segmented Trajectory\n  Optimization with IKD-tree for Uniform Lightweight Coverage","summary":"  This paper introduces a comprehensive planning and navigation framework that\naddress these limitations by integrating semantic mapping, adaptive coverage\nplanning, dynamic obstacle avoidance and precise trajectory tracking. Our\nframework begins by generating panoptic occupancy local semantic maps and\naccurate localization information from data aligned between a monocular camera,\nIMU, and GPS. This information is combined with input terrain point clouds or\npreloaded terrain information to initialize the planning process. We propose\nthe Radiant Field-Informed Coverage Planning algorithm, which utilizes a\ndiffusion field model to dynamically adjust the robot's coverage trajectory and\nspeed based on environmental attributes such as dirtiness and dryness. By\nmodeling the spatial influence of the robot's actions using a Gaussian field,\nensures a speed-optimized, uniform coverage trajectory while adapting to\nvarying environmental conditions.\n","authors":["Zexuan Fan","Sunchun Zhou","Hengye Yang","Junyi Cai","Ran Cheng","Lige Liu","Tao Sun"],"pdf_url":"https://arxiv.org/pdf/2412.10706v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10694v1","updated":"2024-12-14T05:54:32Z","published":"2024-12-14T05:54:32Z","title":"Grasp What You Want: Embodied Dexterous Grasping System Driven by Your\n  Voice","summary":"  In recent years, as robotics has advanced, human-robot collaboration has\ngained increasing importance. However, current robots struggle to fully and\naccurately interpret human intentions from voice commands alone. Traditional\ngripper and suction systems often fail to interact naturally with humans, lack\nadvanced manipulation capabilities, and are not adaptable to diverse tasks,\nespecially in unstructured environments. This paper introduces the Embodied\nDexterous Grasping System (EDGS), designed to tackle object grasping in\ncluttered environments for human-robot interaction. We propose a novel approach\nto semantic-object alignment using a Vision-Language Model (VLM) that fuses\nvoice commands and visual information, significantly enhancing the alignment of\nmulti-dimensional attributes of target objects in complex scenarios. Inspired\nby human hand-object interactions, we develop a robust, precise, and efficient\ngrasping strategy, incorporating principles like the thumb-object axis,\nmulti-finger wrapping, and fingertip interaction with an object's contact\nmechanics. We also design experiments to assess Referring Expression\nRepresentation Enrichment (RERE) in referring expression segmentation,\ndemonstrating that our system accurately detects and matches referring\nexpressions. Extensive experiments confirm that EDGS can effectively handle\ncomplex grasping tasks, achieving stability and high success rates,\nhighlighting its potential for further development in the field of Embodied AI.\n","authors":["Junliang Li","Kai Ye","Haolan Kang","Mingxuan Liang","Yuhang Wu","Zhenhua Liu","Huiping Zhuang","Rui Huang","Yongquan Chen"],"pdf_url":"https://arxiv.org/pdf/2412.10694v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10670v1","updated":"2024-12-14T04:18:22Z","published":"2024-12-14T04:18:22Z","title":"Magnisketch Drone Control","summary":"  The use of Unmanned Aerial Vehicles (UAVs) for aerial tasks and environmental\nmanipulation is increasingly desired. This can be demonstrated via art tasks.\nThis paper presents the development of Magnasketch, capable of translating\nimage inputs into art on a magnetic drawing board via a Bitcraze Crazyflie 2.0\nquadrotor. Optimal trajectories were generated using a Model Predictive Control\n(MPC) formulation newly incorporating magnetic force dynamics. A Z-compliant\nmagnetic drawing apparatus was designed for the quadrotor. Experimental results\nof the novel controller tested against the existing Position High Level\nCommander showed comparable performance. Although slightly outperformed in\nterms of error, with average errors of 3.9 cm, 4.4 cm, and 0.5 cm in x, y, and\nz respectively, the Magnasketch controller produced smoother drawings with the\nadded benefit of full state control.\n","authors":["Ashley Kline","Abirami Elangovan","Dominique Escandon","Scott Wade","Aatish Gupta"],"pdf_url":"https://arxiv.org/pdf/2412.10670v1.pdf","comment":"20 pages, 28 figures"},{"id":"http://arxiv.org/abs/2309.12611v2","updated":"2024-12-14T02:08:11Z","published":"2023-09-22T04:08:05Z","title":"On the Robotic Uncertainty of Fully Autonomous Traffic","summary":"  Recent transportation research highlights the potential of autonomous\nvehicles (AV) to improve traffic flow mobility as they are able to maintain\nsmaller car-following distances. However, as a unique class of ground robots,\nAVs are susceptible to robotic errors, particularly in their perception and\ncontrol modules, leading to uncertainties in their movements and an increased\nrisk of collisions. Consequently, conservative operational strategies, such as\nlarger headway and slower speeds, are implemented to prioritize safety over\nmobility in real-world operations. To reconcile the inconsistency, this paper\npresents an analytical model framework that delineates the endogenous\nreciprocity between traffic safety and mobility that arises from AVs' robotic\nuncertainties. Using both realistic car-following data and a stochastic\nintelligent driving model (IDM), the stochastic car-following distance is\nderived as a key parameter, enabling analysis of single-lane capacity and the\ncollision probability. A semi-Markov process is then employed to model the\ndynamics of the lane capacity, and the resulting collision-inclusive capacity,\nrepresenting expected lane capacity under stationary conditions, serves as the\nprimary performance metric for fully autonomous traffic. The analytical results\nare further utilized to investigate the impacts of critical parameters in AV\nand roadway designs on traffic performance, as well as the properties of\noptimal speed and headway under mobility-targeted or safety-dominated\nmanagement objectives. Extensions to scenarios involving multiple\nnon-independent collisions or multi-lane traffic scenarios are also discussed,\nwhich demonstrates the robustness of the theoretical results and their\npractical applications.\n","authors":["Hangyu Li","Xiaotong Sun","Chenglin Zhuang","Xiaopeng Li"],"pdf_url":"https://arxiv.org/pdf/2309.12611v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10631v1","updated":"2024-12-14T01:07:06Z","published":"2024-12-14T01:07:06Z","title":"ARMADA: Augmented Reality for Robot Manipulation and Robot-Free Data\n  Acquisition","summary":"  Teleoperation for robot imitation learning is bottlenecked by hardware\navailability. Can high-quality robot data be collected without a physical\nrobot? We present a system for augmenting Apple Vision Pro with real-time\nvirtual robot feedback. By providing users with an intuitive understanding of\nhow their actions translate to robot motions, we enable the collection of\nnatural barehanded human data that is compatible with the limitations of\nphysical robot hardware. We conducted a user study with 15 participants\ndemonstrating 3 different tasks each under 3 different feedback conditions and\ndirectly replayed the collected trajectories on physical robot hardware.\nResults suggest live robot feedback dramatically improves the quality of the\ncollected data, suggesting a new avenue for scalable human data collection\nwithout access to robot hardware. Videos and more are available at\nhttps://nataliya.dev/armada.\n","authors":["Nataliya Nechyporenko","Ryan Hoque","Christopher Webb","Mouli Sivapurapu","Jian Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.10631v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10628v1","updated":"2024-12-14T00:40:13Z","published":"2024-12-14T00:40:13Z","title":"Versatile Locomotion Skills for Hexapod Robots","summary":"  Hexapod robots are potentially suitable for carrying out tasks in cluttered\nenvironments since they are stable, compact, and light weight. They also have\nmulti-joint legs and variable height bodies that make them good candidates for\ntasks such as stairs climbing and squeezing under objects in a typical home\nenvironment or an attic. Expanding on our previous work on joist climbing in\nattics, we train a legged hexapod equipped with a depth camera and visual\ninertial odometry (VIO) to perform three tasks: climbing stairs, avoiding\nobstacles, and squeezing under obstacles such as a table. Our policies are\ntrained with simulation data only and can be deployed on lowcost hardware not\nrequiring real-time joint state feedback. We train our model in a\nteacher-student model with 2 phases: In phase 1, we use reinforcement learning\nwith access to privileged information such as height maps and joint feedback.\nIn phase 2, we use supervised learning to distill the model into one with\naccess to only onboard observations, consisting of egocentric depth images and\nrobot pose captured by a tracking VIO camera. By manipulating available\nprivileged information, constructing simulation terrains, and refining reward\nfunctions during phase 1 training, we are able to train the robots with skills\nthat are robust in non-ideal physical environments. We demonstrate successful\nsim-to-real transfer and achieve high success rates across all three tasks in\nphysical experiments.\n","authors":["Tomson Qu","Dichen Li","Avideh Zakhor","Wenhao Yu","Tingnan Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.10628v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12197v1","updated":"2024-12-14T12:44:37Z","published":"2024-12-14T12:44:37Z","title":"Anti-bullying Adaptive Cruise Control: A proactive right-of-way\n  protection approach","summary":"  The current Adaptive Cruise Control (ACC) systems are vulnerable to \"road\nbully\" such as cut-ins. This paper proposed an Anti-bullying Adaptive Cruise\nControl (AACC) approach with proactive right-of-way protection ability. It\nbears the following features: i) with the enhanced capability of preventing\nbullying from cut-ins; ii) optimal but not unsafe; iii) adaptive to various\ndriving styles of cut-in vehicles; iv) with real-time field implementation\ncapability. The proposed approach can identify other road users' driving styles\nonline and conduct game-based motion planning for right-of-way protection. A\ndetailed investigation of the simulation results shows that the proposed\napproach can prevent bullying from cut-ins and be adaptive to different cut-in\nvehicles' driving styles. The proposed approach is capable of enhancing travel\nefficiency by up to 29.55% under different cut-in gaps and can strengthen\ndriving safety compared with the current ACC controller. The proposed approach\nis flexible and robust against traffic congestion levels. It can improve\nmobility by up to 11.93% and robustness by 8.74% in traffic flow. Furthermore,\nthe proposed approach can support real-time field implementation by ensuring\nless than 50 milliseconds computation time.\n","authors":["Jia Hu","Zhexi Lian","Haoran Wang","Zihan Zhang","Ruoxi Qian","Duo Li"," Jaehyun"," So","Junnian Zheng"],"pdf_url":"https://arxiv.org/pdf/2412.12197v1.pdf","comment":"12 pages, 15 figures"},{"id":"http://arxiv.org/abs/2411.01200v2","updated":"2024-12-14T09:51:09Z","published":"2024-11-02T10:09:08Z","title":"GarmentLab: A Unified Simulation and Benchmark for Garment Manipulation","summary":"  Manipulating garments and fabrics has long been a critical endeavor in the\ndevelopment of home-assistant robots. However, due to complex dynamics and\ntopological structures, garment manipulations pose significant challenges.\nRecent successes in reinforcement learning and vision-based methods offer\npromising avenues for learning garment manipulation. Nevertheless, these\napproaches are severely constrained by current benchmarks, which offer limited\ndiversity of tasks and unrealistic simulation behavior. Therefore, we present\nGarmentLab, a content-rich benchmark and realistic simulation designed for\ndeformable object and garment manipulation. Our benchmark encompasses a diverse\nrange of garment types, robotic systems and manipulators. The abundant tasks in\nthe benchmark further explores of the interactions between garments, deformable\nobjects, rigid bodies, fluids, and human body. Moreover, by incorporating\nmultiple simulation methods such as FEM and PBD, along with our proposed\nsim-to-real algorithms and real-world benchmark, we aim to significantly narrow\nthe sim-to-real gap. We evaluate state-of-the-art vision methods, reinforcement\nlearning, and imitation learning approaches on these tasks, highlighting the\nchallenges faced by current algorithms, notably their limited generalization\ncapabilities. Our proposed open-source environments and comprehensive analysis\nshow promising boost to future research in garment manipulation by unlocking\nthe full potential of these methods. We guarantee that we will open-source our\ncode as soon as possible. You can watch the videos in supplementary files to\nlearn more about the details of our work. Our project page is available at:\nhttps://garmentlab.github.io/\n","authors":["Haoran Lu","Ruihai Wu","Yitong Li","Sijie Li","Ziyu Zhu","Chuanruo Ning","Yan Shen","Longzan Luo","Yuanpei Chen","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2411.01200v2.pdf","comment":"NeurIPS 2024"}]},"2024-12-17T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2412.13194v1","updated":"2024-12-17T18:59:50Z","published":"2024-12-17T18:59:50Z","title":"Proposer-Agent-Evaluator(PAE): Autonomous Skill Discovery For Foundation\n  Model Internet Agents","summary":"  The vision of a broadly capable and goal-directed agent, such as an\nInternet-browsing agent in the digital world and a household humanoid in the\nphysical world, has rapidly advanced, thanks to the generalization capability\nof foundation models. Such a generalist agent needs to have a large and diverse\nskill repertoire, such as finding directions between two travel locations and\nbuying specific items from the Internet. If each skill needs to be specified\nmanually through a fixed set of human-annotated instructions, the agent's skill\nrepertoire will necessarily be limited due to the quantity and diversity of\nhuman-annotated instructions. In this work, we address this challenge by\nproposing Proposer-Agent-Evaluator, an effective learning system that enables\nfoundation model agents to autonomously discover and practice skills in the\nwild. At the heart of PAE is a context-aware task proposer that autonomously\nproposes tasks for the agent to practice with context information of the\nenvironment such as user demos or even just the name of the website itself for\nInternet-browsing agents. Then, the agent policy attempts those tasks with\nthoughts and actual grounded operations in the real world with resulting\ntrajectories evaluated by an autonomous VLM-based success evaluator. The\nsuccess evaluation serves as the reward signal for the agent to refine its\npolicies through RL. We validate PAE on challenging vision-based web\nnavigation, using both real-world and self-hosted websites from WebVoyager and\nWebArena.To the best of our knowledge, this work represents the first effective\nlearning system to apply autonomous task proposal with RL for agents that\ngeneralizes real-world human-annotated benchmarks with SOTA performances. Our\nopen-source checkpoints and code can be found in https://yanqval.github.io/PAE/\n","authors":["Yifei Zhou","Qianlan Yang","Kaixiang Lin","Min Bai","Xiong Zhou","Yu-Xiong Wang","Sergey Levine","Erran Li"],"pdf_url":"https://arxiv.org/pdf/2412.13194v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13195v1","updated":"2024-12-17T18:59:50Z","published":"2024-12-17T18:59:50Z","title":"CoMPaSS: Enhancing Spatial Understanding in Text-to-Image Diffusion\n  Models","summary":"  Text-to-image diffusion models excel at generating photorealistic images, but\ncommonly struggle to render accurate spatial relationships described in text\nprompts. We identify two core issues underlying this common failure: 1) the\nambiguous nature of spatial-related data in existing datasets, and 2) the\ninability of current text encoders to accurately interpret the spatial\nsemantics of input descriptions. We address these issues with CoMPaSS, a\nversatile training framework that enhances spatial understanding of any T2I\ndiffusion model. CoMPaSS solves the ambiguity of spatial-related data with the\nSpatial Constraints-Oriented Pairing (SCOP) data engine, which curates\nspatially-accurate training data through a set of principled spatial\nconstraints. To better exploit the curated high-quality spatial priors, CoMPaSS\nfurther introduces a Token ENcoding ORdering (TENOR) module to allow better\nexploitation of high-quality spatial priors, effectively compensating for the\nshortcoming of text encoders. Extensive experiments on four popular open-weight\nT2I diffusion models covering both UNet- and MMDiT-based architectures\ndemonstrate the effectiveness of CoMPaSS by setting new state-of-the-arts with\nsubstantial relative gains across well-known benchmarks on spatial\nrelationships generation, including VISOR (+98%), T2I-CompBench Spatial (+67%),\nand GenEval Position (+131%). Code will be available at\nhttps://github.com/blurgyy/CoMPaSS.\n","authors":["Gaoyang Zhang","Bingtao Fu","Qingnan Fan","Qi Zhang","Runxing Liu","Hong Gu","Huaqi Zhang","Xinguo Liu"],"pdf_url":"https://arxiv.org/pdf/2412.13195v1.pdf","comment":"18 pages, 11 figures"},{"id":"http://arxiv.org/abs/2412.13193v1","updated":"2024-12-17T18:59:46Z","published":"2024-12-17T18:59:46Z","title":"GaussTR: Foundation Model-Aligned Gaussian Transformer for\n  Self-Supervised 3D Spatial Understanding","summary":"  3D Semantic Occupancy Prediction is fundamental for spatial understanding as\nit provides a comprehensive semantic cognition of surrounding environments.\nHowever, prevalent approaches primarily rely on extensive labeled data and\ncomputationally intensive voxel-based modeling, restricting the scalability and\ngeneralizability of 3D representation learning. In this paper, we introduce\nGaussTR, a novel Gaussian Transformer that leverages alignment with foundation\nmodels to advance self-supervised 3D spatial understanding. GaussTR adopts a\nTransformer architecture to predict sparse sets of 3D Gaussians that represent\nscenes in a feed-forward manner. Through aligning rendered Gaussian features\nwith diverse knowledge from pre-trained foundation models, GaussTR facilitates\nthe learning of versatile 3D representations and enables open-vocabulary\noccupancy prediction without explicit annotations. Empirical evaluations on the\nOcc3D-nuScenes dataset showcase GaussTR's state-of-the-art zero-shot\nperformance, achieving 11.70 mIoU while reducing training duration by\napproximately 50%. These experimental results highlight the significant\npotential of GaussTR for scalable and holistic 3D spatial understanding, with\npromising implications for autonomous driving and embodied agents. Code is\navailable at https://github.com/hustvl/GaussTR.\n","authors":["Haoyi Jiang","Liu Liu","Tianheng Cheng","Xinjie Wang","Tianwei Lin","Zhizhong Su","Wenyu Liu","Xinggang Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13190v1","updated":"2024-12-17T18:59:33Z","published":"2024-12-17T18:59:33Z","title":"MotionBridge: Dynamic Video Inbetweening with Flexible Controls","summary":"  By generating plausible and smooth transitions between two image frames,\nvideo inbetweening is an essential tool for video editing and long video\nsynthesis. Traditional works lack the capability to generate complex large\nmotions. While recent video generation techniques are powerful in creating\nhigh-quality results, they often lack fine control over the details of\nintermediate frames, which can lead to results that do not align with the\ncreative mind. We introduce MotionBridge, a unified video inbetweening\nframework that allows flexible controls, including trajectory strokes,\nkeyframes, masks, guide pixels, and text. However, learning such multi-modal\ncontrols in a unified framework is a challenging task. We thus design two\ngenerators to extract the control signal faithfully and encode feature through\ndual-branch embedders to resolve ambiguities. We further introduce a curriculum\ntraining strategy to smoothly learn various controls. Extensive qualitative and\nquantitative experiments have demonstrated that such multi-modal controls\nenable a more dynamic, customizable, and contextually accurate visual\nnarrative.\n","authors":["Maham Tanveer","Yang Zhou","Simon Niklaus","Ali Mahdavi Amiri","Hao Zhang","Krishna Kumar Singh","Nanxuan Zhao"],"pdf_url":"https://arxiv.org/pdf/2412.13190v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20309v4","updated":"2024-12-17T18:59:07Z","published":"2024-03-29T17:29:58Z","title":"InstantSplat: Sparse-view SfM-free Gaussian Splatting in Seconds","summary":"  While neural 3D reconstruction has advanced substantially, it typically\nrequires densely captured multi-view data with carefully initialized poses\n(e.g., using COLMAP). However, this requirement limits its broader\napplicability, as Structure-from-Motion (SfM) is often unreliable in\nsparse-view scenarios where feature matches are limited, resulting in\ncumulative errors. In this paper, we introduce InstantSplat, a novel and\nlightning-fast neural reconstruction system that builds accurate 3D\nrepresentations from as few as 2-3 images. InstantSplat adopts a\nself-supervised framework that bridges the gap between 2D images and 3D\nrepresentations using Gaussian Bundle Adjustment (GauBA) and can be optimized\nin an end-to-end manner. InstantSplat integrates dense stereo priors and\nco-visibility relationships between frames to initialize pixel-aligned geometry\nby progressively expanding the scene avoiding redundancy. Gaussian Bundle\nAdjustment is used to adapt both the scene representation and camera parameters\nquickly by minimizing gradient-based photometric error. Overall, InstantSplat\nachieves large-scale 3D reconstruction in mere seconds by reducing the required\nnumber of input views. It achieves an acceleration of over 20 times in\nreconstruction, improves visual quality (SSIM) from 0.3755 to 0.7624 than\nCOLMAP with 3D-GS, and is compatible with multiple 3D representations (3D-GS,\n2D-GS, and Mip-Splatting).\n","authors":["Zhiwen Fan","Kairun Wen","Wenyan Cong","Kevin Wang","Jian Zhang","Xinghao Ding","Danfei Xu","Boris Ivanovic","Marco Pavone","Georgios Pavlakos","Zhangyang Wang","Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2403.20309v4.pdf","comment":"Project Page: https://instantsplat.github.io/"},{"id":"http://arxiv.org/abs/2412.13188v1","updated":"2024-12-17T18:58:55Z","published":"2024-12-17T18:58:55Z","title":"StreetCrafter: Street View Synthesis with Controllable Video Diffusion\n  Models","summary":"  This paper aims to tackle the problem of photorealistic view synthesis from\nvehicle sensor data. Recent advancements in neural scene representation have\nachieved notable success in rendering high-quality autonomous driving scenes,\nbut the performance significantly degrades as the viewpoint deviates from the\ntraining trajectory. To mitigate this problem, we introduce StreetCrafter, a\nnovel controllable video diffusion model that utilizes LiDAR point cloud\nrenderings as pixel-level conditions, which fully exploits the generative prior\nfor novel view synthesis, while preserving precise camera control. Moreover,\nthe utilization of pixel-level LiDAR conditions allows us to make accurate\npixel-level edits to target scenes. In addition, the generative prior of\nStreetCrafter can be effectively incorporated into dynamic scene\nrepresentations to achieve real-time rendering. Experiments on Waymo Open\nDataset and PandaSet demonstrate that our model enables flexible control over\nviewpoint changes, enlarging the view synthesis regions for satisfying\nrendering, which outperforms existing methods.\n","authors":["Yunzhi Yan","Zhen Xu","Haotong Lin","Haian Jin","Haoyu Guo","Yida Wang","Kun Zhan","Xianpeng Lang","Hujun Bao","Xiaowei Zhou","Sida Peng"],"pdf_url":"https://arxiv.org/pdf/2412.13188v1.pdf","comment":"Project page: https://zju3dv.github.io/street_crafter"},{"id":"http://arxiv.org/abs/2412.13187v1","updated":"2024-12-17T18:58:33Z","published":"2024-12-17T18:58:33Z","title":"HandsOnVLM: Vision-Language Models for Hand-Object Interaction\n  Prediction","summary":"  How can we predict future interaction trajectories of human hands in a scene\ngiven high-level colloquial task specifications in the form of natural\nlanguage? In this paper, we extend the classic hand trajectory prediction task\nto two tasks involving explicit or implicit language queries. Our proposed\ntasks require extensive understanding of human daily activities and reasoning\nabilities about what should be happening next given cues from the current\nscene. We also develop new benchmarks to evaluate the proposed two tasks,\nVanilla Hand Prediction (VHP) and Reasoning-Based Hand Prediction (RBHP). We\nenable solving these tasks by integrating high-level world knowledge and\nreasoning capabilities of Vision-Language Models (VLMs) with the\nauto-regressive nature of low-level ego-centric hand trajectories. Our model,\nHandsOnVLM is a novel VLM that can generate textual responses and produce\nfuture hand trajectories through natural-language conversations. Our\nexperiments show that HandsOnVLM outperforms existing task-specific methods and\nother VLM baselines on proposed tasks, and demonstrates its ability to\neffectively utilize world knowledge for reasoning about low-level human hand\ntrajectories based on the provided context. Our website contains code and\ndetailed video results \\url{https://www.chenbao.tech/handsonvlm/}\n","authors":["Chen Bao","Jiarui Xu","Xiaolong Wang","Abhinav Gupta","Homanga Bharadhwaj"],"pdf_url":"https://arxiv.org/pdf/2412.13187v1.pdf","comment":"Preprint. Under Review"},{"id":"http://arxiv.org/abs/2412.13185v1","updated":"2024-12-17T18:58:07Z","published":"2024-12-17T18:58:07Z","title":"Move-in-2D: 2D-Conditioned Human Motion Generation","summary":"  Generating realistic human videos remains a challenging task, with the most\neffective methods currently relying on a human motion sequence as a control\nsignal. Existing approaches often use existing motion extracted from other\nvideos, which restricts applications to specific motion types and global scene\nmatching. We propose Move-in-2D, a novel approach to generate human motion\nsequences conditioned on a scene image, allowing for diverse motion that adapts\nto different scenes. Our approach utilizes a diffusion model that accepts both\na scene image and text prompt as inputs, producing a motion sequence tailored\nto the scene. To train this model, we collect a large-scale video dataset\nfeaturing single-human activities, annotating each video with the corresponding\nhuman motion as the target output. Experiments demonstrate that our method\neffectively predicts human motion that aligns with the scene image after\nprojection. Furthermore, we show that the generated motion sequence improves\nhuman motion quality in video synthesis tasks.\n","authors":["Hsin-Ping Huang","Yang Zhou","Jui-Hsien Wang","Difan Liu","Feng Liu","Ming-Hsuan Yang","Zhan Xu"],"pdf_url":"https://arxiv.org/pdf/2412.13185v1.pdf","comment":"Project page: https://hhsinping.github.io/Move-in-2D/"},{"id":"http://arxiv.org/abs/2412.13183v1","updated":"2024-12-17T18:57:38Z","published":"2024-12-17T18:57:38Z","title":"Real-time Free-view Human Rendering from Sparse-view RGB Videos using\n  Double Unprojected Textures","summary":"  Real-time free-view human rendering from sparse-view RGB inputs is a\nchallenging task due to the sensor scarcity and the tight time budget. To\nensure efficiency, recent methods leverage 2D CNNs operating in texture space\nto learn rendering primitives. However, they either jointly learn geometry and\nappearance, or completely ignore sparse image information for geometry\nestimation, significantly harming visual quality and robustness to unseen body\nposes. To address these issues, we present Double Unprojected Textures, which\nat the core disentangles coarse geometric deformation estimation from\nappearance synthesis, enabling robust and photorealistic 4K rendering in\nreal-time. Specifically, we first introduce a novel image-conditioned template\ndeformation network, which estimates the coarse deformation of the human\ntemplate from a first unprojected texture. This updated geometry is then used\nto apply a second and more accurate texture unprojection. The resulting texture\nmap has fewer artifacts and better alignment with input views, which benefits\nour learning of finer-level geometry and appearance represented by Gaussian\nsplats. We validate the effectiveness and efficiency of the proposed method in\nquantitative and qualitative experiments, which significantly surpasses other\nstate-of-the-art methods.\n","authors":["Guoxing Sun","Rishabh Dabral","Heming Zhu","Pascal Fua","Christian Theobalt","Marc Habermann"],"pdf_url":"https://arxiv.org/pdf/2412.13183v1.pdf","comment":"Project page: https://vcai.mpi-inf.mpg.de/projects/DUT/"},{"id":"http://arxiv.org/abs/2412.13180v1","updated":"2024-12-17T18:56:50Z","published":"2024-12-17T18:56:50Z","title":"Feather the Throttle: Revisiting Visual Token Pruning for\n  Vision-Language Model Acceleration","summary":"  Recent works on accelerating Vision-Language Models show that strong\nperformance can be maintained across a variety of vision-language tasks despite\nhighly compressing visual information. In this work, we examine the popular\nacceleration approach of early pruning of visual tokens inside the language\nmodel and find that its strong performance across many tasks is not due to an\nexceptional ability to compress visual information, but rather the benchmarks'\nlimited ability to assess fine-grained visual capabilities. Namely, we\ndemonstrate a core issue with the acceleration approach where most tokens\ntowards the top of the image are pruned away. Yet, this issue is only reflected\nin performance for a small subset of tasks such as localization. For the other\nevaluated tasks, strong performance is maintained with the flawed pruning\nstrategy. Noting the limited visual capabilities of the studied acceleration\ntechnique, we propose FEATHER (Fast and Effective Acceleration wiTH Ensemble\ncRiteria), a straightforward approach that (1) resolves the identified issue\nwith early-layer pruning, (2) incorporates uniform sampling to ensure coverage\nacross all image regions, and (3) applies pruning in two stages to allow the\ncriteria to become more effective at a later layer while still achieving\nsignificant speedup through early-layer pruning. With comparable computational\nsavings, we find that FEATHER has more than $5\\times$ performance improvement\non the vision-centric localization benchmarks compared to the original\nacceleration approach.\n","authors":["Mark Endo","Xiaohan Wang","Serena Yeung-Levy"],"pdf_url":"https://arxiv.org/pdf/2412.13180v1.pdf","comment":"Project page:\n  https://web.stanford.edu/~markendo/projects/feather.html"},{"id":"http://arxiv.org/abs/2412.13176v1","updated":"2024-12-17T18:54:28Z","published":"2024-12-17T18:54:28Z","title":"NFL-BA: Improving Endoscopic SLAM with Near-Field Light Bundle\n  Adjustment","summary":"  Simultaneous Localization And Mapping (SLAM) from a monocular endoscopy video\ncan enable autonomous navigation, guidance to unsurveyed regions, and 3D\nvisualizations, which can significantly improve endoscopy experience for\nsurgeons and patient outcomes. Existing dense SLAM algorithms often assume\ndistant and static lighting and textured surfaces, and alternate between\noptimizing scene geometry and camera parameters by minimizing a photometric\nrendering loss, often called Photometric Bundle Adjustment. However, endoscopic\nenvironments exhibit dynamic near-field lighting due to the co-located light\nand camera moving extremely close to the surface, textureless surfaces, and\nstrong specular reflections due to mucus layers. When not considered, these\nnear-field lighting effects can cause significant performance reductions for\nexisting SLAM algorithms from indoor/outdoor scenes when applied to endoscopy\nvideos. To mitigate this problem, we introduce a new Near-Field Lighting Bundle\nAdjustment Loss $(L_{NFL-BA})$ that can also be alternatingly optimized, along\nwith the Photometric Bundle Adjustment loss, such that the captured images'\nintensity variations match the relative distance and orientation between the\nsurface and the co-located light and camera. We derive a general NFL-BA loss\nfunction for 3D Gaussian surface representations and demonstrate that adding\n$L_{NFL-BA}$ can significantly improve the tracking and mapping performance of\ntwo state-of-the-art 3DGS-SLAM systems, MonoGS (35% improvement in tracking,\n48% improvement in mapping with predicted depth maps) and EndoGSLAM (22%\nimprovement in tracking, marginal improvement in mapping with predicted\ndepths), on the C3VD endoscopy dataset for colons. The project page is\navailable at https://asdunnbe.github.io/NFL-BA/\n","authors":["Andrea Dunn Beltran","Daniel Rho","Marc Niethammer","Roni Sengupta"],"pdf_url":"https://arxiv.org/pdf/2412.13176v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13174v1","updated":"2024-12-17T18:53:43Z","published":"2024-12-17T18:53:43Z","title":"ORFormer: Occlusion-Robust Transformer for Accurate Facial Landmark\n  Detection","summary":"  Although facial landmark detection (FLD) has gained significant progress,\nexisting FLD methods still suffer from performance drops on partially\nnon-visible faces, such as faces with occlusions or under extreme lighting\nconditions or poses. To address this issue, we introduce ORFormer, a novel\ntransformer-based method that can detect non-visible regions and recover their\nmissing features from visible parts. Specifically, ORFormer associates each\nimage patch token with one additional learnable token called the messenger\ntoken. The messenger token aggregates features from all but its patch. This\nway, the consensus between a patch and other patches can be assessed by\nreferring to the similarity between its regular and messenger embeddings,\nenabling non-visible region identification. Our method then recovers occluded\npatches with features aggregated by the messenger tokens. Leveraging the\nrecovered features, ORFormer compiles high-quality heatmaps for the downstream\nFLD task. Extensive experiments show that our method generates heatmaps\nresilient to partial occlusions. By integrating the resultant heatmaps into\nexisting FLD methods, our method performs favorably against the state of the\narts on challenging datasets such as WFLW and COFW.\n","authors":["Jui-Che Chiang","Hou-Ning Hu","Bo-Syuan Hou","Chia-Yu Tseng","Yu-Lun Liu","Min-Hung Chen","Yen-Yu Lin"],"pdf_url":"https://arxiv.org/pdf/2412.13174v1.pdf","comment":"WACV 2025"},{"id":"http://arxiv.org/abs/2412.13173v1","updated":"2024-12-17T18:52:30Z","published":"2024-12-17T18:52:30Z","title":"Locate n' Rotate: Two-stage Openable Part Detection with Foundation\n  Model Priors","summary":"  Detecting the openable parts of articulated objects is crucial for downstream\napplications in intelligent robotics, such as pulling a drawer. This task poses\na multitasking challenge due to the necessity of understanding object\ncategories and motion. Most existing methods are either category-specific or\ntrained on specific datasets, lacking generalization to unseen environments and\nobjects. In this paper, we propose a Transformer-based Openable Part Detection\n(OPD) framework named Multi-feature Openable Part Detection (MOPD) that\nincorporates perceptual grouping and geometric priors, outperforming previous\nmethods in performance. In the first stage of the framework, we introduce a\nperceptual grouping feature model that provides perceptual grouping feature\npriors for openable part detection, enhancing detection results through a\ncross-attention mechanism. In the second stage, a geometric understanding\nfeature model offers geometric feature priors for predicting motion parameters.\nCompared to existing methods, our proposed approach shows better performance in\nboth detection and motion parameter prediction. Codes and models are publicly\navailable at https://github.com/lisiqi-zju/MOPD\n","authors":["Siqi Li","Xiaoxue Chen","Haoyu Cheng","Guyue Zhou","Hao Zhao","Guanzhong Tian"],"pdf_url":"https://arxiv.org/pdf/2412.13173v1.pdf","comment":"ACCV 2024 Oral, Project: https://github.com/lisiqi-zju/MOPD"},{"id":"http://arxiv.org/abs/2412.12095v2","updated":"2024-12-17T18:45:55Z","published":"2024-12-16T18:59:29Z","title":"Causal Diffusion Transformers for Generative Modeling","summary":"  We introduce Causal Diffusion as the autoregressive (AR) counterpart of\nDiffusion models. It is a next-token(s) forecasting framework that is friendly\nto both discrete and continuous modalities and compatible with existing\nnext-token prediction models like LLaMA and GPT. While recent works attempt to\ncombine diffusion with AR models, we show that introducing sequential\nfactorization to a diffusion model can substantially improve its performance\nand enables a smooth transition between AR and diffusion generation modes.\nHence, we propose CausalFusion - a decoder-only transformer that\ndual-factorizes data across sequential tokens and diffusion noise levels,\nleading to state-of-the-art results on the ImageNet generation benchmark while\nalso enjoying the AR advantage of generating an arbitrary number of tokens for\nin-context reasoning. We further demonstrate CausalFusion's multimodal\ncapabilities through a joint image generation and captioning model, and\nshowcase CausalFusion's ability for zero-shot in-context image manipulations.\nWe hope that this work could provide the community with a fresh perspective on\ntraining multimodal models over discrete and continuous data.\n","authors":["Chaorui Deng","Deyao Zhu","Kunchang Li","Shi Guang","Haoqi Fan"],"pdf_url":"https://arxiv.org/pdf/2412.12095v2.pdf","comment":"22 figures, 21 pages"},{"id":"http://arxiv.org/abs/2412.13168v1","updated":"2024-12-17T18:45:53Z","published":"2024-12-17T18:45:53Z","title":"Lifting Scheme-Based Implicit Disentanglement of Emotion-Related Facial\n  Dynamics in the Wild","summary":"  In-the-wild Dynamic facial expression recognition (DFER) encounters a\nsignificant challenge in recognizing emotion-related expressions, which are\noften temporally and spatially diluted by emotion-irrelevant expressions and\nglobal context respectively. Most of the prior DFER methods model tightly\ncoupled spatiotemporal representations which may incorporate weakly relevant\nfeatures, leading to information redundancy and emotion-irrelevant context\nbias. Several DFER methods have highlighted the significance of dynamic\ninformation, but utilize explicit manners to extract dynamic features with\noverly strong prior knowledge. In this paper, we propose a novel Implicit\nFacial Dynamics Disentanglement framework (IFDD). Through expanding wavelet\nlifting scheme to fully learnable framework, IFDD disentangles emotion-related\ndynamic information from emotion-irrelevant global context in an implicit\nmanner, i.e., without exploit operations and external guidance. The\ndisentanglement process of IFDD contains two stages, i.e., Inter-frame\nStatic-dynamic Splitting Module (ISSM) for rough disentanglement estimation and\nLifting-based Aggregation-Disentanglement Module (LADM) for further refinement.\nSpecifically, ISSM explores inter-frame correlation to generate content-aware\nsplitting indexes on-the-fly. We preliminarily utilize these indexes to split\nframe features into two groups, one with greater global similarity, and the\nother with more unique dynamic features. Subsequently, LADM first aggregates\nthese two groups of features to obtain fine-grained global context features by\nan updater, and then disentangles emotion-related facial dynamic features from\nthe global context by a predictor. Extensive experiments on in-the-wild\ndatasets have demonstrated that IFDD outperforms prior supervised DFER methods\nwith higher recognition accuracy and comparable efficiency.\n","authors":["Xingjian Wang","Li Chai"],"pdf_url":"https://arxiv.org/pdf/2412.13168v1.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2412.13161v1","updated":"2024-12-17T18:39:10Z","published":"2024-12-17T18:39:10Z","title":"BanglishRev: A Large-Scale Bangla-English and Code-mixed Dataset of\n  Product Reviews in E-Commerce","summary":"  This work presents the BanglishRev Dataset, the largest e-commerce product\nreview dataset to date for reviews written in Bengali, English, a mixture of\nboth and Banglish, Bengali words written with English alphabets. The dataset\ncomprises of 1.74 million written reviews from 3.2 million ratings information\ncollected from a total of 128k products being sold in online e-commerce\nplatforms targeting the Bengali population. It includes an extensive array of\nrelated metadata for each of the reviews including the rating given by the\nreviewer, date the review was posted and date of purchase, number of likes,\ndislikes, response from the seller, images associated with the review etc. With\nsentiment analysis being the most prominent usage of review datasets,\nexperimentation with a binary sentiment analysis model with the review rating\nserving as an indicator of positive or negative sentiment was conducted to\nevaluate the effectiveness of the large amount of data presented in BanglishRev\nfor sentiment analysis tasks. A BanglishBERT model is trained on the data from\nBanglishRev with reviews being considered labeled positive if the rating is\ngreater than 3 and negative if the rating is less than or equal to 3. The model\nis evaluated by being testing against a previously published manually annotated\ndataset for e-commerce reviews written in a mixture of Bangla, English and\nBanglish. The experimental model achieved an exceptional accuracy of 94\\% and\nF1 score of 0.94, demonstrating the dataset's efficacy for sentiment analysis.\nSome of the intriguing patterns and observations seen within the dataset and\nfuture research directions where the dataset can be utilized is also discussed\nand explored. The dataset can be accessed through\nhttps://huggingface.co/datasets/BanglishRev/bangla-english-and-code-mixed-ecommerce-review-dataset.\n","authors":["Mohammad Nazmush Shamael","Sabila Nawshin","Swakkhar Shatabda","Salekul Islam"],"pdf_url":"https://arxiv.org/pdf/2412.13161v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03665v3","updated":"2024-12-17T18:39:00Z","published":"2024-10-04T17:59:57Z","title":"Estimating Body and Hand Motion in an Ego-sensed World","summary":"  We present EgoAllo, a system for human motion estimation from a head-mounted\ndevice. Using only egocentric SLAM poses and images, EgoAllo guides sampling\nfrom a conditional diffusion model to estimate 3D body pose, height, and hand\nparameters that capture a device wearer's actions in the allocentric coordinate\nframe of the scene. To achieve this, our key insight is in representation: we\npropose spatial and temporal invariance criteria for improving model\nperformance, from which we derive a head motion conditioning parameterization\nthat improves estimation by up to 18%. We also show how the bodies estimated by\nour system can improve hand estimation: the resulting kinematic and temporal\nconstraints can reduce world-frame errors in single-frame estimates by 40%.\nProject page: https://egoallo.github.io/\n","authors":["Brent Yi","Vickie Ye","Maya Zheng","Yunqi Li","Lea Müller","Georgios Pavlakos","Yi Ma","Jitendra Malik","Angjoo Kanazawa"],"pdf_url":"https://arxiv.org/pdf/2410.03665v3.pdf","comment":"Project page: https://egoallo.github.io/"},{"id":"http://arxiv.org/abs/2412.13156v1","updated":"2024-12-17T18:30:22Z","published":"2024-12-17T18:30:22Z","title":"S2S2: Semantic Stacking for Robust Semantic Segmentation in Medical\n  Imaging","summary":"  Robustness and generalizability in medical image segmentation are often\nhindered by scarcity and limited diversity of training data, which stands in\ncontrast to the variability encountered during inference. While conventional\nstrategies -- such as domain-specific augmentation, specialized architectures,\nand tailored training procedures -- can alleviate these issues, they depend on\nthe availability and reliability of domain knowledge. When such knowledge is\nunavailable, misleading, or improperly applied, performance may deteriorate. In\nresponse, we introduce a novel, domain-agnostic, add-on, and data-driven\nstrategy inspired by image stacking in image denoising. Termed ``semantic\nstacking,'' our method estimates a denoised semantic representation that\ncomplements the conventional segmentation loss during training. This method\ndoes not depend on domain-specific assumptions, making it broadly applicable\nacross diverse image modalities, model architectures, and augmentation\ntechniques. Through extensive experiments, we validate the superiority of our\napproach in improving segmentation performance under diverse conditions. Code\nis available at https://github.com/ymp5078/Semantic-Stacking.\n","authors":["Yimu Pan","Sitao Zhang","Alison D. Gernand","Jeffery A. Goldstein","James Z. Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13156v1.pdf","comment":"AAAI2025"},{"id":"http://arxiv.org/abs/2412.13155v1","updated":"2024-12-17T18:28:48Z","published":"2024-12-17T18:28:48Z","title":"F-Bench: Rethinking Human Preference Evaluation Metrics for Benchmarking\n  Face Generation, Customization, and Restoration","summary":"  Artificial intelligence generative models exhibit remarkable capabilities in\ncontent creation, particularly in face image generation, customization, and\nrestoration. However, current AI-generated faces (AIGFs) often fall short of\nhuman preferences due to unique distortions, unrealistic details, and\nunexpected identity shifts, underscoring the need for a comprehensive quality\nevaluation framework for AIGFs. To address this need, we introduce FaceQ, a\nlarge-scale, comprehensive database of AI-generated Face images with\nfine-grained Quality annotations reflecting human preferences. The FaceQ\ndatabase comprises 12,255 images generated by 29 models across three tasks: (1)\nface generation, (2) face customization, and (3) face restoration. It includes\n32,742 mean opinion scores (MOSs) from 180 annotators, assessed across multiple\ndimensions: quality, authenticity, identity (ID) fidelity, and text-image\ncorrespondence. Using the FaceQ database, we establish F-Bench, a benchmark for\ncomparing and evaluating face generation, customization, and restoration\nmodels, highlighting strengths and weaknesses across various prompts and\nevaluation dimensions. Additionally, we assess the performance of existing\nimage quality assessment (IQA), face quality assessment (FQA), AI-generated\ncontent image quality assessment (AIGCIQA), and preference evaluation metrics,\nmanifesting that these standard metrics are relatively ineffective in\nevaluating authenticity, ID fidelity, and text-image correspondence. The FaceQ\ndatabase will be publicly available upon publication.\n","authors":["Lu Liu","Huiyu Duan","Qiang Hu","Liu Yang","Chunlei Cai","Tianxiao Ye","Huayu Liu","Xiaoyun Zhang","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2412.13155v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13152v1","updated":"2024-12-17T18:23:33Z","published":"2024-12-17T18:23:33Z","title":"Continuous Patient Monitoring with AI: Real-Time Analysis of Video in\n  Hospital Care Settings","summary":"  This study introduces an AI-driven platform for continuous and passive\npatient monitoring in hospital settings, developed by LookDeep Health.\nLeveraging advanced computer vision, the platform provides real-time insights\ninto patient behavior and interactions through video analysis, securely storing\ninference results in the cloud for retrospective evaluation. The dataset,\ncompiled in collaboration with 11 hospital partners, encompasses over 300\nhigh-risk fall patients and over 1,000 days of inference, enabling applications\nsuch as fall detection and safety monitoring for vulnerable patient\npopulations. To foster innovation and reproducibility, an anonymized subset of\nthis dataset is publicly available. The AI system detects key components in\nhospital rooms, including individual presence and role, furniture location,\nmotion magnitude, and boundary crossings. Performance evaluation demonstrates\nstrong accuracy in object detection (macro F1-score = 0.92) and patient-role\nclassification (F1-score = 0.98), as well as reliable trend analysis for the\n\"patient alone\" metric (mean logistic regression accuracy = 0.82 \\pm 0.15).\nThese capabilities enable automated detection of patient isolation, wandering,\nor unsupervised movement-key indicators for fall risk and other adverse events.\nThis work establishes benchmarks for validating AI-driven patient monitoring\nsystems, highlighting the platform's potential to enhance patient safety and\ncare by providing continuous, data-driven insights into patient behavior and\ninteractions.\n","authors":["Paolo Gabriel","Peter Rehani","Tyler Troy","Tiffany Wyatt","Michael Choma","Narinder Singh"],"pdf_url":"https://arxiv.org/pdf/2412.13152v1.pdf","comment":"21 pages, 9 figures, 3 tables, submitted to Frontiers in Imaging >\n  Imaging Applications > (Research Topic) Deep Learning for Medical Imaging\n  Applications for publication"},{"id":"http://arxiv.org/abs/2412.13140v1","updated":"2024-12-17T18:06:28Z","published":"2024-12-17T18:06:28Z","title":"Label Errors in the Tobacco3482 Dataset","summary":"  Tobacco3482 is a widely used document classification benchmark dataset.\nHowever, our manual inspection of the entire dataset uncovers widespread\nontological issues, especially large amounts of annotation label problems in\nthe dataset. We establish data label guidelines and find that 11.7% of the\ndataset is improperly annotated and should either have an unknown label or a\ncorrected label, and 16.7% of samples in the dataset have multiple valid\nlabels. We then analyze the mistakes of a top-performing model and find that\n35% of the model's mistakes can be directly attributed to these label issues,\nhighlighting the inherent problems with using a noisily labeled dataset as a\nbenchmark. Supplementary material, including dataset annotations and code, is\navailable at https://github.com/gordon-lim/tobacco3482-mistakes/.\n","authors":["Gordon Lim","Stefan Larson","Kevin Leach"],"pdf_url":"https://arxiv.org/pdf/2412.13140v1.pdf","comment":"WACV VisionDocs Workshop 2025"},{"id":"http://arxiv.org/abs/2412.13137v1","updated":"2024-12-17T18:04:33Z","published":"2024-12-17T18:04:33Z","title":"Unlocking the Potential of Digital Pathology: Novel Baselines for\n  Compression","summary":"  Digital pathology offers a groundbreaking opportunity to transform clinical\npractice in histopathological image analysis, yet faces a significant hurdle:\nthe substantial file sizes of pathological Whole Slide Images (WSI). While\ncurrent digital pathology solutions rely on lossy JPEG compression to address\nthis issue, lossy compression can introduce color and texture disparities,\npotentially impacting clinical decision-making. While prior research addresses\nperceptual image quality and downstream performance independently of each\nother, we jointly evaluate compression schemes for perceptual and downstream\ntask quality on four different datasets. In addition, we collect an initially\nuncompressed dataset for an unbiased perceptual evaluation of compression\nschemes. Our results show that deep learning models fine-tuned for perceptual\nquality outperform conventional compression schemes like JPEG-XL or WebP for\nfurther compression of WSI. However, they exhibit a significant bias towards\nthe compression artifacts present in the training data and struggle to\ngeneralize across various compression schemes. We introduce a novel evaluation\nmetric based on feature similarity between original files and compressed files\nthat aligns very well with the actual downstream performance on the compressed\nWSI. Our metric allows for a general and standardized evaluation of lossy\ncompression schemes and mitigates the requirement to independently assess\ndifferent downstream tasks. Our study provides novel insights for the\nassessment of lossy compression schemes for WSI and encourages a unified\nevaluation of lossy compression schemes to accelerate the clinical uptake of\ndigital pathology.\n","authors":["Maximilian Fischer","Peter Neher","Peter Schüffler","Sebastian Ziegler","Shuhan Xiao","Robin Peretzke","David Clunie","Constantin Ulrich","Michael Baumgartner","Alexander Muckenhuber","Silvia Dias Almeida","Michael Götz","Jens Kleesiek","Marco Nolden","Rickmer Braren","Klaus Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2412.13137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13126v1","updated":"2024-12-17T17:45:21Z","published":"2024-12-17T17:45:21Z","title":"A Knowledge-enhanced Pathology Vision-language Foundation Model for\n  Cancer Diagnosis","summary":"  Deep learning has enabled the development of highly robust foundation models\nfor various pathological tasks across diverse diseases and patient cohorts.\nAmong these models, vision-language pre-training, which leverages large-scale\npaired data to align pathology image and text embedding spaces, and provides a\nnovel zero-shot paradigm for downstream tasks. However, existing models have\nbeen primarily data-driven and lack the incorporation of domain-specific\nknowledge, which limits their performance in cancer diagnosis, especially for\nrare tumor subtypes. To address this limitation, we establish a\nKnowledge-enhanced Pathology (KEEP) foundation model that harnesses disease\nknowledge to facilitate vision-language pre-training. Specifically, we first\nconstruct a disease knowledge graph (KG) that covers 11,454 human diseases with\n139,143 disease attributes, including synonyms, definitions, and hypernym\nrelations. We then systematically reorganize the millions of publicly available\nnoisy pathology image-text pairs, into 143K well-structured semantic groups\nlinked through the hierarchical relations of the disease KG. To derive more\nnuanced image and text representations, we propose a novel knowledge-enhanced\nvision-language pre-training approach that integrates disease knowledge into\nthe alignment within hierarchical semantic groups instead of unstructured\nimage-text pairs. Validated on 18 diverse benchmarks with more than 14,000\nwhole slide images (WSIs), KEEP achieves state-of-the-art performance in\nzero-shot cancer diagnostic tasks. Notably, for cancer detection, KEEP\ndemonstrates an average sensitivity of 89.8% at a specificity of 95.0% across 7\ncancer types. For cancer subtyping, KEEP achieves a median balanced accuracy of\n0.456 in subtyping 30 rare brain cancers, indicating strong generalizability\nfor diagnosing rare tumors.\n","authors":["Xiao Zhou","Luoyi Sun","Dexuan He","Wenbin Guan","Ruifen Wang","Lifeng Wang","Xin Sun","Kun Sun","Ya Zhang","Yanfeng Wang","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2412.13126v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.05271v3","updated":"2024-12-17T17:44:38Z","published":"2024-12-06T18:57:08Z","title":"Expanding Performance Boundaries of Open-Source Multimodal Models with\n  Model, Data, and Test-Time Scaling","summary":"  We introduce InternVL 2.5, an advanced multimodal large language model (MLLM)\nseries that builds upon InternVL 2.0, maintaining its core model architecture\nwhile introducing significant enhancements in training and testing strategies\nas well as data quality. In this work, we delve into the relationship between\nmodel scaling and performance, systematically exploring the performance trends\nin vision encoders, language models, dataset sizes, and test-time\nconfigurations. Through extensive evaluations on a wide range of benchmarks,\nincluding multi-discipline reasoning, document understanding, multi-image /\nvideo understanding, real-world comprehension, multimodal hallucination\ndetection, visual grounding, multilingual capabilities, and pure language\nprocessing, InternVL 2.5 exhibits competitive performance, rivaling leading\ncommercial models such as GPT-4o and Claude-3.5-Sonnet. Notably, our model is\nthe first open-source MLLMs to surpass 70% on the MMMU benchmark, achieving a\n3.7-point improvement through Chain-of-Thought (CoT) reasoning and showcasing\nstrong potential for test-time scaling. We hope this model contributes to the\nopen-source community by setting new standards for developing and applying\nmultimodal AI systems. HuggingFace demo see\nhttps://huggingface.co/spaces/OpenGVLab/InternVL\n","authors":["Zhe Chen","Weiyun Wang","Yue Cao","Yangzhou Liu","Zhangwei Gao","Erfei Cui","Jinguo Zhu","Shenglong Ye","Hao Tian","Zhaoyang Liu","Lixin Gu","Xuehui Wang","Qingyun Li","Yimin Ren","Zixuan Chen","Jiapeng Luo","Jiahao Wang","Tan Jiang","Bo Wang","Conghui He","Botian Shi","Xingcheng Zhang","Han Lv","Yi Wang","Wenqi Shao","Pei Chu","Zhongying Tu","Tong He","Zhiyong Wu","Huipeng Deng","Jiaye Ge","Kai Chen","Min Dou","Lewei Lu","Xizhou Zhu","Tong Lu","Dahua Lin","Yu Qiao","Jifeng Dai","Wenhai Wang"],"pdf_url":"https://arxiv.org/pdf/2412.05271v3.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2412.13111v1","updated":"2024-12-17T17:34:52Z","published":"2024-12-17T17:34:52Z","title":"Motion-2-to-3: Leveraging 2D Motion Data to Boost 3D Motion Generation","summary":"  Text-driven human motion synthesis is capturing significant attention for its\nability to effortlessly generate intricate movements from abstract text cues,\nshowcasing its potential for revolutionizing motion design not only in film\nnarratives but also in virtual reality experiences and computer game\ndevelopment. Existing methods often rely on 3D motion capture data, which\nrequire special setups resulting in higher costs for data acquisition,\nultimately limiting the diversity and scope of human motion. In contrast, 2D\nhuman videos offer a vast and accessible source of motion data, covering a\nwider range of styles and activities. In this paper, we explore leveraging 2D\nhuman motion extracted from videos as an alternative data source to improve\ntext-driven 3D motion generation. Our approach introduces a novel framework\nthat disentangles local joint motion from global movements, enabling efficient\nlearning of local motion priors from 2D data. We first train a single-view 2D\nlocal motion generator on a large dataset of text-motion pairs. To enhance this\nmodel to synthesize 3D motion, we fine-tune the generator with 3D data,\ntransforming it into a multi-view generator that predicts view-consistent local\njoint motion and root dynamics. Experiments on the HumanML3D dataset and novel\ntext prompts demonstrate that our method efficiently utilizes 2D data,\nsupporting realistic 3D human motion generation and broadening the range of\nmotion types it supports. Our code will be made publicly available at\nhttps://zju3dv.github.io/Motion-2-to-3/.\n","authors":["Huaijin Pi","Ruoxi Guo","Zehong Shen","Qing Shuai","Zechen Hu","Zhumei Wang","Yajiao Dong","Ruizhen Hu","Taku Komura","Sida Peng","Xiaowei Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.13111v1.pdf","comment":"Project page: https://zju3dv.github.io/Motion-2-to-3/"},{"id":"http://arxiv.org/abs/2412.13099v1","updated":"2024-12-17T17:10:02Z","published":"2024-12-17T17:10:02Z","title":"Accuracy Limits as a Barrier to Biometric System Security","summary":"  Biometric systems are widely used for identity verification and\nidentification, including authentication (i.e., one-to-one matching to verify a\nclaimed identity) and identification (i.e., one-to-many matching to find a\nsubject in a database). The matching process relies on measuring similarities\nor dissimilarities between a fresh biometric template and enrolled templates.\nThe False Match Rate FMR is a key metric for assessing the accuracy and\nreliability of such systems. This paper analyzes biometric systems based on\ntheir FMR, with two main contributions. First, we explore untargeted attacks,\nwhere an adversary aims to impersonate any user within a database. We determine\nthe number of trials required for an attacker to successfully impersonate a\nuser and derive the critical population size (i.e., the maximum number of users\nin the database) required to maintain a given level of security. Furthermore,\nwe compute the critical FMR value needed to ensure resistance against\nuntargeted attacks as the database size increases. Second, we revisit the\nbiometric birthday problem to evaluate the approximate and exact probabilities\nthat two users in a database collide (i.e., can impersonate each other). Based\non this analysis, we derive both the approximate critical population size and\nthe critical FMR value needed to bound the likelihood of such collisions\noccurring with a given probability. These thresholds offer insights for\ndesigning systems that mitigate the risk of impersonation and collisions,\nparticularly in large-scale biometric databases. Our findings indicate that\ncurrent biometric systems fail to deliver sufficient accuracy to achieve an\nadequate security level against untargeted attacks, even in small-scale\ndatabases. Moreover, state-of-the-art systems face significant challenges in\naddressing the biometric birthday problem, especially as database sizes grow.\n","authors":["Axel Durbet","Paul-Marie Grollemund","Pascal Lafourcade","Kevin Thiry-Atighehchi"],"pdf_url":"https://arxiv.org/pdf/2412.13099v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13096v1","updated":"2024-12-17T17:06:33Z","published":"2024-12-17T17:06:33Z","title":"Incremental Online Learning of Randomized Neural Network with Forward\n  Regularization","summary":"  Online learning of deep neural networks suffers from challenges such as\nhysteretic non-incremental updating, increasing memory usage, past\nretrospective retraining, and catastrophic forgetting. To alleviate these\ndrawbacks and achieve progressive immediate decision-making, we propose a novel\nIncremental Online Learning (IOL) process of Randomized Neural Networks\n(Randomized NN), a framework facilitating continuous improvements to Randomized\nNN performance in restrictive online scenarios. Within the framework, we\nfurther introduce IOL with ridge regularization (-R) and IOL with forward\nregularization (-F). -R generates stepwise incremental updates without\nretrospective retraining and avoids catastrophic forgetting. Moreover, we\nsubstituted -R with -F as it enhanced precognition learning ability using\nsemi-supervision and realized better online regrets to offline global experts\ncompared to -R during IOL. The algorithms of IOL for Randomized NN with -R/-F\non non-stationary batch stream were derived respectively, featuring recursive\nweight updates and variable learning rates. Additionally, we conducted a\ndetailed analysis and theoretically derived relative cumulative regret bounds\nof the Randomized NN learners with -R/-F in IOL under adversarial assumptions\nusing a novel methodology and presented several corollaries, from which we\nobserved the superiority on online learning acceleration and regret bounds of\nemploying -F in IOL. Finally, our proposed methods were rigorously examined\nacross regression and classification tasks on diverse datasets, which\ndistinctly validated the efficacy of IOL frameworks of Randomized NN and the\nadvantages of forward regularization.\n","authors":["Junda Wang","Minghui Hu","Ning Li","Abdulaziz Al-Ali","Ponnuthurai Nagaratnam Suganthan"],"pdf_url":"https://arxiv.org/pdf/2412.13096v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13081v1","updated":"2024-12-17T16:54:05Z","published":"2024-12-17T16:54:05Z","title":"Prompt Augmentation for Self-supervised Text-guided Image Manipulation","summary":"  Text-guided image editing finds applications in various creative and\npractical fields. While recent studies in image generation have advanced the\nfield, they often struggle with the dual challenges of coherent image\ntransformation and context preservation. In response, our work introduces\nprompt augmentation, a method amplifying a single input prompt into several\ntarget prompts, strengthening textual context and enabling localised image\nediting. Specifically, we use the augmented prompts to delineate the intended\nmanipulation area. We propose a Contrastive Loss tailored to driving effective\nimage editing by displacing edited areas and drawing preserved regions closer.\nAcknowledging the continuous nature of image manipulations, we further refine\nour approach by incorporating the similarity concept, creating a Soft\nContrastive Loss. The new losses are incorporated to the diffusion model,\ndemonstrating improved or competitive image editing results on public datasets\nand generated images over state-of-the-art approaches.\n","authors":["Rumeysa Bodur","Binod Bhattarai","Tae-Kyun Kim"],"pdf_url":"https://arxiv.org/pdf/2412.13081v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13079v1","updated":"2024-12-17T16:51:44Z","published":"2024-12-17T16:51:44Z","title":"Identifying Bias in Deep Neural Networks Using Image Transforms","summary":"  CNNs have become one of the most commonly used computational tool in the past\ntwo decades. One of the primary downsides of CNNs is that they work as a\n``black box\", where the user cannot necessarily know how the image data are\nanalyzed, and therefore needs to rely on empirical evaluation to test the\nefficacy of a trained CNN. This can lead to hidden biases that affect the\nperformance evaluation of neural networks, but are difficult to identify. Here\nwe discuss examples of such hidden biases in common and widely used benchmark\ndatasets, and propose techniques for identifying dataset biases that can affect\nthe standard performance evaluation metrics. One effective approach to identify\ndataset bias is to perform image classification by using merely blank\nbackground parts of the original images. However, in some situations a blank\nbackground in the images is not available, making it more difficult to separate\nforeground or contextual information from the bias. To overcome this, we\npropose a method to identify dataset bias without the need to crop background\ninformation from the images. That method is based on applying several image\ntransforms to the original images, including Fourier transform, wavelet\ntransforms, median filter, and their combinations. These transforms were\napplied to recover background bias information that CNNs use to classify\nimages. This transformations affect the contextual visual information in a\ndifferent manner than it affects the systemic background bias. Therefore, the\nmethod can distinguish between contextual information and the bias, and alert\non the presence of background bias even without the need to separate sub-images\nparts from the blank background of the original images. Code used in the\nexperiments is publicly available.\n","authors":["Sai Teja Erukude","Akhil Joshi","Lior Shamir"],"pdf_url":"https://arxiv.org/pdf/2412.13079v1.pdf","comment":"Computers, published"},{"id":"http://arxiv.org/abs/2410.14672v2","updated":"2024-12-17T16:47:41Z","published":"2024-10-18T17:59:04Z","title":"BiGR: Harnessing Binary Latent Codes for Image Generation and Improved\n  Visual Representation Capabilities","summary":"  We introduce BiGR, a novel conditional image generation model using compact\nbinary latent codes for generative training, focusing on enhancing both\ngeneration and representation capabilities. BiGR is the first conditional\ngenerative model that unifies generation and discrimination within the same\nframework. BiGR features a binary tokenizer, a masked modeling mechanism, and a\nbinary transcoder for binary code prediction. Additionally, we introduce a\nnovel entropy-ordered sampling method to enable efficient image generation.\nExtensive experiments validate BiGR's superior performance in generation\nquality, as measured by FID-50k, and representation capabilities, as evidenced\nby linear-probe accuracy. Moreover, BiGR showcases zero-shot generalization\nacross various vision tasks, enabling applications such as image inpainting,\noutpainting, editing, interpolation, and enrichment, without the need for\nstructural modifications. Our findings suggest that BiGR unifies generative and\ndiscriminative tasks effectively, paving the way for further advancements in\nthe field. We further enable BiGR to perform text-to-image generation,\nshowcasing its potential for broader applications.\n","authors":["Shaozhe Hao","Xuantong Liu","Xianbiao Qi","Shihao Zhao","Bojia Zi","Rong Xiao","Kai Han","Kwan-Yee K. Wong"],"pdf_url":"https://arxiv.org/pdf/2410.14672v2.pdf","comment":"Updated with additional T2I results; Project page:\n  https://haoosz.github.io/BiGR"},{"id":"http://arxiv.org/abs/2412.13070v1","updated":"2024-12-17T16:34:32Z","published":"2024-12-17T16:34:32Z","title":"Learning of Patch-Based Smooth-Plus-Sparse Models for Image\n  Reconstruction","summary":"  We aim at the solution of inverse problems in imaging, by combining a\npenalized sparse representation of image patches with an unconstrained smooth\none. This allows for a straightforward interpretation of the reconstruction. We\nformulate the optimization as a bilevel problem. The inner problem deploys\nclassical algorithms while the outer problem optimizes the dictionary and the\nregularizer parameters through supervised learning. The process is carried out\nvia implicit differentiation and gradient-based optimization. We evaluate our\nmethod for denoising, super-resolution, and compressed-sensing\nmagnetic-resonance imaging. We compare it to other classical models as well as\ndeep-learning-based methods and show that it always outperforms the former and\nalso the latter in some instances.\n","authors":["Stanislas Ducotterd","Sebastian Neumayer","Michael Unser"],"pdf_url":"https://arxiv.org/pdf/2412.13070v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13063v1","updated":"2024-12-17T16:28:08Z","published":"2024-12-17T16:28:08Z","title":"Smartphone-based Iris Recognition through High-Quality Visible Spectrum\n  Iris Capture","summary":"  Iris recognition is widely acknowledged for its exceptional accuracy in\nbiometric authentication, traditionally relying on near-infrared (NIR) imaging.\nRecently, visible spectrum (VIS) imaging via accessible smartphone cameras has\nbeen explored for biometric capture. However, a thorough study of iris\nrecognition using smartphone-captured 'High-Quality' VIS images and\ncross-spectral matching with previously enrolled NIR images has not been\nconducted. The primary challenge lies in capturing high-quality biometrics, a\nknown limitation of smartphone cameras. This study introduces a novel Android\napplication designed to consistently capture high-quality VIS iris images\nthrough automated focus and zoom adjustments. The application integrates a\nYOLOv3-tiny model for precise eye and iris detection and a lightweight\nGhost-Attention U-Net (G-ATTU-Net) for segmentation, while adhering to ISO/IEC\n29794-6 standards for image quality. The approach was validated using\nsmartphone-captured VIS and NIR iris images from 47 subjects, achieving a True\nAcceptance Rate (TAR) of 96.57% for VIS images and 97.95% for NIR images, with\nconsistent performance across various capture distances and iris colors. This\nrobust solution is expected to significantly advance the field of iris\nbiometrics, with important implications for enhancing smartphone security.\n","authors":["Naveenkumar G Venkataswamy","Yu Liu","Surendra Singh","Soumyabrata Dey","Stephanie Schuckers","Masudul H Imtiaz"],"pdf_url":"https://arxiv.org/pdf/2412.13063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11017v2","updated":"2024-12-17T16:27:21Z","published":"2024-12-15T02:10:18Z","title":"On Distilling the Displacement Knowledge for Few-Shot Class-Incremental\n  Learning","summary":"  Few-shot Class-Incremental Learning (FSCIL) addresses the challenges of\nevolving data distributions and the difficulty of data acquisition in\nreal-world scenarios. To counteract the catastrophic forgetting typically\nencountered in FSCIL, knowledge distillation is employed as a way to maintain\nthe knowledge from learned data distribution. Recognizing the limitations of\ngenerating discriminative feature representations in a few-shot context, our\napproach incorporates structural information between samples into knowledge\ndistillation. This structural information serves as a remedy for the low\nquality of features. Diverging from traditional structured distillation methods\nthat compute sample similarity, we introduce the Displacement Knowledge\nDistillation (DKD) method. DKD utilizes displacement rather than similarity\nbetween samples, incorporating both distance and angular information to\nsignificantly enhance the information density retained through knowledge\ndistillation. Observing performance disparities in feature distribution between\nbase and novel classes, we propose the Dual Distillation Network (DDNet). This\nnetwork applies traditional knowledge distillation to base classes and DKD to\nnovel classes, challenging the conventional integration of novel classes with\nbase classes. Additionally, we implement an instance-aware sample selector\nduring inference to dynamically adjust dual branch weights, thereby leveraging\nthe complementary strengths of each approach. Extensive testing on three\nbenchmarks demonstrates that DDNet achieves state-of-the-art results. Moreover,\nthrough rigorous experimentation and comparison, we establish the robustness\nand general applicability of our proposed DKD method.\n","authors":["Pengfei Fang","Yongchun Qin","Hui Xue"],"pdf_url":"https://arxiv.org/pdf/2412.11017v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13061v1","updated":"2024-12-17T16:27:11Z","published":"2024-12-17T16:27:11Z","title":"VidTok: A Versatile and Open-Source Video Tokenizer","summary":"  Encoding video content into compact latent tokens has become a fundamental\nstep in video generation and understanding, driven by the need to address the\ninherent redundancy in pixel-level representations. Consequently, there is a\ngrowing demand for high-performance, open-source video tokenizers as\nvideo-centric research gains prominence. We introduce VidTok, a versatile video\ntokenizer that delivers state-of-the-art performance in both continuous and\ndiscrete tokenizations. VidTok incorporates several key advancements over\nexisting approaches: 1) model architecture such as convolutional layers and\nup/downsampling modules; 2) to address the training instability and codebook\ncollapse commonly associated with conventional Vector Quantization (VQ), we\nintegrate Finite Scalar Quantization (FSQ) into discrete video tokenization; 3)\nimproved training strategies, including a two-stage training process and the\nuse of reduced frame rates. By integrating these advancements, VidTok achieves\nsubstantial improvements over existing methods, demonstrating superior\nperformance across multiple metrics, including PSNR, SSIM, LPIPS, and FVD,\nunder standardized evaluation settings.\n","authors":["Anni Tang","Tianyu He","Junliang Guo","Xinle Cheng","Li Song","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2412.13061v1.pdf","comment":"Code & Models: https://github.com/microsoft/VidTok"},{"id":"http://arxiv.org/abs/2412.13059v1","updated":"2024-12-17T16:25:40Z","published":"2024-12-17T16:25:40Z","title":"3D MedDiffusion: A 3D Medical Diffusion Model for Controllable and\n  High-quality Medical Image Generation","summary":"  The generation of medical images presents significant challenges due to their\nhigh-resolution and three-dimensional nature. Existing methods often yield\nsuboptimal performance in generating high-quality 3D medical images, and there\nis currently no universal generative framework for medical imaging. In this\npaper, we introduce the 3D Medical Diffusion (3D MedDiffusion) model for\ncontrollable, high-quality 3D medical image generation. 3D MedDiffusion\nincorporates a novel, highly efficient Patch-Volume Autoencoder that compresses\nmedical images into latent space through patch-wise encoding and recovers back\ninto image space through volume-wise decoding. Additionally, we design a new\nnoise estimator to capture both local details and global structure information\nduring diffusion denoising process. 3D MedDiffusion can generate fine-detailed,\nhigh-resolution images (up to 512x512x512) and effectively adapt to various\ndownstream tasks as it is trained on large-scale datasets covering CT and MRI\nmodalities and different anatomical regions (from head to leg). Experimental\nresults demonstrate that 3D MedDiffusion surpasses state-of-the-art methods in\ngenerative quality and exhibits strong generalizability across tasks such as\nsparse-view CT reconstruction, fast MRI reconstruction, and data augmentation.\n","authors":["Haoshen Wang","Zhentao Liu","Kaicong Sun","Xiaodong Wang","Dinggang Shen","Zhiming Cui"],"pdf_url":"https://arxiv.org/pdf/2412.13059v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13058v1","updated":"2024-12-17T16:22:56Z","published":"2024-12-17T16:22:56Z","title":"CondiMen: Conditional Multi-Person Mesh Recovery","summary":"  Multi-person human mesh recovery (HMR) consists in detecting all individuals\nin a given input image, and predicting the body shape, pose, and 3D location\nfor each detected person. The dominant approaches to this task rely on neural\nnetworks trained to output a single prediction for each detected individual. In\ncontrast, we propose CondiMen, a method that outputs a joint parametric\ndistribution over likely poses, body shapes, intrinsics and distances to the\ncamera, using a Bayesian network. This approach offers several advantages.\nFirst, a probability distribution can handle some inherent ambiguities of this\ntask -- such as the uncertainty between a person's size and their distance to\nthe camera, or simply the loss of information when projecting 3D data onto the\n2D image plane. Second, the output distribution can be combined with additional\ninformation to produce better predictions, by using e.g. known camera or body\nshape parameters, or by exploiting multi-view observations. Third, one can\nefficiently extract the most likely predictions from the output distribution,\nmaking our proposed approach suitable for real-time applications. Empirically\nwe find that our model i) achieves performance on par with or better than the\nstate-of-the-art, ii) captures uncertainties and correlations inherent in pose\nestimation and iii) can exploit additional information at test time, such as\nmulti-view consistency or body shape priors. CondiMen spices up the modeling of\nambiguity, using just the right ingredients on hand.\n","authors":["Brégier Romain","Baradel Fabien","Lucas Thomas","Galaaoui Salma","Armando Matthieu","Weinzaepfel Philippe","Rogez Grégory"],"pdf_url":"https://arxiv.org/pdf/2412.13058v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16268v2","updated":"2024-12-17T16:22:55Z","published":"2024-10-21T17:59:19Z","title":"SAM2Long: Enhancing SAM 2 for Long Video Segmentation with a\n  Training-Free Memory Tree","summary":"  The Segment Anything Model 2 (SAM 2) has emerged as a powerful foundation\nmodel for object segmentation in both images and videos, paving the way for\nvarious downstream video applications. The crucial design of SAM 2 for video\nsegmentation is its memory module, which prompts object-aware memories from\nprevious frames for current frame prediction. However, its greedy-selection\nmemory design suffers from the \"error accumulation\" problem, where an errored\nor missed mask will cascade and influence the segmentation of the subsequent\nframes, which limits the performance of SAM 2 toward complex long-term videos.\nTo this end, we introduce SAM2Long, an improved training-free video object\nsegmentation strategy, which considers the segmentation uncertainty within each\nframe and chooses the video-level optimal results from multiple segmentation\npathways in a constrained tree search manner. In practice, we maintain a fixed\nnumber of segmentation pathways throughout the video. For each frame, multiple\nmasks are proposed based on the existing pathways, creating various candidate\nbranches. We then select the same fixed number of branches with higher\ncumulative scores as the new pathways for the next frame. After processing the\nfinal frame, the pathway with the highest cumulative score is chosen as the\nfinal segmentation result. Benefiting from its heuristic search design,\nSAM2Long is robust toward occlusions and object reappearances, and can\neffectively segment and track objects for complex long-term videos. Notably,\nSAM2Long achieves an average improvement of 3.0 points across all 24\nhead-to-head comparisons, with gains of up to 5.3 points in J&F on long-term\nvideo object segmentation benchmarks such as SA-V and LVOS. The code is\nreleased at https://github.com/Mark12Ding/SAM2Long.\n","authors":["Shuangrui Ding","Rui Qian","Xiaoyi Dong","Pan Zhang","Yuhang Zang","Yuhang Cao","Yuwei Guo","Dahua Lin","Jiaqi Wang"],"pdf_url":"https://arxiv.org/pdf/2410.16268v2.pdf","comment":"update results including single VOT, Project page:\n  https://mark12ding.github.io/project/SAM2Long/"},{"id":"http://arxiv.org/abs/2408.08495v2","updated":"2024-12-17T16:21:31Z","published":"2024-08-16T02:33:55Z","title":"FunEditor: Achieving Complex Image Edits via Function Aggregation with\n  Diffusion Models","summary":"  Diffusion models have demonstrated outstanding performance in generative\ntasks, making them ideal candidates for image editing. Recent studies highlight\ntheir ability to apply desired edits effectively by following textual\ninstructions, yet with two key challenges remaining. First, these models\nstruggle to apply multiple edits simultaneously, resulting in computational\ninefficiencies due to their reliance on sequential processing. Second, relying\non textual prompts to determine the editing region can lead to unintended\nalterations to the image. We introduce FunEditor, an efficient diffusion model\ndesigned to learn atomic editing functions and perform complex edits by\naggregating simpler functions. This approach enables complex editing tasks,\nsuch as object movement, by aggregating multiple functions and applying them\nsimultaneously to specific areas. Our experiments demonstrate that FunEditor\nsignificantly outperforms recent inference-time optimization methods and\nfine-tuned models, either quantitatively across various metrics or through\nvisual comparisons or both, on complex tasks like object movement and object\npasting. In the meantime, with only 4 steps of inference, FunEditor achieves\n5-24x inference speedups over existing popular methods. The code is available\nat: mhmdsmdi.github.io/funeditor/.\n","authors":["Mohammadreza Samadi","Fred X. Han","Mohammad Salameh","Hao Wu","Fengyu Sun","Chunhua Zhou","Di Niu"],"pdf_url":"https://arxiv.org/pdf/2408.08495v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13050v1","updated":"2024-12-17T16:13:56Z","published":"2024-12-17T16:13:56Z","title":"Modality-Inconsistent Continual Learning of Multimodal Large Language\n  Models","summary":"  In this paper, we introduce Modality-Inconsistent Continual Learning (MICL),\na new continual learning scenario for Multimodal Large Language Models (MLLMs)\nthat involves tasks with inconsistent modalities (image, audio, or video) and\nvarying task types (captioning or question-answering). Unlike existing\nvision-only or modality-incremental settings, MICL combines modality and task\ntype shifts, both of which drive catastrophic forgetting. To address these\nchallenges, we propose MoInCL, which employs a Pseudo Targets Generation Module\nto mitigate forgetting caused by task type shifts in previously seen\nmodalities. It also incorporates Instruction-based Knowledge Distillation to\npreserve the model's ability to handle previously learned modalities when new\nones are introduced. We benchmark MICL using a total of six tasks and conduct\nexperiments to validate the effectiveness of our proposed MoInCL. The\nexperimental results highlight the superiority of MoInCL, showing significant\nimprovements over representative and state-of-the-art continual learning\nbaselines.\n","authors":["Weiguo Pian","Shijian Deng","Shentong Mo","Yunhui Guo","Yapeng Tian"],"pdf_url":"https://arxiv.org/pdf/2412.13050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10488v2","updated":"2024-12-17T16:13:15Z","published":"2024-12-13T15:24:11Z","title":"SVGBuilder: Component-Based Colored SVG Generation with Text-Guided\n  Autoregressive Transformers","summary":"  Scalable Vector Graphics (SVG) are essential XML-based formats for versatile\ngraphics, offering resolution independence and scalability. Unlike raster\nimages, SVGs use geometric shapes and support interactivity, animation, and\nmanipulation via CSS and JavaScript. Current SVG generation methods face\nchallenges related to high computational costs and complexity. In contrast,\nhuman designers use component-based tools for efficient SVG creation. Inspired\nby this, SVGBuilder introduces a component-based, autoregressive model for\ngenerating high-quality colored SVGs from textual input. It significantly\nreduces computational overhead and improves efficiency compared to traditional\nmethods. Our model generates SVGs up to 604 times faster than\noptimization-based approaches. To address the limitations of existing SVG\ndatasets and support our research, we introduce ColorSVG-100K, the first\nlarge-scale dataset of colored SVGs, comprising 100,000 graphics. This dataset\nfills the gap in color information for SVG generation models and enhances\ndiversity in model training. Evaluation against state-of-the-art models\ndemonstrates SVGBuilder's superior performance in practical applications,\nhighlighting its efficiency and quality in generating complex SVG graphics.\n","authors":["Zehao Chen","Rong Pan"],"pdf_url":"https://arxiv.org/pdf/2412.10488v2.pdf","comment":"Project: https://svgbuilder.github.io"},{"id":"http://arxiv.org/abs/2412.13047v1","updated":"2024-12-17T16:11:14Z","published":"2024-12-17T16:11:14Z","title":"EOGS: Gaussian Splatting for Earth Observation","summary":"  Recently, Gaussian splatting has emerged as a strong alternative to NeRF,\ndemonstrating impressive 3D modeling capabilities while requiring only a\nfraction of the training and rendering time. In this paper, we show how the\nstandard Gaussian splatting framework can be adapted for remote sensing,\nretaining its high efficiency. This enables us to achieve state-of-the-art\nperformance in just a few minutes, compared to the day-long optimization\nrequired by the best-performing NeRF-based Earth observation methods. The\nproposed framework incorporates remote-sensing improvements from EO-NeRF, such\nas radiometric correction and shadow modeling, while introducing novel\ncomponents, including sparsity, view consistency, and opacity regularizations.\n","authors":["Luca Savant Aira","Gabriele Facciolo","Thibaud Ehret"],"pdf_url":"https://arxiv.org/pdf/2412.13047v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07841v2","updated":"2024-12-17T15:54:07Z","published":"2024-07-10T17:00:57Z","title":"Benchmarking Embedding Aggregation Methods in Computational Pathology: A\n  Clinical Data Perspective","summary":"  Recent advances in artificial intelligence (AI), in particular\nself-supervised learning of foundation models (FMs), are revolutionizing\nmedical imaging and computational pathology (CPath). A constant challenge in\nthe analysis of digital Whole Slide Images (WSIs) is the problem of aggregating\ntens of thousands of tile-level image embeddings to a slide-level\nrepresentation. Due to the prevalent use of datasets created for genomic\nresearch, such as TCGA, for method development, the performance of these\ntechniques on diagnostic slides from clinical practice has been inadequately\nexplored. This study conducts a thorough benchmarking analysis of ten\nslide-level aggregation techniques across nine clinically relevant tasks,\nincluding diagnostic assessment, biomarker classification, and outcome\nprediction. The results yield following key insights: (1) Embeddings derived\nfrom domain-specific (histological images) FMs outperform those from generic\nImageNet-based models across aggregation methods. (2) Spatial-aware aggregators\nenhance the performance significantly when using ImageNet pre-trained models\nbut not when using FMs. (3) No single model excels in all tasks and\nspatially-aware models do not show general superiority as it would be expected.\nThese findings underscore the need for more adaptable and universally\napplicable aggregation techniques, guiding future research towards tools that\nbetter meet the evolving needs of clinical-AI in pathology. The code used in\nthis work is available at\n\\url{https://github.com/fuchs-lab-public/CPath_SABenchmark}.\n","authors":["Shengjia Chen","Gabriele Campanella","Abdulkadir Elmas","Aryeh Stock","Jennifer Zeng","Alexandros D. Polydorides","Adam J. Schoenfeld","Kuan-lin Huang","Jane Houldsworth","Chad Vanderbilt","Thomas J. Fuchs"],"pdf_url":"https://arxiv.org/pdf/2407.07841v2.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2412.13026v1","updated":"2024-12-17T15:48:25Z","published":"2024-12-17T15:48:25Z","title":"NAVCON: A Cognitively Inspired and Linguistically Grounded Corpus for\n  Vision and Language Navigation","summary":"  We present NAVCON, a large-scale annotated Vision-Language Navigation (VLN)\ncorpus built on top of two popular datasets (R2R and RxR). The paper introduces\nfour core, cognitively motivated and linguistically grounded, navigation\nconcepts and an algorithm for generating large-scale silver annotations of\nnaturally occurring linguistic realizations of these concepts in navigation\ninstructions. We pair the annotated instructions with video clips of an agent\nacting on these instructions. NAVCON contains 236, 316 concept annotations for\napproximately 30, 0000 instructions and 2.7 million aligned images (from\napproximately 19, 000 instructions) showing what the agent sees when executing\nan instruction. To our knowledge, this is the first comprehensive resource of\nnavigation concepts. We evaluated the quality of the silver annotations by\nconducting human evaluation studies on NAVCON samples. As further validation of\nthe quality and usefulness of the resource, we trained a model for detecting\nnavigation concepts and their linguistic realizations in unseen instructions.\nAdditionally, we show that few-shot learning with GPT-4o performs well on this\ntask using large-scale silver annotations of NAVCON.\n","authors":["Karan Wanchoo","Xiaoye Zuo","Hannah Gonzalez","Soham Dan","Georgios Georgakis","Dan Roth","Kostas Daniilidis","Eleni Miltsakaki"],"pdf_url":"https://arxiv.org/pdf/2412.13026v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13017v1","updated":"2024-12-17T15:36:55Z","published":"2024-12-17T15:36:55Z","title":"A New Adversarial Perspective for LiDAR-based 3D Object Detection","summary":"  Autonomous vehicles (AVs) rely on LiDAR sensors for environmental perception\nand decision-making in driving scenarios. However, ensuring the safety and\nreliability of AVs in complex environments remains a pressing challenge. To\naddress this issue, we introduce a real-world dataset (ROLiD) comprising\nLiDAR-scanned point clouds of two random objects: water mist and smoke. In this\npaper, we introduce a novel adversarial perspective by proposing an attack\nframework that utilizes water mist and smoke to simulate environmental\ninterference. Specifically, we propose a point cloud sequence generation method\nusing a motion and content decomposition generative adversarial network named\nPCS-GAN to simulate the distribution of random objects. Furthermore, leveraging\nthe simulated LiDAR scanning characteristics implemented with Range Image, we\nexamine the effects of introducing random object perturbations at various\npositions on the target vehicle. Extensive experiments demonstrate that\nadversarial perturbations based on random objects effectively deceive vehicle\ndetection and reduce the recognition rate of 3D object detection models.\n","authors":["Shijun Zheng","Weiquan Liu","Yu Guo","Yu Zang","Siqi Shen","Cheng Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13017v1.pdf","comment":"11 pages, 7 figures, AAAI2025"},{"id":"http://arxiv.org/abs/2412.13010v1","updated":"2024-12-17T15:32:12Z","published":"2024-12-17T15:32:12Z","title":"Measurement of Medial Elbow Joint Space using Landmark Detection","summary":"  Ultrasound imaging of the medial elbow is crucial for the early\nidentification of Ulnar Collateral Ligament (UCL) injuries. Specifically,\nmeasuring the elbow joint space in ultrasound images is used to assess the\nvalgus instability of elbow. To automate this measurement, a precisely\nannotated dataset is necessary; however, no publicly available dataset has been\nproposed thus far. This study introduces a novel ultrasound medial elbow\ndataset for measuring joint space to diagnose Ulnar Collateral Ligament (UCL)\ninjuries. The dataset comprises 4,201 medial elbow ultrasound images from 22\nsubjects, with landmark annotations on the humerus and ulna. The annotations\nare made precisely by the authors under the supervision of three orthopedic\nsurgeons. We evaluated joint space measurement methods using our proposed\ndataset with several landmark detection approaches, including ViTPose, HRNet,\nPCT, YOLOv8, and U-Net. In addition, we propose using Shape Subspace (SS) for\nlandmark refinement in heatmap-based landmark detection. The results show that\nthe mean Euclidean distance error of joint space is 0.116 mm when using HRNet.\nFurthermore, the SS landmark refinement improves the mean absolute error of\nlandmark positions by 0.010 mm with HRNet and by 0.103 mm with ViTPose on\naverage. These highlight the potential for high-precision, real-time diagnosis\nof UCL injuries and associated risks, which could be leveraged in large-scale\nscreening. Lastly, we demonstrate point-based segmentation of the humerus and\nulna using the detected landmarks as input. The dataset will be made publicly\navailable upon acceptance of this paper at:\nhttps://github.com/Akahori000/Ultrasound-Medial-Elbow-Dataset.\n","authors":["Shizuka Akahori","Shotaro Teruya","Pragyan Shrestha","Yuichi Yoshii","Ryuhei Michinobu","Satoshi Iizuka","Itaru Kitahara"],"pdf_url":"https://arxiv.org/pdf/2412.13010v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05966v3","updated":"2024-12-17T15:31:49Z","published":"2024-03-09T17:17:07Z","title":"Can Generative Models Improve Self-Supervised Representation Learning?","summary":"  The rapid advancement in self-supervised representation learning has\nhighlighted its potential to leverage unlabeled data for learning rich visual\nrepresentations. However, the existing techniques, particularly those employing\ndifferent augmentations of the same image, often rely on a limited set of\nsimple transformations that cannot fully capture variations in the real world.\nThis constrains the diversity and quality of samples, which leads to\nsub-optimal representations. In this paper, we introduce a framework that\nenriches the self-supervised learning (SSL) paradigm by utilizing generative\nmodels to produce semantically consistent image augmentations. By directly\nconditioning generative models on a source image, our method enables the\ngeneration of diverse augmentations while maintaining the semantics of the\nsource image, thus offering a richer set of data for SSL. Our extensive\nexperimental results on various joint-embedding SSL techniques demonstrate that\nour framework significantly enhances the quality of learned visual\nrepresentations by up to 10\\% Top-1 accuracy in downstream tasks. This research\ndemonstrates that incorporating generative models into the joint-embedding SSL\nworkflow opens new avenues for exploring the potential of synthetic data. This\ndevelopment paves the way for more robust and versatile representation learning\ntechniques.\n","authors":["Sana Ayromlou","Vahid Reza Khazaie","Fereshteh Forghani","Arash Afkanpour"],"pdf_url":"https://arxiv.org/pdf/2403.05966v3.pdf","comment":"To be published in AAAI 2025"},{"id":"http://arxiv.org/abs/2412.13006v1","updated":"2024-12-17T15:26:15Z","published":"2024-12-17T15:26:15Z","title":"What is YOLOv6? A Deep Insight into the Object Detection Model","summary":"  This work explores the YOLOv6 object detection model in depth, concentrating\non its design framework, optimization techniques, and detection capabilities.\nYOLOv6's core elements consist of the EfficientRep Backbone for robust feature\nextraction and the Rep-PAN Neck for seamless feature aggregation, ensuring\nhigh-performance object detection. Evaluated on the COCO dataset, YOLOv6-N\nachieves 37.5\\% AP at 1187 FPS on an NVIDIA Tesla T4 GPU. YOLOv6-S reaches\n45.0\\% AP at 484 FPS, outperforming models like PPYOLOE-S, YOLOv5-S, YOLOX-S,\nand YOLOv8-S in the same class. Moreover, YOLOv6-M and YOLOv6-L also show\nbetter accuracy (50.0\\% and 52.8\\%) while maintaining comparable inference\nspeeds to other detectors. With an upgraded backbone and neck structure,\nYOLOv6-L6 delivers cutting-edge accuracy in real-time.\n","authors":["Athulya Sundaresan Geetha"],"pdf_url":"https://arxiv.org/pdf/2412.13006v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12990v1","updated":"2024-12-17T15:07:50Z","published":"2024-12-17T15:07:50Z","title":"Future Aspects in Human Action Recognition: Exploring Emerging\n  Techniques and Ethical Influences","summary":"  Visual-based human action recognition can be found in various application\nfields, e.g., surveillance systems, sports analytics, medical assistive\ntechnologies, or human-robot interaction frameworks, and it concerns the\nidentification and classification of individuals' activities within a video.\nSince actions typically occur over a sequence of consecutive images, it is\nparticularly challenging due to the inclusion of temporal analysis, which\nintroduces an extra layer of complexity. However, although multiple approaches\ntry to handle temporal analysis, there are still difficulties because of their\ncomputational cost and lack of adaptability. Therefore, different types of\nvision data, containing transition information between consecutive images,\nprovided by next-generation hardware sensors will guide the robotics community\nin tackling the problem of human action recognition. On the other hand, while\nthere is a plethora of still-image datasets, that researchers can adopt to\ntrain new artificial intelligence models, videos representing human activities\nare of limited capabilities, e.g., small and unbalanced datasets or selected\nwithout control from multiple sources. To this end, generating new and\nrealistic synthetic videos is possible since labeling is performed throughout\nthe data creation process, while reinforcement learning techniques can permit\nthe avoidance of considerable dataset dependence. At the same time, human\nfactors' involvement raises ethical issues for the research community, as\ndoubts and concerns about new technologies already exist.\n","authors":["Antonios Gasteratos","Stavros N. Moutsis","Konstantinos A. Tsintotas","Yiannis Aloimonos"],"pdf_url":"https://arxiv.org/pdf/2412.12990v1.pdf","comment":"2 pages, 1 figure, 40th Anniversary of the IEEE Conference on\n  Robotics and Automation (ICRA@40), Rotterdam, Netherlands | September 23-26,\n  2024"},{"id":"http://arxiv.org/abs/2205.10691v2","updated":"2024-12-17T15:04:46Z","published":"2022-05-21T23:04:20Z","title":"Producing Histopathology Phantom Images using Generative Adversarial\n  Networks to improve Tumor Detection","summary":"  Advance in medical imaging is an important part in deep learning research.\nOne of the goals of computer vision is development of a holistic, comprehensive\nmodel which can identify tumors from histology slides obtained via biopsies. A\nmajor problem that stands in the way is lack of data for a few cancer-types. In\nthis paper, we ascertain that data augmentation using GANs can be a viable\nsolution to reduce the unevenness in the distribution of different cancer types\nin our dataset. Our demonstration showed that a dataset augmented to a 50%\nincrease causes an increase in tumor detection from 80% to 87.5%\n","authors":["Vidit Gautam"],"pdf_url":"https://arxiv.org/pdf/2205.10691v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12982v1","updated":"2024-12-17T15:01:35Z","published":"2024-12-17T15:01:35Z","title":"Stable Diffusion is a Natural Cross-Modal Decoder for Layered\n  AI-generated Image Compression","summary":"  Recent advances in Artificial Intelligence Generated Content (AIGC) have\ngarnered significant interest, accompanied by an increasing need to transmit\nand compress the vast number of AI-generated images (AIGIs). However, there is\na noticeable deficiency in research focused on compression methods for AIGIs.\nTo address this critical gap, we introduce a scalable cross-modal compression\nframework that incorporates multiple human-comprehensible modalities, designed\nto efficiently capture and relay essential visual information for AIGIs. In\nparticular, our framework encodes images into a layered bitstream consisting of\na semantic layer that delivers high-level semantic information through text\nprompts; a structural layer that captures spatial details using edge or\nskeleton maps; and a texture layer that preserves local textures via a\ncolormap. Utilizing Stable Diffusion as the backend, the framework effectively\nleverages these multimodal priors for image generation, effectively functioning\nas a decoder when these priors are encoded. Qualitative and quantitative\nresults show that our method proficiently restores both semantic and visual\ndetails, competing against baseline approaches at extremely low bitrates (\n<0.02 bpp). Additionally, our framework facilitates downstream editing\napplications without requiring full decoding, thereby paving a new direction\nfor future research in AIGI compression.\n","authors":["Ruijie Chen","Qi Mao","Zhengxue Cheng"],"pdf_url":"https://arxiv.org/pdf/2412.12982v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12974v1","updated":"2024-12-17T14:56:59Z","published":"2024-12-17T14:56:59Z","title":"Attentive Eraser: Unleashing Diffusion Model's Object Removal Potential\n  via Self-Attention Redirection Guidance","summary":"  Recently, diffusion models have emerged as promising newcomers in the field\nof generative models, shining brightly in image generation. However, when\nemployed for object removal tasks, they still encounter issues such as\ngenerating random artifacts and the incapacity to repaint foreground object\nareas with appropriate content after removal. To tackle these problems, we\npropose Attentive Eraser, a tuning-free method to empower pre-trained diffusion\nmodels for stable and effective object removal. Firstly, in light of the\nobservation that the self-attention maps influence the structure and shape\ndetails of the generated images, we propose Attention Activation and\nSuppression (ASS), which re-engineers the self-attention mechanism within the\npre-trained diffusion models based on the given mask, thereby prioritizing the\nbackground over the foreground object during the reverse generation process.\nMoreover, we introduce Self-Attention Redirection Guidance (SARG), which\nutilizes the self-attention redirected by ASS to guide the generation process,\neffectively removing foreground objects within the mask while simultaneously\ngenerating content that is both plausible and coherent. Experiments demonstrate\nthe stability and effectiveness of Attentive Eraser in object removal across a\nvariety of pre-trained diffusion models, outperforming even training-based\nmethods. Furthermore, Attentive Eraser can be implemented in various diffusion\nmodel architectures and checkpoints, enabling excellent scalability. Code is\navailable at https://github.com/Anonym0u3/AttentiveEraser.\n","authors":["Wenhao Sun","Benlei Cui","Jingqun Tang","Xue-Mei Dong"],"pdf_url":"https://arxiv.org/pdf/2412.12974v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.12966v1","updated":"2024-12-17T14:51:13Z","published":"2024-12-17T14:51:13Z","title":"Fruit Deformity Classification through Single-Input and Multi-Input\n  Architectures based on CNN Models using Real and Synthetic Images","summary":"  The present study focuses on detecting the degree of deformity in fruits such\nas apples, mangoes, and strawberries during the process of inspecting their\nexternal quality, employing Single-Input and Multi-Input architectures based on\nconvolutional neural network (CNN) models using sets of real and synthetic\nimages. The datasets are segmented using the Segment Anything Model (SAM),\nwhich provides the silhouette of the fruits. Regarding the single-input\narchitecture, the evaluation of the CNN models is performed only with real\nimages, but a methodology is proposed to improve these results using a\npre-trained model with synthetic images. In the Multi-Input architecture,\nbranches with RGB images and fruit silhouettes are implemented as inputs for\nevaluating CNN models such as VGG16, MobileNetV2, and CIDIS. However, the\nresults revealed that the Multi-Input architecture with the MobileNetV2 model\nwas the most effective in identifying deformities in the fruits, achieving\naccuracies of 90\\%, 94\\%, and 92\\% for apples, mangoes, and strawberries,\nrespectively. In conclusion, the Multi-Input architecture with the MobileNetV2\nmodel is the most accurate for classifying levels of deformity in fruits.\n","authors":["Tommy D. Beltran","Raul J. Villao","Luis E. Chuquimarca","Boris X. Vintimilla","Sergio A. Velastin"],"pdf_url":"https://arxiv.org/pdf/2412.12966v1.pdf","comment":"15 pages, 9 figures, CIARP 2024"},{"id":"http://arxiv.org/abs/2412.00876v3","updated":"2024-12-17T14:45:12Z","published":"2024-12-01T16:32:31Z","title":"Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification","summary":"  Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .\n","authors":["Wenxuan Huang","Zijie Zhai","Yunhang Shen","Shaosheng Cao","Fei Zhao","Xiangfeng Xu","Zheyu Ye","Shaohui Lin"],"pdf_url":"https://arxiv.org/pdf/2412.00876v3.pdf","comment":"Code is available at https://github.com/Osilly/dynamic_llava"},{"id":"http://arxiv.org/abs/2403.15698v3","updated":"2024-12-17T14:39:07Z","published":"2024-03-23T03:23:29Z","title":"SceneX: Procedural Controllable Large-scale Scene Generation","summary":"  Developing comprehensive explicit world models is crucial for understanding\nand simulating real-world scenarios. Recently, Procedural Controllable\nGeneration (PCG) has gained significant attention in large-scale scene\ngeneration by enabling the creation of scalable, high-quality assets. However,\nPCG faces challenges such as limited modular diversity, high expertise\nrequirements, and challenges in managing the diverse elements and structures in\ncomplex scenes. In this paper, we introduce a large-scale scene generation\nframework, SceneX, which can automatically produce high-quality procedural\nmodels according to designers' textual descriptions. Specifically, the proposed\nmethod comprises two components, PCGHub and PCGPlanner. The former encompasses\nan extensive collection of accessible procedural assets and thousands of\nhand-craft API documents to perform as a standard protocol for PCG controller.\nThe latter aims to generate executable actions for Blender to produce\ncontrollable and precise 3D assets guided by the user's instructions. Extensive\nexperiments demonstrated the capability of our method in controllable\nlarge-scale scene generation, including nature scenes and unbounded cities, as\nwell as scene editing such as asset placement and season translation.\n","authors":["Mengqi Zhou","Yuxi Wang","Jun Hou","Shougao Zhang","Yiwei Li","Chuanchen Luo","Junran Peng","Zhaoxiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.15698v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10702v2","updated":"2024-12-17T14:37:34Z","published":"2024-12-14T06:21:24Z","title":"Memory Efficient Matting with Adaptive Token Routing","summary":"  Transformer-based models have recently achieved outstanding performance in\nimage matting. However, their application to high-resolution images remains\nchallenging due to the quadratic complexity of global self-attention. To\naddress this issue, we propose MEMatte, a \\textbf{m}emory-\\textbf{e}fficient\n\\textbf{m}atting framework for processing high-resolution images. MEMatte\nincorporates a router before each global attention block, directing informative\ntokens to the global attention while routing other tokens to a Lightweight\nToken Refinement Module (LTRM). Specifically, the router employs a local-global\nstrategy to predict the routing probability of each token, and the LTRM\nutilizes efficient modules to simulate global attention. Additionally, we\nintroduce a Batch-constrained Adaptive Token Routing (BATR) mechanism, which\nallows each router to dynamically route tokens based on image content and the\nstages of attention block in the network. Furthermore, we construct an ultra\nhigh-resolution image matting dataset, UHR-395, comprising 35,500 training\nimages and 1,000 test images, with an average resolution of $4872\\times6017$.\nThis dataset is created by compositing 395 different alpha mattes across 11\ncategories onto various backgrounds, all with high-quality manual annotation.\nExtensive experiments demonstrate that MEMatte outperforms existing methods on\nboth high-resolution and real-world datasets, significantly reducing memory\nusage by approximately 88% and latency by 50% on the Composition-1K benchmark.\nOur code is available at https://github.com/linyiheng123/MEMatte.\n","authors":["Yiheng Lin","Yihan Hu","Chenyi Zhang","Ting Liu","Xiaochao Qu","Luoqi Liu","Yao Zhao","Yunchao Wei"],"pdf_url":"https://arxiv.org/pdf/2412.10702v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12949v1","updated":"2024-12-17T14:29:12Z","published":"2024-12-17T14:29:12Z","title":"Synthetic Data Generation for Anomaly Detection on Table Grapes","summary":"  Early detection of illnesses and pest infestations in fruit cultivation is\ncritical for maintaining yield quality and plant health. Computer vision and\nrobotics are increasingly employed for the automatic detection of such issues,\nparticularly using data-driven solutions. However, the rarity of these problems\nmakes acquiring and processing the necessary data to train such algorithms a\nsignificant obstacle. One solution to this scarcity is the generation of\nsynthetic high-quality anomalous samples. While numerous methods exist for this\ntask, most require highly trained individuals for setup.\n  This work addresses the challenge of generating synthetic anomalies in an\nautomatic fashion that requires only an initial collection of normal and\nanomalous samples from the user - a task that is straightforward for farmers.\nWe demonstrate the approach in the context of table grape cultivation.\nSpecifically, based on the observation that normal berries present relatively\nsmooth surfaces, while defects result in more complex textures, we introduce a\nDual-Canny Edge Detection (DCED) filter. This filter emphasizes the additional\ntexture indicative of diseases, pest infestations, or other defects. Using\nsegmentation masks provided by the Segment Anything Model, we then select and\nseamlessly blend anomalous berries onto normal ones. We show that the proposed\ndataset augmentation technique improves the accuracy of an anomaly classifier\nfor table grapes and that the approach can be generalized to other fruit types.\n","authors":["Ionut Marian Motoi","Valerio Belli","Alberto Carpineto","Daniele Nardi","Thomas Alessandro Ciarfuglia"],"pdf_url":"https://arxiv.org/pdf/2412.12949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12944v1","updated":"2024-12-17T14:22:44Z","published":"2024-12-17T14:22:44Z","title":"Online optimisation for dynamic electrical impedance tomography","summary":"  Online optimisation studies the convergence of optimisation methods as the\ndata embedded in the problem changes. Based on this idea, we propose a primal\ndual online method for nonlinear time-discrete inverse problems. We analyse the\nmethod through regret theory and demonstrate its performance in real-time\nmonitoring of moving bodies in a fluid with Electrical Impedance Tomography\n(EIT). To do so, we also prove the second-order differentiability of the\nComplete Electrode Model (CEM) solution operator on $L^\\infty$.\n","authors":["Neil Dizon","Jyrki Jauhiainen","Tuomo Valkonen"],"pdf_url":"https://arxiv.org/pdf/2412.12944v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16424v2","updated":"2024-12-17T14:17:51Z","published":"2024-07-23T12:21:23Z","title":"ESOD: Efficient Small Object Detection on High-Resolution Images","summary":"  Enlarging input images is a straightforward and effective approach to promote\nsmall object detection. However, simple image enlargement is significantly\nexpensive on both computations and GPU memory. In fact, small objects are\nusually sparsely distributed and locally clustered. Therefore, massive feature\nextraction computations are wasted on the non-target background area of images.\nRecent works have tried to pick out target-containing regions using an extra\nnetwork and perform conventional object detection, but the newly introduced\ncomputation limits their final performance. In this paper, we propose to reuse\nthe detector's backbone to conduct feature-level object-seeking and\npatch-slicing, which can avoid redundant feature extraction and reduce the\ncomputation cost. Incorporating a sparse detection head, we are able to detect\nsmall objects on high-resolution inputs (e.g., 1080P or larger) for superior\nperformance. The resulting Efficient Small Object Detection (ESOD) approach is\na generic framework, which can be applied to both CNN- and ViT-based detectors\nto save the computation and GPU memory costs. Extensive experiments demonstrate\nthe efficacy and efficiency of our method. In particular, our method\nconsistently surpasses the SOTA detectors by a large margin (e.g., 8% gains on\nAP) on the representative VisDrone, UAVDT, and TinyPerson datasets. Code is\navailable at https://github.com/alibaba/esod.\n","authors":["Kai Liu","Zhihang Fu","Sheng Jin","Ze Chen","Fan Zhou","Rongxin Jiang","Yaowu Chen","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2407.16424v2.pdf","comment":"This paper has been recerived by IEEE TIP 2024. Code is available at\n  https://github.com/alibaba/esod"},{"id":"http://arxiv.org/abs/2412.11974v2","updated":"2024-12-17T14:12:56Z","published":"2024-12-16T16:58:28Z","title":"Emma-X: An Embodied Multimodal Action Model with Grounded Chain of\n  Thought and Look-ahead Spatial Reasoning","summary":"  Traditional reinforcement learning-based robotic control methods are often\ntask-specific and fail to generalize across diverse environments or unseen\nobjects and instructions. Visual Language Models (VLMs) demonstrate strong\nscene understanding and planning capabilities but lack the ability to generate\nactionable policies tailored to specific robotic embodiments. To address this,\nVisual-Language-Action (VLA) models have emerged, yet they face challenges in\nlong-horizon spatial reasoning and grounded task planning. In this work, we\npropose the Embodied Multimodal Action Model with Grounded Chain of Thought and\nLook-ahead Spatial Reasoning, Emma-X. Emma-X leverages our constructed\nhierarchical embodiment dataset based on BridgeV2, containing 60,000 robot\nmanipulation trajectories auto-annotated with grounded task reasoning and\nspatial guidance. Additionally, we introduce a trajectory segmentation strategy\nbased on gripper states and motion trajectories, which can help mitigate\nhallucination in grounding subtask reasoning generation. Experimental results\ndemonstrate that Emma-X achieves superior performance over competitive\nbaselines, particularly in real-world robotic tasks requiring spatial\nreasoning.\n","authors":["Qi Sun","Pengfei Hong","Tej Deep Pala","Vernon Toh","U-Xuan Tan","Deepanway Ghosal","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2412.11974v2.pdf","comment":"https://github.com/declare-lab/Emma-X,\n  https://huggingface.co/declare-lab/Emma-X"},{"id":"http://arxiv.org/abs/2412.12932v1","updated":"2024-12-17T14:10:16Z","published":"2024-12-17T14:10:16Z","title":"CoMT: A Novel Benchmark for Chain of Multi-modal Thought on Large\n  Vision-Language Models","summary":"  Large Vision-Language Models (LVLMs) have recently demonstrated amazing\nsuccess in multi-modal tasks, including advancements in Multi-modal\nChain-of-Thought (MCoT) reasoning. Despite these successes, current benchmarks\nstill follow a traditional paradigm with multi-modal input and text-modal\noutput, which leads to significant drawbacks such as missing visual operations\nand vague expressions. Motivated by this, we introduce a novel Chain of\nMulti-modal Thought (CoMT) benchmark to address these limitations. Different\nfrom the traditional MCoT benchmark, CoMT requires both multi-modal input and\nmulti-modal reasoning output, aiming to mimic human-like reasoning that\ninherently integrates visual operation. Specifically, CoMT consists of four\ncategories: (1) Visual Creation, (2) Visual Deletion, (3) Visual Update, and\n(4) Visual Selection to comprehensively explore complex visual operations and\nconcise expression in real scenarios. We evaluate various LVLMs and strategies\non CoMT, revealing some key insights into the capabilities and limitations of\nthe current approaches. We hope that CoMT can inspire more research on\nintroducing multi-modal generation into the reasoning process.\n","authors":["Zihui Cheng","Qiguang Chen","Jin Zhang","Hao Fei","Xiaocheng Feng","Wanxiang Che","Min Li","Libo Qin"],"pdf_url":"https://arxiv.org/pdf/2412.12932v1.pdf","comment":"Accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2412.10908v2","updated":"2024-12-17T14:06:48Z","published":"2024-12-14T17:35:27Z","title":"Do large language vision models understand 3D shapes?","summary":"  Large vision language models (LVLM) are the leading A.I approach for\nachieving a general visual understanding of the world. Models such as GPT,\nClaude, Gemini, and LLama can use images to understand and analyze complex\nvisual scenes. 3D objects and shapes are the basic building blocks of the\nworld, recognizing them is a fundamental part of human perception. The goal of\nthis work is to test whether LVLMs truly understand 3D shapes by testing the\nmodels ability to identify and match objects of the exact same 3D shapes but\nwith different orientations and materials/textures. Test images were created\nusing CGI with a huge number of highly diverse objects, materials, and scenes.\nThe results of this test show that the ability of such models to match 3D\nshapes is significantly below humans but much higher than random guesses.\nSuggesting that the models have gained some abstract understanding of 3D shapes\nbut still trail far beyond humans in this task. Mainly it seems that the models\ncan easily identify the same object with a different orientation as well as\nmatching identical 3D shapes of the same orientation but with different\nmaterial textures. However, when both the object material and orientation are\nchanged, all models perform poorly relative to humans.\n","authors":["Sagi Eppel"],"pdf_url":"https://arxiv.org/pdf/2412.10908v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12919v1","updated":"2024-12-17T13:51:56Z","published":"2024-12-17T13:51:56Z","title":"4DRGS: 4D Radiative Gaussian Splatting for Efficient 3D Vessel\n  Reconstruction from Sparse-View Dynamic DSA Images","summary":"  Reconstructing 3D vessel structures from sparse-view dynamic digital\nsubtraction angiography (DSA) images enables accurate medical assessment while\nreducing radiation exposure. Existing methods often produce suboptimal results\nor require excessive computation time. In this work, we propose 4D radiative\nGaussian splatting (4DRGS) to achieve high-quality reconstruction efficiently.\nIn detail, we represent the vessels with 4D radiative Gaussian kernels. Each\nkernel has time-invariant geometry parameters, including position, rotation,\nand scale, to model static vessel structures. The time-dependent central\nattenuation of each kernel is predicted from a compact neural network to\ncapture the temporal varying response of contrast agent flow. We splat these\nGaussian kernels to synthesize DSA images via X-ray rasterization and optimize\nthe model with real captured ones. The final 3D vessel volume is voxelized from\nthe well-trained kernels. Moreover, we introduce accumulated attenuation\npruning and bounded scaling activation to improve reconstruction quality.\nExtensive experiments on real-world patient data demonstrate that 4DRGS\nachieves impressive results in 5 minutes training, which is 32x faster than the\nstate-of-the-art method. This underscores the potential of 4DRGS for real-world\nclinics.\n","authors":["Zhentao Liu","Ruyi Zha","Huangxuan Zhao","Hongdong Li","Zhiming Cui"],"pdf_url":"https://arxiv.org/pdf/2412.12919v1.pdf","comment":"Zhentao Liu and Ruyi Zha made equal contributions"},{"id":"http://arxiv.org/abs/2412.06418v2","updated":"2024-12-17T13:49:59Z","published":"2024-12-09T11:51:28Z","title":"Continual Learning for Segment Anything Model Adaptation","summary":"  Although the current different types of SAM adaptation methods have achieved\npromising performance for various downstream tasks, such as prompt-based ones\nand adapter-based ones, most of them belong to the one-step adaptation\nparadigm. In real-world scenarios, we are generally confronted with the dynamic\nscenario where the data comes in a streaming manner. Driven by the practical\nneed, in this paper, we first propose a novel Continual SAM adaptation (CoSAM)\nbenchmark with 8 different task domains and carefully analyze the limitations\nof the existing SAM one-step adaptation methods in the continual segmentation\nscenario. Then we propose a novel simple-yet-effective Mixture of Domain\nAdapters (MoDA) algorithm which utilizes the Global Feature Tokens (GFT) and\nGlobal Assistant Tokens (GAT) modules to help the SAM encoder extract\nwell-separated features for different task domains, and then provide the\naccurate task-specific information for continual learning. Extensive\nexperiments demonstrate that our proposed MoDA obviously surpasses the existing\nclassic continual learning methods, as well as prompt-based and adapter-based\napproaches for continual segmentation. Moreover, after sequential learning on\nthe CoSAM benchmark with diverse data distributions, our MoDA maintains highly\ncompetitive results in the natural image domain, approaching the zero-shot\nperformance of the original SAM, demonstrating its superior capability in\nknowledge preservation. Notably, the proposed MoDA can be seamlessly integrated\ninto various one-step adaptation methods of SAM, which can consistently bring\nobvious performance gains. Code is available at\n\\url{https://github.com/yangjl1215/CoSAM}\n","authors":["Jinglong Yang","Yichen Wu","Jun Cen","Wenjian Huang","Hong Wang","Jianguo Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.06418v2.pdf","comment":"Code is available at \\url{https://github.com/yangjl1215/CoSAM}"},{"id":"http://arxiv.org/abs/2412.12912v1","updated":"2024-12-17T13:46:12Z","published":"2024-12-17T13:46:12Z","title":"Unsupervised Region-Based Image Editing of Denoising Diffusion Models","summary":"  Although diffusion models have achieved remarkable success in the field of\nimage generation, their latent space remains under-explored. Current methods\nfor identifying semantics within latent space often rely on external\nsupervision, such as textual information and segmentation masks. In this paper,\nwe propose a method to identify semantic attributes in the latent space of\npre-trained diffusion models without any further training. By projecting the\nJacobian of the targeted semantic region into a low-dimensional subspace which\nis orthogonal to the non-masked regions, our approach facilitates precise\nsemantic discovery and control over local masked areas, eliminating the need\nfor annotations. We conducted extensive experiments across multiple datasets\nand various architectures of diffusion models, achieving state-of-the-art\nperformance. In particular, for some specific face attributes, the performance\nof our proposed method even surpasses that of supervised approaches,\ndemonstrating its superior ability in editing local image properties.\n","authors":["Zixiang Li","Yue Song","Renshuai Tao","Xiaohong Jia","Yao Zhao","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2412.12912v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10710v2","updated":"2024-12-17T13:41:32Z","published":"2024-12-14T06:50:10Z","title":"Virtual Trial Room with Computer Vision and Machine Learning","summary":"  Online shopping has revolutionized the retail industry, providing customers\nwith convenience and accessibility. However, customers often hesitate to\npurchase wearable products such as watches, jewelry, glasses, shoes, and\nclothes due to the lack of certainty regarding fit and suitability. This leads\nto significant return rates, causing problems for both customers and vendors.\nTo address this issue, a platform called the Virtual Trial Room with Computer\nVision and Machine Learning is designed which enables customers to easily check\nwhether a product will fit and suit them or not. To achieve this, an\nAI-generated 3D model of the human head was created from a single 2D image\nusing the DECA model. This 3D model was then superimposed with a custom-made 3D\nmodel of glass which is based on real-world measurements and fitted over the\nhuman head. To replicate the real-world look and feel, the model was retouched\nwith textures, lightness, and smoothness. Furthermore, a full-stack application\nwas developed utilizing various fornt-end and back-end technologies. This\napplication enables users to view 3D-generated results on the website,\nproviding an immersive and interactive experience.\n","authors":["Tulashi Prasad Joshi","Amrendra Kumar Yadav","Arjun Chhetri","Suraj Agrahari","Umesh Kanta Ghimire"],"pdf_url":"https://arxiv.org/pdf/2412.10710v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12906v1","updated":"2024-12-17T13:32:04Z","published":"2024-12-17T13:32:04Z","title":"CATSplat: Context-Aware Transformer with Spatial Guidance for\n  Generalizable 3D Gaussian Splatting from A Single-View Image","summary":"  Recently, generalizable feed-forward methods based on 3D Gaussian Splatting\nhave gained significant attention for their potential to reconstruct 3D scenes\nusing finite resources. These approaches create a 3D radiance field,\nparameterized by per-pixel 3D Gaussian primitives, from just a few images in a\nsingle forward pass. However, unlike multi-view methods that benefit from\ncross-view correspondences, 3D scene reconstruction with a single-view image\nremains an underexplored area. In this work, we introduce CATSplat, a novel\ngeneralizable transformer-based framework designed to break through the\ninherent constraints in monocular settings. First, we propose leveraging\ntextual guidance from a visual-language model to complement insufficient\ninformation from a single image. By incorporating scene-specific contextual\ndetails from text embeddings through cross-attention, we pave the way for\ncontext-aware 3D scene reconstruction beyond relying solely on visual cues.\nMoreover, we advocate utilizing spatial guidance from 3D point features toward\ncomprehensive geometric understanding under single-view settings. With 3D\npriors, image features can capture rich structural insights for predicting 3D\nGaussians without multi-view techniques. Extensive experiments on large-scale\ndatasets demonstrate the state-of-the-art performance of CATSplat in\nsingle-view 3D scene reconstruction with high-quality novel view synthesis.\n","authors":["Wonseok Roh","Hwanhee Jung","Jong Wook Kim","Seunggwan Lee","Innfarn Yoo","Andreas Lugmayr","Seunggeun Chi","Karthik Ramani","Sangpil Kim"],"pdf_url":"https://arxiv.org/pdf/2412.12906v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12902v1","updated":"2024-12-17T13:26:31Z","published":"2024-12-17T13:26:31Z","title":"DoPTA: Improving Document Layout Analysis using Patch-Text Alignment","summary":"  The advent of multimodal learning has brought a significant improvement in\ndocument AI. Documents are now treated as multimodal entities, incorporating\nboth textual and visual information for downstream analysis. However, works in\nthis space are often focused on the textual aspect, using the visual space as\nauxiliary information. While some works have explored pure vision based\ntechniques for document image understanding, they require OCR identified text\nas input during inference, or do not align with text in their learning\nprocedure. Therefore, we present a novel image-text alignment technique\nspecially designed for leveraging the textual information in document images to\nimprove performance on visual tasks. Our document encoder model DoPTA - trained\nwith this technique demonstrates strong performance on a wide range of document\nimage understanding tasks, without requiring OCR during inference. Combined\nwith an auxiliary reconstruction objective, DoPTA consistently outperforms\nlarger models, while using significantly lesser pre-training compute. DoPTA\nalso sets new state-of-the art results on D4LA, and FUNSD, two challenging\ndocument visual analysis benchmarks.\n","authors":["Nikitha SR","Tarun Ram Menta","Mausoom Sarkar"],"pdf_url":"https://arxiv.org/pdf/2412.12902v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12892v1","updated":"2024-12-17T13:18:41Z","published":"2024-12-17T13:18:41Z","title":"SAUGE: Taming SAM for Uncertainty-Aligned Multi-Granularity Edge\n  Detection","summary":"  Edge labels are typically at various granularity levels owing to the varying\npreferences of annotators, thus handling the subjectivity of per-pixel labels\nhas been a focal point for edge detection. Previous methods often employ a\nsimple voting strategy to diminish such label uncertainty or impose a strong\nassumption of labels with a pre-defined distribution, e.g., Gaussian. In this\nwork, we unveil that the segment anything model (SAM) provides strong prior\nknowledge to model the uncertainty in edge labels. Our key insight is that the\nintermediate SAM features inherently correspond to object edges at various\ngranularities, which reflects different edge options due to uncertainty.\nTherefore, we attempt to align uncertainty with granularity by regressing\nintermediate SAM features from different layers to object edges at\nmulti-granularity levels. In doing so, the model can fully and explicitly\nexplore diverse ``uncertainties'' in a data-driven fashion. Specifically, we\ninject a lightweight module (~ 1.5% additional parameters) into the frozen SAM\nto progressively fuse and adapt its intermediate features to estimate edges\nfrom coarse to fine. It is crucial to normalize the granularity level of human\nedge labels to match their innate uncertainty. For this, we simply perform\nlinear blending to the real edge labels at hand to create pseudo labels with\nvarying granularities. Consequently, our uncertainty-aligned edge detector can\nflexibly produce edges at any desired granularity (including an optimal one).\nThanks to SAM, our model uniquely demonstrates strong generalizability for\ncross-dataset edge detection. Extensive experimental results on BSDS500,\nMuticue and NYUDv2 validate our model's superiority.\n","authors":["Xing Liufu","Chaolei Tan","Xiaotong Lin","Yonggang Qi","Jinxuan Li","Jian-Fang Hu"],"pdf_url":"https://arxiv.org/pdf/2412.12892v1.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2412.12890v1","updated":"2024-12-17T13:17:19Z","published":"2024-12-17T13:17:19Z","title":"Suppressing Uncertainty in Gaze Estimation","summary":"  Uncertainty in gaze estimation manifests in two aspects: 1) low-quality\nimages caused by occlusion, blurriness, inconsistent eye movements, or even\nnon-face images; 2) incorrect labels resulting from the misalignment between\nthe labeled and actual gaze points during the annotation process. Allowing\nthese uncertainties to participate in training hinders the improvement of gaze\nestimation. To tackle these challenges, in this paper, we propose an effective\nsolution, named Suppressing Uncertainty in Gaze Estimation (SUGE), which\nintroduces a novel triplet-label consistency measurement to estimate and reduce\nthe uncertainties. Specifically, for each training sample, we propose to\nestimate a novel ``neighboring label'' calculated by a linearly weighted\nprojection from the neighbors to capture the similarity relationship between\nimage features and their corresponding labels, which can be incorporated with\nthe predicted pseudo label and ground-truth label for uncertainty estimation.\nBy modeling such triplet-label consistency, we can measure the qualities of\nboth images and labels, and further largely reduce the negative effects of\nunqualified images and wrong labels through our designed sample weighting and\nlabel correction strategies. Experimental results on the gaze estimation\nbenchmarks indicate that our proposed SUGE achieves state-of-the-art\nperformance.\n","authors":["Shijing Wang","Yaping Huang"],"pdf_url":"https://arxiv.org/pdf/2412.12890v1.pdf","comment":"This paper has been accepted to AAAI 2024"},{"id":"http://arxiv.org/abs/2412.12888v1","updated":"2024-12-17T13:12:31Z","published":"2024-12-17T13:12:31Z","title":"ArtAug: Enhancing Text-to-Image Generation through\n  Synthesis-Understanding Interaction","summary":"  The emergence of diffusion models has significantly advanced image synthesis.\nThe recent studies of model interaction and self-corrective reasoning approach\nin large language models offer new insights for enhancing text-to-image models.\nInspired by these studies, we propose a novel method called ArtAug for\nenhancing text-to-image models in this paper. To the best of our knowledge,\nArtAug is the first one that improves image synthesis models via model\ninteractions with understanding models. In the interactions, we leverage human\npreferences implicitly learned by image understanding models to provide\nfine-grained suggestions for image synthesis models. The interactions can\nmodify the image content to make it aesthetically pleasing, such as adjusting\nexposure, changing shooting angles, and adding atmospheric effects. The\nenhancements brought by the interaction are iteratively fused into the\nsynthesis model itself through an additional enhancement module. This enables\nthe synthesis model to directly produce aesthetically pleasing images without\nany extra computational cost. In the experiments, we train the ArtAug\nenhancement module on existing text-to-image models. Various evaluation metrics\nconsistently demonstrate that ArtAug enhances the generative capabilities of\ntext-to-image models without incurring additional computational costs. The\nsource code and models will be released publicly.\n","authors":["Zhongjie Duan","Qianyi Zhao","Cen Chen","Daoyuan Chen","Wenmeng Zhou","Yaliang Li","Yingda Chen"],"pdf_url":"https://arxiv.org/pdf/2412.12888v1.pdf","comment":"18 pages, 8 figures"},{"id":"http://arxiv.org/abs/2412.12887v1","updated":"2024-12-17T13:11:48Z","published":"2024-12-17T13:11:48Z","title":"Learning Coarse-to-Fine Pruning of Graph Convolutional Networks for\n  Skeleton-based Recognition","summary":"  Magnitude Pruning is a staple lightweight network design method which seeks\nto remove connections with the smallest magnitude. This process is either\nachieved in a structured or unstructured manner. While structured pruning\nallows reaching high efficiency, unstructured one is more flexible and leads to\nbetter accuracy, but this is achieved at the expense of low computational\nperformance. In this paper, we devise a novel coarse-to-fine (CTF) method that\ngathers the advantages of structured and unstructured pruning while discarding\ntheir inconveniences to some extent. Our method relies on a novel CTF\nparametrization that models the mask of each connection as the Hadamard product\ninvolving four parametrizations which capture channel-wise, column-wise,\nrow-wise and entry-wise pruning respectively. Hence, fine-grained pruning is\nenabled only when the coarse-grained one is disabled, and this leads to highly\nefficient networks while being effective. Extensive experiments conducted on\nthe challenging task of skeleton-based recognition, using the standard SBU and\nFPHA datasets, show the clear advantage of our CTF approach against different\nbaselines as well as the related work.\n","authors":["Hichem Sahbi"],"pdf_url":"https://arxiv.org/pdf/2412.12887v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14322v2","updated":"2024-12-17T13:02:10Z","published":"2024-11-21T17:12:47Z","title":"SplatR : Experience Goal Visual Rearrangement with 3D Gaussian Splatting\n  and Dense Feature Matching","summary":"  Experience Goal Visual Rearrangement task stands as a foundational challenge\nwithin Embodied AI, requiring an agent to construct a robust world model that\naccurately captures the goal state. The agent uses this world model to restore\na shuffled scene to its original configuration, making an accurate\nrepresentation of the world essential for successfully completing the task. In\nthis work, we present a novel framework that leverages on 3D Gaussian Splatting\nas a 3D scene representation for experience goal visual rearrangement task.\nRecent advances in volumetric scene representation like 3D Gaussian Splatting,\noffer fast rendering of high quality and photo-realistic novel views. Our\napproach enables the agent to have consistent views of the current and the goal\nsetting of the rearrangement task, which enables the agent to directly compare\nthe goal state and the shuffled state of the world in image space. To compare\nthese views, we propose to use a dense feature matching method with visual\nfeatures extracted from a foundation model, leveraging its advantages of a more\nuniversal feature representation, which facilitates robustness, and\ngeneralization. We validate our approach on the AI2-THOR rearrangement\nchallenge benchmark and demonstrate improvements over the current state of the\nart methods\n","authors":["Arjun P S","Andrew Melnik","Gora Chand Nandi"],"pdf_url":"https://arxiv.org/pdf/2411.14322v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12877v1","updated":"2024-12-17T13:00:04Z","published":"2024-12-17T13:00:04Z","title":"MIVE: New Design and Benchmark for Multi-Instance Video Editing","summary":"  Recent AI-based video editing has enabled users to edit videos through simple\ntext prompts, significantly simplifying the editing process. However, recent\nzero-shot video editing techniques primarily focus on global or single-object\nedits, which can lead to unintended changes in other parts of the video. When\nmultiple objects require localized edits, existing methods face challenges,\nsuch as unfaithful editing, editing leakage, and lack of suitable evaluation\ndatasets and metrics. To overcome these limitations, we propose a zero-shot\n$\\textbf{M}$ulti-$\\textbf{I}$nstance $\\textbf{V}$ideo $\\textbf{E}$diting\nframework, called MIVE. MIVE is a general-purpose mask-based framework, not\ndedicated to specific objects (e.g., people). MIVE introduces two key modules:\n(i) Disentangled Multi-instance Sampling (DMS) to prevent editing leakage and\n(ii) Instance-centric Probability Redistribution (IPR) to ensure precise\nlocalization and faithful editing. Additionally, we present our new MIVE\nDataset featuring diverse video scenarios and introduce the Cross-Instance\nAccuracy (CIA) Score to evaluate editing leakage in multi-instance video\nediting tasks. Our extensive qualitative, quantitative, and user study\nevaluations demonstrate that MIVE significantly outperforms recent\nstate-of-the-art methods in terms of editing faithfulness, accuracy, and\nleakage prevention, setting a new benchmark for multi-instance video editing.\nThe project page is available at https://kaist-viclab.github.io/mive-site/\n","authors":["Samuel Teodoro","Agus Gunawan","Soo Ye Kim","Jihyong Oh","Munchurl Kim"],"pdf_url":"https://arxiv.org/pdf/2412.12877v1.pdf","comment":"The first two authors contributed equally to this work. The last two\n  authors are co-corresponding authors. Please visit our project page at\n  https://kaist-viclab.github.io/mive-site/"},{"id":"http://arxiv.org/abs/2312.16476v6","updated":"2024-12-17T12:55:57Z","published":"2023-12-27T08:50:01Z","title":"SVGDreamer: Text Guided SVG Generation with Diffusion Model","summary":"  Recently, text-guided scalable vector graphics (SVGs) synthesis has shown\npromise in domains such as iconography and sketch. However, existing\ntext-to-SVG generation methods lack editability and struggle with visual\nquality and result diversity. To address these limitations, we propose a novel\ntext-guided vector graphics synthesis method called SVGDreamer. SVGDreamer\nincorporates a semantic-driven image vectorization (SIVE) process that enables\nthe decomposition of synthesis into foreground objects and background, thereby\nenhancing editability. Specifically, the SIVE process introduces\nattention-based primitive control and an attention-mask loss function for\neffective control and manipulation of individual elements. Additionally, we\npropose a Vectorized Particle-based Score Distillation (VPSD) approach to\naddress issues of shape over-smoothing, color over-saturation, limited\ndiversity, and slow convergence of the existing text-to-SVG generation methods\nby modeling SVGs as distributions of control points and colors. Furthermore,\nVPSD leverages a reward model to re-weight vector particles, which improves\naesthetic appeal and accelerates convergence. Extensive experiments are\nconducted to validate the effectiveness of SVGDreamer, demonstrating its\nsuperiority over baseline methods in terms of editability, visual quality, and\ndiversity. Project page: https://ximinng.github.io/SVGDreamer-project/\n","authors":["Ximing Xing","Haitao Zhou","Chuang Wang","Jing Zhang","Dong Xu","Qian Yu"],"pdf_url":"https://arxiv.org/pdf/2312.16476v6.pdf","comment":"Accepted by CVPR 2024. project link:\n  https://ximinng.github.io/SVGDreamer-project/"},{"id":"http://arxiv.org/abs/2412.12861v1","updated":"2024-12-17T12:43:10Z","published":"2024-12-17T12:43:10Z","title":"Dyn-HaMR: Recovering 4D Interacting Hand Motion from a Dynamic Camera","summary":"  We propose Dyn-HaMR, to the best of our knowledge, the first approach to\nreconstruct 4D global hand motion from monocular videos recorded by dynamic\ncameras in the wild. Reconstructing accurate 3D hand meshes from monocular\nvideos is a crucial task for understanding human behaviour, with significant\napplications in augmented and virtual reality (AR/VR). However, existing\nmethods for monocular hand reconstruction typically rely on a weak perspective\ncamera model, which simulates hand motion within a limited camera frustum. As a\nresult, these approaches struggle to recover the full 3D global trajectory and\noften produce noisy or incorrect depth estimations, particularly when the video\nis captured by dynamic or moving cameras, which is common in egocentric\nscenarios. Our Dyn-HaMR consists of a multi-stage, multi-objective optimization\npipeline, that factors in (i) simultaneous localization and mapping (SLAM) to\nrobustly estimate relative camera motion, (ii) an interacting-hand prior for\ngenerative infilling and to refine the interaction dynamics, ensuring plausible\nrecovery under (self-)occlusions, and (iii) hierarchical initialization through\na combination of state-of-the-art hand tracking methods. Through extensive\nevaluations on both in-the-wild and indoor datasets, we show that our approach\nsignificantly outperforms state-of-the-art methods in terms of 4D global mesh\nrecovery. This establishes a new benchmark for hand motion reconstruction from\nmonocular video with moving cameras. Our project page is at\nhttps://dyn-hamr.github.io/.\n","authors":["Zhengdi Yu","Stefanos Zafeiriou","Tolga Birdal"],"pdf_url":"https://arxiv.org/pdf/2412.12861v1.pdf","comment":"Project page is available at https://dyn-hamr.github.io/"},{"id":"http://arxiv.org/abs/2412.12853v1","updated":"2024-12-17T12:29:32Z","published":"2024-12-17T12:29:32Z","title":"Automatic Left Ventricular Cavity Segmentation via Deep Spatial\n  Sequential Network in 4D Computed Tomography Studies","summary":"  Automated segmentation of left ventricular cavity (LVC) in temporal cardiac\nimage sequences (multiple time points) is a fundamental requirement for\nquantitative analysis of its structural and functional changes. Deep learning\nbased methods for the segmentation of LVC are the state of the art; however,\nthese methods are generally formulated to work on single time points, and fails\nto exploit the complementary information from the temporal image sequences that\ncan aid in segmentation accuracy and consistency among the images across the\ntime points. Furthermore, these segmentation methods perform poorly in\nsegmenting the end-systole (ES) phase images, where the left ventricle deforms\nto the smallest irregular shape, and the boundary between the blood chamber and\nmyocardium becomes inconspicuous. To overcome these limitations, we propose a\nnew method to automatically segment temporal cardiac images where we introduce\na spatial sequential (SS) network to learn the deformation and motion\ncharacteristics of the LVC in an unsupervised manner; these characteristics\nwere then integrated with sequential context information derived from\nbi-directional learning (BL) where both chronological and reverse-chronological\ndirections of the image sequence were used. Our experimental results on a\ncardiac computed tomography (CT) dataset demonstrated that our\nspatial-sequential network with bi-directional learning (SS-BL) method\noutperformed existing methods for LVC segmentation. Our method was also applied\nto MRI cardiac dataset and the results demonstrated the generalizability of our\nmethod.\n","authors":["Yuyu Guo","Lei Bi","Zhengbin Zhu","David Dagan Feng","Ruiyan Zhang","Qian Wang","Jinman Kim"],"pdf_url":"https://arxiv.org/pdf/2412.12853v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2412.12850v1","updated":"2024-12-17T12:24:08Z","published":"2024-12-17T12:24:08Z","title":"Boosting Fine-Grained Visual Anomaly Detection with\n  Coarse-Knowledge-Aware Adversarial Learning","summary":"  Many unsupervised visual anomaly detection methods train an auto-encoder to\nreconstruct normal samples and then leverage the reconstruction error map to\ndetect and localize the anomalies. However, due to the powerful modeling and\ngeneralization ability of neural networks, some anomalies can also be well\nreconstructed, resulting in unsatisfactory detection and localization accuracy.\nIn this paper, a small coarsely-labeled anomaly dataset is first collected.\nThen, a coarse-knowledge-aware adversarial learning method is developed to\nalign the distribution of reconstructed features with that of normal features.\nThe alignment can effectively suppress the auto-encoder's reconstruction\nability on anomalies and thus improve the detection accuracy. Considering that\nanomalies often only occupy very small areas in anomalous images, a patch-level\nadversarial learning strategy is further developed. Although no patch-level\nanomalous information is available, we rigorously prove that by simply viewing\nany patch features from anomalous images as anomalies, the proposed\nknowledge-aware method can also align the distribution of reconstructed patch\nfeatures with the normal ones. Experimental results on four medical datasets\nand two industrial datasets demonstrate the effectiveness of our method in\nimproving the detection and localization performance.\n","authors":["Qingqing Fang","Qinliang Su","Wenxi Lv","Wenchao Xu","Jianxing Yu"],"pdf_url":"https://arxiv.org/pdf/2412.12850v1.pdf","comment":"The paper is accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.12849v1","updated":"2024-12-17T12:23:07Z","published":"2024-12-17T12:23:07Z","title":"HyperGS: Hyperspectral 3D Gaussian Splatting","summary":"  We introduce HyperGS, a novel framework for Hyperspectral Novel View\nSynthesis (HNVS), based on a new latent 3D Gaussian Splatting (3DGS) technique.\nOur approach enables simultaneous spatial and spectral renderings by encoding\nmaterial properties from multi-view 3D hyperspectral datasets. HyperGS\nreconstructs high-fidelity views from arbitrary perspectives with improved\naccuracy and speed, outperforming currently existing methods. To address the\nchallenges of high-dimensional data, we perform view synthesis in a learned\nlatent space, incorporating a pixel-wise adaptive density function and a\npruning technique for increased training stability and efficiency.\nAdditionally, we introduce the first HNVS benchmark, implementing a number of\nnew baselines based on recent SOTA RGB-NVS techniques, alongside the small\nnumber of prior works on HNVS. We demonstrate HyperGS's robustness through\nextensive evaluation of real and simulated hyperspectral scenes with a 14db\naccuracy improvement upon previously published models.\n","authors":["Christopher Thirgood","Oscar Mendez","Erin Chao Ling","Jon Storey","Simon Hadfield"],"pdf_url":"https://arxiv.org/pdf/2412.12849v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12843v1","updated":"2024-12-17T12:11:04Z","published":"2024-12-17T12:11:04Z","title":"Efficient Event-based Semantic Segmentation with Spike-driven\n  Lightweight Transformer-based Networks","summary":"  Event-based semantic segmentation has great potential in autonomous driving\nand robotics due to the advantages of event cameras, such as high dynamic\nrange, low latency, and low power cost. Unfortunately, current artificial\nneural network (ANN)-based segmentation methods suffer from high computational\ndemands, the requirements for image frames, and massive energy consumption,\nlimiting their efficiency and application on resource-constrained edge/mobile\nplatforms. To address these problems, we introduce SLTNet, a spike-driven\nlightweight transformer-based network designed for event-based semantic\nsegmentation. Specifically, SLTNet is built on efficient spike-driven\nconvolution blocks (SCBs) to extract rich semantic features while reducing the\nmodel's parameters. Then, to enhance the long-range contextural feature\ninteraction, we propose novel spike-driven transformer blocks (STBs) with\nbinary mask operations. Based on these basic blocks, SLTNet employs a\nhigh-efficiency single-branch architecture while maintaining the low energy\nconsumption of the Spiking Neural Network (SNN). Finally, extensive experiments\non DDD17 and DSEC-Semantic datasets demonstrate that SLTNet outperforms\nstate-of-the-art (SOTA) SNN-based methods by at least 7.30% and 3.30% mIoU,\nrespectively, with extremely 5.48x lower energy consumption and 1.14x faster\ninference speed.\n","authors":["Xiaxin Zhu","Fangming Guo","Xianlei Long","Qingyi Gu","Chao Chen","Fuqiang Gu"],"pdf_url":"https://arxiv.org/pdf/2412.12843v1.pdf","comment":"Submitted to IEEE ICRA 2025"},{"id":"http://arxiv.org/abs/2409.03487v3","updated":"2024-12-17T11:57:52Z","published":"2024-09-05T12:52:24Z","title":"ScreenMark: Watermarking Arbitrary Visual Content on Screen","summary":"  Digital watermarking has shown its effectiveness in protecting multimedia\ncontent. However, existing watermarking is predominantly tailored for specific\nmedia types, rendering them less effective for the protection of content\ndisplayed on computer screens, which is often multi-modal and dynamic. Visual\nScreen Content (VSC), is particularly susceptible to theft and leakage through\nscreenshots, a vulnerability that current watermarking methods fail to\nadequately address.To address these challenges, we propose ScreenMark, a robust\nand practical watermarking method designed specifically for arbitrary VSC\nprotection. ScreenMark utilizes a three-stage progressive watermarking\nframework. Initially, inspired by diffusion principles, we initialize the\nmutual transformation between regular watermark information and irregular\nwatermark patterns. Subsequently, these patterns are integrated with screen\ncontent using a pre-multiplication alpha blending technique, supported by a\npre-trained screen decoder for accurate watermark retrieval. The progressively\ncomplex distorter enhances the robustness of the watermark in real-world\nscreenshot scenarios. Finally, the model undergoes fine-tuning guided by a\njoint-level distorter to ensure optimal performance. To validate the\neffectiveness of ScreenMark, we compiled a dataset comprising 100,000\nscreenshots from various devices and resolutions. Extensive experiments on\ndifferent datasets confirm the superior robustness, imperceptibility, and\npractical applicability of the method.\n","authors":["Xiujian Liang","Gaozhi Liu","Yichao Si","Xiaoxiao Hu","Zhenxing Qian"],"pdf_url":"https://arxiv.org/pdf/2409.03487v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12833v1","updated":"2024-12-17T11:54:47Z","published":"2024-12-17T11:54:47Z","title":"FocusChat: Text-guided Long Video Understanding via Spatiotemporal\n  Information Filtering","summary":"  Recently, multi-modal large language models have made significant progress.\nHowever, visual information lacking of guidance from the user's intention may\nlead to redundant computation and involve unnecessary visual noise, especially\nin long, untrimmed videos. To address this issue, we propose FocusChat, a\ntext-guided multi-modal large language model (LLM) that emphasizes visual\ninformation correlated to the user's prompt. In detail, Our model first\nundergoes the semantic extraction module, which comprises a visual semantic\nbranch and a text semantic branch to extract image and text semantics,\nrespectively. The two branches are combined using the Spatial-Temporal\nFiltering Module (STFM). STFM enables explicit spatial-level information\nfiltering and implicit temporal-level feature filtering, ensuring that the\nvisual tokens are closely aligned with the user's query. It lowers the\nessential number of visual tokens inputted into the LLM. FocusChat\nsignificantly outperforms Video-LLaMA in zero-shot experiments, using an order\nof magnitude less training data with only 16 visual tokens occupied. It\nachieves results comparable to the state-of-the-art in few-shot experiments,\nwith only 0.72M pre-training data.\n","authors":["Zheng Cheng","Rendong Wang","Zhicheng Wang"],"pdf_url":"https://arxiv.org/pdf/2412.12833v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2412.12830v1","updated":"2024-12-17T11:52:10Z","published":"2024-12-17T11:52:10Z","title":"Differential Alignment for Domain Adaptive Object Detection","summary":"  Domain adaptive object detection (DAOD) aims to generalize an object detector\ntrained on labeled source-domain data to a target domain without annotations,\nthe core principle of which is \\emph{source-target feature alignment}.\nTypically, existing approaches employ adversarial learning to align the\ndistributions of the source and target domains as a whole, barely considering\nthe varying significance of distinct regions, say instances under different\ncircumstances and foreground \\emph{vs} background areas, during feature\nalignment. To overcome the shortcoming, we investigates a differential feature\nalignment strategy. Specifically, a prediction-discrepancy feedback instance\nalignment module (dubbed PDFA) is designed to adaptively assign higher weights\nto instances of higher teacher-student detection discrepancy, effectively\nhandling heavier domain-specific information. Additionally, an\nuncertainty-based foreground-oriented image alignment module (UFOA) is proposed\nto explicitly guide the model to focus more on regions of interest. Extensive\nexperiments on widely-used DAOD datasets together with ablation studies are\nconducted to demonstrate the efficacy of our proposed method and reveal its\nsuperiority over other SOTA alternatives. Our code is available at\nhttps://github.com/EstrellaXyu/Differential-Alignment-for-DAOD.\n","authors":["Xinyu He","Xinhui Li","Xiaojie Guo"],"pdf_url":"https://arxiv.org/pdf/2412.12830v1.pdf","comment":"11 pages, 8 figures, accepted by aaai25"},{"id":"http://arxiv.org/abs/2412.12829v1","updated":"2024-12-17T11:49:36Z","published":"2024-12-17T11:49:36Z","title":"2by2: Weakly-Supervised Learning for Global Action Segmentation","summary":"  This paper presents a simple yet effective approach for the poorly\ninvestigated task of global action segmentation, aiming at grouping frames\ncapturing the same action across videos of different activities. Unlike the\ncase of videos depicting all the same activity, the temporal order of actions\nis not roughly shared among all videos, making the task even more challenging.\nWe propose to use activity labels to learn, in a weakly-supervised fashion,\naction representations suitable for global action segmentation. For this\npurpose, we introduce a triadic learning approach for video pairs, to ensure\nintra-video action discrimination, as well as inter-video and inter-activity\naction association. For the backbone architecture, we use a Siamese network\nbased on sparse transformers that takes as input video pairs and determine\nwhether they belong to the same activity. The proposed approach is validated on\ntwo challenging benchmark datasets: Breakfast and YouTube Instructions,\noutperforming state-of-the-art methods.\n","authors":["Elena Bueno-Benito","Mariella Dimiccoli"],"pdf_url":"https://arxiv.org/pdf/2412.12829v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12827v1","updated":"2024-12-17T11:47:59Z","published":"2024-12-17T11:47:59Z","title":"TabSniper: Towards Accurate Table Detection & Structure Recognition for\n  Bank Statements","summary":"  Extraction of transaction information from bank statements is required to\nassess one's financial well-being for credit rating and underwriting decisions.\nUnlike other financial documents such as tax forms or financial statements,\nextracting the transaction descriptions from bank statements can provide a\ncomprehensive and recent view into the cash flows and spending patterns. With\nmultiple variations in layout and templates across several banks, extracting\ntransactional level information from different table categories is an arduous\ntask. Existing table structure recognition approaches produce sub optimal\nresults for long, complex tables and are unable to capture all transactions\naccurately. This paper proposes TabSniper, a novel approach for efficient table\ndetection, categorization and structure recognition from bank statements. The\npipeline starts with detecting and categorizing tables of interest from the\nbank statements. The extracted table regions are then processed by the table\nstructure recognition model followed by a post-processing module to transform\nthe transactional data into a structured and standardised format. The detection\nand structure recognition architectures are based on DETR, fine-tuned with\ndiverse bank statements along with additional feature enhancements. Results on\nchallenging datasets demonstrate that TabSniper outperforms strong baselines\nand produces high-quality extraction of transaction information from bank and\nother financial documents across multiple layouts and templates.\n","authors":["Abhishek Trivedi","Sourajit Mukherjee","Rajat Kumar Singh","Vani Agarwal","Sriranjani Ramakrishnan","Himanshu S. Bhatt"],"pdf_url":"https://arxiv.org/pdf/2412.12827v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12821v1","updated":"2024-12-17T11:41:49Z","published":"2024-12-17T11:41:49Z","title":"ComprehendEdit: A Comprehensive Dataset and Evaluation Framework for\n  Multimodal Knowledge Editing","summary":"  Large multimodal language models (MLLMs) have revolutionized natural language\nprocessing and visual understanding, but often contain outdated or inaccurate\ninformation. Current multimodal knowledge editing evaluations are limited in\nscope and potentially biased, focusing on narrow tasks and failing to assess\nthe impact on in-domain samples. To address these issues, we introduce\nComprehendEdit, a comprehensive benchmark comprising eight diverse tasks from\nmultiple datasets. We propose two novel metrics: Knowledge Generalization Index\n(KGI) and Knowledge Preservation Index (KPI), which evaluate editing effects on\nin-domain samples without relying on AI-synthetic samples. Based on insights\nfrom our framework, we establish Hierarchical In-Context Editing (HICE), a\nbaseline method employing a two-stage approach that balances performance across\nall metrics. This study provides a more comprehensive evaluation framework for\nmultimodal knowledge editing, reveals unique challenges in this field, and\noffers a baseline method demonstrating improved performance. Our work opens new\nperspectives for future research and provides a foundation for developing more\nrobust and effective editing techniques for MLLMs. The ComprehendEdit benchmark\nand implementation code are available at\nhttps://github.com/yaohui120/ComprehendEdit.\n","authors":["Yaohui Ma","Xiaopeng Hong","Shizhou Zhang","Huiyun Li","Zhilin Zhu","Wei Luo","Zhiheng Ma"],"pdf_url":"https://arxiv.org/pdf/2412.12821v1.pdf","comment":"Extended version for paper accepted to AAAI 2025. Project Page:\n  https://github.com/yaohui120/ComprehendEdit"},{"id":"http://arxiv.org/abs/2412.12801v1","updated":"2024-12-17T11:10:46Z","published":"2024-12-17T11:10:46Z","title":"Multi-View Incremental Learning with Structured Hebbian Plasticity for\n  Enhanced Fusion Efficiency","summary":"  The rapid evolution of multimedia technology has revolutionized human\nperception, paving the way for multi-view learning. However, traditional\nmulti-view learning approaches are tailored for scenarios with fixed data\nviews, falling short of emulating the intricate cognitive procedures of the\nhuman brain processing signals sequentially. Our cerebral architecture\nseamlessly integrates sequential data through intricate feed-forward and\nfeedback mechanisms. In stark contrast, traditional methods struggle to\ngeneralize effectively when confronted with data spanning diverse domains,\nhighlighting the need for innovative strategies that can mimic the brain's\nadaptability and dynamic integration capabilities. In this paper, we propose a\nbio-neurologically inspired multi-view incremental framework named MVIL aimed\nat emulating the brain's fine-grained fusion of sequentially arriving views.\nMVIL lies two fundamental modules: structured Hebbian plasticity and synaptic\npartition learning. The structured Hebbian plasticity reshapes the structure of\nweights to express the high correlation between view representations,\nfacilitating a fine-grained fusion of view representations. Moreover, synaptic\npartition learning is efficient in alleviating drastic changes in weights and\nalso retaining old knowledge by inhibiting partial synapses. These modules\nbionically play a central role in reinforcing crucial associations between\nnewly acquired information and existing knowledge repositories, thereby\nenhancing the network's capacity for generalization. Experimental results on\nsix benchmark datasets show MVIL's effectiveness over state-of-the-art methods.\n","authors":["Yuhong Chen","Ailin Song","Huifeng Yin","Shuai Zhong","Fuhai Chen","Qi Xu","Shiping Wang","Mingkun Xu"],"pdf_url":"https://arxiv.org/pdf/2412.12801v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2412.12799v1","updated":"2024-12-17T11:02:36Z","published":"2024-12-17T11:02:36Z","title":"RCTrans: Radar-Camera Transformer via Radar Densifier and Sequential\n  Decoder for 3D Object Detection","summary":"  In radar-camera 3D object detection, the radar point clouds are sparse and\nnoisy, which causes difficulties in fusing camera and radar modalities. To\nsolve this, we introduce a novel query-based detection method named\nRadar-Camera Transformer (RCTrans). Specifically, we first design a Radar Dense\nEncoder to enrich the sparse valid radar tokens, and then concatenate them with\nthe image tokens. By doing this, we can fully explore the 3D information of\neach interest region and reduce the interference of empty tokens during the\nfusing stage. We then design a Pruning Sequential Decoder to predict 3D boxes\nbased on the obtained tokens and random initialized queries. To alleviate the\neffect of elevation ambiguity in radar point clouds, we gradually locate the\nposition of the object via a sequential fusion structure. It helps to get more\nprecise and flexible correspondences between tokens and queries. A pruning\ntraining strategy is adopted in the decoder, which can save much time during\ninference and inhibit queries from losing their distinctiveness. Extensive\nexperiments on the large-scale nuScenes dataset prove the superiority of our\nmethod, and we also achieve new state-of-the-art radar-camera 3D detection\nresults. Our implementation is available at https://github.com/liyih/RCTrans.\n","authors":["Yiheng Li","Yang Yang","Zhen Lei"],"pdf_url":"https://arxiv.org/pdf/2412.12799v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.12798v1","updated":"2024-12-17T11:00:56Z","published":"2024-12-17T11:00:56Z","title":"ZoRI: Towards Discriminative Zero-Shot Remote Sensing Instance\n  Segmentation","summary":"  Instance segmentation algorithms in remote sensing are typically based on\nconventional methods, limiting their application to seen scenarios and\nclosed-set predictions. In this work, we propose a novel task called zero-shot\nremote sensing instance segmentation, aimed at identifying aerial objects that\nare absent from training data. Challenges arise when classifying aerial\ncategories with high inter-class similarity and intra-class variance. Besides,\nthe domain gap between vision-language models' pretraining datasets and remote\nsensing datasets hinders the zero-shot capabilities of the pretrained model\nwhen it is directly applied to remote sensing images. To address these\nchallenges, we propose a $\\textbf{Z}$ero-Sh$\\textbf{o}$t $\\textbf{R}$emote\nSensing $\\textbf{I}$nstance Segmentation framework, dubbed $\\textbf{ZoRI}$. Our\napproach features a discrimination-enhanced classifier that uses refined\ntextual embeddings to increase the awareness of class disparities. Instead of\ndirect fine-tuning, we propose a knowledge-maintained adaptation strategy that\ndecouples semantic-related information to preserve the pretrained\nvision-language alignment while adjusting features to capture remote sensing\ndomain-specific visual cues. Additionally, we introduce a prior-injected\nprediction with cache bank of aerial visual prototypes to supplement the\nsemantic richness of text embeddings and seamlessly integrate aerial\nrepresentations, adapting to the remote sensing domain. We establish new\nexperimental protocols and benchmarks, and extensive experiments convincingly\ndemonstrate that ZoRI achieves the state-of-art performance on the zero-shot\nremote sensing instance segmentation task. Our code is available at\nhttps://github.com/HuangShiqi128/ZoRI.\n","authors":["Shiqi Huang","Shuting He","Bihan Wen"],"pdf_url":"https://arxiv.org/pdf/2412.12798v1.pdf","comment":"AAAI 2025, code see https://github.com/HuangShiqi128/ZoRI"},{"id":"http://arxiv.org/abs/2412.12793v1","updated":"2024-12-17T10:56:18Z","published":"2024-12-17T10:56:18Z","title":"CRoF: CLIP-based Robust Few-shot Learning on Noisy Labels","summary":"  Noisy labels threaten the robustness of few-shot learning (FSL) due to the\ninexact features in a new domain. CLIP, a large-scale vision-language model,\nperforms well in FSL on image-text embedding similarities, but it is\nsusceptible to misclassification caused by noisy labels. How to enhance domain\ngeneralization of CLIP on noisy data within FSL tasks is a critical challenge.\nIn this paper, we provide a novel view to mitigate the influence of noisy\nlabels, CLIP-based Robust Few-shot learning (CRoF). CRoF is a general plug-in\nmodule for CLIP-based models. To avoid misclassification and confused label\nembedding, we design the few-shot task-oriented prompt generator to give more\ndiscriminative descriptions of each category. The proposed prompt achieves\nlarger distances of inter-class textual embedding. Furthermore, rather than\nfully trusting zero-shot classification by CLIP, we fine-tune CLIP on noisy\nfew-shot data in a new domain with a weighting strategy like label-smooth. The\nweights for multiple potentially correct labels consider the relationship\nbetween CLIP's prior knowledge and original label information to ensure\nreliability. Our multiple label loss function further supports robust training\nunder this paradigm. Comprehensive experiments show that CRoF, as a plug-in,\noutperforms fine-tuned and vanilla CLIP models on different noise types and\nnoise ratios.\n","authors":["Shizhuo Deng","Bowen Han","Jiaqi Chen","Hao Wang","Dongyue Chen","Tong Jia"],"pdf_url":"https://arxiv.org/pdf/2412.12793v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12791v1","updated":"2024-12-17T10:52:50Z","published":"2024-12-17T10:52:50Z","title":"Implicit Location-Caption Alignment via Complementary Masking for\n  Weakly-Supervised Dense Video Captioning","summary":"  Weakly-Supervised Dense Video Captioning (WSDVC) aims to localize and\ndescribe all events of interest in a video without requiring annotations of\nevent boundaries. This setting poses a great challenge in accurately locating\nthe temporal location of event, as the relevant supervision is unavailable.\nExisting methods rely on explicit alignment constraints between event locations\nand captions, which involve complex event proposal procedures during both\ntraining and inference. To tackle this problem, we propose a novel implicit\nlocation-caption alignment paradigm by complementary masking, which simplifies\nthe complex event proposal and localization process while maintaining\neffectiveness. Specifically, our model comprises two components: a dual-mode\nvideo captioning module and a mask generation module. The dual-mode video\ncaptioning module captures global event information and generates descriptive\ncaptions, while the mask generation module generates differentiable positive\nand negative masks for localizing the events. These masks enable the implicit\nalignment of event locations and captions by ensuring that captions generated\nfrom positively and negatively masked videos are complementary, thereby forming\na complete video description. In this way, even under weak supervision, the\nevent location and event caption can be aligned implicitly. Extensive\nexperiments on the public datasets demonstrate that our method outperforms\nexisting weakly-supervised methods and achieves competitive results compared to\nfully-supervised methods.\n","authors":["Shiping Ge","Qiang Chen","Zhiwei Jiang","Yafeng Yin","Liu Qin","Ziyao Chen","Qing Gu"],"pdf_url":"https://arxiv.org/pdf/2412.12791v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.12788v1","updated":"2024-12-17T10:47:13Z","published":"2024-12-17T10:47:13Z","title":"RA-SGG: Retrieval-Augmented Scene Graph Generation Framework via\n  Multi-Prototype Learning","summary":"  Scene Graph Generation (SGG) research has suffered from two fundamental\nchallenges: the long-tailed predicate distribution and semantic ambiguity\nbetween predicates. These challenges lead to a bias towards head predicates in\nSGG models, favoring dominant general predicates while overlooking fine-grained\npredicates. In this paper, we address the challenges of SGG by framing it as\nmulti-label classification problem with partial annotation, where relevant\nlabels of fine-grained predicates are missing. Under the new frame, we propose\nRetrieval-Augmented Scene Graph Generation (RA-SGG), which identifies potential\ninstances to be multi-labeled and enriches the single-label with multi-labels\nthat are semantically similar to the original label by retrieving relevant\nsamples from our established memory bank. Based on augmented relations (i.e.,\ndiscovered multi-labels), we apply multi-prototype learning to train our SGG\nmodel. Several comprehensive experiments have demonstrated that RA-SGG\noutperforms state-of-the-art baselines by up to 3.6% on VG and 5.9% on GQA,\nparticularly in terms of F@K, showing that RA-SGG effectively alleviates the\nissue of biased prediction caused by the long-tailed distribution and semantic\nambiguity of predicates.\n","authors":["Kanghoon Yoon","Kibum Kim","Jaehyung Jeon","Yeonjun In","Donghyun Kim","Chanyoung Park"],"pdf_url":"https://arxiv.org/pdf/2412.12788v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2310.12848v2","updated":"2024-12-17T10:44:49Z","published":"2023-10-19T15:59:24Z","title":"Neural Degradation Representation Learning for All-In-One Image\n  Restoration","summary":"  Existing methods have demonstrated effective performance on a single\ndegradation type. In practical applications, however, the degradation is often\nunknown, and the mismatch between the model and the degradation will result in\na severe performance drop. In this paper, we propose an all-in-one image\nrestoration network that tackles multiple degradations. Due to the\nheterogeneous nature of different types of degradations, it is difficult to\nprocess multiple degradations in a single network. To this end, we propose to\nlearn a neural degradation representation (NDR) that captures the underlying\ncharacteristics of various degradations. The learned NDR decomposes different\ntypes of degradations adaptively, similar to a neural dictionary that\nrepresents basic degradation components. Subsequently, we develop a degradation\nquery module and a degradation injection module to effectively recognize and\nutilize the specific degradation based on NDR, enabling the all-in-one\nrestoration ability for multiple degradations. Moreover, we propose a\nbidirectional optimization strategy to effectively drive NDR to learn the\ndegradation representation by optimizing the degradation and restoration\nprocesses alternately. Comprehensive experiments on representative types of\ndegradations (including noise, haze, rain, and downsampling) demonstrate the\neffectiveness and generalization capability of our method.\n","authors":["Mingde Yao","Ruikang Xu","Yuanshen Guan","Jie Huang","Zhiwei Xiong"],"pdf_url":"https://arxiv.org/pdf/2310.12848v2.pdf","comment":"Code: https://github.com/mdyao/NDR-Restore"},{"id":"http://arxiv.org/abs/2412.12785v1","updated":"2024-12-17T10:44:47Z","published":"2024-12-17T10:44:47Z","title":"Activating Distributed Visual Region within LLMs for Efficient and\n  Effective Vision-Language Training and Inference","summary":"  Large Vision-Language Models (LVLMs) typically learn visual capacity through\nvisual instruction tuning, involving updates to both a projector and their LLM\nbackbones. Drawing inspiration from the concept of visual region in the human\nbrain, we investigate the existence of an analogous \\textit{visual region}\nwithin LLMs that functions as a cognitive core, and explore the possibility of\nefficient training of LVLMs via selective layers tuning. We use\nBunny-Llama-3-8B-V for detailed experiments and LLaVA-1.5-7B and LLaVA-1.5-13B\nfor validation across a range of visual and textual tasks. Our findings reveal\nthat selectively updating 25\\% of LLMs layers, when sparsely and uniformly\ndistributed, can preserve nearly 99\\% of visual performance while maintaining\nor enhancing textual task results, and also effectively reducing training time.\nBased on this targeted training approach, we further propose a novel visual\nregion-based pruning paradigm, removing non-critical layers outside the visual\nregion, which can achieve minimal performance loss. This study offers an\neffective and efficient strategy for LVLM training and inference by activating\na layer-wise visual region within LLMs, which is consistently effective across\ndifferent models and parameter scales.\n","authors":["Siyuan Wang","Dianyi Wang","Chengxing Zhou","Zejun Li","Zhihao Fan","Xuanjing Huang","Zhongyu Wei"],"pdf_url":"https://arxiv.org/pdf/2412.12785v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12782v1","updated":"2024-12-17T10:42:19Z","published":"2024-12-17T10:42:19Z","title":"Bidirectional Logits Tree: Pursuing Granularity Reconcilement in\n  Fine-Grained Classification","summary":"  This paper addresses the challenge of Granularity Competition in fine-grained\nclassification tasks, which arises due to the semantic gap between\nmulti-granularity labels. Existing approaches typically develop independent\nhierarchy-aware models based on shared features extracted from a common base\nencoder. However, because coarse-grained levels are inherently easier to learn\nthan finer ones, the base encoder tends to prioritize coarse feature\nabstractions, which impedes the learning of fine-grained features. To overcome\nthis challenge, we propose a novel framework called the Bidirectional Logits\nTree (BiLT) for Granularity Reconcilement. The key idea is to develop\nclassifiers sequentially from the finest to the coarsest granularities, rather\nthan parallelly constructing a set of classifiers based on the same input\nfeatures. In this setup, the outputs of finer-grained classifiers serve as\ninputs for coarser-grained ones, facilitating the flow of hierarchical semantic\ninformation across different granularities. On top of this, we further\nintroduce an Adaptive Intra-Granularity Difference Learning (AIGDL) approach to\nuncover subtle semantic differences between classes within the same\ngranularity. Extensive experiments demonstrate the effectiveness of our\nproposed method.\n","authors":["Zhiguang Lu","Qianqian Xu","Shilong Bao","Zhiyong Yang","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2412.12782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12778v1","updated":"2024-12-17T10:37:46Z","published":"2024-12-17T10:37:46Z","title":"Rethinking Diffusion-Based Image Generators for Fundus Fluorescein\n  Angiography Synthesis on Limited Data","summary":"  Fundus imaging is a critical tool in ophthalmology, with different imaging\nmodalities offering unique advantages. For instance, fundus fluorescein\nangiography (FFA) can accurately identify eye diseases. However, traditional\ninvasive FFA involves the injection of sodium fluorescein, which can cause\ndiscomfort and risks. Generating corresponding FFA images from non-invasive\nfundus images holds significant practical value but also presents challenges.\nFirst, limited datasets constrain the performance and effectiveness of models.\nSecond, previous studies have primarily focused on generating FFA for single\ndiseases or single modalities, often resulting in poor performance for patients\nwith various ophthalmic conditions. To address these issues, we propose a novel\nlatent diffusion model-based framework, Diffusion, which introduces a\nfine-tuning protocol to overcome the challenge of limited medical data and\nunleash the generative capabilities of diffusion models. Furthermore, we\ndesigned a new approach to tackle the challenges of generating across different\nmodalities and disease types. On limited datasets, our framework achieves\nstate-of-the-art results compared to existing methods, offering significant\npotential to enhance ophthalmic diagnostics and patient care. Our code will be\nreleased soon to support further research in this field.\n","authors":["Chengzhou Yu","Huihui Fang","Hongqiu Wang","Ting Deng","Qing Du","Yanwu Xu","Weihua Yang"],"pdf_url":"https://arxiv.org/pdf/2412.12778v1.pdf","comment":"15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.12774v1","updated":"2024-12-17T10:35:27Z","published":"2024-12-17T10:35:27Z","title":"A Framework for Critical Evaluation of Text-to-Image Models: Integrating\n  Art Historical Analysis, Artistic Exploration, and Critical Prompt\n  Engineering","summary":"  This paper proposes a novel interdisciplinary framework for the critical\nevaluation of text-to-image models, addressing the limitations of current\ntechnical metrics and bias studies. By integrating art historical analysis,\nartistic exploration, and critical prompt engineering, the framework offers a\nmore nuanced understanding of these models' capabilities and societal\nimplications. Art historical analysis provides a structured approach to examine\nvisual and symbolic elements, revealing potential biases and\nmisrepresentations. Artistic exploration, through creative experimentation,\nuncovers hidden potentials and limitations, prompting critical reflection on\nthe algorithms' assumptions. Critical prompt engineering actively challenges\nthe model's assumptions, exposing embedded biases. Case studies demonstrate the\nframework's practical application, showcasing how it can reveal biases related\nto gender, race, and cultural representation. This comprehensive approach not\nonly enhances the evaluation of text-to-image models but also contributes to\nthe development of more equitable, responsible, and culturally aware AI\nsystems.\n","authors":["Amalia Foka"],"pdf_url":"https://arxiv.org/pdf/2412.12774v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12772v1","updated":"2024-12-17T10:33:36Z","published":"2024-12-17T10:33:36Z","title":"Optimize the Unseen -- Fast NeRF Cleanup with Free Space Prior","summary":"  Neural Radiance Fields (NeRF) have advanced photorealistic novel view\nsynthesis, but their reliance on photometric reconstruction introduces\nartifacts, commonly known as \"floaters\". These artifacts degrade novel view\nquality, especially in areas unseen by the training cameras. We present a fast,\npost-hoc NeRF cleanup method that eliminates such artifacts by enforcing our\nFree Space Prior, effectively minimizing floaters without disrupting the NeRF's\nrepresentation of observed regions. Unlike existing approaches that rely on\neither Maximum Likelihood (ML) estimation to fit the data or a complex, local\ndata-driven prior, our method adopts a Maximum-a-Posteriori (MAP) approach,\nselecting the optimal model parameters under a simple global prior assumption\nthat unseen regions should remain empty. This enables our method to clean\nartifacts in both seen and unseen areas, enhancing novel view quality even in\nchallenging scene regions. Our method is comparable with existing NeRF cleanup\nmodels while being 2.5x faster in inference time, requires no additional memory\nbeyond the original NeRF, and achieves cleanup training in less than 30\nseconds. Our code will be made publically available.\n","authors":["Leo Segre","Shai Avidan"],"pdf_url":"https://arxiv.org/pdf/2412.12772v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12771v1","updated":"2024-12-17T10:33:34Z","published":"2024-12-17T10:33:34Z","title":"Guided and Variance-Corrected Fusion with One-shot Style Alignment for\n  Large-Content Image Generation","summary":"  Producing large images using small diffusion models is gaining increasing\npopularity, as the cost of training large models could be prohibitive. A common\napproach involves jointly generating a series of overlapped image patches and\nobtaining large images by merging adjacent patches. However, results from\nexisting methods often exhibit obvious artifacts, e.g., seams and inconsistent\nobjects and styles. To address the issues, we proposed Guided Fusion (GF),\nwhich mitigates the negative impact from distant image regions by applying a\nweighted average to the overlapping regions. Moreover, we proposed\nVariance-Corrected Fusion (VCF), which corrects data variance at\npost-averaging, generating more accurate fusion for the Denoising Diffusion\nProbabilistic Model. Furthermore, we proposed a one-shot Style Alignment (SA),\nwhich generates a coherent style for large images by adjusting the initial\ninput noise without adding extra computational burden. Extensive experiments\ndemonstrated that the proposed fusion methods improved the quality of the\ngenerated image significantly. As a plug-and-play module, the proposed method\ncan be widely applied to enhance other fusion-based methods for large image\ngeneration.\n","authors":["Shoukun Sun","Min Xian","Tiankai Yao","Fei Xu","Luca Capriotti"],"pdf_url":"https://arxiv.org/pdf/2412.12771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12766v1","updated":"2024-12-17T10:31:03Z","published":"2024-12-17T10:31:03Z","title":"Towards a Training Free Approach for 3D Scene Editing","summary":"  Text driven diffusion models have shown remarkable capabilities in editing\nimages. However, when editing 3D scenes, existing works mostly rely on training\na NeRF for 3D editing. Recent NeRF editing methods leverages edit operations by\ndeploying 2D diffusion models and project these edits into 3D space. They\nrequire strong positional priors alongside text prompt to identify the edit\nlocation. These methods are operational on small 3D scenes and are more\ngeneralized to particular scene. They require training for each specific edit\nand cannot be exploited in real-time edits. To address these limitations, we\npropose a novel method, FreeEdit, to make edits in training free manner using\nmesh representations as a substitute for NeRF. Training-free methods are now a\npossibility because of the advances in foundation model's space. We leverage\nthese models to bring a training-free alternative and introduce solutions for\ninsertion, replacement and deletion. We consider insertion, replacement and\ndeletion as basic blocks for performing intricate edits with certain\ncombinations of these operations. Given a text prompt and a 3D scene, our model\nis capable of identifying what object should be inserted/replaced or deleted\nand location where edit should be performed. We also introduce a novel\nalgorithm as part of FreeEdit to find the optimal location on grounding object\nfor placement. We evaluate our model by comparing it with baseline models on a\nwide range of scenes using quantitative and qualitative metrics and showcase\nthe merits of our method with respect to others.\n","authors":["Vivek Madhavaram","Shivangana Rawat","Chaitanya Devaguptapu","Charu Sharma","Manohar Kaul"],"pdf_url":"https://arxiv.org/pdf/2412.12766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12765v1","updated":"2024-12-17T10:30:56Z","published":"2024-12-17T10:30:56Z","title":"Monocular Facial Appearance Capture in the Wild","summary":"  We present a new method for reconstructing the appearance properties of human\nfaces from a lightweight capture procedure in an unconstrained environment. Our\nmethod recovers the surface geometry, diffuse albedo, specular intensity and\nspecular roughness from a monocular video containing a simple head rotation\nin-the-wild. Notably, we make no simplifying assumptions on the environment\nlighting, and we explicitly take visibility and occlusions into account. As a\nresult, our method can produce facial appearance maps that approach the\nfidelity of studio-based multi-view captures, but with a far easier and cheaper\nprocedure.\n","authors":["Yingyan Xu","Kate Gadola","Prashanth Chandran","Sebastian Weiss","Markus Gross","Gaspard Zoss","Derek Bradley"],"pdf_url":"https://arxiv.org/pdf/2412.12765v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12755v1","updated":"2024-12-17T10:20:29Z","published":"2024-12-17T10:20:29Z","title":"Progressive Monitoring of Generative Model Training Evolution","summary":"  While deep generative models (DGMs) have gained popularity, their\nsusceptibility to biases and other inefficiencies that lead to undesirable\noutcomes remains an issue. With their growing complexity, there is a critical\nneed for early detection of issues to achieve desired results and optimize\nresources. Hence, we introduce a progressive analysis framework to monitor the\ntraining process of DGMs. Our method utilizes dimensionality reduction\ntechniques to facilitate the inspection of latent representations, the\ngenerated and real distributions, and their evolution across training\niterations. This monitoring allows us to pause and fix the training method if\nthe representations or distributions progress undesirably. This approach allows\nfor the analysis of a models' training dynamics and the timely identification\nof biases and failures, minimizing computational loads. We demonstrate how our\nmethod supports identifying and mitigating biases early in training a\nGenerative Adversarial Network (GAN) and improving the quality of the generated\ndata distribution.\n","authors":["Vidya Prasad","Anna Vilanova","Nicola Pezzotti"],"pdf_url":"https://arxiv.org/pdf/2412.12755v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12743v1","updated":"2024-12-17T10:06:42Z","published":"2024-12-17T10:06:42Z","title":"Training a Distributed Acoustic Sensing Traffic Monitoring Network With\n  Video Inputs","summary":"  Distributed Acoustic Sensing (DAS) has emerged as a promising tool for\nreal-time traffic monitoring in densely populated areas. In this paper, we\npresent a novel concept that integrates DAS data with co-located visual\ninformation. We use YOLO-derived vehicle location and classification from\ncamera inputs as labeled data to train a detection and classification neural\nnetwork utilizing DAS data only. Our model achieves a performance exceeding 94%\nfor detection and classification, and about 1.2% false alarm rate. We\nillustrate the model's application in monitoring traffic over a week, yielding\nstatistical insights that could benefit future smart city developments. Our\napproach highlights the potential of combining fiber-optic sensors with visual\ninformation, focusing on practicality and scalability, protecting privacy, and\nminimizing infrastructure costs. To encourage future research, we share our\ndataset.\n","authors":["Khen Cohen","Liav Hen","Ariel Lellouch"],"pdf_url":"https://arxiv.org/pdf/2412.12743v1.pdf","comment":"12 pages, 11 figures, 5 appendices. Shared dataset in:\n  https://zenodo.org/records/14502092"},{"id":"http://arxiv.org/abs/2412.12740v1","updated":"2024-12-17T10:03:39Z","published":"2024-12-17T10:03:39Z","title":"Open-World Panoptic Segmentation","summary":"  Perception is a key building block of autonomously acting vision systems such\nas autonomous vehicles. It is crucial that these systems are able to understand\ntheir surroundings in order to operate safely and robustly. Additionally,\nautonomous systems deployed in unconstrained real-world scenarios must be able\nof dealing with novel situations and object that have never been seen before.\nIn this article, we tackle the problem of open-world panoptic segmentation,\ni.e., the task of discovering new semantic categories and new object instances\nat test time, while enforcing consistency among the categories that we\nincrementally discover. We propose Con2MAV, an approach for open-world panoptic\nsegmentation that extends our previous work, ContMAV, which was developed for\nopen-world semantic segmentation. Through extensive experiments across multiple\ndatasets, we show that our model achieves state-of-the-art results on\nopen-world segmentation tasks, while still performing competitively on the\nknown categories. We will open-source our implementation upon acceptance.\nAdditionally, we propose PANIC (Panoptic ANomalies In Context), a benchmark for\nevaluating open-world panoptic segmentation in autonomous driving scenarios.\nThis dataset, recorded with a multi-modal sensor suite mounted on a car,\nprovides high-quality, pixel-wise annotations of anomalous objects at both\nsemantic and instance level. Our dataset contains 800 images, with more than 50\nunknown classes, i.e., classes that do not appear in the training set, and 4000\nobject instances, making it an extremely challenging dataset for open-world\nsegmentation tasks in the autonomous driving scenario. We provide competitions\nfor multiple open-world tasks on a hidden test set. Our dataset and\ncompetitions are available at https://www.ipb.uni-bonn.de/data/panic.\n","authors":["Matteo Sodano","Federico Magistri","Jens Behley","Cyrill Stachniss"],"pdf_url":"https://arxiv.org/pdf/2412.12740v1.pdf","comment":"Submitted to PAMI"},{"id":"http://arxiv.org/abs/2410.15628v2","updated":"2024-12-17T10:01:59Z","published":"2024-10-21T04:24:10Z","title":"Towards Kriging-informed Conditional Diffusion for Regional Sea-Level\n  Data Downscaling","summary":"  Given coarser-resolution projections from global climate models or satellite\ndata, the downscaling problem aims to estimate finer-resolution regional\nclimate data, capturing fine-scale spatial patterns and variability.\nDownscaling is any method to derive high-resolution data from low-resolution\nvariables, often to provide more detailed and local predictions and analyses.\nThis problem is societally crucial for effective adaptation, mitigation, and\nresilience against significant risks from climate change. The challenge arises\nfrom spatial heterogeneity and the need to recover finer-scale features while\nensuring model generalization. Most downscaling methods \\cite{Li2020} fail to\ncapture the spatial dependencies at finer scales and underperform on real-world\nclimate datasets, such as sea-level rise. We propose a novel Kriging-informed\nConditional Diffusion Probabilistic Model (Ki-CDPM) to capture spatial\nvariability while preserving fine-scale features. Experimental results on\nclimate data show that our proposed method is more accurate than\nstate-of-the-art downscaling techniques.\n","authors":["Subhankar Ghosh","Arun Sharma","Jayant Gupta","Aneesh Subramanian","Shashi Shekhar"],"pdf_url":"https://arxiv.org/pdf/2410.15628v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12737v1","updated":"2024-12-17T09:59:53Z","published":"2024-12-17T09:59:53Z","title":"PolSAM: Polarimetric Scattering Mechanism Informed Segment Anything\n  Model","summary":"  PolSAR data presents unique challenges due to its rich and complex\ncharacteristics. Existing data representations, such as complex-valued data,\npolarimetric features, and amplitude images, are widely used. However, these\nformats often face issues related to usability, interpretability, and data\nintegrity. Most feature extraction networks for PolSAR are small, limiting\ntheir ability to capture features effectively. To address these issues, We\npropose the Polarimetric Scattering Mechanism-Informed SAM (PolSAM), an\nenhanced Segment Anything Model (SAM) that integrates domain-specific\nscattering characteristics and a novel prompt generation strategy. PolSAM\nintroduces Microwave Vision Data (MVD), a lightweight and interpretable data\nrepresentation derived from polarimetric decomposition and semantic\ncorrelations. We propose two key components: the Feature-Level Fusion Prompt\n(FFP), which fuses visual tokens from pseudo-colored SAR images and MVD to\naddress modality incompatibility in the frozen SAM encoder, and the\nSemantic-Level Fusion Prompt (SFP), which refines sparse and dense segmentation\nprompts using semantic information. Experimental results on the PhySAR-Seg\ndatasets demonstrate that PolSAM significantly outperforms existing SAM-based\nand multimodal fusion models, improving segmentation accuracy, reducing data\nstorage, and accelerating inference time. The source code and datasets will be\nmade publicly available at \\url{https://github.com/XAI4SAR/PolSAM}.\n","authors":["Yuqing Wang","Zhongling Huang","Shuxin Yang","Hao Tang","Xiaolan Qiu","Junwei Han","Dingwen Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.12737v1.pdf","comment":"The manuscript is 15 pages long, includes 14 figures and 5 tables"},{"id":"http://arxiv.org/abs/2412.12735v1","updated":"2024-12-17T09:57:21Z","published":"2024-12-17T09:57:21Z","title":"GIRAFFE: Design Choices for Extending the Context Length of Visual\n  Language Models","summary":"  Visual Language Models (VLMs) demonstrate impressive capabilities in\nprocessing multimodal inputs, yet applications such as visual agents, which\nrequire handling multiple images and high-resolution videos, demand enhanced\nlong-range modeling. Moreover, existing open-source VLMs lack systematic\nexploration into extending their context length, and commercial models often\nprovide limited details. To tackle this, we aim to establish an effective\nsolution that enhances long context performance of VLMs while preserving their\ncapacities in short context scenarios. Towards this goal, we make the best\ndesign choice through extensive experiment settings from data curation to\ncontext window extending and utilizing: (1) we analyze data sources and length\ndistributions to construct ETVLM - a data recipe to balance the performance\nacross scenarios; (2) we examine existing position extending methods, identify\ntheir limitations and propose M-RoPE++ as an enhanced approach; we also choose\nto solely instruction-tune the backbone with mixed-source data; (3) we discuss\nhow to better utilize extended context windows and propose hybrid-resolution\ntraining. Built on the Qwen-VL series model, we propose Giraffe, which is\neffectively extended to 128K lengths. Evaluated on extensive long context VLM\nbenchmarks such as VideoMME and Viusal Haystacks, our Giraffe achieves\nstate-of-the-art performance among similarly sized open-source long VLMs and is\ncompetitive with commercial model GPT-4V. We will open-source the code, data,\nand models.\n","authors":["Mukai Li","Lei Li","Shansan Gong","Qi Liu"],"pdf_url":"https://arxiv.org/pdf/2412.12735v1.pdf","comment":"Working in progress"},{"id":"http://arxiv.org/abs/2412.12734v1","updated":"2024-12-17T09:57:04Z","published":"2024-12-17T09:57:04Z","title":"Gaussian Billboards: Expressive 2D Gaussian Splatting with Textures","summary":"  Gaussian Splatting has recently emerged as the go-to representation for\nreconstructing and rendering 3D scenes. The transition from 3D to 2D Gaussian\nprimitives has further improved multi-view consistency and surface\nreconstruction accuracy. In this work we highlight the similarity between 2D\nGaussian Splatting (2DGS) and billboards from traditional computer graphics.\nBoth use flat semi-transparent 2D geometry that is positioned, oriented and\nscaled in 3D space. However 2DGS uses a solid color per splat and an opacity\nmodulated by a Gaussian distribution, where billboards are more expressive,\nmodulating the color with a uv-parameterized texture. We propose to unify these\nconcepts by presenting Gaussian Billboards, a modification of 2DGS to add\nspatially-varying color achieved using per-splat texture interpolation. The\nresult is a mixture of the two representations, which benefits from both the\nrobust scene optimization power of 2DGS and the expressiveness of texture\nmapping. We show that our method can improve the sharpness and quality of the\nscene representation in a wide range of qualitative and quantitative\nevaluations compared to the original 2DGS implementation.\n","authors":["Sebastian Weiss","Derek Bradley"],"pdf_url":"https://arxiv.org/pdf/2412.12734v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12725v1","updated":"2024-12-17T09:47:48Z","published":"2024-12-17T09:47:48Z","title":"RaCFormer: Towards High-Quality 3D Object Detection via Query-based\n  Radar-Camera Fusion","summary":"  We propose Radar-Camera fusion transformer (RaCFormer) to boost the accuracy\nof 3D object detection by the following insight. The Radar-Camera fusion in\noutdoor 3D scene perception is capped by the image-to-BEV transformation--if\nthe depth of pixels is not accurately estimated, the naive combination of BEV\nfeatures actually integrates unaligned visual content. To avoid this problem,\nwe propose a query-based framework that enables adaptively sample\ninstance-relevant features from both the BEV and the original image view.\nFurthermore, we enhance system performance by two key designs: optimizing query\ninitialization and strengthening the representational capacity of BEV. For the\nformer, we introduce an adaptive circular distribution in polar coordinates to\nrefine the initialization of object queries, allowing for a distance-based\nadjustment of query density. For the latter, we initially incorporate a\nradar-guided depth head to refine the transformation from image view to BEV.\nSubsequently, we focus on leveraging the Doppler effect of radar and introduce\nan implicit dynamic catcher to capture the temporal elements within the BEV.\nExtensive experiments on nuScenes and View-of-Delft (VoD) datasets validate the\nmerits of our design. Remarkably, our method achieves superior results of 64.9%\nmAP and 70.2% NDS on nuScenes, even outperforming several LiDAR-based\ndetectors. RaCFormer also secures the 1st ranking on the VoD dataset. The code\nwill be released.\n","authors":["Xiaomeng Chu","Jiajun Deng","Guoliang You","Yifan Duan","Houqiang Li","Yanyong Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.12725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13362v2","updated":"2024-12-17T09:46:19Z","published":"2024-06-19T09:07:31Z","title":"VisualRWKV: Exploring Recurrent Neural Networks for Visual Language\n  Models","summary":"  Visual Language Models (VLMs) have rapidly progressed with the recent success\nof large language models. However, there have been few attempts to incorporate\nefficient linear Recurrent Neural Networks (RNNs) architectures into VLMs. In\nthis study, we introduce VisualRWKV, the first application of a linear RNN\nmodel to multimodal learning tasks, leveraging the pre-trained RWKV language\nmodel. We propose a data-dependent recurrence and sandwich prompts to enhance\nour modeling capabilities, along with a 2D image scanning mechanism to enrich\nthe processing of visual sequences. Extensive experiments demonstrate that\nVisualRWKV achieves competitive performance compared to Transformer-based\nmodels like LLaVA-1.5 on various benchmarks. Compared to LLaVA-1.5, VisualRWKV\nhas a speed advantage of 3.98 times and can save 54% of GPU memory when\nreaching an inference length of 24K tokens. To facilitate further research and\nanalysis, we have made the checkpoints and the associated code publicly\naccessible at the following GitHub repository: see\nhttps://github.com/howard-hou/VisualRWKV.\n","authors":["Haowen Hou","Peigen Zeng","Fei Ma","Fei Richard Yu"],"pdf_url":"https://arxiv.org/pdf/2406.13362v2.pdf","comment":"Accepted at COLING 2025 main conference"},{"id":"http://arxiv.org/abs/2412.12722v1","updated":"2024-12-17T09:38:58Z","published":"2024-12-17T09:38:58Z","title":"Defending LVLMs Against Vision Attacks through Partial-Perception\n  Supervision","summary":"  Recent studies have raised significant concerns regarding the vulnerability\nof Large Vision Language Models (LVLMs) to maliciously injected or perturbed\ninput images, which can mislead their responses. Existing defense methods show\nthat such vision attacks are sensitive to image modifications especially\ncropping, using majority voting across responses of modified images as\ncorrected responses. However, these modifications often result in partial\nimages and distort the semantics, which reduces response quality on clean\nimages after voting. Instead of directly using responses from partial images\nfor voting, we investigate using them to supervise the LVLM's responses to the\noriginal images. We propose a black-box, training-free method called DPS\n(Defense through Partial-Perception Supervision). In this approach, the model\nis prompted using the responses generated by a model that perceives only a\npartial image. With DPS, the model can adjust its response based on partial\nimage understanding when under attack, while confidently maintaining its\noriginal response for clean input. Our findings show that the weak model can\nsupervise the strong model: when faced with an attacked input, the strong model\nbecomes less confident and adjusts its response based on the weak model's\npartial understanding, effectively defending against the attack. With clean\ninput, it confidently maintains its original response. Empirical experiments\nshow our method outperforms the baseline, cutting the average attack success\nrate by 76.3% across six datasets on three popular models.\n","authors":["Qi Zhou","Tianlin Li","Qing Guo","Dongxia Wang","Yun Lin","Yang Liu","Jin Song Dong"],"pdf_url":"https://arxiv.org/pdf/2412.12722v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.18263v2","updated":"2024-12-17T09:34:49Z","published":"2024-11-27T12:01:08Z","title":"TSD-SR: One-Step Diffusion with Target Score Distillation for Real-World\n  Image Super-Resolution","summary":"  Pre-trained text-to-image diffusion models are increasingly applied to\nreal-world image super-resolution (Real-ISR) task. Given the iterative\nrefinement nature of diffusion models, most existing approaches are\ncomputationally expensive. While methods such as SinSR and OSEDiff have emerged\nto condense inference steps via distillation, their performance in image\nrestoration or details recovery is not satisfied. To address this, we propose\nTSD-SR, a novel distillation framework specifically designed for real-world\nimage super-resolution, aiming to construct an efficient and effective one-step\nmodel. We first introduce the Target Score Distillation, which leverages the\npriors of diffusion models and real image references to achieve more realistic\nimage restoration. Secondly, we propose a Distribution-Aware Sampling Module to\nmake detail-oriented gradients more readily accessible, addressing the\nchallenge of recovering fine details. Extensive experiments demonstrate that\nour TSD-SR has superior restoration results (most of the metrics perform the\nbest) and the fastest inference speed (e.g. 40 times faster than SeeSR)\ncompared to the past Real-ISR approaches based on pre-trained diffusion priors.\n","authors":["Linwei Dong","Qingnan Fan","Yihong Guo","Zhonghao Wang","Qi Zhang","Jinwei Chen","Yawei Luo","Changqing Zou"],"pdf_url":"https://arxiv.org/pdf/2411.18263v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12718v1","updated":"2024-12-17T09:33:06Z","published":"2024-12-17T09:33:06Z","title":"ASAP: Advancing Semantic Alignment Promotes Multi-Modal Manipulation\n  Detecting and Grounding","summary":"  We present ASAP, a new framework for detecting and grounding multi-modal\nmedia manipulation (DGM4).Upon thorough examination, we observe that accurate\nfine-grained cross-modal semantic alignment between the image and text is vital\nfor accurately manipulation detection and grounding. While existing DGM4\nmethods pay rare attention to the cross-modal alignment, hampering the accuracy\nof manipulation detecting to step further. To remedy this issue, this work\ntargets to advance the semantic alignment learning to promote this task.\nParticularly, we utilize the off-the-shelf Multimodal Large-Language Models\n(MLLMs) and Large Language Models (LLMs) to construct paired image-text pairs,\nespecially for the manipulated instances. Subsequently, a cross-modal alignment\nlearning is performed to enhance the semantic alignment. Besides the explicit\nauxiliary clues, we further design a Manipulation-Guided Cross Attention (MGCA)\nto provide implicit guidance for augmenting the manipulation perceiving. With\nthe grounding truth available during training, MGCA encourages the model to\nconcentrate more on manipulated components while downplaying normal ones,\nenhancing the model's ability to capture manipulations. Extensive experiments\nare conducted on the DGM4 dataset, the results demonstrate that our model can\nsurpass the comparison method with a clear margin.\n","authors":["Zhenxing Zhang","Yaxiong Wang","Lechao Cheng","Zhun Zhong","Dan Guo","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2412.12718v1.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.12716v1","updated":"2024-12-17T09:30:31Z","published":"2024-12-17T09:30:31Z","title":"Unsupervised UAV 3D Trajectories Estimation with Sparse Point Clouds","summary":"  Compact UAV systems, while advancing delivery and surveillance, pose\nsignificant security challenges due to their small size, which hinders\ndetection by traditional methods. This paper presents a cost-effective,\nunsupervised UAV detection method using spatial-temporal sequence processing to\nfuse multiple LiDAR scans for accurate UAV tracking in real-world scenarios.\nOur approach segments point clouds into foreground and background, analyzes\nspatial-temporal data, and employs a scoring mechanism to enhance detection\naccuracy. Tested on a public dataset, our solution placed 4th in the CVPR 2024\nUG2+ Challenge, demonstrating its practical effectiveness. We plan to\nopen-source all designs, code, and sample data for the research community\ngithub.com/lianghanfang/UnLiDAR-UAV-Est.\n","authors":["Hanfang Liang","Yizhuo Yang","Jinming Hu","Jianfei Yang","Fen Liu","Shenghai Yuan"],"pdf_url":"https://arxiv.org/pdf/2412.12716v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09951v2","updated":"2024-12-17T09:27:33Z","published":"2024-12-13T08:14:24Z","title":"WiseAD: Knowledge Augmented End-to-End Autonomous Driving with\n  Vision-Language Model","summary":"  The emergence of general human knowledge and impressive logical reasoning\ncapacity in rapidly progressed vision-language models (VLMs) have driven\nincreasing interest in applying VLMs to high-level autonomous driving tasks,\nsuch as scene understanding and decision-making. However, an in-depth study on\nthe relationship between knowledge proficiency, especially essential driving\nexpertise, and closed-loop autonomous driving performance requires further\nexploration. In this paper, we investigate the effects of the depth and breadth\nof fundamental driving knowledge on closed-loop trajectory planning and\nintroduce WiseAD, a specialized VLM tailored for end-to-end autonomous driving\ncapable of driving reasoning, action justification, object recognition, risk\nanalysis, driving suggestions, and trajectory planning across diverse\nscenarios. We employ joint training on driving knowledge and planning datasets,\nenabling the model to perform knowledge-aligned trajectory planning\naccordingly. Extensive experiments indicate that as the diversity of driving\nknowledge extends, critical accidents are notably reduced, contributing 11.9%\nand 12.4% improvements in the driving score and route completion on the Carla\nclosed-loop evaluations, achieving state-of-the-art performance. Moreover,\nWiseAD also demonstrates remarkable performance in knowledge evaluations on\nboth in-domain and out-of-domain datasets.\n","authors":["Songyan Zhang","Wenhui Huang","Zihui Gao","Hao Chen","Chen Lv"],"pdf_url":"https://arxiv.org/pdf/2412.09951v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12709v1","updated":"2024-12-17T09:23:46Z","published":"2024-12-17T09:23:46Z","title":"Accelerating lensed quasars discovery and modeling with physics-informed\n  variational autoencoders","summary":"  Strongly lensed quasars provide valuable insights into the rate of cosmic\nexpansion, the distribution of dark matter in foreground deflectors, and the\ncharacteristics of quasar hosts. However, detecting them in astronomical images\nis difficult due to the prevalence of non-lensing objects. To address this\nchallenge, we developed a generative deep learning model called VariLens, built\nupon a physics-informed variational autoencoder. This model seamlessly\nintegrates three essential modules: image reconstruction, object\nclassification, and lens modeling, offering a fast and comprehensive approach\nto strong lens analysis. VariLens is capable of rapidly determining both (1)\nthe probability that an object is a lens system and (2) key parameters of a\nsingular isothermal ellipsoid (SIE) mass model -- including the Einstein radius\n($\\theta_\\mathrm{E}$), lens center, and ellipticity -- in just milliseconds\nusing a single CPU. A direct comparison of VariLens estimates with traditional\nlens modeling for 20 known lensed quasars within the Subaru Hyper Suprime-Cam\n(HSC) footprint shows good agreement, with both results consistent within\n$2\\sigma$ for systems with $\\theta_\\mathrm{E}<3$ arcsecs. To identify new\nlensed quasar candidates, we begin with an initial sample of approximately 80\nmillion sources, combining HSC data with multiwavelength information from\nvarious surveys. After applying a photometric preselection aimed at locating\n$z>1.5$ sources, the number of candidates is reduced to 710,966. Subsequently,\nVariLens highlights 13,831 sources, each showing a high likelihood of being a\nlens. A visual assessment of these objects results in 42 promising candidates\nthat await spectroscopic confirmation. These results underscore the potential\nof automated deep learning pipelines to efficiently detect and model strong\nlenses in large datasets.\n","authors":["Irham T. Andika","Stefan Schuldt","Sherry H. Suyu","Satadru Bag","Raoul Cañameras","Alejandra Melo","Claudio Grillo","James H. H. Chan"],"pdf_url":"https://arxiv.org/pdf/2412.12709v1.pdf","comment":"Submitted to the Astronomy & Astrophysics journal. The paper consists\n  of 17 main pages, 14 figures, and 5 tables. We welcome feedback and comments\n  from readers!"},{"id":"http://arxiv.org/abs/2408.09429v2","updated":"2024-12-17T09:19:46Z","published":"2024-08-18T10:07:02Z","title":"Reefknot: A Comprehensive Benchmark for Relation Hallucination\n  Evaluation, Analysis and Mitigation in Multimodal Large Language Models","summary":"  Hallucination issues continue to affect multimodal large language models\n(MLLMs), with existing research mainly addressing object-level or\nattribute-level hallucinations, neglecting the more complex relation\nhallucinations that require advanced reasoning. Current benchmarks for relation\nhallucinations lack detailed evaluation and effective mitigation, and their\ndatasets often suffer from biases due to systematic annotation processes. To\naddress these challenges, we introduce Reefknot, a comprehensive benchmark\ntargeting relation hallucinations, comprising over 20,000 real-world samples.\nWe provide a systematic definition of relation hallucinations, integrating\nperceptive and cognitive perspectives, and construct a relation-based corpus\nusing the Visual Genome scene graph dataset. Our comparative evaluation reveals\nsignificant limitations in current MLLMs' ability to handle relation\nhallucinations. Additionally, we propose a novel confidence-based mitigation\nstrategy, which reduces the hallucination rate by an average of 9.75% across\nthree datasets, including Reefknot. Our work offers valuable insights for\nachieving trustworthy multimodal intelligence.\n","authors":["Kening Zheng","Junkai Chen","Yibo Yan","Xin Zou","Xuming Hu"],"pdf_url":"https://arxiv.org/pdf/2408.09429v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12704v1","updated":"2024-12-17T09:19:44Z","published":"2024-12-17T09:19:44Z","title":"MapExpert: Online HD Map Construction with Simple and Efficient Sparse\n  Map Element Expert","summary":"  Constructing online High-Definition (HD) maps is crucial for the static\nenvironment perception of autonomous driving systems (ADS). Existing solutions\ntypically attempt to detect vectorized HD map elements with unified models;\nhowever, these methods often overlook the distinct characteristics of different\nnon-cubic map elements, making accurate distinction challenging. To address\nthese issues, we introduce an expert-based online HD map method, termed\nMapExpert. MapExpert utilizes sparse experts, distributed by our routers, to\ndescribe various non-cubic map elements accurately. Additionally, we propose an\nauxiliary balance loss function to distribute the load evenly across experts.\nFurthermore, we theoretically analyze the limitations of prevalent bird's-eye\nview (BEV) feature temporal fusion methods and introduce an efficient temporal\nfusion module called Learnable Weighted Moving Descentage. This module\neffectively integrates relevant historical information into the final BEV\nfeatures. Combined with an enhanced slice head branch, the proposed MapExpert\nachieves state-of-the-art performance and maintains good efficiency on both\nnuScenes and Argoverse2 datasets.\n","authors":["Dapeng Zhang","Dayu Chen","Peng Zhi","Yinda Chen","Zhenlong Yuan","Chenyang Li"," Sunjing","Rui Zhou","Qingguo Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.12704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12696v1","updated":"2024-12-17T09:13:22Z","published":"2024-12-17T09:13:22Z","title":"ALADE-SNN: Adaptive Logit Alignment in Dynamically Expandable Spiking\n  Neural Networks for Class Incremental Learning","summary":"  Inspired by the human brain's ability to adapt to new tasks without erasing\nprior knowledge, we develop spiking neural networks (SNNs) with dynamic\nstructures for Class Incremental Learning (CIL). Our comparative experiments\nreveal that limited datasets introduce biases in logits distributions among\ntasks. Fixed features from frozen past-task extractors can cause overfitting\nand hinder the learning of new tasks. To address these challenges, we propose\nthe ALADE-SNN framework, which includes adaptive logit alignment for balanced\nfeature representation and OtoN suppression to manage weights mapping frozen\nold features to new classes during training, releasing them during fine-tuning.\nThis approach dynamically adjusts the network architecture based on analytical\nobservations, improving feature extraction and balancing performance between\nnew and old tasks. Experiment results show that ALADE-SNN achieves an average\nincremental accuracy of 75.42 on the CIFAR100-B0 benchmark over 10 incremental\nsteps. ALADE-SNN not only matches the performance of DNN-based methods but also\nsurpasses state-of-the-art SNN-based continual learning algorithms. This\nadvancement enhances continual learning in neuromorphic computing, offering a\nbrain-inspired, energy-efficient solution for real-time data processing.\n","authors":["Wenyao Ni","Jiangrong Shen","Qi Xu","Huajin Tang"],"pdf_url":"https://arxiv.org/pdf/2412.12696v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12693v1","updated":"2024-12-17T09:10:55Z","published":"2024-12-17T09:10:55Z","title":"SPHERE: A Hierarchical Evaluation on Spatial Perception and Reasoning\n  for Vision-Language Models","summary":"  Current vision-language models may incorporate single-dimensional spatial\ncues, such as depth, object boundary, and basic spatial directions (e.g. left,\nright, front, back), yet often lack the multi-dimensional spatial reasoning\nnecessary for human-like understanding and real-world applications. To address\nthis gap, we develop SPHERE (Spatial Perception and Hierarchical Evaluation of\nREasoning), a hierarchical evaluation framework with a new human-annotated\ndataset to pinpoint model strengths and weaknesses, advancing from single-skill\ntasks to multi-skill tasks, and ultimately to complex reasoning tasks that\nrequire the integration of multiple spatial and visual cues with logical\nreasoning. Benchmark evaluation of state-of-the-art open-source models reveal\nsignificant shortcomings, especially in the abilities to understand distance\nand proximity, to reason from both allocentric and egocentric viewpoints, and\nto perform complex reasoning in a physical context. This work underscores the\nneed for more advanced approaches to spatial understanding and reasoning,\npaving the way for improvements in vision-language models and their alignment\nwith human-like spatial capabilities. The dataset will be open-sourced upon\npublication.\n","authors":["Wenyu Zhang","Wei En Ng","Lixin Ma","Yuwen Wang","Jungqi Zhao","Boyang Li","Lu Wang"],"pdf_url":"https://arxiv.org/pdf/2412.12693v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12685v1","updated":"2024-12-17T09:02:55Z","published":"2024-12-17T09:02:55Z","title":"SemStereo: Semantic-Constrained Stereo Matching Network for Remote\n  Sensing","summary":"  Semantic segmentation and 3D reconstruction are two fundamental tasks in\nremote sensing, typically treated as separate or loosely coupled tasks. Despite\nattempts to integrate them into a unified network, the constraints between the\ntwo heterogeneous tasks are not explicitly modeled, since the pioneering\nstudies either utilize a loosely coupled parallel structure or engage in only\nimplicit interactions, failing to capture the inherent connections. In this\nwork, we explore the connections between the two tasks and propose a new\nnetwork that imposes semantic constraints on the stereo matching task, both\nimplicitly and explicitly. Implicitly, we transform the traditional parallel\nstructure to a new cascade structure termed Semantic-Guided Cascade structure,\nwhere the deep features enriched with semantic information are utilized for the\ncomputation of initial disparity maps, enhancing semantic guidance. Explicitly,\nwe propose a Semantic Selective Refinement (SSR) module and a Left-Right\nSemantic Consistency (LRSC) module. The SSR refines the initial disparity map\nunder the guidance of the semantic map. The LRSC ensures semantic consistency\nbetween two views via reducing the semantic divergence after transforming the\nsemantic map from one view to the other using the disparity map. Experiments on\nthe US3D and WHU datasets demonstrate that our method achieves state-of-the-art\nperformance for both semantic segmentation and stereo matching.\n","authors":["Chen Chen","Liangjin Zhao","Yuanchun He","Yingxuan Long","Kaiqiang Chen","Zhirui Wang","Yanfeng Hu","Xian Sun"],"pdf_url":"https://arxiv.org/pdf/2412.12685v1.pdf","comment":"9 pages, 6 figures, AAAI 2025"},{"id":"http://arxiv.org/abs/2412.12683v1","updated":"2024-12-17T08:56:59Z","published":"2024-12-17T08:56:59Z","title":"ShiftedBronzes: Benchmarking and Analysis of Domain Fine-Grained\n  Classification in Open-World Settings","summary":"  In real-world applications across specialized domains, addressing complex\nout-of-distribution (OOD) challenges is a common and significant concern. In\nthis study, we concentrate on the task of fine-grained bronze ware dating, a\ncritical aspect in the study of ancient Chinese history, and developed a\nbenchmark dataset named ShiftedBronzes. By extensively expanding the bronze\nDing dataset, ShiftedBronzes incorporates two types of bronze ware data and\nseven types of OOD data, which exhibit distribution shifts commonly encountered\nin bronze ware dating scenarios. We conduct benchmarking experiments on\nShiftedBronzes and five commonly used general OOD datasets, employing a variety\nof widely adopted post-hoc, pre-trained Vision Large Model (VLM)-based and\ngeneration-based OOD detection methods. Through analysis of the experimental\nresults, we validate previous conclusions regarding post-hoc, VLM-based, and\ngeneration-based methods, while also highlighting their distinct behaviors on\nspecialized datasets. These findings underscore the unique challenges of\napplying general OOD detection methods to domain-specific tasks such as bronze\nware dating. We hope that the ShiftedBronzes benchmark provides valuable\ninsights into both the field of bronze ware dating and the and the development\nof OOD detection methods. The dataset and associated code will be available\nlater.\n","authors":["Rixin Zhou","Honglin Pang","Qian Zhang","Ruihua Qi","Xi Yang","Chuntao Li"],"pdf_url":"https://arxiv.org/pdf/2412.12683v1.pdf","comment":"9pages, 7 figures, 4 tables"},{"id":"http://arxiv.org/abs/2412.11594v2","updated":"2024-12-17T08:52:51Z","published":"2024-12-16T09:32:23Z","title":"VersaGen: Unleashing Versatile Visual Control for Text-to-Image\n  Synthesis","summary":"  Despite the rapid advancements in text-to-image (T2I) synthesis, enabling\nprecise visual control remains a significant challenge. Existing works\nattempted to incorporate multi-facet controls (text and sketch), aiming to\nenhance the creative control over generated images. However, our pilot study\nreveals that the expressive power of humans far surpasses the capabilities of\ncurrent methods. Users desire a more versatile approach that can accommodate\ntheir diverse creative intents, ranging from controlling individual subjects to\nmanipulating the entire scene composition. We present VersaGen, a generative AI\nagent that enables versatile visual control in T2I synthesis. VersaGen admits\nfour types of visual controls: i) single visual subject; ii) multiple visual\nsubjects; iii) scene background; iv) any combination of the three above or\nmerely no control at all. We train an adaptor upon a frozen T2I model to\naccommodate the visual information into the text-dominated diffusion process.\nWe introduce three optimization strategies during the inference phase of\nVersaGen to improve generation results and enhance user experience.\nComprehensive experiments on COCO and Sketchy validate the effectiveness and\nflexibility of VersaGen, as evidenced by both qualitative and quantitative\nresults.\n","authors":["Zhipeng Chen","Lan Yang","Yonggang Qi","Honggang Zhang","Kaiyue Pang","Ke Li","Yi-Zhe Song"],"pdf_url":"https://arxiv.org/pdf/2412.11594v2.pdf","comment":"The paper has been accepted by AAAI 2025. Paper code:\n  https://github.com/FelixChan9527/VersaGen_official"},{"id":"http://arxiv.org/abs/2412.12675v1","updated":"2024-12-17T08:44:29Z","published":"2024-12-17T08:44:29Z","title":"ShotVL: Human-Centric Highlight Frame Retrieval via Language Queries","summary":"  Existing works on human-centric video understanding typically focus on\nanalyzing specific moment or entire videos. However, many applications require\nhigher precision at the frame level. In this work, we propose a novel task,\nBestShot, which aims to locate highlight frames within human-centric videos via\nlanguage queries. This task demands not only a deep semantic comprehension of\nhuman actions but also precise temporal localization. To support this task, we\nintroduce the BestShot Benchmark. %The benchmark is meticulously constructed by\ncombining human detection and tracking, potential frame selection based on\nhuman judgment, and detailed textual descriptions crafted by human input to\nensure precision. The benchmark is meticulously constructed by combining\nhuman-annotated highlight frames, detailed textual descriptions and duration\nlabeling. These descriptions encompass three critical elements: (1) Visual\ncontent; (2) Fine-grained action; and (3) Human Pose Description. Together,\nthese elements provide the necessary precision to identify the exact highlight\nframes in videos.\n  To tackle this problem, we have collected two distinct datasets: (i)\nShotGPT4o Dataset, which is algorithmically generated by GPT-4o and (ii)\nImage-SMPLText Dataset, a dataset with large-scale and accurate per-frame pose\ndescription leveraging PoseScript and existing pose estimation datasets. Based\non these datasets, we present a strong baseline model, ShotVL, fine-tuned from\nInternVL, specifically for BestShot. We highlight the impressive zero-shot\ncapabilities of our model and offer comparative analyses with existing SOTA\nmodels. ShotVL demonstrates a significant 52% improvement over InternVL on the\nBestShot Benchmark and a notable 57% improvement on the THUMOS14 Benchmark, all\nwhile maintaining the SOTA performance in general image classification and\nretrieval.\n","authors":["Wangyu Xue","Chen Qian","Jiayi Wu","Yang Zhou","Wentao Liu","Ju Ren","Siming Fan","Yaoxue Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.12675v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12672v1","updated":"2024-12-17T08:41:50Z","published":"2024-12-17T08:41:50Z","title":"Structural Pruning via Spatial-aware Information Redundancy for Semantic\n  Segmentation","summary":"  In recent years, semantic segmentation has flourished in various\napplications. However, the high computational cost remains a significant\nchallenge that hinders its further adoption. The filter pruning method for\nstructured network slimming offers a direct and effective solution for the\nreduction of segmentation networks. Nevertheless, we argue that most existing\npruning methods, originally designed for image classification, overlook the\nfact that segmentation is a location-sensitive task, which consequently leads\nto their suboptimal performance when applied to segmentation networks. To\naddress this issue, this paper proposes a novel approach, denoted as\nSpatial-aware Information Redundancy Filter Pruning~(SIRFP), which aims to\nreduce feature redundancy between channels. First, we formulate the pruning\nprocess as a maximum edge weight clique problem~(MEWCP) in graph theory,\nthereby minimizing the redundancy among the remaining features after pruning.\nWithin this framework, we introduce a spatial-aware redundancy metric based on\nfeature maps, thus endowing the pruning process with location sensitivity to\nbetter adapt to pruning segmentation networks. Additionally, based on the\nMEWCP, we propose a low computational complexity greedy strategy to solve this\nNP-hard problem, making it feasible and efficient for structured pruning. To\nvalidate the effectiveness of our method, we conducted extensive comparative\nexperiments on various challenging datasets. The results demonstrate the\nsuperior performance of SIRFP for semantic segmentation tasks.\n","authors":["Dongyue Wu","Zilin Guo","Li Yu","Nong Sang","Changxin Gao"],"pdf_url":"https://arxiv.org/pdf/2412.12672v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.12669v1","updated":"2024-12-17T08:40:23Z","published":"2024-12-17T08:40:23Z","title":"Adaptive Prototype Replay for Class Incremental Semantic Segmentation","summary":"  Class incremental semantic segmentation (CISS) aims to segment new classes\nduring continual steps while preventing the forgetting of old knowledge.\nExisting methods alleviate catastrophic forgetting by replaying distributions\nof previously learned classes using stored prototypes or features. However,\nthey overlook a critical issue: in CISS, the representation of class knowledge\nis updated continuously through incremental learning, whereas prototype replay\nmethods maintain fixed prototypes. This mismatch between updated representation\nand fixed prototypes limits the effectiveness of the prototype replay strategy.\nTo address this issue, we propose the Adaptive prototype replay (Adapter) for\nCISS in this paper. Adapter comprises an adaptive deviation compen sation (ADC)\nstrategy and an uncertainty-aware constraint (UAC) loss. Specifically, the ADC\nstrategy dynamically updates the stored prototypes based on the estimated\nrepresentation shift distance to match the updated representation of old class.\nThe UAC loss reduces prediction uncertainty, aggregating discriminative\nfeatures to aid in generating compact prototypes. Additionally, we introduce a\ncompensation-based prototype similarity discriminative (CPD) loss to ensure\nadequate differentiation between similar prototypes, thereby enhancing the\nefficiency of the adaptive prototype replay strategy. Extensive experiments on\nPascal VOC and ADE20K datasets demonstrate that Adapter achieves\nstate-of-the-art results and proves effective across various CISS tasks,\nparticularly in challenging multi-step scenarios. The code and model is\navailable at https://github.com/zhu-gl-ux/Adapter.\n","authors":["Guilin Zhu","Dongyue Wu","Changxin Gao","Runmin Wang","Weidong Yang","Nong Sang"],"pdf_url":"https://arxiv.org/pdf/2412.12669v1.pdf","comment":"Accepted by the Main Technical Track of the 39th Annual AAAI\n  Conference on Artificial Intelligence (AAAI-2025)"},{"id":"http://arxiv.org/abs/2412.12667v1","updated":"2024-12-17T08:36:47Z","published":"2024-12-17T08:36:47Z","title":"A Two-Fold Patch Selection Approach for Improved 360-Degree Image\n  Quality Assessment","summary":"  This article presents a novel approach to improving the accuracy of\n360-degree perceptual image quality assessment (IQA) through a two-fold patch\nselection process. Our methodology combines visual patch selection with\nembedding similarity-based refinement. The first stage focuses on selecting\npatches from 360-degree images using three distinct sampling methods to ensure\ncomprehensive coverage of visual content for IQA. The second stage, which is\nthe core of our approach, employs an embedding similarity-based selection\nprocess to filter and prioritize the most informative patches based on their\nembeddings similarity distances. This dual selection mechanism ensures that the\ntraining data is both relevant and informative, enhancing the model's learning\nefficiency. Extensive experiments and statistical analyses using three distance\nmetrics across three benchmark datasets validate the effectiveness of our\nselection algorithm. The results highlight its potential to deliver robust and\naccurate 360-degree IQA, with performance gains of up to 4.5% in accuracy and\nmonotonicity of quality score prediction, while using only 40% to 50% of the\ntraining patches. These improvements are consistent across various\nconfigurations and evaluation metrics, demonstrating the strength of the\nproposed method. The code for the selection process is available at:\nhttps://github.com/sendjasni/patch-selection-360-image-quality.\n","authors":["Abderrezzaq Sendjasni","Seif-Eddine Benkabou","Mohamed-Chaker Larabi"],"pdf_url":"https://arxiv.org/pdf/2412.12667v1.pdf","comment":"Submitted to IEEE Transactions on Image Processing"},{"id":"http://arxiv.org/abs/2412.12661v1","updated":"2024-12-17T08:30:00Z","published":"2024-12-17T08:30:00Z","title":"MedMax: Mixed-Modal Instruction Tuning for Training Biomedical\n  Assistants","summary":"  Recent advancements in mixed-modal generative models have enabled flexible\nintegration of information across image-text content. These models have opened\nnew avenues for developing unified biomedical assistants capable of analyzing\nbiomedical images, answering complex questions about them, and predicting the\nimpact of medical procedures on a patient's health. However, existing resources\nface challenges such as limited data availability, narrow domain coverage, and\nrestricted sources (e.g., medical papers). To address these gaps, we present\nMedMax, the first large-scale multimodal biomedical instruction-tuning dataset\nfor mixed-modal foundation models. With 1.47 million instances, MedMax\nencompasses a diverse range of tasks, including multimodal content generation\n(interleaved image-text data), biomedical image captioning and generation,\nvisual chatting, and report understanding. These tasks span diverse medical\ndomains such as radiology and histopathology. Subsequently, we fine-tune a\nmixed-modal foundation model on the MedMax dataset, achieving significant\nperformance improvements: a 26% gain over the Chameleon model and an 18.3%\nimprovement over GPT-4o across 12 downstream biomedical visual\nquestion-answering tasks. Additionally, we introduce a unified evaluation suite\nfor biomedical tasks, providing a robust framework to guide the development of\nnext-generation mixed-modal biomedical AI assistants.\n","authors":["Hritik Bansal","Daniel Israel","Siyan Zhao","Shufan Li","Tung Nguyen","Aditya Grover"],"pdf_url":"https://arxiv.org/pdf/2412.12661v1.pdf","comment":"12 figures, 15 tables"},{"id":"http://arxiv.org/abs/2412.12660v1","updated":"2024-12-17T08:29:13Z","published":"2024-12-17T08:29:13Z","title":"SEG-SAM: Semantic-Guided SAM for Unified Medical Image Segmentation","summary":"  Recently, developing unified medical image segmentation models gains\nincreasing attention, especially with the advent of the Segment Anything Model\n(SAM). SAM has shown promising binary segmentation performance in natural\ndomains, however, transferring it to the medical domain remains challenging, as\nmedical images often possess substantial inter-category overlaps. To address\nthis, we propose the SEmantic-Guided SAM (SEG-SAM), a unified medical\nsegmentation model that incorporates semantic medical knowledge to enhance\nmedical segmentation performance. First, to avoid the potential conflict\nbetween binary and semantic predictions, we introduce a semantic-aware decoder\nindependent of SAM's original decoder, specialized for both semantic\nsegmentation on the prompted object and classification on unprompted objects in\nimages. To further enhance the model's semantic understanding, we solicit key\ncharacteristics of medical categories from large language models and\nincorporate them into SEG-SAM through a text-to-vision semantic module,\nadaptively transferring the language information into the visual segmentation\ntask. In the end, we introduce the cross-mask spatial alignment strategy to\nencourage greater overlap between the predicted masks from SEG-SAM's two\ndecoders, thereby benefiting both predictions. Extensive experiments\ndemonstrate that SEG-SAM outperforms state-of-the-art SAM-based methods in\nunified binary medical segmentation and task-specific methods in semantic\nmedical segmentation, showcasing promising results and potential for broader\nmedical applications.\n","authors":["Shuangping Huang","Hao Liang","Qingfeng Wang","Chulong Zhong","Zijian Zhou","Miaojing Shi"],"pdf_url":"https://arxiv.org/pdf/2412.12660v1.pdf","comment":"12 pages, 3 figures"},{"id":"http://arxiv.org/abs/2412.12654v1","updated":"2024-12-17T08:21:46Z","published":"2024-12-17T08:21:46Z","title":"CALA: A Class-Aware Logit Adapter for Few-Shot Class-Incremental\n  Learning","summary":"  Few-Shot Class-Incremental Learning (FSCIL) defines a practical but\nchallenging task where models are required to continuously learn novel concepts\nwith only a few training samples. Due to data scarcity, existing FSCIL methods\nresort to training a backbone with abundant base data and then keeping it\nfrozen afterward. However, the above operation often causes the backbone to\noverfit to base classes while overlooking the novel ones, leading to severe\nconfusion between them. To address this issue, we propose Class-Aware Logit\nAdapter (CALA). Our method involves a lightweight adapter that learns to\nrectify biased predictions through a pseudo-incremental learning paradigm. In\nthe real FSCIL process, we use the learned adapter to dynamically generate\nrobust balancing factors. These factors can adjust confused novel instances\nback to their true label space based on their similarity to base classes.\nSpecifically, when confusion is more likely to occur in novel instances that\nclosely resemble base classes, greater rectification is required. Notably, CALA\noperates on the classifier level, preserving the original feature space, thus\nit can be flexibly plugged into most of the existing FSCIL works for improved\nperformance. Experiments on three benchmark datasets consistently validate the\neffectiveness and flexibility of CALA. Codes will be available upon acceptance.\n","authors":["Chengyan Liu","Linglan Zhao","Fan Lyu","Kaile Du","Fuyuan Hu","Tao Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.12654v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2405.05095v4","updated":"2024-12-17T08:12:37Z","published":"2024-05-08T14:44:34Z","title":"Approximation properties relative to continuous scale space for hybrid\n  discretizations of Gaussian derivative operators","summary":"  This paper presents an analysis of properties of two hybrid discretization\nmethods for Gaussian derivatives, based on convolutions with either the\nnormalized sampled Gaussian kernel or the integrated Gaussian kernel followed\nby central differences. The motivation for studying these discretization\nmethods is that in situations when multiple spatial derivatives of different\norder are needed at the same scale level, they can be computed significantly\nmore efficiently compared to more direct derivative approximations based on\nexplicit convolutions with either sampled Gaussian kernels or integrated\nGaussian kernels.\n  While these computational benefits do also hold for the genuinely discrete\napproach for computing discrete analogues of Gaussian derivatives, based on\nconvolution with the discrete analogue of the Gaussian kernel followed by\ncentral differences, the underlying mathematical primitives for the discrete\nanalogue of the Gaussian kernel, in terms of modified Bessel functions of\ninteger order, may not be available in certain frameworks for image processing,\nsuch as when performing deep learning based on scale-parameterized filters in\nterms of Gaussian derivatives, with learning of the scale levels.\n  In this paper, we present a characterization of the properties of these\nhybrid discretization methods, in terms of quantitative performance measures\nconcerning the amount of spatial smoothing that they imply, as well as the\nrelative consistency of scale estimates obtained from scale-invariant feature\ndetectors with automatic scale selection, with an emphasis on the behaviour for\nvery small values of the scale parameter, which may differ significantly from\ncorresponding results obtained from the fully continuous scale-space theory, as\nwell as between different types of discretization methods.\n","authors":["Tony Lindeberg"],"pdf_url":"https://arxiv.org/pdf/2405.05095v4.pdf","comment":"23 pages, 9 figures. arXiv admin note: text overlap with\n  arXiv:2311.11317"},{"id":"http://arxiv.org/abs/2408.13854v2","updated":"2024-12-17T08:12:25Z","published":"2024-08-25T14:47:25Z","title":"Tangram: Benchmark for Evaluating Geometric Element Recognition in Large\n  Multimodal Models","summary":"  Significant advancements in Large Multimodal Models (LMMs) have enabled them\nto tackle complex problems involving visual-mathematical reasoning. However,\ntheir ability to identify geometric elements remains underexplored. To address\nthis gap, we introduce Tangram, a novel benchmark designed to evaluate the\nperformance of LMMs on geometric element recognition. Tangram comprises 1,080\ndiverse geometric diagrams sourced from primary and secondary school exams,\ncompetitions, and textbooks, ranging from simple geometric shapes to complex\ncombinations. Each diagram is paired with four questions, resulting in 4,320\nvisual-question-answer pairs. Unlike existing benchmarks that emphasize\nhigher-level cognition and reasoning, Tangram focuses on understanding\ngeometric elements, requiring models to perform a ``simple yet challenging\"\ncounting task. Systematic evaluation of 13 prominent LMMs, such as GPT-4o and\nClaude 3.5 Sonnet, reveals that these models face significant challenges even\nin seemingly straightforward tasks. The top-performing model achieves an\naccuracy of only 53.0%, highlighting a substantial gap compared to human\nperformance. These findings underscore the limitations of current multimodal AI\nsystems in handling basic perception tasks and serve to inspire the development\nof the next generation of expert-level multimodal foundational models. The data\nand code will be released soon.\n","authors":["Chao Zhang","Jiamin Tang","Jing Xiao"],"pdf_url":"https://arxiv.org/pdf/2408.13854v2.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2412.11024v2","updated":"2024-12-17T07:45:29Z","published":"2024-12-15T02:35:31Z","title":"Exploring Diffusion and Flow Matching Under Generator Matching","summary":"  In this paper, we present a comprehensive theoretical comparison of diffusion\nand flow matching under the Generator Matching framework. Despite their\napparent differences, both diffusion and flow matching can be viewed under the\nunified framework of Generator Matching. By recasting both diffusion and flow\nmatching under the same generative Markov framework, we provide theoretical\ninsights into why flow matching models can be more robust empirically and how\nnovel model classes can be constructed by mixing deterministic and stochastic\ncomponents. Our analysis offers a fresh perspective on the relationships\nbetween state-of-the-art generative modeling paradigms.\n","authors":["Zeeshan Patel","James DeLoye","Lance Mathias"],"pdf_url":"https://arxiv.org/pdf/2412.11024v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12629v1","updated":"2024-12-17T07:44:25Z","published":"2024-12-17T07:44:25Z","title":"a2z-1 for Multi-Disease Detection in Abdomen-Pelvis CT: External\n  Validation and Performance Analysis Across 21 Conditions","summary":"  We present a comprehensive evaluation of a2z-1, an artificial intelligence\n(AI) model designed to analyze abdomen-pelvis CT scans for 21 time-sensitive\nand actionable findings. Our study focuses on rigorous assessment of the\nmodel's performance and generalizability. Large-scale retrospective analysis\ndemonstrates an average AUC of 0.931 across 21 conditions. External validation\nacross two distinct health systems confirms consistent performance (AUC 0.923),\nestablishing generalizability to different evaluation scenarios, with notable\nperformance in critical findings such as small bowel obstruction (AUC 0.958)\nand acute pancreatitis (AUC 0.961). Subgroup analysis shows consistent accuracy\nacross patient sex, age groups, and varied imaging protocols, including\ndifferent slice thicknesses and contrast administration types. Comparison of\nhigh-confidence model outputs to radiologist reports reveals instances where\na2z-1 identified overlooked findings, suggesting potential for quality\nassurance applications.\n","authors":["Pranav Rajpurkar","Julian N. Acosta","Siddhant Dogra","Jaehwan Jeong","Deepanshu Jindal","Michael Moritz","Samir Rajpurkar"],"pdf_url":"https://arxiv.org/pdf/2412.12629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12628v1","updated":"2024-12-17T07:43:36Z","published":"2024-12-17T07:43:36Z","title":"Dense Audio-Visual Event Localization under Cross-Modal Consistency and\n  Multi-Temporal Granularity Collaboration","summary":"  In the field of audio-visual learning, most research tasks focus exclusively\non short videos. This paper focuses on the more practical Dense Audio-Visual\nEvent Localization (DAVEL) task, advancing audio-visual scene understanding for\nlonger, {untrimmed} videos. This task seeks to identify and temporally pinpoint\nall events simultaneously occurring in both audio and visual streams.\nTypically, each video encompasses dense events of multiple classes, which may\noverlap on the timeline, each exhibiting varied durations. Given these\nchallenges, effectively exploiting the audio-visual relations and the temporal\nfeatures encoded at various granularities becomes crucial. To address these\nchallenges, we introduce a novel \\ul{CC}Net, comprising two core modules: the\nCross-Modal Consistency \\ul{C}ollaboration (CMCC) and the Multi-Temporal\nGranularity \\ul{C}ollaboration (MTGC). Specifically, the CMCC module contains\ntwo branches: a cross-modal interaction branch and a temporal consistency-gated\nbranch. The former branch facilitates the aggregation of consistent event\nsemantics across modalities through the encoding of audio-visual relations,\nwhile the latter branch guides one modality's focus to pivotal event-relevant\ntemporal areas as discerned in the other modality. The MTGC module includes a\ncoarse-to-fine collaboration block and a fine-to-coarse collaboration block,\nproviding bidirectional support among coarse- and fine-grained temporal\nfeatures. Extensive experiments on the UnAV-100 dataset validate our module\ndesign, resulting in a new state-of-the-art performance in dense audio-visual\nevent localization. The code is available at\n\\url{https://github.com/zzhhfut/CCNet-AAAI2025}.\n","authors":["Ziheng Zhou","Jinxing Zhou","Wei Qian","Shengeng Tang","Xiaojun Chang","Dan Guo"],"pdf_url":"https://arxiv.org/pdf/2412.12628v1.pdf","comment":"Accepted by AAAI 2025. Project page:\n  https://github.com/zzhhfut/CCNet-AAAI2025. Jinxing Zhou and Dan Guo are the\n  corresponding authors"},{"id":"http://arxiv.org/abs/2412.12626v1","updated":"2024-12-17T07:41:06Z","published":"2024-12-17T07:41:06Z","title":"Improving the Transferability of 3D Point Cloud Attack via\n  Spectral-aware Admix and Optimization Designs","summary":"  Deep learning models for point clouds have shown to be vulnerable to\nadversarial attacks, which have received increasing attention in various\nsafety-critical applications such as autonomous driving, robotics, and\nsurveillance. Existing 3D attackers generally design various attack strategies\nin the white-box setting, requiring the prior knowledge of 3D model details.\nHowever, real-world 3D applications are in the black-box setting, where we can\nonly acquire the outputs of the target classifier. Although few recent works\ntry to explore the black-box attack, they still achieve limited attack success\nrates (ASR). To alleviate this issue, this paper focuses on attacking the 3D\nmodels in a transfer-based black-box setting, where we first carefully design\nadversarial examples in a white-box surrogate model and then transfer them to\nattack other black-box victim models. Specifically, we propose a novel\nSpectral-aware Admix with Augmented Optimization method (SAAO) to improve the\nadversarial transferability. In particular, since traditional Admix strategy\nare deployed in the 2D domain that adds pixel-wise images for perturbing, we\ncan not directly follow it to merge point clouds in coordinate domain as it\nwill destroy the geometric shapes. Therefore, we design spectral-aware fusion\nthat performs Graph Fourier Transform (GFT) to get spectral features of the\npoint clouds and add them in the spectral domain. Afterward, we run a few steps\nwith spectral-aware weighted Admix to select better optimization paths as well\nas to adjust corresponding learning weights. At last, we run more steps to\ngenerate adversarial spectral feature along the optimization path and perform\nInverse-GFT on the adversarial spectral feature to obtain the adversarial\nexample in the data domain. Experiments show that our SAAO achieves better\ntransferability compared to existing 3D attack methods.\n","authors":["Shiyu Hu","Daizong Liu","Wei Hu"],"pdf_url":"https://arxiv.org/pdf/2412.12626v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12620v1","updated":"2024-12-17T07:33:07Z","published":"2024-12-17T07:33:07Z","title":"Multi-Domain Features Guided Supervised Contrastive Learning for Radar\n  Target Detection","summary":"  Detecting small targets in sea clutter is challenging due to dynamic maritime\nconditions. Existing solutions either model sea clutter for detection or\nextract target features based on clutter-target echo differences, including\nstatistical and deep features. While more common, the latter often excels in\ncontrolled scenarios but struggles with robust detection and generalization in\ndiverse environments, limiting practical use. In this letter, we propose a\nmulti-domain features guided supervised contrastive learning (MDFG_SCL) method,\nwhich integrates statistical features derived from multi-domain differences\nwith deep features obtained through supervised contrastive learning, thereby\ncapturing both low-level domain-specific variations and high-level semantic\ninformation. This comprehensive feature integration enables the model to\neffectively distinguish between small targets and sea clutter, even under\nchallenging conditions. Experiments conducted on real-world datasets\ndemonstrate that the proposed shallow-to-deep detector not only achieves\neffective identification of small maritime targets but also maintains superior\ndetection performance across varying sea conditions, outperforming the\nmainstream unsupervised contrastive learning and supervised contrastive\nlearning methods.\n","authors":["Junjie Wang","Yuze Gao","Dongying Li","Wenxian Yu"],"pdf_url":"https://arxiv.org/pdf/2412.12620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11248v2","updated":"2024-12-17T07:31:27Z","published":"2024-12-15T16:54:53Z","title":"Multimodal Class-aware Semantic Enhancement Network for Audio-Visual\n  Video Parsing","summary":"  The Audio-Visual Video Parsing task aims to recognize and temporally localize\nall events occurring in either the audio or visual stream, or both. Capturing\naccurate event semantics for each audio/visual segment is vital. Prior works\ndirectly utilize the extracted holistic audio and visual features for intra-\nand cross-modal temporal interactions. However, each segment may contain\nmultiple events, resulting in semantically mixed holistic features that can\nlead to semantic interference during intra- or cross-modal interactions: the\nevent semantics of one segment may incorporate semantics of unrelated events\nfrom other segments. To address this issue, our method begins with a\nClass-Aware Feature Decoupling (CAFD) module, which explicitly decouples the\nsemantically mixed features into distinct class-wise features, including\nmultiple event-specific features and a dedicated background feature. The\ndecoupled class-wise features enable our model to selectively aggregate useful\nsemantics for each segment from clearly matched classes contained in other\nsegments, preventing semantic interference from irrelevant classes.\nSpecifically, we further design a Fine-Grained Semantic Enhancement module for\nencoding intra- and cross-modal relations. It comprises a Segment-wise Event\nCo-occurrence Modeling (SECM) block and a Local-Global Semantic Fusion (LGSF)\nblock. The SECM exploits inter-class dependencies of concurrent events within\nthe same timestamp with the aid of a new event co-occurrence loss. The LGSF\nfurther enhances the event semantics of each segment by incorporating relevant\nsemantics from more informative global video features. Extensive experiments\nvalidate the effectiveness of the proposed modules and loss functions,\nresulting in a new state-of-the-art parsing performance.\n","authors":["Pengcheng Zhao","Jinxing Zhou","Yang Zhao","Dan Guo","Yanxiang Chen"],"pdf_url":"https://arxiv.org/pdf/2412.11248v2.pdf","comment":"Accepted by AAAI-2025"},{"id":"http://arxiv.org/abs/2412.12617v1","updated":"2024-12-17T07:30:09Z","published":"2024-12-17T07:30:09Z","title":"PO3AD: Predicting Point Offsets toward Better 3D Point Cloud Anomaly\n  Detection","summary":"  Point cloud anomaly detection under the anomaly-free setting poses\nsignificant challenges as it requires accurately capturing the features of 3D\nnormal data to identify deviations indicative of anomalies. Current efforts\nfocus on devising reconstruction tasks, such as acquiring normal data\nrepresentations by restoring normal samples from altered, pseudo-anomalous\ncounterparts. Our findings reveal that distributing attention equally across\nnormal and pseudo-anomalous data tends to dilute the model's focus on anomalous\ndeviations. The challenge is further compounded by the inherently disordered\nand sparse nature of 3D point cloud data. In response to those predicaments, we\nintroduce an innovative approach that emphasizes learning point offsets,\ntargeting more informative pseudo-abnormal points, thus fostering more\neffective distillation of normal data representations. We also have crafted an\naugmentation technique that is steered by normal vectors, facilitating the\ncreation of credible pseudo anomalies that enhance the efficiency of the\ntraining process. Our comprehensive experimental evaluation on the\nAnomaly-ShapeNet and Real3D-AD datasets evidences that our proposed method\noutperforms existing state-of-the-art approaches, achieving an average\nenhancement of 9.0% and 1.4% in the AUC-ROC detection metric across these\ndatasets, respectively.\n","authors":["Jianan Ye","Weiguang Zhao","Xi Yang","Guangliang Cheng","Kaizhu Huang"],"pdf_url":"https://arxiv.org/pdf/2412.12617v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.20962v3","updated":"2024-12-17T07:13:38Z","published":"2024-07-30T16:43:24Z","title":"MMTrail: A Multimodal Trailer Video Dataset with Language and Music\n  Descriptions","summary":"  Massive multi-modality datasets play a significant role in facilitating the\nsuccess of large video-language models. However, current video-language\ndatasets primarily provide text descriptions for visual frames, considering\naudio to be weakly related information. They usually overlook exploring the\npotential of inherent audio-visual correlation, leading to monotonous\nannotation within each modality instead of comprehensive and precise\ndescriptions. Such ignorance results in the difficulty of multiple\ncross-modality studies. To fulfill this gap, we present MMTrail, a large-scale\nmulti-modality video-language dataset incorporating more than 20M trailer clips\nwith visual captions, and 2M high-quality clips with multimodal captions.\nTrailers preview full-length video works and integrate context, visual frames,\nand background music. In particular, the trailer has two main advantages: (1)\nthe topics are diverse, and the content characters are of various types, e.g.,\nfilm, news, and gaming. (2) the corresponding background music is\ncustom-designed, making it more coherent with the visual context. Upon these\ninsights, we propose a systemic captioning framework, achieving various\nmodality annotations with more than 27.1k hours of trailer videos. Here, to\nensure the caption retains music perspective while preserving the authority of\nvisual context, we leverage the advanced LLM to merge all annotations\nadaptively. In this fashion, our MMtrail dataset potentially paves the path for\nfine-grained large multimodal-language model training. In experiments, we\nprovide evaluation metrics and benchmark results on our dataset, demonstrating\nthe high quality of our annotation and its effectiveness for model training.\n","authors":["Xiaowei Chi","Yatian Wang","Aosong Cheng","Pengjun Fang","Zeyue Tian","Yingqing He","Zhaoyang Liu","Xingqun Qi","Jiahao Pan","Rongyu Zhang","Mengfei Li","Ruibin Yuan","Yanbing Jiang","Wei Xue","Wenhan Luo","Qifeng Chen","Shanghang Zhang","Qifeng Liu","Yike Guo"],"pdf_url":"https://arxiv.org/pdf/2407.20962v3.pdf","comment":"15 Pages. Dataset report"},{"id":"http://arxiv.org/abs/2410.05993v3","updated":"2024-12-17T07:13:18Z","published":"2024-10-08T12:44:57Z","title":"Aria: An Open Multimodal Native Mixture-of-Experts Model","summary":"  Information comes in diverse modalities. Multimodal native AI models are\nessential to integrate real-world information and deliver comprehensive\nunderstanding. While proprietary multimodal native models exist, their lack of\nopenness imposes obstacles for adoptions, let alone adaptations. To fill this\ngap, we introduce Aria, an open multimodal native model with best-in-class\nperformance across a wide range of multimodal, language, and coding tasks. Aria\nis a mixture-of-expert model with 3.9B and 3.5B activated parameters per visual\ntoken and text token, respectively. It outperforms Pixtral-12B and\nLlama3.2-11B, and is competitive against the best proprietary models on various\nmultimodal tasks. We pre-train Aria from scratch following a 4-stage pipeline,\nwhich progressively equips the model with strong capabilities in language\nunderstanding, multimodal understanding, long context window, and instruction\nfollowing. We open-source the model weights along with a codebase that\nfacilitates easy adoptions and adaptations of Aria in real-world applications.\n","authors":["Dongxu Li","Yudong Liu","Haoning Wu","Yue Wang","Zhiqi Shen","Bowen Qu","Xinyao Niu","Fan Zhou","Chengen Huang","Yanpeng Li","Chongyan Zhu","Xiaoyi Ren","Chao Li","Yifan Ye","Lihuan Zhang","Hanshu Yan","Guoyin Wang","Bei Chen","Junnan Li"],"pdf_url":"https://arxiv.org/pdf/2410.05993v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12606v1","updated":"2024-12-17T07:06:10Z","published":"2024-12-17T07:06:10Z","title":"Multi-Dimensional Insights: Benchmarking Real-World Personalization in\n  Large Multimodal Models","summary":"  The rapidly developing field of large multimodal models (LMMs) has led to the\nemergence of diverse models with remarkable capabilities. However, existing\nbenchmarks fail to comprehensively, objectively and accurately evaluate whether\nLMMs align with the diverse needs of humans in real-world scenarios. To bridge\nthis gap, we propose the Multi-Dimensional Insights (MDI) benchmark, which\nincludes over 500 images covering six common scenarios of human life. Notably,\nthe MDI-Benchmark offers two significant advantages over existing evaluations:\n(1) Each image is accompanied by two types of questions: simple questions to\nassess the model's understanding of the image, and complex questions to\nevaluate the model's ability to analyze and reason beyond basic content. (2)\nRecognizing that people of different age groups have varying needs and\nperspectives when faced with the same scenario, our benchmark stratifies\nquestions into three age categories: young people, middle-aged people, and\nolder people. This design allows for a detailed assessment of LMMs'\ncapabilities in meeting the preferences and needs of different age groups. With\nMDI-Benchmark, the strong model like GPT-4o achieve 79% accuracy on age-related\ntasks, indicating that existing LMMs still have considerable room for\nimprovement in addressing real-world applications. Looking ahead, we anticipate\nthat the MDI-Benchmark will open new pathways for aligning real-world\npersonalization in LMMs. The MDI-Benchmark data and evaluation code are\navailable at https://mdi-benchmark.github.io/\n","authors":["YiFan Zhang","Shanglin Lei","Runqi Qiao","Zhuoma GongQue","Xiaoshuai Song","Guanting Dong","Qiuna Tan","Zhe Wei","Peiqing Yang","Ye Tian","Yadong Xue","Xiaofei Wang","Honggang Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.12606v1.pdf","comment":"33 pages, 33 figures, Work in progress"},{"id":"http://arxiv.org/abs/2412.12603v1","updated":"2024-12-17T07:00:07Z","published":"2024-12-17T07:00:07Z","title":"RemoteTrimmer: Adaptive Structural Pruning for Remote Sensing Image\n  Classification","summary":"  Since high resolution remote sensing image classification often requires a\nrelatively high computation complexity, lightweight models tend to be practical\nand efficient. Model pruning is an effective method for model compression.\nHowever, existing methods rarely take into account the specificity of remote\nsensing images, resulting in significant accuracy loss after pruning. To this\nend, we propose an effective structural pruning approach for remote sensing\nimage classification. Specifically, a pruning strategy that amplifies the\ndifferences in channel importance of the model is introduced. Then an adaptive\nmining loss function is designed for the fine-tuning process of the pruned\nmodel. Finally, we conducted experiments on two remote sensing classification\ndatasets. The experimental results demonstrate that our method achieves minimal\naccuracy loss after compressing remote sensing classification models, achieving\nstate-of-the-art (SoTA) performance.\n","authors":["Guanwenjie Zou","Liang Yao","Fan Liu","Chuanyi Zhang","Xin Li","Ning Chen","Shengxiang Xu","Jun Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.12603v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12596v1","updated":"2024-12-17T06:54:54Z","published":"2024-12-17T06:54:54Z","title":"OpenViewer: Openness-Aware Multi-View Learning","summary":"  Multi-view learning methods leverage multiple data sources to enhance\nperception by mining correlations across views, typically relying on predefined\ncategories. However, deploying these models in real-world scenarios presents\ntwo primary openness challenges. 1) Lack of Interpretability: The integration\nmechanisms of multi-view data in existing black-box models remain poorly\nexplained; 2) Insufficient Generalization: Most models are not adapted to\nmulti-view scenarios involving unknown categories. To address these challenges,\nwe propose OpenViewer, an openness-aware multi-view learning framework with\ntheoretical support. This framework begins with a Pseudo-Unknown Sample\nGeneration Mechanism to efficiently simulate open multi-view environments and\npreviously adapt to potential unknown samples. Subsequently, we introduce an\nExpression-Enhanced Deep Unfolding Network to intuitively promote\ninterpretability by systematically constructing functional prior-mapping\nmodules and effectively providing a more transparent integration mechanism for\nmulti-view data. Additionally, we establish a Perception-Augmented Open-Set\nTraining Regime to significantly enhance generalization by precisely boosting\nconfidences for known categories and carefully suppressing inappropriate\nconfidences for unknown ones. Experimental results demonstrate that OpenViewer\neffectively addresses openness challenges while ensuring recognition\nperformance for both known and unknown samples. The code is released at\nhttps://github.com/dushide/OpenViewer.\n","authors":["Shide Du","Zihan Fang","Yanchao Tan","Changwei Wang","Shiping Wang","Wenzhong Guo"],"pdf_url":"https://arxiv.org/pdf/2412.12596v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2411.16657v2","updated":"2024-12-17T06:52:46Z","published":"2024-11-25T18:41:56Z","title":"DreamRunner: Fine-Grained Storytelling Video Generation with\n  Retrieval-Augmented Motion Adaptation","summary":"  Storytelling video generation (SVG) has recently emerged as a task to create\nlong, multi-motion, multi-scene videos that consistently represent the story\ndescribed in the input text script. SVG holds great potential for diverse\ncontent creation in media and entertainment; however, it also presents\nsignificant challenges: (1) objects must exhibit a range of fine-grained,\ncomplex motions, (2) multiple objects need to appear consistently across\nscenes, and (3) subjects may require multiple motions with seamless transitions\nwithin a single scene. To address these challenges, we propose DreamRunner, a\nnovel story-to-video generation method: First, we structure the input script\nusing a large language model (LLM) to facilitate both coarse-grained scene\nplanning as well as fine-grained object-level layout and motion planning. Next,\nDreamRunner presents retrieval-augmented test-time adaptation to capture target\nmotion priors for objects in each scene, supporting diverse motion\ncustomization based on retrieved videos, thus facilitating the generation of\nnew videos with complex, scripted motions. Lastly, we propose a novel\nspatial-temporal region-based 3D attention and prior injection module SR3AI for\nfine-grained object-motion binding and frame-by-frame semantic control. We\ncompare DreamRunner with various SVG baselines, demonstrating state-of-the-art\nperformance in character consistency, text alignment, and smooth transitions.\nAdditionally, DreamRunner exhibits strong fine-grained condition-following\nability in compositional text-to-video generation, significantly outperforming\nbaselines on T2V-ComBench. Finally, we validate DreamRunner's robust ability to\ngenerate multi-object interactions with qualitative examples.\n","authors":["Zun Wang","Jialu Li","Han Lin","Jaehong Yoon","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2411.16657v2.pdf","comment":"Project website: https://zunwang1.github.io/DreamRunner"},{"id":"http://arxiv.org/abs/2412.12594v1","updated":"2024-12-17T06:50:23Z","published":"2024-12-17T06:50:23Z","title":"A Simple and Efficient Baseline for Zero-Shot Generative Classification","summary":"  Large diffusion models have become mainstream generative models in both\nacademic studies and industrial AIGC applications. Recently, a number of works\nfurther explored how to employ the power of large diffusion models as zero-shot\nclassifiers. While recent zero-shot diffusion-based classifiers have made\nperformance advancement on benchmark datasets, they still suffered badly from\nextremely slow classification speed (e.g., ~1000 seconds per classifying single\nimage on ImageNet). The extremely slow classification speed strongly prohibits\nexisting zero-shot diffusion-based classifiers from practical applications. In\nthis paper, we propose an embarrassingly simple and efficient zero-shot\nGaussian Diffusion Classifiers (GDC) via pretrained text-to-image diffusion\nmodels and DINOv2. The proposed GDC can not only significantly surpass previous\nzero-shot diffusion-based classifiers by over 10 points (61.40% - 71.44%) on\nImageNet, but also accelerate more than 30000 times (1000 - 0.03 seconds)\nclassifying a single image on ImageNet. Additionally, it provides probability\ninterpretation of the results. Our extensive experiments further demonstrate\nthat GDC can achieve highly competitive zero-shot classification performance\nover various datasets and can promisingly self-improve with stronger diffusion\nmodels. To the best of our knowledge, the proposed GDC is the first zero-shot\ndiffusionbased classifier that exhibits both competitive accuracy and practical\nefficiency.\n","authors":["Zipeng Qi","Buhua Liu","Shiyan Zhang","Bao Li","Zhiqiang Xu","Haoyi Xiong","Zeke Xie"],"pdf_url":"https://arxiv.org/pdf/2412.12594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10638v2","updated":"2024-12-17T06:48:10Z","published":"2024-06-15T13:58:26Z","title":"Unveiling the Ignorance of MLLMs: Seeing Clearly, Answering Incorrectly","summary":"  Multimodal Large Language Models (MLLMs) have displayed remarkable\nperformance in multi-modal tasks, particularly in visual comprehension.\nHowever, we reveal that MLLMs often generate incorrect answers even when they\nunderstand the visual content. To this end, we manually construct a benchmark\nwith 12 categories and design evaluation metrics that assess the degree of\nerror in MLLM responses even when the visual content is seemingly understood.\nBased on this benchmark, we test 15 leading MLLMs and analyze the distribution\nof attention maps and logits of some MLLMs. Our investigation identifies two\nprimary issues: 1) most instruction tuning datasets predominantly feature\nquestions that 'directly' relate to the visual content, leading to a bias in\nMLLMs' responses to other indirect questions, and 2) MLLMs' attention to visual\ntokens is notably lower than to system and question tokens. We further observe\nthat attention scores between questions and visual tokens as well as the\nmodel's confidence in the answers are lower in response to misleading questions\nthan to straightforward ones. To address the first challenge, we introduce a\npaired positive and negative data construction pipeline to diversify the\ndataset. For the second challenge, we propose to enhance the model's focus on\nvisual content during decoding by refining the text and visual prompt. For the\ntext prompt, we propose a content guided refinement strategy that performs\npreliminary visual content analysis to generate structured information before\nanswering the question. Additionally, we employ a visual attention refinement\nstrategy that highlights question-relevant visual tokens to increase the\nmodel's attention to visual content that aligns with the question. Extensive\nexperiments demonstrate that these challenges can be significantly mitigated\nwith our proposed dataset and techniques.\n","authors":["Yexin Liu","Zhengyang Liang","Yueze Wang","Xianfeng Wu","Feilong Tang","Muyang He","Jian Li","Zheng Liu","Harry Yang","Sernam Lim","Bo Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.10638v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14358v2","updated":"2024-12-17T06:40:05Z","published":"2024-11-21T17:58:07Z","title":"InCrowd-VI: A Realistic Visual-Inertial Dataset for Evaluating SLAM in\n  Indoor Pedestrian-Rich Spaces for Human Navigation","summary":"  Simultaneous localization and mapping (SLAM) techniques can be used to\nnavigate the visually impaired, but the development of robust SLAM solutions\nfor crowded spaces is limited by the lack of realistic datasets. To address\nthis, we introduce InCrowd-VI, a novel visual-inertial dataset specifically\ndesigned for human navigation in indoor pedestrian-rich environments. Recorded\nusing Meta Aria Project glasses, it captures realistic scenarios without\nenvironmental control. InCrowd-VI features 58 sequences totaling a 5 km\ntrajectory length and 1.5 hours of recording time, including RGB, stereo\nimages, and IMU measurements. The dataset captures important challenges such as\npedestrian occlusions, varying crowd densities, complex layouts, and lighting\nchanges. Ground-truth trajectories, accurate to approximately 2 cm, are\nprovided in the dataset, originating from the Meta Aria project machine\nperception SLAM service. In addition, a semi-dense 3D point cloud of scenes is\nprovided for each sequence. The evaluation of state-of-the-art visual odometry\n(VO) and SLAM algorithms on InCrowd-VI revealed severe performance limitations\nin these realistic scenarios. Under challenging conditions, systems exceeded\nthe required localization accuracy of 0.5 meters and the 1\\% drift threshold,\nwith classical methods showing drift up to 5-10\\%. While deep learning-based\napproaches maintained high pose estimation coverage (>90\\%), they failed to\nachieve real-time processing speeds necessary for walking pace navigation.\nThese results demonstrate the need and value of a new dataset to advance SLAM\nresearch for visually impaired navigation in complex indoor environments. The\ndataset and associated tools are publicly available at\nhttps://incrowd-vi.cloudlab.zhaw.ch/.\n","authors":["Marziyeh Bamdad","Hans-Peter Hutter","Alireza Darvishy"],"pdf_url":"https://arxiv.org/pdf/2411.14358v2.pdf","comment":"24 pages, 8 figures, 6 tables"},{"id":"http://arxiv.org/abs/2412.08939v2","updated":"2024-12-17T06:30:00Z","published":"2024-12-12T05:01:17Z","title":"Dynamic Contrastive Knowledge Distillation for Efficient Image\n  Restoration","summary":"  Knowledge distillation (KD) is a valuable yet challenging approach that\nenhances a compact student network by learning from a high-performance but\ncumbersome teacher model. However, previous KD methods for image restoration\noverlook the state of the student during the distillation, adopting a fixed\nsolution space that limits the capability of KD. Additionally, relying solely\non L1-type loss struggles to leverage the distribution information of images.\nIn this work, we propose a novel dynamic contrastive knowledge distillation\n(DCKD) framework for image restoration. Specifically, we introduce dynamic\ncontrastive regularization to perceive the student's learning state and\ndynamically adjust the distilled solution space using contrastive learning.\nAdditionally, we also propose a distribution mapping module to extract and\nalign the pixel-level category distribution of the teacher and student models.\nNote that the proposed DCKD is a structure-agnostic distillation framework,\nwhich can adapt to different backbones and can be combined with methods that\noptimize upper-bound constraints to further enhance model performance.\nExtensive experiments demonstrate that DCKD significantly outperforms the\nstate-of-the-art KD methods across various image restoration tasks and\nbackbones.\n","authors":["Yunshuai Zhou","Junbo Qiao","Jincheng Liao","Wei Li","Simiao Li","Jiao Xie","Yunhang Shen","Jie Hu","Shaohui Lin"],"pdf_url":"https://arxiv.org/pdf/2412.08939v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09224v3","updated":"2024-12-17T06:21:52Z","published":"2024-12-12T12:26:08Z","title":"DASK: Distribution Rehearsing via Adaptive Style Kernel Learning for\n  Exemplar-Free Lifelong Person Re-Identification","summary":"  Lifelong person re-identification (LReID) is an important but challenging\ntask that suffers from catastrophic forgetting due to significant domain gaps\nbetween training steps. Existing LReID approaches typically rely on data replay\nand knowledge distillation to mitigate this issue. However, data replay methods\ncompromise data privacy by storing historical exemplars, while knowledge\ndistillation methods suffer from limited performance due to the cumulative\nforgetting of undistilled knowledge. To overcome these challenges, we propose a\nnovel paradigm that models and rehearses the distribution of the old domains to\nenhance knowledge consolidation during the new data learning, possessing a\nstrong anti-forgetting capacity without storing any exemplars. Specifically, we\nintroduce an exemplar-free LReID method called Distribution Rehearsing via\nAdaptive Style Kernel Learning (DASK). DASK includes a Distribution Rehearser\nLearning (DRL) mechanism that learns to transform arbitrary distribution data\ninto the current data style at each learning step. To enhance the style\ntransfer capacity of DRL, an Adaptive Kernel Prediction Network (AKPNet) is\nexplored to achieve an instance-specific distribution adjustment. Additionally,\nwe design a Distribution Rehearsing-driven LReID Training (DRRT) module, which\nrehearses old distribution based on the new data via the old AKPNet model,\nachieving effective new-old knowledge accumulation under a joint knowledge\nconsolidation scheme. Experimental results show our DASK outperforms the\nexisting methods by 3.6%-6.8% and 4.5%-6.5% on anti-forgetting and\ngeneralization capacity, respectively. Our code is available at\nhttps://github.com/zhoujiahuan1991/AAAI2025-LReID-DASK\n","authors":["Kunlun Xu","Chenghao Jiang","Peixi Xiong","Yuxin Peng","Jiahuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.09224v3.pdf","comment":"in Proceedings of the 39th AAAI Conference on Artificial Intelligence\n  (AAAI-25)"},{"id":"http://arxiv.org/abs/2412.00473v4","updated":"2024-12-17T06:09:08Z","published":"2024-11-30T13:21:15Z","title":"Jailbreak Large Vision-Language Models Through Multi-Modal Linkage","summary":"  With the significant advancement of Large Vision-Language Models (VLMs),\nconcerns about their potential misuse and abuse have grown rapidly. Previous\nstudies have highlighted VLMs' vulnerability to jailbreak attacks, where\ncarefully crafted inputs can lead the model to produce content that violates\nethical and legal standards. However, existing methods struggle against\nstate-of-the-art VLMs like GPT-4o, due to the over-exposure of harmful content\nand lack of stealthy malicious guidance. In this work, we propose a novel\njailbreak attack framework: Multi-Modal Linkage (MML) Attack. Drawing\ninspiration from cryptography, MML utilizes an encryption-decryption process\nacross text and image modalities to mitigate over-exposure of malicious\ninformation. To align the model's output with malicious intent covertly, MML\nemploys a technique called \"evil alignment\", framing the attack within a video\ngame production scenario. Comprehensive experiments demonstrate MML's\neffectiveness. Specifically, MML jailbreaks GPT-4o with attack success rates of\n97.80% on SafeBench, 98.81% on MM-SafeBench and 99.07% on HADES-Dataset. Our\ncode is available at https://github.com/wangyu-ovo/MML\n","authors":["Yu Wang","Xiaofei Zhou","Yichen Wang","Geyuan Zhang","Tianxing He"],"pdf_url":"https://arxiv.org/pdf/2412.00473v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16900v2","updated":"2024-12-17T06:06:02Z","published":"2023-11-28T15:58:13Z","title":"Lane-Keeping Control of Autonomous Vehicles Through a Soft-Constrained\n  Iterative LQR","summary":"  The accurate prediction of smooth steering inputs is crucial for automotive\napplications because control actions with jitter might cause the vehicle system\nto become unstable. To address this problem in automobile lane-keeping control\nwithout the use of additional smoothing algorithms, we developed a novel\nsoft-constrained iterative linear quadratic regulator (soft-CILQR) algorithm by\nintegrating CILQR algorithm and a model predictive control (MPC) constraint\nrelaxation method. We incorporated slack variables into the state and control\nbarrier functions of the soft-CILQR solver to soften the constraints in the\noptimization process such that control input stabilization can be achieved in a\ncomputationally simple manner. Two types of automotive lane-keeping experiments\n(numerical simulations and experiments involving challenging vision-based\nmaneuvers) were conducted with a linear system dynamics model to test the\nperformance of the proposed soft-CILQR algorithm, and its performance was\ncompared with that of the CILQR algorithm. In the numerical simulations, the\nsoft-CILQR and CILQR solvers managed to drive the system toward the reference\nstate asymptotically; however, the soft-CILQR solver obtained smooth steering\ninput trajectories more easily than did the CILQR solver under conditions\ninvolving additive disturbances. The results of the vision-based experiments in\nwhich an ego vehicle drove in perturbed TORCS environments with various road\nfriction settings were consistent with those of the numerical tests. The\nproposed soft-CILQR algorithm achieved an average runtime of 2.55 ms and is\nthus applicable for real-time autonomous driving scenarios.\n","authors":["Der-Hau Lee"],"pdf_url":"https://arxiv.org/pdf/2311.16900v2.pdf","comment":"17 figures, 13 pages"},{"id":"http://arxiv.org/abs/2412.12572v1","updated":"2024-12-17T06:03:42Z","published":"2024-12-17T06:03:42Z","title":"License Plate Detection and Character Recognition Using Deep Learning\n  and Font Evaluation","summary":"  License plate detection (LPD) is essential for traffic management, vehicle\ntracking, and law enforcement but faces challenges like variable lighting and\ndiverse font types, impacting accuracy. Traditionally reliant on image\nprocessing and machine learning, the field is now shifting towards deep\nlearning for its robust performance in various conditions. Current methods,\nhowever, often require tailoring to specific regional datasets. This paper\nproposes a dual deep learning strategy using a Faster R-CNN for detection and a\nCNN-RNN model with Connectionist Temporal Classification (CTC) loss and a\nMobileNet V3 backbone for recognition. This approach aims to improve model\nperformance using datasets from Ontario, Quebec, California, and New York\nState, achieving a recall rate of 92% on the Centre for Pattern Recognition and\nMachine Intelligence (CENPARMI) dataset and 90% on the UFPR-ALPR dataset. It\nincludes a detailed error analysis to identify the causes of false positives.\nAdditionally, the research examines the role of font features in license plate\n(LP) recognition, analyzing fonts like Driver Gothic, Dreadnought, California\nClarendon, and Zurich Extra Condensed with the OpenALPR system. It discovers\nsignificant performance discrepancies influenced by font characteristics,\noffering insights for future LPD system enhancements.\n  Keywords: Deep Learning, License Plate, Font Evaluation\n","authors":["Zahra Ebrahimi Vargoorani","Ching Yee Suen"],"pdf_url":"https://arxiv.org/pdf/2412.12572v1.pdf","comment":"12 pages, 5 figures. This is the pre-Springer final accepted version.\n  The final version is published in Springer, Lecture Notes in Computer Science\n  (LNCS), Volume 14731, 2024. Springer Version of Record"},{"id":"http://arxiv.org/abs/2412.12571v1","updated":"2024-12-17T06:03:05Z","published":"2024-12-17T06:03:05Z","title":"ChatDiT: A Training-Free Baseline for Task-Agnostic Free-Form Chatting\n  with Diffusion Transformers","summary":"  Recent research arXiv:2410.15027 arXiv:2410.23775 has highlighted the\ninherent in-context generation capabilities of pretrained diffusion\ntransformers (DiTs), enabling them to seamlessly adapt to diverse visual tasks\nwith minimal or no architectural modifications. These capabilities are unlocked\nby concatenating self-attention tokens across multiple input and target images,\ncombined with grouped and masked generation pipelines. Building upon this\nfoundation, we present ChatDiT, a zero-shot, general-purpose, and interactive\nvisual generation framework that leverages pretrained diffusion transformers in\ntheir original form, requiring no additional tuning, adapters, or\nmodifications. Users can interact with ChatDiT to create interleaved text-image\narticles, multi-page picture books, edit images, design IP derivatives, or\ndevelop character design settings, all through free-form natural language\nacross one or more conversational rounds. At its core, ChatDiT employs a\nmulti-agent system comprising three key components: an Instruction-Parsing\nagent that interprets user-uploaded images and instructions, a\nStrategy-Planning agent that devises single-step or multi-step generation\nactions, and an Execution agent that performs these actions using an in-context\ntoolkit of diffusion transformers. We thoroughly evaluate ChatDiT on IDEA-Bench\narXiv:2412.11767, comprising 100 real-world design tasks and 275 cases with\ndiverse instructions and varying numbers of input and target images. Despite\nits simplicity and training-free approach, ChatDiT surpasses all competitors,\nincluding those specifically designed and trained on extensive multi-task\ndatasets. We further identify key limitations of pretrained DiTs in zero-shot\nadapting to tasks. We release all code, agents, results, and intermediate\noutputs to facilitate further research at https://github.com/ali-vilab/ChatDiT\n","authors":["Lianghua Huang","Wei Wang","Zhi-Fan Wu","Yupeng Shi","Chen Liang","Tong Shen","Han Zhang","Huanzhang Dou","Yu Liu","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.12571v1.pdf","comment":"Tech report. Project page: https://ali-vilab.github.io/ChatDiT-Page/"},{"id":"http://arxiv.org/abs/2406.16469v2","updated":"2024-12-17T05:51:01Z","published":"2024-06-24T09:18:15Z","title":"Evaluating Visual and Cultural Interpretation: The K-Viscuit Benchmark\n  with Human-VLM Collaboration","summary":"  To create culturally inclusive vision-language models (VLMs), developing a\nbenchmark that tests their ability to address culturally relevant questions is\nessential. Existing approaches typically rely on human annotators, making the\nprocess labor-intensive and creating a cognitive burden in generating diverse\nquestions. To address this, we propose a semi-automated framework for\nconstructing cultural VLM benchmarks, specifically targeting multiple-choice\nQA. This framework combines human-VLM collaboration, where VLMs generate\nquestions based on guidelines, a small set of annotated examples, and relevant\nknowledge, followed by a verification process by native speakers. We\ndemonstrate the effectiveness of this framework through the creation of\nK-Viscuit, a dataset focused on Korean culture. Our experiments on this dataset\nreveal that open-source models lag behind proprietary ones in understanding\nKorean culture, highlighting key areas for improvement. We also present a\nseries of further analyses, including human evaluation, augmenting VLMs with\nexternal knowledge, and the evaluation beyond multiple-choice QA. Our dataset\nis available at https://huggingface.co/datasets/ddehun/k-viscuit.\n","authors":["Yujin Baek","ChaeHun Park","Jaeseok Kim","Yu-Jung Heo","Du-Seong Chang","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2406.16469v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12566v1","updated":"2024-12-17T05:49:26Z","published":"2024-12-17T05:49:26Z","title":"ITP: Instance-Aware Test Pruning for Out-of-Distribution Detection","summary":"  Out-of-distribution (OOD) detection is crucial for ensuring the reliable\ndeployment of deep models in real-world scenarios. Recently, from the\nperspective of over-parameterization, a series of methods leveraging weight\nsparsification techniques have shown promising performance. These methods\ntypically focus on selecting important parameters for in-distribution (ID) data\nto reduce the negative impact of redundant parameters on OOD detection.\nHowever, we empirically find that these selected parameters may behave\noverconfidently toward OOD data and hurt OOD detection. To address this issue,\nwe propose a simple yet effective post-hoc method called Instance-aware Test\nPruning (ITP), which performs OOD detection by considering both coarse-grained\nand fine-grained levels of parameter pruning. Specifically, ITP first estimates\nthe class-specific parameter contribution distribution by exploring the ID\ndata. By using the contribution distribution, ITP conducts coarse-grained\npruning to eliminate redundant parameters. More importantly, ITP further adopts\na fine-grained test pruning process based on the right-tailed Z-score test,\nwhich can adaptively remove instance-level overconfident parameters. Finally,\nITP derives OOD scores from the pruned model to achieve more reliable\npredictions. Extensive experiments on widely adopted benchmarks verify the\neffectiveness of ITP, demonstrating its competitive performance.\n","authors":["Haonan Xu","Yang Yang"],"pdf_url":"https://arxiv.org/pdf/2412.12566v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12565v1","updated":"2024-12-17T05:49:16Z","published":"2024-12-17T05:49:16Z","title":"PBVS 2024 Solution: Self-Supervised Learning and Sampling Strategies for\n  SAR Classification in Extreme Long-Tail Distribution","summary":"  The Multimodal Learning Workshop (PBVS 2024) aims to improve the performance\nof automatic target recognition (ATR) systems by leveraging both Synthetic\nAperture Radar (SAR) data, which is difficult to interpret but remains\nunaffected by weather conditions and visible light, and Electro-Optical (EO)\ndata for simultaneous learning. The subtask, known as the Multi-modal Aerial\nView Imagery Challenge - Classification, focuses on predicting the class label\nof a low-resolution aerial image based on a set of SAR-EO image pairs and their\nrespective class labels. The provided dataset consists of SAR-EO pairs,\ncharacterized by a severe long-tail distribution with over a 1000-fold\ndifference between the largest and smallest classes, making typical long-tail\nmethods difficult to apply. Additionally, the domain disparity between the SAR\nand EO datasets complicates the effectiveness of standard multimodal methods.\nTo address these significant challenges, we propose a two-stage learning\napproach that utilizes self-supervised techniques, combined with multimodal\nlearning and inference through SAR-to-EO translation for effective EO\nutilization. In the final testing phase of the PBVS 2024 Multi-modal Aerial\nView Image Challenge - Classification (SAR Classification) task, our model\nachieved an accuracy of 21.45%, an AUC of 0.56, and a total score of 0.30,\nplacing us 9th in the competition.\n","authors":["Yuhyun Kim","Minwoo Kim","Hyobin Park","Jinwook Jung","Dong-Geol Choi"],"pdf_url":"https://arxiv.org/pdf/2412.12565v1.pdf","comment":"4 pages, 3 figures, 1 Table"},{"id":"http://arxiv.org/abs/2412.12562v1","updated":"2024-12-17T05:45:48Z","published":"2024-12-17T05:45:48Z","title":"Efficient Oriented Object Detection with Enhanced Small Object\n  Recognition in Aerial Images","summary":"  Achieving a balance between computational efficiency and detection accuracy\nin the realm of rotated bounding box object detection within aerial imagery is\na significant challenge. While prior research has aimed at creating lightweight\nmodels that enhance computational performance and feature extraction, there\nremains a gap in the performance of these networks when it comes to the\ndetection of small and multi-scale objects in remote sensing (RS) imagery. To\naddress these challenges, we present a novel enhancement to the YOLOv8 model,\ntailored for oriented object detection tasks and optimized for environments\nwith limited computational resources. Our model features a wavelet\ntransform-based C2f module for capturing associative features and an Adaptive\nScale Feature Pyramid (ASFP) module that leverages P2 layer details.\nAdditionally, the incorporation of GhostDynamicConv significantly contributes\nto the model's lightweight nature, ensuring high efficiency in aerial imagery\nanalysis. Featuring a parameter count of 21.6M, our approach provides a more\nefficient architectural design than DecoupleNet, which has 23.3M parameters,\nall while maintaining detection accuracy. On the DOTAv1.0 dataset, our model\ndemonstrates a mean Average Precision (mAP) that is competitive with leading\nmethods such as DecoupleNet. The model's efficiency, combined with its reduced\nparameter count, makes it a strong candidate for aerial object detection,\nparticularly in resource-constrained environments.\n","authors":["Zhifei Shi","Zongyao Yin","Sheng Chang","Xiao Yi","Xianchuan Yu"],"pdf_url":"https://arxiv.org/pdf/2412.12562v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12561v1","updated":"2024-12-17T05:43:35Z","published":"2024-12-17T05:43:35Z","title":"Tell Me What to Track: Infusing Robust Language Guidance for Enhanced\n  Referring Multi-Object Tracking","summary":"  Referring multi-object tracking (RMOT) is an emerging cross-modal task that\naims to localize an arbitrary number of targets based on a language expression\nand continuously track them in a video. This intricate task involves reasoning\non multi-modal data and precise target localization with temporal association.\nHowever, prior studies overlook the imbalanced data distribution between\nnewborn targets and existing targets due to the nature of the task. In\naddition, they only indirectly fuse multi-modal features, struggling to deliver\nclear guidance on newborn target detection. To solve the above issues, we\nconduct a collaborative matching strategy to alleviate the impact of the\nimbalance, boosting the ability to detect newborn targets while maintaining\ntracking performance. In the encoder, we integrate and enhance the cross-modal\nand multi-scale fusion, overcoming the bottlenecks in previous work, where\nlimited multi-modal information is shared and interacted between feature maps.\nIn the decoder, we also develop a referring-infused adaptation that provides\nexplicit referring guidance through the query tokens. The experiments showcase\nthe superior performance of our model (+3.42%) compared to prior works,\ndemonstrating the effectiveness of our designs.\n","authors":["Wenjun Huang","Yang Ni","Hanning Chen","Yirui He","Ian Bryant","Yezi Liu","Mohsen Imani"],"pdf_url":"https://arxiv.org/pdf/2412.12561v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19391v2","updated":"2024-12-17T05:37:37Z","published":"2024-06-27T17:59:40Z","title":"Fibottention: Inceptive Visual Representation Learning with Diverse\n  Attention Across Heads","summary":"  Transformer architectures such as Vision Transformers (ViT) have proven\neffective for solving visual perception tasks. However, they suffer from two\nmajor limitations; first, the quadratic complexity of self-attention limits the\nnumber of tokens that can be processed, and second, Transformers often require\nlarge amounts of training data to attain state-of-the-art performance. In this\npaper, we propose a new multi-head self-attention (MHSA) variant named\nFibottention, which can replace MHSA in Transformer architectures. Fibottention\nis data-efficient and computationally more suitable for processing large\nnumbers of tokens than the standard MHSA. It employs structured sparse\nattention based on dilated Fibonacci sequences, which, uniquely, differ across\nattention heads, resulting in inception-like diverse features across heads. The\nspacing of the Fibonacci sequences follows the Wythoff array, which minimizes\nthe redundancy of token interactions aggregated across different attention\nheads, while still capturing sufficient complementary information through token\npair interactions. These sparse attention patterns are unique among the\nexisting sparse attention and lead to an $O(N \\log N)$ complexity, where $N$ is\nthe number of tokens. Leveraging only 2-6% of the elements in the\nself-attention heads, Fibottention embedded into popular, state-of-the-art\nTransformer architectures can achieve significantly improved predictive\nperformance for domains with limited data such as image classification, video\nunderstanding, and robot learning tasks, and render reduced computational\ncomplexity. We further validated the improved diversity of feature\nrepresentations resulting from different self-attention heads, and our model\ndesign against other sparse attention mechanisms.\n","authors":["Ali Khaleghi Rahimian","Manish Kumar Govind","Subhajit Maity","Dominick Reilly","Christian Kümmerle","Srijan Das","Aritra Dutta"],"pdf_url":"https://arxiv.org/pdf/2406.19391v2.pdf","comment":"The complete implementation, including source code and evaluation\n  scripts, is publicly available at:\n  https://github.com/Charlotte-CharMLab/Fibottention"},{"id":"http://arxiv.org/abs/2412.10718v2","updated":"2024-12-17T05:24:57Z","published":"2024-12-14T07:22:03Z","title":"GridShow: Omni Visual Generation","summary":"  In this paper, we introduce GRID, a novel paradigm that reframes a broad\nrange of visual generation tasks as the problem of arranging grids, akin to\nfilm strips. At its core, GRID transforms temporal sequences into grid layouts,\nenabling image generation models to process visual sequences holistically. To\nachieve both layout consistency and motion coherence, we develop a parallel\nflow-matching training strategy that combines layout matching and temporal\nlosses, guided by a coarse-to-fine schedule that evolves from basic layouts to\nprecise motion control. Our approach demonstrates remarkable efficiency,\nachieving up to 35 faster inference speeds while using 1/1000 of the\ncomputational resources compared to specialized models. Extensive experiments\nshow that GRID exhibits exceptional versatility across diverse visual\ngeneration tasks, from Text-to-Video to 3D Editing, while maintaining its\nfoundational image generation capabilities. This dual strength in both expanded\napplications and preserved core competencies establishes GRID as an efficient\nand versatile omni-solution for visual generation.\n","authors":["Cong Wan","Xiangyang Luo","Zijian Cai","Yiren Song","Yunlong Zhao","Yifan Bai","Yuhang He","Yihong Gong"],"pdf_url":"https://arxiv.org/pdf/2412.10718v2.pdf","comment":"Codes: https://github.com/Should-AI-Lab/GRID"},{"id":"http://arxiv.org/abs/2412.10891v2","updated":"2024-12-17T05:23:42Z","published":"2024-12-14T16:42:41Z","title":"Zigzag Diffusion Sampling: Diffusion Models Can Self-Improve via\n  Self-Reflection","summary":"  Diffusion models, the most popular generative paradigm so far, can inject\nconditional information into the generation path to guide the latent towards\ndesired directions. However, existing text-to-image diffusion models often fail\nto maintain high image quality and high prompt-image alignment for those\nchallenging prompts. To mitigate this issue and enhance existing pretrained\ndiffusion models, we mainly made three contributions in this paper. First, we\npropose diffusion self-reflection that alternately performs denoising and\ninversion and demonstrate that such diffusion self-reflection can leverage the\nguidance gap between denoising and inversion to capture prompt-related semantic\ninformation with theoretical and empirical evidence. Second, motivated by\ntheoretical analysis, we derive Zigzag Diffusion Sampling (Z-Sampling), a novel\nself-reflection-based diffusion sampling method that leverages the guidance gap\nbetween denosing and inversion to accumulate semantic information step by step\nalong the sampling path, leading to improved sampling results. Moreover, as a\nplug-and-play method, Z-Sampling can be generally applied to various diffusion\nmodels (e.g., accelerated ones and Transformer-based ones) with very limited\ncoding and computational costs. Third, our extensive experiments demonstrate\nthat Z-Sampling can generally and significantly enhance generation quality\nacross various benchmark datasets, diffusion models, and performance evaluation\nmetrics. For example, DreamShaper with Z-Sampling can self-improve with the\nHPSv2 winning rate up to 94% over the original results. Moreover, Z-Sampling\ncan further enhance existing diffusion models combined with other orthogonal\nmethods, including Diffusion-DPO.\n","authors":["Lichen Bai","Shitong Shao","Zikai Zhou","Zipeng Qi","Zhiqiang Xu","Haoyi Xiong","Zeke Xie"],"pdf_url":"https://arxiv.org/pdf/2412.10891v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12552v1","updated":"2024-12-17T05:23:00Z","published":"2024-12-17T05:23:00Z","title":"SAModified: A Foundation Model-Based Zero-Shot Approach for Refining\n  Noisy Land-Use Land-Cover Maps","summary":"  Land-use and land cover (LULC) analysis is critical in remote sensing, with\nwide-ranging applications across diverse fields such as agriculture, utilities,\nand urban planning. However, automating LULC map generation using machine\nlearning is rendered challenging due to noisy labels. Typically, the ground\ntruths (e.g. ESRI LULC, MapBioMass) have noisy labels that hamper the model's\nability to learn to accurately classify the pixels. Further, these erroneous\nlabels can significantly distort the performance metrics of a model, leading to\nmisleading evaluations. Traditionally, the ambiguous labels are rectified using\nunsupervised algorithms. These algorithms struggle not only with scalability\nbut also with generalization across different geographies. To overcome these\nchallenges, we propose a zero-shot approach using the foundation model, Segment\nAnything Model (SAM), to automatically delineate different land parcels/regions\nand leverage them to relabel the unsure pixels by using the local label\nstatistics within each detected region. We achieve a significant reduction in\nlabel noise and an improvement in the performance of the downstream\nsegmentation model by $\\approx 5\\%$ when trained with denoised labels.\n","authors":["Sparsh Pekhale","Rakshith Sathish","Sathisha Basavaraju","Divya Sharma"],"pdf_url":"https://arxiv.org/pdf/2412.12552v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09191v2","updated":"2024-12-17T05:21:52Z","published":"2024-12-12T11:38:46Z","title":"RAD: Region-Aware Diffusion Models for Image Inpainting","summary":"  Diffusion models have achieved remarkable success in image generation, with\napplications broadening across various domains. Inpainting is one such\napplication that can benefit significantly from diffusion models. Existing\nmethods either hijack the reverse process of a pretrained diffusion model or\ncast the problem into a larger framework, \\ie, conditioned generation. However,\nthese approaches often require nested loops in the generation process or\nadditional components for conditioning. In this paper, we present region-aware\ndiffusion models (RAD) for inpainting with a simple yet effective reformulation\nof the vanilla diffusion models. RAD utilizes a different noise schedule for\neach pixel, which allows local regions to be generated asynchronously while\nconsidering the global image context. A plain reverse process requires no\nadditional components, enabling RAD to achieve inference time up to 100 times\nfaster than the state-of-the-art approaches. Moreover, we employ low-rank\nadaptation (LoRA) to fine-tune RAD based on other pretrained diffusion models,\nreducing computational burdens in training as well. Experiments demonstrated\nthat RAD provides state-of-the-art results both qualitatively and\nquantitatively, on the FFHQ, LSUN Bedroom, and ImageNet datasets.\n","authors":["Sora Kim","Sungho Suh","Minsik Lee"],"pdf_url":"https://arxiv.org/pdf/2412.09191v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12550v1","updated":"2024-12-17T05:21:16Z","published":"2024-12-17T05:21:16Z","title":"Consistent Diffusion: Denoising Diffusion Model with Data-Consistent\n  Training for Image Restoration","summary":"  In this work, we address the limitations of denoising diffusion models (DDMs)\nin image restoration tasks, particularly the shape and color distortions that\ncan compromise image quality. While DDMs have demonstrated a promising\nperformance in many applications such as text-to-image synthesis, their\neffectiveness in image restoration is often hindered by shape and color\ndistortions. We observe that these issues arise from inconsistencies between\nthe training and testing data used by DDMs. Based on our observation, we\npropose a novel training method, named data-consistent training, which allows\nthe DDMs to access images with accumulated errors during training, thereby\nensuring the model to learn to correct these errors. Experimental results show\nthat, across five image restoration tasks, our method has significant\nimprovements over state-of-the-art methods while effectively minimizing\ndistortions and preserving image fidelity.\n","authors":["Xinlong Cheng","Tiantian Cao","Guoan Cheng","Bangxuan Huang","Xinghan Tian","Ye Wang","Xiaoyu He","Weixin Li","Tianfan Xue","Xuan Dong"],"pdf_url":"https://arxiv.org/pdf/2412.12550v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17418v2","updated":"2024-12-17T05:15:54Z","published":"2024-07-24T16:53:17Z","title":"3D Gaussian Splatting: Survey, Technologies, Challenges, and\n  Opportunities","summary":"  3D Gaussian Splatting (3DGS) has emerged as a prominent technique with the\npotential to become a mainstream method for 3D representations. It can\neffectively transform multi-view images into explicit 3D Gaussian through\nefficient training, and achieve real-time rendering of novel views. This survey\naims to analyze existing 3DGS-related works from multiple intersecting\nperspectives, including related tasks, technologies, challenges, and\nopportunities. The primary objective is to provide newcomers with a rapid\nunderstanding of the field and to assist researchers in methodically organizing\nexisting technologies and challenges. Specifically, we delve into the\noptimization, application, and extension of 3DGS, categorizing them based on\ntheir focuses or motivations. Additionally, we summarize and classify nine\ntypes of technical modules and corresponding improvements identified in\nexisting works. Based on these analyses, we further examine the common\nchallenges and technologies across various tasks, proposing potential research\nopportunities.\n","authors":["Yanqi Bao","Tianyu Ding","Jing Huo","Yaoli Liu","Yuxin Li","Wenbin Li","Yang Gao","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2407.17418v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09777v4","updated":"2024-12-17T05:14:03Z","published":"2024-09-15T15:55:24Z","title":"DiFSD: Ego-Centric Fully Sparse Paradigm with Uncertainty Denoising and\n  Iterative Refinement for Efficient End-to-End Self-Driving","summary":"  Current end-to-end autonomous driving methods resort to unifying modular\ndesigns for various tasks (e.g. perception, prediction and planning). Although\noptimized in a planning-oriented spirit with a fully differentiable framework,\nexisting end-to-end driving systems without ego-centric designs still suffer\nfrom unsatisfactory performance and inferior efficiency, owing to the\nrasterized scene representation learning and redundant information\ntransmission. In this paper, we revisit the human driving behavior and propose\nan ego-centric fully sparse paradigm, named DiFSD, for end-to-end self-driving.\nSpecifically, DiFSD mainly consists of sparse perception, hierarchical\ninteraction and iterative motion planner. The sparse perception module performs\ndetection, tracking and online mapping based on sparse representation of the\ndriving scene. The hierarchical interaction module aims to select the Closest\nIn-Path Vehicle / Stationary (CIPV / CIPS) from coarse to fine, benefiting from\nan additional geometric prior. As for the iterative motion planner, both\nselected interactive agents and ego-vehicle are considered for joint motion\nprediction, where the output multi-modal ego-trajectories are optimized in an\niterative fashion. Besides, both position-level motion diffusion and\ntrajectory-level planning denoising are introduced for uncertainty modeling,\nthus facilitating the training stability and convergence of the whole\nframework. Extensive experiments conducted on nuScenes and Bench2Drive datasets\ndemonstrate the superior planning performance and great efficiency of DiFSD.\n","authors":["Haisheng Su","Wei Wu","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2409.09777v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17886v2","updated":"2024-12-17T04:59:06Z","published":"2024-11-26T21:16:36Z","title":"The Context of Crash Occurrence: A Complexity-Infused Approach\n  Integrating Semantic, Contextual, and Kinematic Features","summary":"  Understanding the context of crash occurrence in complex driving environments\nis essential for improving traffic safety and advancing automated driving.\nPrevious studies have used statistical models and deep learning to predict\ncrashes based on semantic, contextual, or vehicle kinematic features, but none\nhave examined the combined influence of these factors. In this study, we term\nthe integration of these features ``roadway complexity''. This paper introduces\na two-stage framework that integrates roadway complexity features for crash\nprediction. In the first stage, an encoder extracts hidden contextual\ninformation from these features, generating complexity-infused features. The\nsecond stage uses both original and complexity-infused features to predict\ncrash likelihood, achieving an accuracy of 87.98\\% with original features alone\nand 90.15\\% with the added complexity-infused features. Ablation studies\nconfirm that a combination of semantic, kinematic, and contextual features\nyields the best results, which emphasize their role in capturing roadway\ncomplexity. Additionally, complexity index annotations generated by the Large\nLanguage Model outperform those by Amazon Mechanical Turk, highlighting the\npotential of AI-based tools for accurate, scalable crash prediction systems.\n","authors":["Meng Wang","Zach Noonan","Pnina Gershon","Bruce Mehler","Bryan Reimer","Shannon C. Roberts"],"pdf_url":"https://arxiv.org/pdf/2411.17886v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10538v4","updated":"2024-12-17T04:58:18Z","published":"2024-08-20T04:32:50Z","title":"Surgical Workflow Recognition and Blocking Effectiveness Detection in\n  Laparoscopic Liver Resections with Pringle Maneuver","summary":"  Pringle maneuver (PM) in laparoscopic liver resection aims to reduce blood\nloss and provide a clear surgical view by intermittently blocking blood inflow\nof the liver, whereas prolonged PM may cause ischemic injury. To\ncomprehensively monitor this surgical procedure and provide timely warnings of\nineffective and prolonged blocking, we suggest two complementary AI-assisted\nsurgical monitoring tasks: workflow recognition and blocking effectiveness\ndetection in liver resections. The former presents challenges in real-time\ncapturing of short-term PM, while the latter involves the intraoperative\ndiscrimination of long-term liver ischemia states. To address these challenges,\nwe meticulously collect a novel dataset, called PmLR50, consisting of 25,037\nvideo frames covering various surgical phases from 50 laparoscopic liver\nresection procedures. Additionally, we develop an online baseline for PmLR50,\ntermed PmNet. This model embraces Masked Temporal Encoding (MTE) and Compressed\nSequence Modeling (CSM) for efficient short-term and long-term temporal\ninformation modeling, and embeds Contrastive Prototype Separation (CPS) to\nenhance action discrimination between similar intraoperative operations.\nExperimental results demonstrate that PmNet outperforms existing\nstate-of-the-art surgical workflow recognition methods on the PmLR50 benchmark.\nOur research offers potential clinical applications for the laparoscopic liver\nsurgery community. Codes are available at https://github.com/RascalGdd/PmNet.\n","authors":["Diandian Guo","Weixin Si","Zhixi Li","Jialun Pei","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2408.10538v4.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2404.14239v2","updated":"2024-12-17T04:47:44Z","published":"2024-04-22T14:47:54Z","title":"MultiBooth: Towards Generating All Your Concepts in an Image from Text","summary":"  This paper introduces MultiBooth, a novel and efficient technique for\nmulti-concept customization in image generation from text. Despite the\nsignificant advancements in customized generation methods, particularly with\nthe success of diffusion models, existing methods often struggle with\nmulti-concept scenarios due to low concept fidelity and high inference cost.\nMultiBooth addresses these issues by dividing the multi-concept generation\nprocess into two phases: a single-concept learning phase and a multi-concept\nintegration phase. During the single-concept learning phase, we employ a\nmulti-modal image encoder and an efficient concept encoding technique to learn\na concise and discriminative representation for each concept. In the\nmulti-concept integration phase, we use bounding boxes to define the generation\narea for each concept within the cross-attention map. This method enables the\ncreation of individual concepts within their specified regions, thereby\nfacilitating the formation of multi-concept images. This strategy not only\nimproves concept fidelity but also reduces additional inference cost.\nMultiBooth surpasses various baselines in both qualitative and quantitative\nevaluations, showcasing its superior performance and computational efficiency.\nProject Page: https://multibooth.github.io/\n","authors":["Chenyang Zhu","Kai Li","Yue Ma","Chunming He","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2404.14239v2.pdf","comment":"To be published in AAAI 2025"},{"id":"http://arxiv.org/abs/2412.12532v1","updated":"2024-12-17T04:42:50Z","published":"2024-12-17T04:42:50Z","title":"Addressing Small and Imbalanced Medical Image Datasets Using Generative\n  Models: A Comparative Study of DDPM and PGGANs with Random and Greedy K\n  Sampling","summary":"  The development of accurate medical image classification models is often\nconstrained by privacy concerns and data scarcity for certain conditions,\nleading to small and imbalanced datasets. To address these limitations, this\nstudy explores the use of generative models, such as Denoising Diffusion\nProbabilistic Models (DDPM) and Progressive Growing Generative Adversarial\nNetworks (PGGANs), for dataset augmentation. The research introduces a\nframework to assess the impact of synthetic images generated by DDPM and PGGANs\non the performance of four models: a custom CNN, Untrained VGG16, Pretrained\nVGG16, and Pretrained ResNet50. Experiments were conducted using Random\nSampling and Greedy K Sampling to create small, imbalanced datasets. The\nsynthetic images were evaluated using Frechet Inception Distance (FID) and\ncompared to original datasets through classification metrics. The results show\nthat DDPM consistently generated more realistic images with lower FID scores\nand significantly outperformed PGGANs in improving classification metrics\nacross all models and datasets. Incorporating DDPM-generated images into the\noriginal datasets increased accuracy by up to 6%, enhancing model robustness\nand stability, particularly in imbalanced scenarios. Random Sampling\ndemonstrated superior stability, while Greedy K Sampling offered diversity at\nthe cost of higher FID scores. This study highlights the efficacy of DDPM in\naugmenting small, imbalanced medical image datasets, improving model\nperformance by balancing the dataset and expanding its size.\n","authors":["Iman Khazrak","Shakhnoza Takhirova","Mostafa M. Rezaee","Mehrdad Yadollahi","Robert C. Green II","Shuteng Niu"],"pdf_url":"https://arxiv.org/pdf/2412.12532v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11892v2","updated":"2024-12-17T04:38:28Z","published":"2024-12-16T15:41:14Z","title":"From 2D CAD Drawings to 3D Parametric Models: A Vision-Language Approach","summary":"  In this paper, we present CAD2Program, a new method for reconstructing 3D\nparametric models from 2D CAD drawings. Our proposed method is inspired by\nrecent successes in vision-language models (VLMs), and departs from traditional\nmethods which rely on task-specific data representations and/or algorithms.\nSpecifically, on the input side, we simply treat the 2D CAD drawing as a raster\nimage, regardless of its original format, and encode the image with a standard\nViT model. We show that such an encoding scheme achieves competitive\nperformance against existing methods that operate on vector-graphics inputs,\nwhile imposing substantially fewer restrictions on the 2D drawings. On the\noutput side, our method auto-regressively predicts a general-purpose language\ndescribing 3D parametric models in text form. Compared to other sequence\nmodeling methods for CAD which use domain-specific sequence representations\nwith fixed-size slots, our text-based representation is more flexible, and can\nbe easily extended to arbitrary geometric entities and semantic or functional\nproperties. Experimental results on a large-scale dataset of cabinet models\ndemonstrate the effectiveness of our method.\n","authors":["Xilin Wang","Jia Zheng","Yuanchao Hu","Hao Zhu","Qian Yu","Zihan Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.11892v2.pdf","comment":"To Appear in AAAI 2025. The project page is at\n  https://manycore-research.github.io/CAD2Program"},{"id":"http://arxiv.org/abs/2412.12525v1","updated":"2024-12-17T04:33:31Z","published":"2024-12-17T04:33:31Z","title":"CREST: An Efficient Conjointly-trained Spike-driven Framework for\n  Event-based Object Detection Exploiting Spatiotemporal Dynamics","summary":"  Event-based cameras feature high temporal resolution, wide dynamic range, and\nlow power consumption, which is ideal for high-speed and low-light object\ndetection. Spiking neural networks (SNNs) are promising for event-based object\nrecognition and detection due to their spiking nature but lack efficient\ntraining methods, leading to gradient vanishing and high computational\ncomplexity, especially in deep SNNs. Additionally, existing SNN frameworks\noften fail to effectively handle multi-scale spatiotemporal features, leading\nto increased data redundancy and reduced accuracy. To address these issues, we\npropose CREST, a novel conjointly-trained spike-driven framework to exploit\nspatiotemporal dynamics in event-based object detection. We introduce the\nconjoint learning rule to accelerate SNN learning and alleviate gradient\nvanishing. It also supports dual operation modes for efficient and flexible\nimplementation on different hardware types. Additionally, CREST features a\nfully spike-driven framework with a multi-scale spatiotemporal event integrator\n(MESTOR) and a spatiotemporal-IoU (ST-IoU) loss. Our approach achieves superior\nobject recognition & detection performance and up to 100X energy efficiency\ncompared with state-of-the-art SNN algorithms on three datasets, providing an\nefficient solution for event-based object detection algorithms suitable for SNN\nhardware implementation.\n","authors":["Ruixin Mao","Aoyu Shen","Lin Tang","Jun Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.12525v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2401.11791v3","updated":"2024-12-17T04:27:31Z","published":"2024-01-22T09:41:05Z","title":"Semantic Prompt Learning for Weakly-Supervised Semantic Segmentation","summary":"  Weakly-Supervised Semantic Segmentation (WSSS) aims to train segmentation\nmodels using image data with only image-level supervision. Since precise\npixel-level annotations are not accessible, existing methods typically focus on\nproducing pseudo masks for training segmentation models by refining CAM-like\nheatmaps. However, the produced heatmaps may capture only the discriminative\nimage regions of object categories or the associated co-occurring backgrounds.\nTo address the issues, we propose a Semantic Prompt Learning for WSSS (SemPLeS)\nframework, which learns to effectively prompt the CLIP latent space to enhance\nthe semantic alignment between the segmented regions and the target object\ncategories. More specifically, we propose Contrastive Prompt Learning and\nPrompt-guided Semantic Refinement to learn the prompts that adequately describe\nand suppress the co-occurring backgrounds associated with each object category.\nIn this way, SemPLeS can perform better semantic alignment between object\nregions and class labels, resulting in desired pseudo masks for training\nsegmentation models. The proposed SemPLeS framework achieves competitive\nperformance on standard WSSS benchmarks, PASCAL VOC 2012 and MS COCO2014, and\nshows compatibility with other WSSS methods.\n","authors":["Ci-Siang Lin","Chien-Yi Wang","Yu-Chiang Frank Wang","Min-Hung Chen"],"pdf_url":"https://arxiv.org/pdf/2401.11791v3.pdf","comment":"WACV 2025. Project page: https://projectdisr.github.io/semples"},{"id":"http://arxiv.org/abs/2412.11620v2","updated":"2024-12-17T04:26:08Z","published":"2024-12-16T10:07:15Z","title":"Combating Semantic Contamination in Learning with Label Noise","summary":"  Noisy labels can negatively impact the performance of deep neural networks.\nOne common solution is label refurbishment, which involves reconstructing noisy\nlabels through predictions and distributions. However, these methods may\nintroduce problematic semantic associations, a phenomenon that we identify as\nSemantic Contamination. Through an analysis of Robust LR, a representative\nlabel refurbishment method, we found that utilizing the logits of views for\nrefurbishment does not adequately balance the semantic information of\nindividual classes. Conversely, using the logits of models fails to maintain\nconsistent semantic relationships across models, which explains why label\nrefurbishment methods frequently encounter issues related to Semantic\nContamination. To address this issue, we propose a novel method called\nCollaborative Cross Learning, which utilizes semi-supervised learning on\nrefurbished labels to extract appropriate semantic associations from embeddings\nacross views and models. Experimental results show that our method outperforms\nexisting approaches on both synthetic and real-world noisy datasets,\neffectively mitigating the impact of label noise and Semantic Contamination.\n","authors":["Wenxiao Fan","Kan Li"],"pdf_url":"https://arxiv.org/pdf/2412.11620v2.pdf","comment":"AAAI2025"},{"id":"http://arxiv.org/abs/2310.19180v4","updated":"2024-12-17T04:08:33Z","published":"2023-10-29T22:51:49Z","title":"JEN-1 Composer: A Unified Framework for High-Fidelity Multi-Track Music\n  Generation","summary":"  With rapid advances in generative artificial intelligence, the text-to-music\nsynthesis task has emerged as a promising direction for music generation.\nNevertheless, achieving precise control over multi-track generation remains an\nopen challenge. While existing models excel in directly generating multi-track\nmix, their limitations become evident when it comes to composing individual\ntracks and integrating them in a controllable manner. This departure from the\ntypical workflows of professional composers hinders the ability to refine\ndetails in specific tracks. To address this gap, we propose JEN-1 Composer, a\nunified framework designed to efficiently model marginal, conditional, and\njoint distributions over multi-track music using a single model. Building upon\nan audio latent diffusion model, JEN-1 Composer extends the versatility of\nmulti-track music generation. We introduce a progressive curriculum training\nstrategy, which gradually escalates the difficulty of training tasks while\nensuring the model's generalization ability and facilitating smooth transitions\nbetween different scenarios. During inference, users can iteratively generate\nand select music tracks, thus incrementally composing entire musical pieces in\naccordance with the Human-AI co-composition workflow. Our approach demonstrates\nstate-of-the-art performance in controllable and high-fidelity multi-track\nmusic synthesis, marking a significant advancement in interactive AI-assisted\nmusic creation. Our demo pages are available at www.jenmusic.ai/research.\n","authors":["Yao Yao","Peike Li","Boyu Chen","Alex Wang"],"pdf_url":"https://arxiv.org/pdf/2310.19180v4.pdf","comment":"9 pages, 3 figures, 3 tables, accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.10443v2","updated":"2024-12-17T03:55:34Z","published":"2024-12-11T13:48:06Z","title":"SweetTokenizer: Semantic-Aware Spatial-Temporal Tokenizer for Compact\n  Visual Discretization","summary":"  This paper presents the \\textbf{S}emantic-a\\textbf{W}ar\\textbf{E}\nspatial-t\\textbf{E}mporal \\textbf{T}okenizer (SweetTokenizer), a compact yet\neffective discretization approach for vision data. Our goal is to boost\ntokenizers' compression ratio while maintaining reconstruction fidelity in the\nVQ-VAE paradigm. Firstly, to obtain compact latent representations, we decouple\nimages or videos into spatial-temporal dimensions, translating visual\ninformation into learnable querying spatial and temporal tokens through a\n\\textbf{C}ross-attention \\textbf{Q}uery \\textbf{A}uto\\textbf{E}ncoder (CQAE).\nSecondly, to complement visual information during compression, we quantize\nthese tokens via a specialized codebook derived from off-the-shelf LLM\nembeddings to leverage the rich semantics from language modality. Finally, to\nenhance training stability and convergence, we also introduce a curriculum\nlearning strategy, which proves critical for effective discrete visual\nrepresentation learning. SweetTokenizer achieves comparable video\nreconstruction fidelity with only \\textbf{25\\%} of the tokens used in previous\nstate-of-the-art video tokenizers, and boost video generation results by\n\\textbf{32.9\\%} w.r.t gFVD. When using the same token number, we significantly\nimproves video and image reconstruction results by \\textbf{57.1\\%} w.r.t rFVD\non UCF-101 and \\textbf{37.2\\%} w.r.t rFID on ImageNet-1K. Additionally, the\ncompressed tokens are imbued with semantic information, enabling few-shot\nrecognition capabilities powered by LLMs in downstream applications.\n","authors":["Zhentao Tan","Ben Xue","Jian Jia","Junhao Wang","Wencai Ye","Shaoyun Shi","Mingjie Sun","Wenjin Wu","Quan Chen","Peng Jiang"],"pdf_url":"https://arxiv.org/pdf/2412.10443v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12511v1","updated":"2024-12-17T03:50:13Z","published":"2024-12-17T03:50:13Z","title":"Invisible Watermarks: Attacks and Robustness","summary":"  As Generative AI continues to become more accessible, the case for robust\ndetection of generated images in order to combat misinformation is stronger\nthan ever. Invisible watermarking methods act as identifiers of generated\ncontent, embedding image- and latent-space messages that are robust to many\nforms of perturbations. The majority of current research investigates\nfull-image attacks against images with a single watermarking method applied. We\nintroduce novel improvements to watermarking robustness as well as minimizing\ndegradation on image quality during attack. Firstly, we examine the application\nof both image-space and latent-space watermarking methods on a single image,\nwhere we propose a custom watermark remover network which preserves one of the\nwatermarking modalities while completely removing the other during decoding.\nThen, we investigate localized blurring attacks (LBA) on watermarked images\nbased on the GradCAM heatmap acquired from the watermark decoder in order to\nreduce the amount of degradation to the target image. Our evaluation suggests\nthat 1) implementing the watermark remover model to preserve one of the\nwatermark modalities when decoding the other modality slightly improves on the\nbaseline performance, and that 2) LBA degrades the image significantly less\ncompared to uniform blurring of the entire image. Code is available at:\nhttps://github.com/tomputer-g/IDL_WAR\n","authors":["Dongjun Hwang","Sungwon Woo","Tom Gao","Raymond Luo","Sunghwan Baek"],"pdf_url":"https://arxiv.org/pdf/2412.12511v1.pdf","comment":"YouTube link for the presentation:\n  https://www.youtube.com/watch?v=0vwFG1HSrUE"},{"id":"http://arxiv.org/abs/2412.11409v2","updated":"2024-12-17T03:50:05Z","published":"2024-12-16T03:25:23Z","title":"Multi-modal and Multi-scale Spatial Environment Understanding for\n  Immersive Visual Text-to-Speech","summary":"  Visual Text-to-Speech (VTTS) aims to take the environmental image as the\nprompt to synthesize the reverberant speech for the spoken content. The\nchallenge of this task lies in understanding the spatial environment from the\nimage. Many attempts have been made to extract global spatial visual\ninformation from the RGB space of an spatial image. However, local and depth\nimage information are crucial for understanding the spatial environment, which\nprevious works have ignored. To address the issues, we propose a novel\nmulti-modal and multi-scale spatial environment understanding scheme to achieve\nimmersive VTTS, termed M2SE-VTTS. The multi-modal aims to take both the RGB and\nDepth spaces of the spatial image to learn more comprehensive spatial\ninformation, and the multi-scale seeks to model the local and global spatial\nknowledge simultaneously. Specifically, we first split the RGB and Depth images\ninto patches and adopt the Gemini-generated environment captions to guide the\nlocal spatial understanding. After that, the multi-modal and multi-scale\nfeatures are integrated by the local-aware global spatial understanding. In\nthis way, M2SE-VTTS effectively models the interactions between local and\nglobal spatial contexts in the multi-modal spatial environment. Objective and\nsubjective evaluations suggest that our model outperforms the advanced\nbaselines in environmental speech generation. The code and audio samples are\navailable at: https://github.com/AI-S2-Lab/M2SE-VTTS.\n","authors":["Rui Liu","Shuwei He","Yifan Hu","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2412.11409v2.pdf","comment":"9 pages,2 figures, Accepted by AAAI'2025"},{"id":"http://arxiv.org/abs/2412.12507v1","updated":"2024-12-17T03:21:25Z","published":"2024-12-17T03:21:25Z","title":"3DGUT: Enabling Distorted Cameras and Secondary Rays in Gaussian\n  Splatting","summary":"  3D Gaussian Splatting (3DGS) has shown great potential for efficient\nreconstruction and high-fidelity real-time rendering of complex scenes on\nconsumer hardware. However, due to its rasterization-based formulation, 3DGS is\nconstrained to ideal pinhole cameras and lacks support for secondary lighting\neffects. Recent methods address these limitations by tracing volumetric\nparticles instead, however, this comes at the cost of significantly slower\nrendering speeds. In this work, we propose 3D Gaussian Unscented Transform\n(3DGUT), replacing the EWA splatting formulation in 3DGS with the Unscented\nTransform that approximates the particles through sigma points, which can be\nprojected exactly under any nonlinear projection function. This modification\nenables trivial support of distorted cameras with time dependent effects such\nas rolling shutter, while retaining the efficiency of rasterization.\nAdditionally, we align our rendering formulation with that of tracing-based\nmethods, enabling secondary ray tracing required to represent phenomena such as\nreflections and refraction within the same 3D representation.\n","authors":["Qi Wu","Janick Martinez Esturo","Ashkan Mirzaei","Nicolas Moenne-Loccoz","Zan Gojcic"],"pdf_url":"https://arxiv.org/pdf/2412.12507v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12503v1","updated":"2024-12-17T03:10:04Z","published":"2024-12-17T03:10:04Z","title":"Multi-Scale Cross-Fusion and Edge-Supervision Network for Image Splicing\n  Localization","summary":"  Image Splicing Localization (ISL) is a fundamental yet challenging task in\ndigital forensics. Although current approaches have achieved promising\nperformance, the edge information is insufficiently exploited, resulting in\npoor integrality and high false alarms. To tackle this problem, we propose a\nmulti-scale cross-fusion and edge-supervision network for ISL. Specifically,\nour framework consists of three key steps: multi-scale features cross-fusion,\nedge mask prediction and edge-supervision localization. Firstly, we input the\nRGB image and its noise image into a segmentation network to learn multi-scale\nfeatures, which are then aggregated via a cross-scale fusion followed by a\ncross-domain fusion to enhance feature representation. Secondly, we design an\nedge mask prediction module to effectively mine the reliable boundary\nartifacts. Finally, the cross-fused features and the reliable edge mask\ninformation are seamlessly integrated via an attention mechanism to\nincrementally supervise and facilitate model training. Extensive experiments on\npublicly available datasets demonstrate that our proposed method is superior to\nstate-of-the-art schemes.\n","authors":["Yakun Niu","Pei Chen","Lei Zhang","Hongjian Yin","Qi Chang"],"pdf_url":"https://arxiv.org/pdf/2412.12503v1.pdf","comment":"5 pages,3 figures"},{"id":"http://arxiv.org/abs/2412.12502v1","updated":"2024-12-17T03:06:12Z","published":"2024-12-17T03:06:12Z","title":"Track the Answer: Extending TextVQA from Image to Video with\n  Spatio-Temporal Clues","summary":"  Video text-based visual question answering (Video TextVQA) is a practical\ntask that aims to answer questions by jointly reasoning textual and visual\ninformation in a given video. Inspired by the development of TextVQA in image\ndomain, existing Video TextVQA approaches leverage a language model (e.g. T5)\nto process text-rich multiple frames and generate answers auto-regressively.\nNevertheless, the spatio-temporal relationships among visual entities\n(including scene text and objects) will be disrupted and models are susceptible\nto interference from unrelated information, resulting in irrational reasoning\nand inaccurate answering. To tackle these challenges, we propose the TEA\n(stands for ``\\textbf{T}rack th\\textbf{E} \\textbf{A}nswer'') method that better\nextends the generative TextVQA framework from image to video. TEA recovers the\nspatio-temporal relationships in a complementary way and incorporates OCR-aware\nclues to enhance the quality of reasoning questions. Extensive experiments on\nseveral public Video TextVQA datasets validate the effectiveness and\ngeneralization of our framework. TEA outperforms existing TextVQA methods,\nvideo-language pretraining methods and video large language models by great\nmargins.\n","authors":["Yan Zhang","Gangyan Zeng","Huawen Shen","Daiqing Wu","Yu Zhou","Can Ma"],"pdf_url":"https://arxiv.org/pdf/2412.12502v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.12501v1","updated":"2024-12-17T03:05:27Z","published":"2024-12-17T03:05:27Z","title":"Unleashing the Potential of Model Bias for Generalized Category\n  Discovery","summary":"  Generalized Category Discovery is a significant and complex task that aims to\nidentify both known and undefined novel categories from a set of unlabeled\ndata, leveraging another labeled dataset containing only known categories. The\nprimary challenges stem from model bias induced by pre-training on only known\ncategories and the lack of precise supervision for novel ones, leading to\ncategory bias towards known categories and category confusion among different\nnovel categories, which hinders models' ability to identify novel categories\neffectively. To address these challenges, we propose a novel framework named\nSelf-Debiasing Calibration (SDC). Unlike prior methods that regard model bias\ntowards known categories as an obstacle to novel category identification, SDC\nprovides a novel insight into unleashing the potential of the bias to\nfacilitate novel category learning. Specifically, the output of the biased\nmodel serves two key purposes. First, it provides an accurate modeling of\ncategory bias, which can be utilized to measure the degree of bias and debias\nthe output of the current training model. Second, it offers valuable insights\nfor distinguishing different novel categories by transferring knowledge between\nsimilar categories. Based on these insights, SDC dynamically adjusts the output\nlogits of the current training model using the output of the biased model. This\napproach produces less biased logits to effectively address the issue of\ncategory bias towards known categories, and generates more accurate pseudo\nlabels for unlabeled data, thereby mitigating category confusion for novel\ncategories. Experiments on three benchmark datasets show that SDC outperforms\nSOTA methods, especially in the identification of novel categories. Our code\nand data are available at \\url{https://github.com/Lackel/SDC}.\n","authors":["Wenbin An","Haonan Lin","Jiahao Nie","Feng Tian","Wenkai Shi","Yaqiang Wu","Qianying Wang","Ping Chen"],"pdf_url":"https://arxiv.org/pdf/2412.12501v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2411.17214v2","updated":"2024-12-17T03:01:53Z","published":"2024-11-26T08:30:31Z","title":"MAT: Multi-Range Attention Transformer for Efficient Image\n  Super-Resolution","summary":"  Recent advances in image super-resolution (SR) have significantly benefited\nfrom the incorporation of Transformer architectures. However, conventional\ntechniques aimed at enlarging the self-attention window to capture broader\ncontexts come with inherent drawbacks, especially the significantly increased\ncomputational demands. Moreover, the feature perception within a fixed-size\nwindow of existing models restricts the effective receptive fields and the\nintermediate feature diversity. This study demonstrates that a flexible\nintegration of attention across diverse spatial extents can yield significant\nperformance enhancements. In line with this insight, we introduce Multi-Range\nAttention Transformer (MAT) tailored for SR tasks. MAT leverages the\ncomputational advantages inherent in dilation operation, in conjunction with\nself-attention mechanism, to facilitate both multi-range attention (MA) and\nsparse multi-range attention (SMA), enabling efficient capture of both regional\nand sparse global features. Further coupled with local feature extraction, MAT\nadeptly capture dependencies across various spatial ranges, improving the\ndiversity and efficacy of its feature representations. We also introduce the\nMSConvStar module, which augments the model's ability for multi-range\nrepresentation learning. Comprehensive experiments show that our MAT exhibits\nsuperior performance to existing state-of-the-art SR models with remarkable\nefficiency (~3.3 faster than SRFormer-light).\n","authors":["Chengxing Xie","Xiaoming Zhang","Linze Li","Yuqian Fu","Biao Gong","Tianrui Li","Kai Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.17214v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12496v1","updated":"2024-12-17T02:56:35Z","published":"2024-12-17T02:56:35Z","title":"Faster Vision Mamba is Rebuilt in Minutes via Merged Token Re-training","summary":"  Vision Mamba (e.g., Vim) has successfully been integrated into computer\nvision, and token reduction has yielded promising outcomes in Vision\nTransformers (ViTs). However, token reduction performs less effectively on\nVision Mamba compared to ViTs. Pruning informative tokens in Mamba leads to a\nhigh loss of key knowledge and bad performance. This makes it not a good\nsolution for enhancing efficiency in Mamba. Token merging, which preserves more\ntoken information than pruning, has demonstrated commendable performance in\nViTs. Nevertheless, vanilla merging performance decreases as the reduction\nratio increases either, failing to maintain the key knowledge in Mamba.\nRe-training the token-reduced model enhances the performance of Mamba, by\neffectively rebuilding the key knowledge. Empirically, pruned Vims only drop up\nto 0.9% accuracy on ImageNet-1K, recovered by our proposed framework R-MeeTo in\nour main evaluation. We show how simple and effective the fast recovery can be\nachieved at minute-level, in particular, a 35.9% accuracy spike over 3 epochs\nof training on Vim-Ti. Moreover, Vim-Ti/S/B are re-trained within 5/7/17\nminutes, and Vim-S only drop 1.3% with 1.2x (up to 1.5x) speed up in inference.\n","authors":["Mingjia Shi","Yuhao Zhou","Ruiji Yu","Zekai Li","Zhiyuan Liang","Xuanlei Zhao","Xiaojiang Peng","Tanmay Rajpurohit","Shanmukha Ramakrishna Vedantam","Wangbo Zhao","Kai Wang","Yang You"],"pdf_url":"https://arxiv.org/pdf/2412.12496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12492v1","updated":"2024-12-17T02:47:00Z","published":"2024-12-17T02:47:00Z","title":"DuSSS: Dual Semantic Similarity-Supervised Vision-Language Model for\n  Semi-Supervised Medical Image Segmentation","summary":"  Semi-supervised medical image segmentation (SSMIS) uses consistency learning\nto regularize model training, which alleviates the burden of pixel-wise manual\nannotations. However, it often suffers from error supervision from low-quality\npseudo labels. Vision-Language Model (VLM) has great potential to enhance\npseudo labels by introducing text prompt guided multimodal supervision\ninformation. It nevertheless faces the cross-modal problem: the obtained\nmessages tend to correspond to multiple targets. To address aforementioned\nproblems, we propose a Dual Semantic Similarity-Supervised VLM (DuSSS) for\nSSMIS. Specifically, 1) a Dual Contrastive Learning (DCL) is designed to\nimprove cross-modal semantic consistency by capturing intrinsic representations\nwithin each modality and semantic correlations across modalities. 2) To\nencourage the learning of multiple semantic correspondences, a Semantic\nSimilarity-Supervision strategy (SSS) is proposed and injected into each\ncontrastive learning process in DCL, supervising semantic similarity via the\ndistribution-based uncertainty levels. Furthermore, a novel VLM-based SSMIS\nnetwork is designed to compensate for the quality deficiencies of\npseudo-labels. It utilizes the pretrained VLM to generate text prompt guided\nsupervision information, refining the pseudo label for better consistency\nregularization. Experimental results demonstrate that our DuSSS achieves\noutstanding performance with Dice of 82.52%, 74.61% and 78.03% on three public\ndatasets (QaTa-COV19, BM-Seg and MoNuSeg).\n","authors":["Qingtao Pan","Wenhao Qiao","Jingjiao Lou","Bing Ji","Shuo Li"],"pdf_url":"https://arxiv.org/pdf/2412.12492v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10817v2","updated":"2024-12-17T02:35:10Z","published":"2024-12-14T12:58:15Z","title":"Enhance Vision-Language Alignment with Noise","summary":"  With the advancement of pre-trained vision-language (VL) models, enhancing\nthe alignment between visual and linguistic modalities in downstream tasks has\nemerged as a critical challenge. Different from existing fine-tuning methods\nthat add extra modules to these two modalities, we investigate whether the\nfrozen model can be fine-tuned by customized noise. Our approach is motivated\nby the scientific study of beneficial noise, namely Positive-incentive Noise\n(Pi-noise or $\\pi$-noise) , which quantitatively analyzes the impact of noise.\nIt therefore implies a new scheme to learn beneficial noise distribution that\ncan be employed to fine-tune VL models. Focusing on few-shot classification\ntasks based on CLIP, we reformulate the inference process of CLIP and apply\nvariational inference, demonstrating how to generate $\\pi$-noise towards visual\nand linguistic modalities. Then, we propose Positive-incentive Noise Injector\n(PiNI), which can fine-tune CLIP via injecting noise into both visual and text\nencoders. Since the proposed method can learn the distribution of beneficial\nnoise, we can obtain more diverse embeddings of vision and language to better\nalign these two modalities for specific downstream tasks within limited\ncomputational resources. We evaluate different noise incorporation approaches\nand network architectures of PiNI. The evaluation across 11 datasets\ndemonstrates its effectiveness.\n","authors":["Sida Huang","Hongyuan Zhang","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2412.10817v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2404.07846v3","updated":"2024-12-17T02:26:54Z","published":"2024-04-11T15:39:10Z","title":"Rethinking Transformer-Based Blind-Spot Network for Self-Supervised\n  Image Denoising","summary":"  Blind-spot networks (BSN) have been prevalent neural architectures in\nself-supervised image denoising (SSID). However, most existing BSNs are\nconducted with convolution layers. Although transformers have shown the\npotential to overcome the limitations of convolutions in many image restoration\ntasks, the attention mechanisms may violate the blind-spot requirement, thereby\nrestricting their applicability in BSN. To this end, we propose to analyze and\nredesign the channel and spatial attentions to meet the blind-spot requirement.\nSpecifically, channel self-attention may leak the blind-spot information in\nmulti-scale architectures, since the downsampling shuffles the spatial feature\ninto channel dimensions. To alleviate this problem, we divide the channel into\nseveral groups and perform channel attention separately. For spatial\nselfattention, we apply an elaborate mask to the attention matrix to restrict\nand mimic the receptive field of dilated convolution. Based on the redesigned\nchannel and window attentions, we build a Transformer-based Blind-Spot Network\n(TBSN), which shows strong local fitting and global perspective abilities.\nFurthermore, we introduce a knowledge distillation strategy that distills TBSN\ninto smaller denoisers to improve computational efficiency while maintaining\nperformance. Extensive experiments on real-world image denoising datasets show\nthat TBSN largely extends the receptive field and exhibits favorable\nperformance against state-of-theart SSID methods.\n","authors":["Junyi Li","Zhilu Zhang","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2404.07846v3.pdf","comment":"AAAI 2025 Camera Ready"},{"id":"http://arxiv.org/abs/2412.11067v2","updated":"2024-12-17T02:23:03Z","published":"2024-12-15T05:57:36Z","title":"CFSynthesis: Controllable and Free-view 3D Human Video Synthesis","summary":"  Human video synthesis aims to create lifelike characters in various\nenvironments, with wide applications in VR, storytelling, and content creation.\nWhile 2D diffusion-based methods have made significant progress, they struggle\nto generalize to complex 3D poses and varying scene backgrounds. To address\nthese limitations, we introduce CFSynthesis, a novel framework for generating\nhigh-quality human videos with customizable attributes, including identity,\nmotion, and scene configurations. Our method leverages a texture-SMPL-based\nrepresentation to ensure consistent and stable character appearances across\nfree viewpoints. Additionally, we introduce a novel foreground-background\nseparation strategy that effectively decomposes the scene as foreground and\nbackground, enabling seamless integration of user-defined backgrounds.\nExperimental results on multiple datasets show that CFSynthesis not only\nachieves state-of-the-art performance in complex human animations but also\nadapts effectively to 3D motions in free-view and user-specified scenarios.\n","authors":["Liyuan Cui","Xiaogang Xu","Wenqi Dong","Zesong Yang","Hujun Bao","Zhaopeng Cui"],"pdf_url":"https://arxiv.org/pdf/2412.11067v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.13213v3","updated":"2024-12-17T01:59:42Z","published":"2024-01-24T03:56:07Z","title":"Common-Sense Bias Discovery and Mitigation for Classification Tasks","summary":"  Machine learning model bias can arise from dataset composition: correlated\nsensitive features can disturb the downstream classification model's decision\nboundary and lead to performance differences along these features. Existing\nde-biasing works tackle most prominent bias features, like colors of digits or\nbackground of animals. However, a real-world dataset often includes a large\nnumber of feature correlations, that manifest intrinsically in the data as\ncommon sense information. Such spurious visual cues can further reduce model\nrobustness. Thus, practitioners desire the whole picture of correlations and\nflexibility to treat concerned bias for specific domain tasks. With this goal,\nwe propose a novel framework to extract comprehensive bias information in image\ndatasets based on textual descriptions, a common sense-rich modality.\nSpecifically, features are constructed by clustering noun phrase embeddings of\nsimilar semantics. Each feature's appearance across a dataset is inferred and\ntheir co-occurrence statistics are measured, with spurious correlations\noptionally examined by a human-in-the-loop interface. Downstream experiments\nshow that our method discovers novel model biases on multiple image benchmark\ndatasets. Furthermore, the discovered bias can be mitigated by a simple data\nre-weighting strategy that de-correlates the features, and outperforms\nstate-of-the-art unsupervised bias mitigation methods.\n","authors":["Miao Zhang","Zee fryer","Ben Colman","Ali Shahriyari","Gaurav Bharaj"],"pdf_url":"https://arxiv.org/pdf/2401.13213v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12463v1","updated":"2024-12-17T01:52:12Z","published":"2024-12-17T01:52:12Z","title":"Pattern Analogies: Learning to Perform Programmatic Image Edits by\n  Analogy","summary":"  Pattern images are everywhere in the digital and physical worlds, and tools\nto edit them are valuable. But editing pattern images is tricky: desired edits\nare often programmatic: structure-aware edits that alter the underlying program\nwhich generates the pattern. One could attempt to infer this underlying\nprogram, but current methods for doing so struggle with complex images and\nproduce unorganized programs that make editing tedious. In this work, we\nintroduce a novel approach to perform programmatic edits on pattern images. By\nusing a pattern analogy -- a pair of simple patterns to demonstrate the\nintended edit -- and a learning-based generative model to execute these edits,\nour method allows users to intuitively edit patterns. To enable this paradigm,\nwe introduce SplitWeave, a domain-specific language that, combined with a\nframework for sampling synthetic pattern analogies, enables the creation of a\nlarge, high-quality synthetic training dataset. We also present TriFuser, a\nLatent Diffusion Model (LDM) designed to overcome critical issues that arise\nwhen naively deploying LDMs to this task. Extensive experiments on real-world,\nartist-sourced patterns reveals that our method faithfully performs the\ndemonstrated edit while also generalizing to related pattern styles beyond its\ntraining distribution.\n","authors":["Aditya Ganeshan","Thibault Groueix","Paul Guerrero","Radomír Měch","Matthew Fisher","Daniel Ritchie"],"pdf_url":"https://arxiv.org/pdf/2412.12463v1.pdf","comment":"Website: https://bardofcodes.github.io/patterns/"},{"id":"http://arxiv.org/abs/2412.12460v1","updated":"2024-12-17T01:45:15Z","published":"2024-12-17T01:45:15Z","title":"PromptDet: A Lightweight 3D Object Detection Framework with LiDAR\n  Prompts","summary":"  Multi-camera 3D object detection aims to detect and localize objects in 3D\nspace using multiple cameras, which has attracted more attention due to its\ncost-effectiveness trade-off. However, these methods often struggle with the\nlack of accurate depth estimation caused by the natural weakness of the camera\nin ranging. Recently, multi-modal fusion and knowledge distillation methods for\n3D object detection have been proposed to solve this problem, which are\ntime-consuming during the training phase and not friendly to memory cost. In\nlight of this, we propose PromptDet, a lightweight yet effective 3D object\ndetection framework motivated by the success of prompt learning in 2D\nfoundation model. Our proposed framework, PromptDet, comprises two integral\ncomponents: a general camera-based detection module, exemplified by models like\nBEVDet and BEVDepth, and a LiDAR-assisted prompter. The LiDAR-assisted prompter\nleverages the LiDAR points as a complementary signal, enriched with a minimal\nset of additional trainable parameters. Notably, our framework is flexible due\nto our prompt-like design, which can not only be used as a lightweight\nmulti-modal fusion method but also as a camera-only method for 3D object\ndetection during the inference phase. Extensive experiments on nuScenes\nvalidate the effectiveness of the proposed PromptDet. As a multi-modal\ndetector, PromptDet improves the mAP and NDS by at most 22.8\\% and 21.1\\% with\nfewer than 2\\% extra parameters compared with the camera-only baseline. Without\nLiDAR points, PromptDet still achieves an improvement of at most 2.4\\% mAP and\n4.0\\% NDS with almost no impact on camera detection inference time.\n","authors":["Kun Guo","Qiang Ling"],"pdf_url":"https://arxiv.org/pdf/2412.12460v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.07375v2","updated":"2024-12-17T01:06:09Z","published":"2024-12-10T10:16:50Z","title":"StoryWeaver: A Unified World Model for Knowledge-Enhanced Story\n  Character Customization","summary":"  Story visualization has gained increasing attention in artificial\nintelligence. However, existing methods still struggle with maintaining a\nbalance between character identity preservation and text-semantics alignment,\nlargely due to a lack of detailed semantic modeling of the story scene. To\ntackle this challenge, we propose a novel knowledge graph, namely Character\nGraph (\\textbf{CG}), which comprehensively represents various story-related\nknowledge, including the characters, the attributes related to characters, and\nthe relationship between characters. We then introduce StoryWeaver, an image\ngenerator that achieve Customization via Character Graph (\\textbf{C-CG}),\ncapable of consistent story visualization with rich text semantics. To further\nimprove the multi-character generation performance, we incorporate\nknowledge-enhanced spatial guidance (\\textbf{KE-SG}) into StoryWeaver to\nprecisely inject character semantics into generation. To validate the\neffectiveness of our proposed method, extensive experiments are conducted using\na new benchmark called TBC-Bench. The experiments confirm that our StoryWeaver\nexcels not only in creating vivid visual story plots but also in accurately\nconveying character identities across various scenarios with considerable\nstorage efficiency, \\emph{e.g.}, achieving an average increase of +9.03\\%\nDINO-I and +13.44\\% CLIP-T. Furthermore, ablation experiments are conducted to\nverify the superiority of the proposed module. Codes and datasets are released\nat https://github.com/Aria-Zhangjl/StoryWeaver.\n","authors":["Jinlu Zhang","Jiji Tang","Rongsheng Zhang","Tangjie Lv","Xiaoshuai Sun"],"pdf_url":"https://arxiv.org/pdf/2412.07375v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12432v1","updated":"2024-12-17T00:49:12Z","published":"2024-12-17T00:49:12Z","title":"Three Things to Know about Deep Metric Learning","summary":"  This paper addresses supervised deep metric learning for open-set image\nretrieval, focusing on three key aspects: the loss function, mixup\nregularization, and model initialization. In deep metric learning, optimizing\nthe retrieval evaluation metric, recall@k, via gradient descent is desirable\nbut challenging due to its non-differentiable nature. To overcome this, we\npropose a differentiable surrogate loss that is computed on large batches,\nnearly equivalent to the entire training set. This computationally intensive\nprocess is made feasible through an implementation that bypasses the GPU memory\nlimitations. Additionally, we introduce an efficient mixup regularization\ntechnique that operates on pairwise scalar similarities, effectively increasing\nthe batch size even further. The training process is further enhanced by\ninitializing the vision encoder using foundational models, which are\npre-trained on large-scale datasets. Through a systematic study of these\ncomponents, we demonstrate that their synergy enables large models to nearly\nsolve popular benchmarks.\n","authors":["Yash Patel","Giorgos Tolias","Jiri Matas"],"pdf_url":"https://arxiv.org/pdf/2412.12432v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13376v1","updated":"2024-12-17T23:23:25Z","published":"2024-12-17T23:23:25Z","title":"Targeted View-Invariant Adversarial Perturbations for 3D Object\n  Recognition","summary":"  Adversarial attacks pose significant challenges in 3D object recognition,\nespecially in scenarios involving multi-view analysis where objects can be\nobserved from varying angles. This paper introduces View-Invariant Adversarial\nPerturbations (VIAP), a novel method for crafting robust adversarial examples\nthat remain effective across multiple viewpoints. Unlike traditional methods,\nVIAP enables targeted attacks capable of manipulating recognition systems to\nclassify objects as specific, pre-determined labels, all while using a single\nuniversal perturbation. Leveraging a dataset of 1,210 images across 121 diverse\nrendered 3D objects, we demonstrate the effectiveness of VIAP in both targeted\nand untargeted settings. Our untargeted perturbations successfully generate a\nsingular adversarial noise robust to 3D transformations, while targeted attacks\nachieve exceptional results, with top-1 accuracies exceeding 95% across various\nepsilon values. These findings highlight VIAPs potential for real-world\napplications, such as testing the robustness of 3D recognition systems. The\nproposed method sets a new benchmark for view-invariant adversarial robustness,\nadvancing the field of adversarial machine learning for 3D object recognition.\n","authors":["Christian Green","Mehmet Ergezer","Abdurrahman Zeybey"],"pdf_url":"https://arxiv.org/pdf/2412.13376v1.pdf","comment":"Accepted to AAAI-25 Workshop on Artificial Intelligence for Cyber\n  Security (AICS): http://aics.site/AICS2025/index.html"},{"id":"http://arxiv.org/abs/2403.13680v4","updated":"2024-12-17T23:17:00Z","published":"2024-03-20T15:38:53Z","title":"Step-Calibrated Diffusion for Biomedical Optical Image Restoration","summary":"  High-quality, high-resolution medical imaging is essential for clinical care.\nRaman-based biomedical optical imaging uses non-ionizing infrared radiation to\nevaluate human tissues in real time and is used for early cancer detection,\nbrain tumor diagnosis, and intraoperative tissue analysis. Unfortunately,\noptical imaging is vulnerable to image degradation due to laser scattering and\nabsorption, which can result in diagnostic errors and misguided treatment.\nRestoration of optical images is a challenging computer vision task because the\nsources of image degradation are multi-factorial, stochastic, and\ntissue-dependent, preventing a straightforward method to obtain paired\nlow-quality/high-quality data. Here, we present Restorative Step-Calibrated\nDiffusion (RSCD), an unpaired diffusion-based image restoration method that\nuses a step calibrator model to dynamically determine the number of steps\nrequired to complete the reverse diffusion process for image restoration. RSCD\noutperforms other widely used unpaired image restoration methods on both image\nquality and perceptual evaluation metrics for restoring optical images. Medical\nimaging experts consistently prefer images restored using RSCD in blinded\ncomparison experiments and report minimal to no hallucinations. Finally, we\nshow that RSCD improves performance on downstream clinical imaging tasks,\nincluding automated brain tumor diagnosis and deep tissue imaging. Our code is\navailable at\nhttps://github.com/MLNeurosurg/restorative_step-calibrated_diffusion.\n","authors":["Yiwei Lyu","Sung Jik Cha","Cheng Jiang","Asadur Chowdury","Xinhai Hou","Edward Harake","Akhil Kondepudi","Christian Freudiger","Honglak Lee","Todd C. Hollon"],"pdf_url":"https://arxiv.org/pdf/2403.13680v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13364v1","updated":"2024-12-17T22:45:25Z","published":"2024-12-17T22:45:25Z","title":"Bringing Multimodality to Amazon Visual Search System","summary":"  Image to image matching has been well studied in the computer vision\ncommunity. Previous studies mainly focus on training a deep metric learning\nmodel matching visual patterns between the query image and gallery images. In\nthis study, we show that pure image-to-image matching suffers from false\npositives caused by matching to local visual patterns. To alleviate this issue,\nwe propose to leverage recent advances in vision-language pretraining research.\nSpecifically, we introduce additional image-text alignment losses into deep\nmetric learning, which serve as constraints to the image-to-image matching\nloss. With additional alignments between the text (e.g., product title) and\nimage pairs, the model can learn concepts from both modalities explicitly,\nwhich avoids matching low-level visual features. We progressively develop two\nvariants, a 3-tower and a 4-tower model, where the latter takes one more short\ntext query input. Through extensive experiments, we show that this change leads\nto a substantial improvement to the image to image matching problem. We further\nleveraged this model for multimodal search, which takes both image and\nreformulation text queries to improve search quality. Both offline and online\nexperiments show strong improvements on the main metrics. Specifically, we see\n4.95% relative improvement on image matching click through rate with the\n3-tower model and 1.13% further improvement from the 4-tower model.\n","authors":["Xinliang Zhu","Michael Huang","Han Ding","Jinyu Yang","Kelvin Chen","Tao Zhou","Tal Neiman","Ouye Xie","Son Tran","Benjamin Yao","Doug Gray","Anuj Bindal","Arnab Dhua"],"pdf_url":"https://arxiv.org/pdf/2412.13364v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19821v2","updated":"2024-12-17T22:40:26Z","published":"2024-10-18T11:14:54Z","title":"Explainable AI in Handwriting Detection for Dyslexia Using Transfer\n  Learning","summary":"  This study introduces an explainable AI (XAI) framework for the detection of\ndyslexia through handwriting analysis, achieving an impressive test precision\nof 99.65%. The framework integrates transfer learning and transformer-based\nmodels, identifying handwriting features associated with dyslexia while\nensuring transparency in decision-making via Grad-CAM visualizations. Its\nadaptability to different languages and writing systems underscores its\npotential for global applicability. By surpassing the classification accuracy\nof state-of-the-art methods, this approach demonstrates the reliability of\nhandwriting analysis as a diagnostic tool. The findings emphasize the\nframework's ability to support early detection, build stakeholder trust, and\nenable personalized educational strategies.\n","authors":["Mahmoud Robaa","Mazen Balat","Rewaa Awaad","Esraa Omar","Salah A. Aly"],"pdf_url":"https://arxiv.org/pdf/2410.19821v2.pdf","comment":"6 pages, 5 figures, JAC-ECC Conference"},{"id":"http://arxiv.org/abs/2405.14877v2","updated":"2024-12-17T22:12:21Z","published":"2024-04-02T01:58:53Z","title":"Visual Deformation Detection Using Soft Material Simulation for\n  Pre-training of Condition Assessment Models","summary":"  This paper addresses the challenge of geometric quality assurance in\nmanufacturing, particularly when human assessment is required. It proposes\nusing Blender, an open-source simulation tool, to create synthetic datasets for\nmachine learning (ML) models. The process involves translating expert\ninformation into shape key parameters to simulate deformations, generating\nimages for both deformed and non-deformed objects. The study explores the\nimpact of discrepancies between real and simulated environments on ML model\nperformance and investigates the effect of different simulation backgrounds on\nmodel sensitivity. Additionally, the study aims to enhance the model's\nrobustness to camera positioning by generating datasets with a variety of\nrandomized viewpoints. The entire process, from data synthesis to model\ntraining and testing, is implemented using a Python API interfacing with\nBlender. An experiment with a soda can object validates the accuracy of the\nproposed pipeline.\n","authors":["Joel Sol","Amir M. Soufi Enayati","Homayoun Najjaran"],"pdf_url":"https://arxiv.org/pdf/2405.14877v2.pdf","comment":"6 pages, 4 figures, 2024 IEEE 20th International Conference on\n  Automation Science and Engineering (CASE)"},{"id":"http://arxiv.org/abs/2412.09754v2","updated":"2024-12-17T21:14:50Z","published":"2024-12-12T23:10:54Z","title":"ViCaS: A Dataset for Combining Holistic and Pixel-level Video\n  Understanding using Captions with Grounded Segmentation","summary":"  Recent advances in multimodal large language models (MLLMs) have expanded\nresearch in video understanding, primarily focusing on high-level tasks such as\nvideo captioning and question-answering. Meanwhile, a smaller body of work\naddresses dense, pixel-precise segmentation tasks, which typically involve\ncategory-guided or referral-based object segmentation. Although both research\ndirections are essential for developing models with human-level video\ncomprehension, they have largely evolved separately, with distinct benchmarks\nand architectures. This paper aims to unify these efforts by introducing ViCaS,\na new dataset containing thousands of challenging videos, each annotated with\ndetailed, human-written captions and temporally consistent, pixel-accurate\nmasks for multiple objects with phrase grounding. Our benchmark evaluates\nmodels on both holistic/high-level understanding and language-guided,\npixel-precise segmentation. We also present carefully validated evaluation\nmeasures and propose an effective model architecture that can tackle our\nbenchmark. The project page is at https://ali2500.github.io/vicas-project/\n","authors":["Ali Athar","Xueqing Deng","Liang-Chieh Chen"],"pdf_url":"https://arxiv.org/pdf/2412.09754v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17740v3","updated":"2024-12-17T21:11:27Z","published":"2024-06-25T17:26:05Z","title":"Structured Unrestricted-Rank Matrices for Parameter Efficient\n  Fine-tuning","summary":"  Recent efforts to scale Transformer models have demonstrated rapid progress\nacross a wide range of tasks (Wei et al., 2022). However, fine-tuning these\nmodels for downstream tasks is expensive due to their large parameter counts.\nParameter-efficient fine-tuning (PEFT) approaches have emerged as a viable\nalternative by allowing us to fine-tune models by updating only a small number\nof parameters. In this work, we propose a general framework for parameter\nefficient fine-tuning (PEFT), based on structured unrestricted-rank matrices\n(SURM) which can serve as a drop-in replacement for popular approaches such as\nAdapters and LoRA. Unlike other methods like LoRA, SURMs provides more\nflexibility in finding the right balance between compactness and\nexpressiveness. This is achieved by using low displacement rank matrices\n(LDRMs), which hasn't been used in this context before. SURMs remain\ncompetitive with baselines, often providing significant quality improvements\nwhile using a smaller parameter budget. SURMs achieve 5-7% accuracy gains on\nvarious image classification tasks while replacing low-rank matrices in LoRA.\nIt also results in up to 12x reduction of the number of parameters in adapters\n(with virtually no loss in quality) on the GLUE benchmark.\n","authors":["Arijit Sehanobish","Avinava Dubey","Krzysztof Choromanski","Somnath Basu Roy Chowdhury","Deepali Jain","Vikas Sindhwani","Snigdha Chaturvedi"],"pdf_url":"https://arxiv.org/pdf/2406.17740v3.pdf","comment":"Accepted at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2404.02877v4","updated":"2024-12-17T20:58:44Z","published":"2024-04-03T17:24:27Z","title":"FlightScope: An Experimental Comparative Review of Aircraft Detection\n  Algorithms in Satellite Imagery","summary":"  Object detection in remotely sensed satellite pictures is fundamental in many\nfields such as biophysical, and environmental monitoring. While deep learning\nalgorithms are constantly evolving, they have been mostly implemented and\ntested on popular ground-based taken photos. This paper critically evaluates\nand compares a suite of advanced object detection algorithms customized for the\ntask of identifying aircraft within satellite imagery. Using the large\nHRPlanesV2 dataset, together with a rigorous validation with the GDIT dataset,\nthis research encompasses an array of methodologies including YOLO versions 5\nand 8, Faster RCNN, CenterNet, RetinaNet, RTMDet, and DETR, all trained from\nscratch. This exhaustive training and validation study reveal YOLOv5 as the\npreeminent model for the specific case of identifying airplanes from remote\nsensing data, showcasing high precision and adaptability across diverse imaging\nconditions. This research highlight the nuanced performance landscapes of these\nalgorithms, with YOLOv5 emerging as a robust solution for aerial object\ndetection, underlining its importance through superior mean average precision,\nRecall, and Intersection over Union scores. The findings described here\nunderscore the fundamental role of algorithm selection aligned with the\nspecific demands of satellite imagery analysis and extend a comprehensive\nframework to evaluate model efficacy. The benchmark toolkit and codes,\navailable via https://github.com/toelt-llc/FlightScope_Bench, aims to further\nexploration and innovation in the realm of remote sensing object detection,\npaving the way for improved analytical methodologies in satellite imagery\napplications.\n","authors":["Safouane El Ghazouali","Arnaud Gucciardi","Francesca Venturini","Nicola Venturi","Michael Rueegsegger","Umberto Michelucci"],"pdf_url":"https://arxiv.org/pdf/2404.02877v4.pdf","comment":"16 figures, 5 tables, comprehensive survey, comparative study"},{"id":"http://arxiv.org/abs/2412.13324v1","updated":"2024-12-17T20:52:56Z","published":"2024-12-17T20:52:56Z","title":"BadSAD: Clean-Label Backdoor Attacks against Deep Semi-Supervised\n  Anomaly Detection","summary":"  Image anomaly detection (IAD) is essential in applications such as industrial\ninspection, medical imaging, and security. Despite the progress achieved with\ndeep learning models like Deep Semi-Supervised Anomaly Detection (DeepSAD),\nthese models remain susceptible to backdoor attacks, presenting significant\nsecurity challenges. In this paper, we introduce BadSAD, a novel backdoor\nattack framework specifically designed to target DeepSAD models. Our approach\ninvolves two key phases: trigger injection, where subtle triggers are embedded\ninto normal images, and latent space manipulation, which positions and clusters\nthe poisoned images near normal images to make the triggers appear benign.\nExtensive experiments on benchmark datasets validate the effectiveness of our\nattack strategy, highlighting the severe risks that backdoor attacks pose to\ndeep learning-based anomaly detection systems.\n","authors":["He Cheng","Depeng Xu","Shuhan Yuan"],"pdf_url":"https://arxiv.org/pdf/2412.13324v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05212v5","updated":"2024-12-17T20:44:28Z","published":"2023-10-08T16:05:17Z","title":"Semiotics Networks Representing Perceptual Inference","summary":"  Every day, humans perceive objects and communicate these perceptions through\nvarious channels. In this paper, we present a computational model designed to\ntrack and simulate the perception of objects, as well as their representations\nas conveyed in communication.\n  We delineate two fundamental components of our internal representation,\ntermed \"observed\" and \"seen\", which we correlate with established concepts in\ncomputer vision, namely encoding and decoding. These components are integrated\ninto semiotic networks, which simulate perceptual inference of object\nperception and human communication.\n  Our model of object perception by a person allows us to define object\nperception by {\\em a network}. We demonstrate this with an example of an image\nbaseline classifier by constructing a new network that includes the baseline\nclassifier and an additional layer. This layer produces the images \"perceived\"\nby the entire network, transforming it into a perceptualized image classifier.\nThis facilitates visualization of the acquired network.\n  Within our network, the image representations become more efficient for\nclassification tasks when they are assembled and randomized. In our\nexperiments, the perceptualized network outperformed the baseline classifier on\nMNIST training databases consisting of a restricted number of images.\n  Our model is not limited to persons and can be applied to any system\nfeaturing a loop involving the processing from \"internal\" to \"external\"\nrepresentations.\n","authors":["David Kupeev","Eyal Nitzany"],"pdf_url":"https://arxiv.org/pdf/2310.05212v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07030v2","updated":"2024-12-17T20:38:21Z","published":"2024-12-09T22:35:44Z","title":"FM2DS: Few-Shot Multimodal Multihop Data Synthesis with Knowledge\n  Distillation for Question Answering","summary":"  Multimodal multihop question answering is a complex task that requires\nreasoning over multiple sources of information, such as images and text, to\nanswer questions. While there has been significant progress in visual question\nanswering, the multihop setting remains unexplored due to the lack of\nhigh-quality datasets. Current methods focus on single-hop question answering\nor a single modality, which makes them unsuitable for real-world scenarios such\nas analyzing multimodal educational materials, summarizing lengthy academic\narticles, or interpreting scientific studies that combine charts, images, and\ntext. To address this gap, we propose a novel methodology, introducing the\nfirst framework for creating a high-quality dataset that enables training\nmodels for multimodal multihop question answering. Our approach consists of a\n5-stage pipeline that involves acquiring relevant multimodal documents from\nWikipedia, synthetically generating high-level questions and answers, and\nvalidating them through rigorous criteria to ensure quality data. We evaluate\nour methodology by training models on our synthesized dataset and testing on\ntwo benchmarks, our results demonstrate that, with an equal sample size, models\ntrained on our synthesized data outperform those trained on human-collected\ndata by 1.9 in exact match (EM) on average. We believe our data synthesis\nmethod will serve as a strong foundation for training and evaluating multimodal\nmultihop question answering models.\n","authors":["Amirhossein Abaskohi","Spandana Gella","Giuseppe Carenini","Issam H. Laradji"],"pdf_url":"https://arxiv.org/pdf/2412.07030v2.pdf","comment":"20 pages, 11 figures, 10 tables, Submitted to CVPR 2025"},{"id":"http://arxiv.org/abs/2404.10476v4","updated":"2024-12-17T20:22:23Z","published":"2024-04-16T11:38:44Z","title":"Enhanced Facial Feature Extraction and Recignation Using Optimal Fully\n  Dispersed Haar-like Filters","summary":"  Haar-like filters are renowned for their simplicity, speed, and accuracy in\nvarious computer vision tasks. This paper proposes a novel algorithm to\nidentify optimal fully dispersed Haar-like filters for enhanced facial feature\nextraction and recognation. Unlike traditional Haar-like filters, these novel\nfilters allow pixels to move freely within images, enabling more effictive\ncapture of intricate local features...\n","authors":["Zeinab Sedaghatjoo","Hossein Hosseinzadeh","Ahmad shirzadi"],"pdf_url":"https://arxiv.org/pdf/2404.10476v4.pdf","comment":"The idea of the manuscript is basically incorrect"},{"id":"http://arxiv.org/abs/2407.05910v2","updated":"2024-12-17T20:14:23Z","published":"2024-07-08T13:15:11Z","title":"Enhancing Vision-Language Models with Scene Graphs for Traffic Accident\n  Understanding","summary":"  Recognizing a traffic accident is an essential part of any autonomous driving\nor road monitoring system. An accident can appear in a wide variety of forms,\nand understanding what type of accident is taking place may be useful to\nprevent it from reoccurring. This work focuses on classification of traffic\nscenes into specific accident types. We approach the problem by representing a\ntraffic scene as a graph, where objects such as cars can be represented as\nnodes, and relative distances and directions between them as edges. This\nrepresentation of a traffic scene is referred to as a scene graph, and can be\nused as input for an accident classifier. Better results are obtained with a\nclassifier that fuses the scene graph input with visual and textual\nrepresentations. This work introduces a multi-stage, multimodal pipeline that\npre-processes videos of traffic accidents, encodes them as scene graphs, and\naligns this representation with vision and language modalities before executing\nthe classification task. When trained on 4 classes, our method achieves a\nbalanced accuracy score of 57.77% on an (unbalanced) subset of the popular\nDetection of Traffic Anomaly (DoTA) benchmark, representing an increase of\nclose to 5 percentage points from the case where scene graph information is not\ntaken into account.\n","authors":["Aaron Lohner","Francesco Compagno","Jonathan Francis","Alessandro Oltramari"],"pdf_url":"https://arxiv.org/pdf/2407.05910v2.pdf","comment":"Won the 'Best Paper Runner-up Award' at the 2024 IEEE International\n  Automated Vehicle Validation Conference (IAVVC 2024). Also accepted at the\n  1st Workshop on Semantic Reasoning and Goal Understanding in Robotics, at the\n  Robotics Science and Systems Conference (RSS SemRob 2024)"},{"id":"http://arxiv.org/abs/2412.13303v1","updated":"2024-12-17T20:09:55Z","published":"2024-12-17T20:09:55Z","title":"FastVLM: Efficient Vision Encoding for Vision Language Models","summary":"  Scaling the input image resolution is essential for enhancing the performance\nof Vision Language Models (VLMs), particularly in text-rich image understanding\ntasks. However, popular visual encoders such as ViTs become inefficient at high\nresolutions due to the large number of tokens and high encoding latency caused\nby stacked self-attention layers. At different operational resolutions, the\nvision encoder of a VLM can be optimized along two axes: reducing encoding\nlatency and minimizing the number of visual tokens passed to the LLM, thereby\nlowering overall latency. Based on a comprehensive efficiency analysis of the\ninterplay between image resolution, vision latency, token count, and LLM size,\nwe introduce FastVLM, a model that achieves an optimized trade-off between\nlatency, model size and accuracy. FastVLM incorporates FastViTHD, a novel\nhybrid vision encoder designed to output fewer tokens and significantly reduce\nencoding time for high-resolution images. Unlike previous methods, FastVLM\nachieves the optimal balance between visual token count and image resolution\nsolely by scaling the input image, eliminating the need for additional token\npruning and simplifying the model design. In the LLaVA-1.5 setup, FastVLM\nachieves 3.2$\\times$ improvement in time-to-first-token (TTFT) while\nmaintaining similar performance on VLM benchmarks compared to prior works.\nCompared to LLaVa-OneVision at the highest resolution (1152$\\times$1152),\nFastVLM achieves comparable performance on key benchmarks like SeedBench and\nMMMU, using the same 0.5B LLM, but with 85$\\times$ faster TTFT and a vision\nencoder that is 3.4$\\times$ smaller.\n","authors":["Pavan Kumar Anasosalu Vasu","Fartash Faghri","Chun-Liang Li","Cem Koc","Nate True","Albert Antony","Gokul Santhanam","James Gabriel","Peter Grasch","Oncel Tuzel","Hadi Pouransari"],"pdf_url":"https://arxiv.org/pdf/2412.13303v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13299v1","updated":"2024-12-17T19:59:08Z","published":"2024-12-17T19:59:08Z","title":"In-context learning for medical image segmentation","summary":"  Annotation of medical images, such as MRI and CT scans, is crucial for\nevaluating treatment efficacy and planning radiotherapy. However, the extensive\nworkload of medical professionals limits their ability to annotate large image\ndatasets, posing a bottleneck for AI applications in medical imaging. To\naddress this, we propose In-context Cascade Segmentation (ICS), a novel method\nthat minimizes annotation requirements while achieving high segmentation\naccuracy for sequential medical images. ICS builds on the UniverSeg framework,\nwhich performs few-shot segmentation using support images without additional\ntraining. By iteratively adding the inference results of each slice to the\nsupport set, ICS propagates information forward and backward through the\nsequence, ensuring inter-slice consistency. We evaluate the proposed method on\nthe HVSMR dataset, which includes segmentation tasks for eight cardiac regions.\nExperimental results demonstrate that ICS significantly improves segmentation\nperformance in complex anatomical regions, particularly in maintaining boundary\nconsistency across slices, compared to baseline methods. The study also\nhighlights the impact of the number and position of initial support slices on\nsegmentation accuracy. ICS offers a promising solution for reducing annotation\nburdens while delivering robust segmentation results, paving the way for its\nbroader adoption in clinical and research applications.\n","authors":["Eichi Takaya","Shinnosuke Yamamoto"],"pdf_url":"https://arxiv.org/pdf/2412.13299v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13294v1","updated":"2024-12-17T19:47:10Z","published":"2024-12-17T19:47:10Z","title":"Image registration is a geometric deep learning task","summary":"  Data-driven deformable image registration methods predominantly rely on\noperations that process grid-like inputs. However, applying deformable\ntransformations to an image results in a warped space that deviates from a\nrigid grid structure. Consequently, data-driven approaches with sequential\ndeformations have to apply grid resampling operations between each deformation\nstep. While artifacts caused by resampling are negligible in high-resolution\nimages, the resampling of sparse, high-dimensional feature grids introduces\nerrors that affect the deformation modeling process. Taking inspiration from\nLagrangian reference frames of deformation fields, our work introduces a novel\nparadigm for data-driven deformable image registration that utilizes geometric\ndeep-learning principles to model deformations without grid requirements.\nSpecifically, we model image features as a set of nodes that freely move in\nEuclidean space, update their coordinates under graph operations, and\ndynamically readjust their local neighborhoods. We employ this formulation to\nconstruct a multi-resolution deformable registration model, where deformation\nlayers iteratively refine the overall transformation at each resolution without\nintermediate resampling operations on the feature grids. We investigate our\nmethod's ability to fully deformably capture large deformations across a number\nof medical imaging registration tasks. In particular, we apply our approach\n(GeoReg) to the registration of inter-subject brain MR images and inhale-exhale\nlung CT images, showing on par performance with the current state-of-the-art\nmethods. We believe our contribution open up avenues of research to reduce the\nblack-box nature of current learned registration paradigms by explicitly\nmodeling the transformation within the architecture.\n","authors":["Vasiliki Sideri-Lampretsa","Nil Stolt-Ansó","Martin Menten","Huaqi Qiu","Julian McGinnis","Daniel Rueckert"],"pdf_url":"https://arxiv.org/pdf/2412.13294v1.pdf","comment":"22 Pages"},{"id":"http://arxiv.org/abs/2407.16727v2","updated":"2024-12-17T19:37:34Z","published":"2024-07-23T14:22:16Z","title":"A study of animal action segmentation algorithms across supervised,\n  unsupervised, and semi-supervised learning paradigms","summary":"  Action segmentation of behavioral videos is the process of labeling each\nframe as belonging to one or more discrete classes, and is a crucial component\nof many studies that investigate animal behavior. A wide range of algorithms\nexist to automatically parse discrete animal behavior, encompassing supervised,\nunsupervised, and semi-supervised learning paradigms. These algorithms -- which\ninclude tree-based models, deep neural networks, and graphical models -- differ\nwidely in their structure and assumptions on the data. Using four datasets\nspanning multiple species -- fly, mouse, and human -- we systematically study\nhow the outputs of these various algorithms align with manually annotated\nbehaviors of interest. Along the way, we introduce a semi-supervised action\nsegmentation model that bridges the gap between supervised deep neural networks\nand unsupervised graphical models. We find that fully supervised temporal\nconvolutional networks with the addition of temporal information in the\nobservations perform the best on our supervised metrics across all datasets.\n","authors":["Ari Blau","Evan S Schaffer","Neeli Mishra","Nathaniel J Miska","The International Brain Laboratory","Liam Paninski","Matthew R Whiteway"],"pdf_url":"https://arxiv.org/pdf/2407.16727v2.pdf","comment":"33 pages, 15 figures"},{"id":"http://arxiv.org/abs/2406.06342v3","updated":"2024-12-17T19:21:39Z","published":"2024-06-10T15:02:30Z","title":"A Guide to Stochastic Optimisation for Large-Scale Inverse Problems","summary":"  Stochastic optimisation algorithms are the de facto standard for machine\nlearning with large amounts of data. Handling only a subset of available data\nin each optimisation step dramatically reduces the per-iteration computational\ncosts, while still ensuring significant progress towards the solution. Driven\nby the need to solve large-scale optimisation problems as efficiently as\npossible, the last decade has witnessed an explosion of research in this area.\nLeveraging the parallels between machine learning and inverse problems has\nallowed harnessing the power of this research wave for solving inverse\nproblems. In this survey, we provide a comprehensive account of the\nstate-of-the-art in stochastic optimisation from the viewpoint of variational\nregularisation for inverse problems where the solution is modelled as\nminimising an objective function. We present algorithms with diverse modalities\nof problem randomisation and discuss the roles of variance reduction,\nacceleration, higher-order methods, and other algorithmic modifications, and\ncompare theoretical results with practical behaviour. We focus on the potential\nand the challenges for stochastic optimisation that are unique to variational\nregularisation for inverse imaging problems and are not commonly encountered in\nmachine learning. We conclude the survey with illustrative examples from\nimaging on linear inverse problems to examine the advantages and disadvantages\nthat this new generation of algorithms bring to the field of inverse problems.\n","authors":["Matthias J. Ehrhardt","Zeljko Kereta","Jingwei Liang","Junqi Tang"],"pdf_url":"https://arxiv.org/pdf/2406.06342v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13273v1","updated":"2024-12-17T19:06:12Z","published":"2024-12-17T19:06:12Z","title":"CompactFlowNet: Efficient Real-time Optical Flow Estimation on Mobile\n  Devices","summary":"  We present CompactFlowNet, the first real-time mobile neural network for\noptical flow prediction, which involves determining the displacement of each\npixel in an initial frame relative to the corresponding pixel in a subsequent\nframe. Optical flow serves as a fundamental building block for various\nvideo-related tasks, such as video restoration, motion estimation, video\nstabilization, object tracking, action recognition, and video generation. While\ncurrent state-of-the-art methods prioritize accuracy, they often overlook\nconstraints regarding speed and memory usage. Existing light models typically\nfocus on reducing size but still exhibit high latency, compromise significantly\non quality, or are optimized for high-performance GPUs, resulting in\nsub-optimal performance on mobile devices. This study aims to develop a\nmobile-optimized optical flow model by proposing a novel mobile\ndevice-compatible architecture, as well as enhancements to the training\npipeline, which optimize the model for reduced weight, low memory utilization,\nand increased speed while maintaining minimal error. Our approach demonstrates\nsuperior or comparable performance to the state-of-the-art lightweight models\non the challenging KITTI and Sintel benchmarks. Furthermore, it attains a\nsignificantly accelerated inference speed, thereby yielding real-time\noperational efficiency on the iPhone 8, while surpassing real-time performance\nlevels on more advanced mobile devices.\n","authors":["Andrei Znobishchev","Valerii Filev","Oleg Kudashev","Nikita Orlov","Humphrey Shi"],"pdf_url":"https://arxiv.org/pdf/2412.13273v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13244v1","updated":"2024-12-17T18:51:27Z","published":"2024-12-17T18:51:27Z","title":"iRBSM: A Deep Implicit 3D Breast Shape Model","summary":"  We present the first deep implicit 3D shape model of the female breast,\nbuilding upon and improving the recently proposed Regensburg Breast Shape Model\n(RBSM). Compared to its PCA-based predecessor, our model employs implicit\nneural representations; hence, it can be trained on raw 3D breast scans and\neliminates the need for computationally demanding non-rigid registration -- a\ntask that is particularly difficult for feature-less breast shapes. The\nresulting model, dubbed iRBSM, captures detailed surface geometry including\nfine structures such as nipples and belly buttons, is highly expressive, and\noutperforms the RBSM on different surface reconstruction tasks. Finally,\nleveraging the iRBSM, we present a prototype application to 3D reconstruct\nbreast shapes from just a single image. Model and code publicly available at\nhttps://rbsm.re-mic.de/implicit.\n","authors":["Maximilian Weiherer","Antonia von Riedheim","Vanessa Brébant","Bernhard Egger","Christoph Palm"],"pdf_url":"https://arxiv.org/pdf/2412.13244v1.pdf","comment":"6 pages, 5 figures"},{"id":"http://arxiv.org/abs/2412.13237v1","updated":"2024-12-17T16:42:55Z","published":"2024-12-17T16:42:55Z","title":"Optimized two-stage AI-based Neural Decoding for Enhanced Visual\n  Stimulus Reconstruction from fMRI Data","summary":"  AI-based neural decoding reconstructs visual perception by leveraging\ngenerative models to map brain activity, measured through functional MRI\n(fMRI), into latent hierarchical representations. Traditionally, ridge linear\nmodels transform fMRI into a latent space, which is then decoded using latent\ndiffusion models (LDM) via a pre-trained variational autoencoder (VAE). Due to\nthe complexity and noisiness of fMRI data, newer approaches split the\nreconstruction into two sequential steps, the first one providing a rough\nvisual approximation, the second on improving the stimulus prediction via LDM\nendowed by CLIP embeddings. This work proposes a non-linear deep network to\nimprove fMRI latent space representation, optimizing the dimensionality alike.\nExperiments on the Natural Scenes Dataset showed that the proposed architecture\nimproved the structural similarity of the reconstructed image by about 2\\% with\nrespect to the state-of-the-art model, based on ridge linear transform. The\nreconstructed image's semantics improved by about 4\\%, measured by perceptual\nsimilarity, with respect to the state-of-the-art. The noise sensitivity\nanalysis of the LDM showed that the role of the first stage was fundamental to\npredict the stimulus featuring high structural similarity. Conversely,\nproviding a large noise stimulus affected less the semantics of the predicted\nstimulus, while the structural similarity between the ground truth and\npredicted stimulus was very poor. The findings underscore the importance of\nleveraging non-linear relationships between BOLD signal and the latent\nrepresentation and two-stage generative AI for optimizing the fidelity of\nreconstructed visual stimuli from noisy fMRI data.\n","authors":["Lorenzo Veronese","Andrea Moglia","Luca Mainardi","Pietro Cerveri"],"pdf_url":"https://arxiv.org/pdf/2412.13237v1.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2409.10994v3","updated":"2024-12-17T02:05:27Z","published":"2024-09-17T08:56:27Z","title":"Less is More: A Simple yet Effective Token Reduction Method for\n  Efficient Multi-modal LLMs","summary":"  The rapid advancement of Multimodal Large Language Models (MLLMs) has led to\nremarkable performances across various domains. However, this progress is\naccompanied by a substantial surge in the resource consumption of these models.\nWe address this pressing issue by introducing a new approach, Token Reduction\nusing CLIP Metric (TRIM), aimed at improving the efficiency of MLLMs without\nsacrificing their performance. Inspired by human attention patterns in Visual\nQuestion Answering (VQA) tasks, TRIM presents a fresh perspective on the\nselection and reduction of image tokens. The TRIM method has been extensively\ntested across 12 datasets, and the results demonstrate a significant reduction\nin computational overhead while maintaining a consistent level of performance.\nThis research marks a critical stride in efficient MLLM development, promoting\ngreater accessibility and sustainability of high-performing models.\n","authors":["Dingjie Song","Wenjun Wang","Shunian Chen","Xidong Wang","Michael Guan","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2409.10994v3.pdf","comment":"Accepted to COLING 2025"}],"Robotics":[{"id":"http://arxiv.org/abs/2412.13196v1","updated":"2024-12-17T18:59:51Z","published":"2024-12-17T18:59:51Z","title":"ExBody2: Advanced Expressive Humanoid Whole-Body Control","summary":"  This paper enables real-world humanoid robots to maintain stability while\nperforming expressive motions like humans do. We propose ExBody2, a generalized\nwhole-body tracking framework that can take any reference motion inputs and\ncontrol the humanoid to mimic the motion. The model is trained in simulation\nwith Reinforcement Learning and then transferred to the real world. It\ndecouples keypoint tracking with velocity control, and effectively leverages a\nprivileged teacher policy to distill precise mimic skills into the target\nstudent policy, which enables high-fidelity replication of dynamic movements\nsuch as running, crouching, dancing, and other challenging motions. We present\na comprehensive qualitative and quantitative analysis of crucial design factors\nin the paper. We conduct our experiments on two humanoid platforms and\ndemonstrate the superiority of our approach against state-of-the-arts,\nproviding practical guidelines to pursue the extreme of whole-body control for\nhumanoid robots.\n","authors":["Mazeyu Ji","Xuanbin Peng","Fangchen Liu","Jialong Li","Ge Yang","Xuxin Cheng","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13196v1.pdf","comment":"website: https://exbody2.github.io"},{"id":"http://arxiv.org/abs/2412.13178v1","updated":"2024-12-17T18:55:58Z","published":"2024-12-17T18:55:58Z","title":"SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM\n  Agents","summary":"  With the integration of large language models (LLMs), embodied agents have\nstrong capabilities to execute complicated instructions in natural language,\npaving a way for the potential deployment of embodied robots. However, a\nforeseeable issue is that those embodied agents can also flawlessly execute\nsome hazardous tasks, potentially causing damages in real world. To study this\nissue, we present SafeAgentBench -- a new benchmark for safety-aware task\nplanning of embodied LLM agents. SafeAgentBench includes: (1) a new dataset\nwith 750 tasks, covering 10 potential hazards and 3 task types; (2)\nSafeAgentEnv, a universal embodied environment with a low-level controller,\nsupporting multi-agent execution with 17 high-level actions for 8\nstate-of-the-art baselines; and (3) reliable evaluation methods from both\nexecution and semantic perspectives. Experimental results show that the\nbest-performing baseline gets 69% success rate for safe tasks, but only 5%\nrejection rate for hazardous tasks, indicating significant safety risks. More\ndetails and codes are available at\nhttps://github.com/shengyin1224/SafeAgentBench.\n","authors":["Sheng Yin","Xianghe Pang","Yuanzhuo Ding","Menglan Chen","Yutong Bi","Yichen Xiong","Wenhao Huang","Zhen Xiang","Jing Shao","Siheng Chen"],"pdf_url":"https://arxiv.org/pdf/2412.13178v1.pdf","comment":"21 pages, 14 tables, 7 figures, submitted to ICRA 2024"},{"id":"http://arxiv.org/abs/2412.13157v1","updated":"2024-12-17T18:33:05Z","published":"2024-12-17T18:33:05Z","title":"Learning Visuotactile Estimation and Control for Non-prehensile\n  Manipulation under Occlusions","summary":"  Manipulation without grasping, known as non-prehensile manipulation, is\nessential for dexterous robots in contact-rich environments, but presents many\nchallenges relating with underactuation, hybrid-dynamics, and frictional\nuncertainty. Additionally, object occlusions in a scenario of contact\nuncertainty and where the motion of the object evolves independently from the\nrobot becomes a critical problem, which previous literature fails to address.\nWe present a method for learning visuotactile state estimators and\nuncertainty-aware control policies for non-prehensile manipulation under\nocclusions, by leveraging diverse interaction data from privileged policies\ntrained in simulation. We formulate the estimator within a Bayesian deep\nlearning framework, to model its uncertainty, and then train uncertainty-aware\ncontrol policies by incorporating the pre-learned estimator into the\nreinforcement learning (RL) loop, both of which lead to significantly improved\nestimator and policy performance. Therefore, unlike prior non-prehensile\nresearch that relies on complex external perception set-ups, our method\nsuccessfully handles occlusions after sim-to-real transfer to robotic hardware\nwith a simple onboard camera. See our video: https://youtu.be/hW-C8i_HWgs.\n","authors":["Juan Del Aguila Ferrandis","João Moura","Sethu Vijayakumar"],"pdf_url":"https://arxiv.org/pdf/2412.13157v1.pdf","comment":"Conference on Robot Learning (CoRL 2024)"},{"id":"http://arxiv.org/abs/2412.13128v1","updated":"2024-12-17T17:45:58Z","published":"2024-12-17T17:45:58Z","title":"Previous Knowledge Utilization In Online Anytime Belief Space Planning","summary":"  Online planning under uncertainty remains a critical challenge in robotics\nand autonomous systems. While tree search techniques are commonly employed to\nconstruct partial future trajectories within computational constraints, most\nexisting methods discard information from previous planning sessions\nconsidering continuous spaces. This study presents a novel, computationally\nefficient approach that leverages historical planning data in current\ndecision-making processes. We provide theoretical foundations for our\ninformation reuse strategy and introduce an algorithm based on Monte Carlo Tree\nSearch (MCTS) that implements this approach. Experimental results demonstrate\nthat our method significantly reduces computation time while maintaining high\nperformance levels. Our findings suggest that integrating historical planning\ninformation can substantially improve the efficiency of online decision-making\nin uncertain environments, paving the way for more responsive and adaptive\nautonomous systems.\n","authors":["Michael Novitsky","Moran Barenboim","Vadim Indelman"],"pdf_url":"https://arxiv.org/pdf/2412.13128v1.pdf","comment":"10 pages, 4 figures, will be submitted to IEEE Robotics and\n  Automation Letters (RA-L)"},{"id":"http://arxiv.org/abs/2412.13119v1","updated":"2024-12-17T17:41:42Z","published":"2024-12-17T17:41:42Z","title":"Flight Patterns for Swarms of Drones","summary":"  We present flight patterns for a collision-free passage of swarms of drones\nthrough one or more openings. The narrow openings provide drones with access to\nan infrastructure component such as charging stations to charge their depleted\nbatteries and hangars for storage. The flight patterns are a staging area\n(queues) that match the rate at which an infrastructure component and its\nopenings process drones. They prevent collisions and may implement different\npolicies that control the order in which drones pass through an opening. We\nillustrate the flight patterns with a 3D display that uses drones configured\nwith light sources to illuminate shapes.\n","authors":["Shuqin Zhu","Shahram Ghandeharizadeh"],"pdf_url":"https://arxiv.org/pdf/2412.13119v1.pdf","comment":"Appeared in the First International Conference on Holodecks, December\n  15, 2023. Shuqin Zhou and Shahram Ghandeharizadeh. Flight Patterns for Swarms\n  of Drones. In the Proceedings of the First International Conference on\n  Holodecks (Holodecks '23), December 15 2023, Los Angeles, California, USA,\n  29-33. https://doi.org/10.61981/ZFSH2303"},{"id":"http://arxiv.org/abs/2405.08359v2","updated":"2024-12-17T17:31:46Z","published":"2024-05-14T06:55:16Z","title":"GPS-IDS: An Anomaly-based GPS Spoofing Attack Detection Framework for\n  Autonomous Vehicles","summary":"  Autonomous Vehicles (AVs) heavily rely on sensors and communication networks\nlike Global Positioning System (GPS) to navigate autonomously. Prior research\nhas indicated that networks like GPS are vulnerable to cyber-attacks such as\nspoofing and jamming, thus posing serious risks like navigation errors and\nsystem failures. These threats are expected to intensify with the widespread\ndeployment of AVs, making it crucial to detect and mitigate such attacks. This\npaper proposes GPS Intrusion Detection System, or GPS-IDS, an Anomaly-based\nintrusion detection framework to detect GPS spoofing attacks on AVs. The\nframework uses a novel physics-based vehicle behavior model where a GPS\nnavigation model is integrated into the conventional dynamic bicycle model for\naccurate AV behavior representation. Temporal features derived from this\nbehavior model are analyzed using machine learning to detect normal and\nabnormal navigation behaviors. The performance of the GPS-IDS framework is\nevaluated on the AV-GPS-Dataset -- a GPS security dataset for AVs comprising\nreal-world data collected using an AV testbed, and simulated data representing\nurban traffic environments. To the best of our knowledge, this dataset is the\nfirst of its kind and has been publicly released for the global research\ncommunity to address such security challenges.\n","authors":["Murad Mehrab Abrar","Amal Youssef","Raian Islam","Shalaka Satam","Banafsheh Saber Latibari","Salim Hariri","Sicong Shao","Soheil Salehi","Pratik Satam"],"pdf_url":"https://arxiv.org/pdf/2405.08359v2.pdf","comment":"Article under review at IEEE Transactions on Dependable and Secure\n  Computing. For associated AV-GPS-Dataset, see\n  https://github.com/mehrab-abrar/AV-GPS-Dataset"},{"id":"http://arxiv.org/abs/2412.13033v1","updated":"2024-12-17T15:56:40Z","published":"2024-12-17T15:56:40Z","title":"Singularity-Free Guiding Vector Field over Bézier's Curves Applied to\n  Rovers Path Planning and Path Following","summary":"  This paper presents a guidance algorithm for solving the problem of following\nparametric paths, as well as a curvature-varying speed setpoint for land-based\ncar-type wheeled mobile robots (WMRs). The guidance algorithm relies on\nSingularity-Free Guiding Vector Fields SF-GVF. This novel GVF approach expands\nthe desired robot path and the Guiding vector field to a higher dimensional\nspace, in which an angular control function can be found to ensure global\nasymptotic convergence to the desired parametric path while avoiding field\nsingularities. In SF-GVF, paths should follow a parametric definition. This\nfeature makes using Bezier's curves attractive to define the robot's desired\npatch. The curvature-varying speed setpoint, combined with the guidance\nalgorithm, eases the convergence to the path when physical restrictions exist,\nsuch as minimal turning radius or maximal lateral acceleration. We provide\ntheoretical results, simulations, and outdoor experiments using a WMR platform\nassembled with off-the-shelf components.\n","authors":["Alfredo González-Calvin","Lía García-Pérez","Juan Jiménez"],"pdf_url":"https://arxiv.org/pdf/2412.13033v1.pdf","comment":"26 pages, 15 figures"},{"id":"http://arxiv.org/abs/2403.12589v2","updated":"2024-12-17T15:28:10Z","published":"2024-03-19T09:48:18Z","title":"FootstepNet: an Efficient Actor-Critic Method for Fast On-line Bipedal\n  Footstep Planning and Forecasting","summary":"  Designing a humanoid locomotion controller is challenging and classically\nsplit up in sub-problems. Footstep planning is one of those, where the sequence\nof footsteps is defined. Even in simpler environments, finding a minimal\nsequence, or even a feasible sequence, yields a complex optimization problem.\nIn the literature, this problem is usually addressed by search-based algorithms\n(e.g. variants of A*). However, such approaches are either computationally\nexpensive or rely on hand-crafted tuning of several parameters. In this work,\nat first, we propose an efficient footstep planning method to navigate in local\nenvironments with obstacles, based on state-of-the art Deep Reinforcement\nLearning (DRL) techniques, with very low computational requirements for on-line\ninference. Our approach is heuristic-free and relies on a continuous set of\nactions to generate feasible footsteps. In contrast, other methods necessitate\nthe selection of a relevant discrete set of actions. Second, we propose a\nforecasting method, allowing to quickly estimate the number of footsteps\nrequired to reach different candidates of local targets. This approach relies\non inherent computations made by the actor-critic DRL architecture. We\ndemonstrate the validity of our approach with simulation results, and by a\ndeployment on a kid-size humanoid robot during the RoboCup 2023 competition.\n","authors":["Clément Gaspard","Grégoire Passault","Mélodie Daniel","Olivier Ly"],"pdf_url":"https://arxiv.org/pdf/2403.12589v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12990v1","updated":"2024-12-17T15:07:50Z","published":"2024-12-17T15:07:50Z","title":"Future Aspects in Human Action Recognition: Exploring Emerging\n  Techniques and Ethical Influences","summary":"  Visual-based human action recognition can be found in various application\nfields, e.g., surveillance systems, sports analytics, medical assistive\ntechnologies, or human-robot interaction frameworks, and it concerns the\nidentification and classification of individuals' activities within a video.\nSince actions typically occur over a sequence of consecutive images, it is\nparticularly challenging due to the inclusion of temporal analysis, which\nintroduces an extra layer of complexity. However, although multiple approaches\ntry to handle temporal analysis, there are still difficulties because of their\ncomputational cost and lack of adaptability. Therefore, different types of\nvision data, containing transition information between consecutive images,\nprovided by next-generation hardware sensors will guide the robotics community\nin tackling the problem of human action recognition. On the other hand, while\nthere is a plethora of still-image datasets, that researchers can adopt to\ntrain new artificial intelligence models, videos representing human activities\nare of limited capabilities, e.g., small and unbalanced datasets or selected\nwithout control from multiple sources. To this end, generating new and\nrealistic synthetic videos is possible since labeling is performed throughout\nthe data creation process, while reinforcement learning techniques can permit\nthe avoidance of considerable dataset dependence. At the same time, human\nfactors' involvement raises ethical issues for the research community, as\ndoubts and concerns about new technologies already exist.\n","authors":["Antonios Gasteratos","Stavros N. Moutsis","Konstantinos A. Tsintotas","Yiannis Aloimonos"],"pdf_url":"https://arxiv.org/pdf/2412.12990v1.pdf","comment":"2 pages, 1 figure, 40th Anniversary of the IEEE Conference on\n  Robotics and Automation (ICRA@40), Rotterdam, Netherlands | September 23-26,\n  2024"},{"id":"http://arxiv.org/abs/2208.07442v2","updated":"2024-12-17T14:39:42Z","published":"2022-08-15T21:33:30Z","title":"Viability of Robot-supported Flipped Classes in English for Medical Use\n  Reading Comprehension","summary":"  This study delved into the viability of Robot-supported flipped classes in\nEnglish for Medical Purposes reading comprehension. In a 16-session course, the\nreading comprehension and then workspace performance of 444 students, with\nCommercially-Off-The-Shelf and Self-Generated robot flipped classes were\ncompared. The results indicated that the flipped classes brought about a good\ninstructional-learning ambience in postsecondary education for English for\nMedical Purposes (EMP) reading comprehension and adopting proactive approach\nfor workspace performance. In tandem, the Mixed Effect Model revealed that\nstudent participation in the self-generated robot-supported flipped classes\nyielded a larger effect size (+17.6%) than Commercially-Off-The-Shelf\nrobot-supported flipped classes. Analyses produced five contributing moderators\nof EMP reading comprehension and workspace performance: reading proficiency,\nattitude, manner of practicing, as well as student and teacher role.\n","authors":["Amin Rezasoltani","Ehsa Saffari","Farzam Tajdari"],"pdf_url":"https://arxiv.org/pdf/2208.07442v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12953v1","updated":"2024-12-17T14:34:51Z","published":"2024-12-17T14:34:51Z","title":"Efficient Diffusion Transformer Policies with Mixture of Expert\n  Denoisers for Multitask Learning","summary":"  Diffusion Policies have become widely used in Imitation Learning, offering\nseveral appealing properties, such as generating multimodal and discontinuous\nbehavior. As models are becoming larger to capture more complex capabilities,\ntheir computational demands increase, as shown by recent scaling laws.\nTherefore, continuing with the current architectures will present a\ncomputational roadblock. To address this gap, we propose Mixture-of-Denoising\nExperts (MoDE) as a novel policy for Imitation Learning. MoDE surpasses current\nstate-of-the-art Transformer-based Diffusion Policies while enabling\nparameter-efficient scaling through sparse experts and noise-conditioned\nrouting, reducing both active parameters by 40% and inference costs by 90% via\nexpert caching. Our architecture combines this efficient scaling with\nnoise-conditioned self-attention mechanism, enabling more effective denoising\nacross different noise levels. MoDE achieves state-of-the-art performance on\n134 tasks in four established imitation learning benchmarks (CALVIN and\nLIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01\non CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and\nTransformer Diffusion Policies by an average of 57% across 4 benchmarks, while\nusing 90% fewer FLOPs and fewer active parameters compared to default Diffusion\nTransformer architectures. Furthermore, we conduct comprehensive ablations on\nMoDE's components, providing insights for designing efficient and scalable\nTransformer architectures for Diffusion Policies. Code and demonstrations are\navailable at https://mbreuss.github.io/MoDE_Diffusion_Policy/.\n","authors":["Moritz Reuss","Jyothish Pari","Pulkit Agrawal","Rudolf Lioutikov"],"pdf_url":"https://arxiv.org/pdf/2412.12953v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12949v1","updated":"2024-12-17T14:29:12Z","published":"2024-12-17T14:29:12Z","title":"Synthetic Data Generation for Anomaly Detection on Table Grapes","summary":"  Early detection of illnesses and pest infestations in fruit cultivation is\ncritical for maintaining yield quality and plant health. Computer vision and\nrobotics are increasingly employed for the automatic detection of such issues,\nparticularly using data-driven solutions. However, the rarity of these problems\nmakes acquiring and processing the necessary data to train such algorithms a\nsignificant obstacle. One solution to this scarcity is the generation of\nsynthetic high-quality anomalous samples. While numerous methods exist for this\ntask, most require highly trained individuals for setup.\n  This work addresses the challenge of generating synthetic anomalies in an\nautomatic fashion that requires only an initial collection of normal and\nanomalous samples from the user - a task that is straightforward for farmers.\nWe demonstrate the approach in the context of table grape cultivation.\nSpecifically, based on the observation that normal berries present relatively\nsmooth surfaces, while defects result in more complex textures, we introduce a\nDual-Canny Edge Detection (DCED) filter. This filter emphasizes the additional\ntexture indicative of diseases, pest infestations, or other defects. Using\nsegmentation masks provided by the Segment Anything Model, we then select and\nseamlessly blend anomalous berries onto normal ones. We show that the proposed\ndataset augmentation technique improves the accuracy of an anomaly classifier\nfor table grapes and that the approach can be generalized to other fruit types.\n","authors":["Ionut Marian Motoi","Valerio Belli","Alberto Carpineto","Daniele Nardi","Thomas Alessandro Ciarfuglia"],"pdf_url":"https://arxiv.org/pdf/2412.12949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11974v2","updated":"2024-12-17T14:12:56Z","published":"2024-12-16T16:58:28Z","title":"Emma-X: An Embodied Multimodal Action Model with Grounded Chain of\n  Thought and Look-ahead Spatial Reasoning","summary":"  Traditional reinforcement learning-based robotic control methods are often\ntask-specific and fail to generalize across diverse environments or unseen\nobjects and instructions. Visual Language Models (VLMs) demonstrate strong\nscene understanding and planning capabilities but lack the ability to generate\nactionable policies tailored to specific robotic embodiments. To address this,\nVisual-Language-Action (VLA) models have emerged, yet they face challenges in\nlong-horizon spatial reasoning and grounded task planning. In this work, we\npropose the Embodied Multimodal Action Model with Grounded Chain of Thought and\nLook-ahead Spatial Reasoning, Emma-X. Emma-X leverages our constructed\nhierarchical embodiment dataset based on BridgeV2, containing 60,000 robot\nmanipulation trajectories auto-annotated with grounded task reasoning and\nspatial guidance. Additionally, we introduce a trajectory segmentation strategy\nbased on gripper states and motion trajectories, which can help mitigate\nhallucination in grounding subtask reasoning generation. Experimental results\ndemonstrate that Emma-X achieves superior performance over competitive\nbaselines, particularly in real-world robotic tasks requiring spatial\nreasoning.\n","authors":["Qi Sun","Pengfei Hong","Tej Deep Pala","Vernon Toh","U-Xuan Tan","Deepanway Ghosal","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2412.11974v2.pdf","comment":"https://github.com/declare-lab/Emma-X,\n  https://huggingface.co/declare-lab/Emma-X"},{"id":"http://arxiv.org/abs/2412.12894v1","updated":"2024-12-17T13:19:55Z","published":"2024-12-17T13:19:55Z","title":"Design of Restricted Normalizing Flow towards Arbitrary Stochastic\n  Policy with Computational Efficiency","summary":"  This paper proposes a new design method for a stochastic control policy using\na normalizing flow (NF). In reinforcement learning (RL), the policy is usually\nmodeled as a distribution model with trainable parameters. When this\nparameterization has less expressiveness, it would fail to acquiring the\noptimal policy. A mixture model has capability of a universal approximation,\nbut it with too much redundancy increases the computational cost, which can\nbecome a bottleneck when considering the use of real-time robot control. As\nanother approach, NF, which is with additional parameters for invertible\ntransformation from a simple stochastic model as a base, is expected to exert\nhigh expressiveness and lower computational cost. However, NF cannot compute\nits mean analytically due to complexity of the invertible transformation, and\nit lacks reliability because it retains stochastic behaviors after deployment\nfor robot controller. This paper therefore designs a restricted NF (RNF) that\nachieves an analytic mean by appropriately restricting the invertible\ntransformation. In addition, the expressiveness impaired by this restriction is\nregained using bimodal student-t distribution as its base, so-called Bit-RNF.\nIn RL benchmarks, Bit-RNF policy outperformed the previous models. Finally, a\nreal robot experiment demonstrated the applicability of Bit-RNF policy to real\nworld. The attached video is uploaded on youtube: https://youtu.be/R_GJVZDW9bk\n","authors":["Taisuke Kobayashi","Takumi Aotani"],"pdf_url":"https://arxiv.org/pdf/2412.12894v1.pdf","comment":"27 pages, 13 figures"},{"id":"http://arxiv.org/abs/2411.14322v2","updated":"2024-12-17T13:02:10Z","published":"2024-11-21T17:12:47Z","title":"SplatR : Experience Goal Visual Rearrangement with 3D Gaussian Splatting\n  and Dense Feature Matching","summary":"  Experience Goal Visual Rearrangement task stands as a foundational challenge\nwithin Embodied AI, requiring an agent to construct a robust world model that\naccurately captures the goal state. The agent uses this world model to restore\na shuffled scene to its original configuration, making an accurate\nrepresentation of the world essential for successfully completing the task. In\nthis work, we present a novel framework that leverages on 3D Gaussian Splatting\nas a 3D scene representation for experience goal visual rearrangement task.\nRecent advances in volumetric scene representation like 3D Gaussian Splatting,\noffer fast rendering of high quality and photo-realistic novel views. Our\napproach enables the agent to have consistent views of the current and the goal\nsetting of the rearrangement task, which enables the agent to directly compare\nthe goal state and the shuffled state of the world in image space. To compare\nthese views, we propose to use a dense feature matching method with visual\nfeatures extracted from a foundation model, leveraging its advantages of a more\nuniversal feature representation, which facilitates robustness, and\ngeneralization. We validate our approach on the AI2-THOR rearrangement\nchallenge benchmark and demonstrate improvements over the current state of the\nart methods\n","authors":["Arjun P S","Andrew Melnik","Gora Chand Nandi"],"pdf_url":"https://arxiv.org/pdf/2411.14322v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10083v2","updated":"2024-12-17T12:39:09Z","published":"2024-12-13T12:17:30Z","title":"Heterogeneous Multi-Robot Graph Coverage with Proximity and Movement\n  Constraints","summary":"  Multi-Robot Coverage problems have been extensively studied in robotics,\nplanning and multi-agent systems. In this work, we consider the coverage\nproblem when there are constraints on the proximity (e.g., maximum distance\nbetween the agents, or a blue agent must be adjacent to a red agent) and the\nmovement (e.g., terrain traversability and material load capacity) of the\nrobots. Such constraints naturally arise in many real-world applications, e.g.\nin search-and-rescue and maintenance operations. Given such a setting, the goal\nis to compute a covering tour of the graph with a minimum number of steps, and\nthat adheres to the proximity and movement constraints. For this problem, our\ncontributions are four: (i) a formal formulation of the problem, (ii) an exact\nalgorithm that is FPT in F, d and tw, the set of robot formations that encode\nthe proximity constraints, the maximum nodes degree, and the tree-width of the\ngraph, respectively, (iii) for the case that the graph is a tree: a PTAS\napproximation scheme, that given an approximation parameter epsilon, produces a\ntour that is within a epsilon times error(||F||, d) of the optimal one, and the\ncomputation runs in time poly(n) times h(1/epsilon,||F||). (iv) for the case\nthat the graph is a tree, with $k=3$ robots, and the constraint is that all\nagents are connected: a PTAS scheme with multiplicative approximation error of\n1+O(epsilon), independent of the maximal degree d.\n","authors":["Dolev Mutzari","Yonatan Aumann","Sarit Kraus"],"pdf_url":"https://arxiv.org/pdf/2412.10083v2.pdf","comment":"12 pages, 7 figures, to be published in the 39th Annual AAAI\n  Conference on Artificial Intelligence"},{"id":"http://arxiv.org/abs/2412.11764v2","updated":"2024-12-17T12:04:49Z","published":"2024-12-16T13:31:26Z","title":"What Matters in Learning A Zero-Shot Sim-to-Real RL Policy for Quadrotor\n  Control? A Comprehensive Study","summary":"  Executing precise and agile flight maneuvers is critical for quadrotors in\nvarious applications. Traditional quadrotor control approaches are limited by\ntheir reliance on flat trajectories or time-consuming optimization, which\nrestricts their flexibility. Recently, RL-based policy has emerged as a\npromising alternative due to its ability to directly map observations to\nactions, reducing the need for detailed system knowledge and actuation\nconstraints. However, a significant challenge remains in bridging the\nsim-to-real gap, where RL-based policies often experience instability when\ndeployed in real world. In this paper, we investigate key factors for learning\nrobust RL-based control policies that are capable of zero-shot deployment in\nreal-world quadrotors. We identify five critical factors and we develop a\nPPO-based training framework named SimpleFlight, which integrates these five\ntechniques. We validate the efficacy of SimpleFlight on Crazyflie quadrotor,\ndemonstrating that it achieves more than a 50% reduction in trajectory tracking\nerror compared to state-of-the-art RL baselines, and achieves 70% improvement\nover the traditional MPC. The policy derived by SimpleFlight consistently\nexcels across both smooth polynominal trajectories and challenging infeasible\nzigzag trajectories on small thrust-to-weight quadrotors. In contrast, baseline\nmethods struggle with high-speed or infeasible trajectories. To support further\nresearch and reproducibility, we integrate SimpleFlight into a GPU-based\nsimulator Omnidrones and provide open-source access to the code and model\ncheckpoints. We hope SimpleFlight will offer valuable insights for advancing\nRL-based quadrotor control. For more details, visit our project website at\nhttps://sites.google.com/view/simpleflight/.\n","authors":["Jiayu Chen","Chao Yu","Yuqing Xie","Feng Gao","Yinuo Chen","Shu'ang Yu","Wenhao Tang","Shilong Ji","Mo Mu","Yi Wu","Huazhong Yang","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2412.11764v2.pdf","comment":"The first two authors contribute equally"},{"id":"http://arxiv.org/abs/2412.00395v2","updated":"2024-12-17T12:04:32Z","published":"2024-11-30T08:34:10Z","title":"On Foundation Models for Dynamical Systems from Purely Synthetic Data","summary":"  Foundation models have demonstrated remarkable generalization, data\nefficiency, and robustness properties across various domains. In this paper, we\nexplore the feasibility of foundation models for applications in the control\ndomain. The success of these models is enabled by large-scale pretaining on\nInternet-scale datasets. These are available in fields like natural language\nprocessing and computer vision, but do not exist for dynamical systems. We\naddress this challenge by pretraining a transformer-based foundation model\nexclusively on synthetic data and propose to sample dynamics functions from a\nreproducing kernel Hilbert space. Our pretrained model generalizes for\nprediction tasks across different dynamical systems, which we validate in\nsimulation and hardware experiments, including cart-pole and Furuta pendulum\nsetups. Additionally, the model can be fine-tuned effectively to new systems to\nincrease performance even further. Our results demonstrate the feasibility of\nfoundation models for dynamical systems that outperform specialist models in\nterms of generalization, data efficiency, and robustness.\n","authors":["Martin Ziegler","Andres Felipe Posada-Moreno","Friedrich Solowjow","Sebastian Trimpe"],"pdf_url":"https://arxiv.org/pdf/2412.00395v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2412.12825v1","updated":"2024-12-17T11:47:06Z","published":"2024-12-17T11:47:06Z","title":"Enhancing Exploration Efficiency using Uncertainty-Aware Information\n  Prediction","summary":"  Autonomous exploration is a crucial aspect of robotics, enabling robots to\nexplore unknown environments and generate maps without prior knowledge. This\npaper proposes a method to enhance exploration efficiency by integrating neural\nnetwork-based occupancy grid map prediction with uncertainty-aware Bayesian\nneural network. Uncertainty from neural network-based occupancy grid map\nprediction is probabilistically integrated into mutual information for\nexploration. To demonstrate the effectiveness of the proposed method, we\nconducted comparative simulations within a frontier exploration framework in a\nrealistic simulator environment against various information metrics. The\nproposed method showed superior performance in terms of exploration efficiency.\n","authors":["Seunghwan Kim","Heejung Shin","Gaeun Yim","Changseung Kim","Hyondong Oh"],"pdf_url":"https://arxiv.org/pdf/2412.12825v1.pdf","comment":"7pages"},{"id":"http://arxiv.org/abs/2403.16238v3","updated":"2024-12-17T11:08:51Z","published":"2024-03-24T17:00:01Z","title":"KITchen: A Real-World Benchmark and Dataset for 6D Object Pose\n  Estimation in Kitchen Environments","summary":"  Despite the recent progress on 6D object pose estimation methods for robotic\ngrasping, a substantial performance gap persists between the capabilities of\nthese methods on existing datasets and their efficacy in real-world grasping\nand mobile manipulation tasks, particularly when robots rely solely on their\nmonocular egocentric field of view (FOV). Existing real-world datasets\nprimarily focus on table-top grasping scenarios, where a robot arm is placed in\na fixed position and the objects are centralized within the FOV of fixed\nexternal camera(s). Assessing performance on such datasets may not accurately\nreflect the challenges encountered in everyday grasping and mobile manipulation\ntasks within kitchen environments such as retrieving objects from higher\nshelves, sinks, dishwashers, ovens, refrigerators, or microwaves. To address\nthis gap, we present KITchen, a novel benchmark designed specifically for\nestimating the 6D poses of objects located in diverse positions within kitchen\nsettings. For this purpose, we recorded a comprehensive dataset comprising\naround 205k real-world RGBD images for 111 kitchen objects captured in two\ndistinct kitchens, utilizing a humanoid robot with its egocentric perspectives.\nSubsequently, we developed a semi-automated annotation pipeline, to streamline\nthe labeling process of such datasets, resulting in the generation of 2D object\nlabels, 2D object segmentation masks, and 6D object poses with minimal human\neffort. The benchmark, the dataset, and the annotation pipeline will be\npublicly available at https://kitchen-dataset.github.io/KITchen.\n","authors":["Abdelrahman Younes","Tamim Asfour"],"pdf_url":"https://arxiv.org/pdf/2403.16238v3.pdf","comment":"This work has been accepted for publishing at The 2024 IEEE-RAS\n  International Conference on Humanoid Robots"},{"id":"http://arxiv.org/abs/2412.12776v1","updated":"2024-12-17T10:37:00Z","published":"2024-12-17T10:37:00Z","title":"Physical simulation of Marsupial UAV-UGV Systems Connected by a Hanging\n  Tether using Gazebo","summary":"  This paper presents a ROS 2-based simulator framework for tethered UAV-UGV\nmarsupial systems in Gazebo. The framework models interactions among a UAV, a\nUGV, and a winch with dynamically adjustable length and slack of the tether. It\nsupports both manual control and automated trajectory tracking, with the winch\nadjusting the length of the tether based on the relative distance between the\nrobots. The simulator's performance is demonstrated through experiments,\nincluding comparisons with real-world data, showcasing its capability to\nsimulate tethered robotic systems. The framework offers a flexible tool for\nresearchers exploring tethered robot dynamics. The source code of the simulator\nis publicly available for the research community.\n","authors":["Jose Enrique Maese","Fernando Caballero","Luis Merino"],"pdf_url":"https://arxiv.org/pdf/2412.12776v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12740v1","updated":"2024-12-17T10:03:39Z","published":"2024-12-17T10:03:39Z","title":"Open-World Panoptic Segmentation","summary":"  Perception is a key building block of autonomously acting vision systems such\nas autonomous vehicles. It is crucial that these systems are able to understand\ntheir surroundings in order to operate safely and robustly. Additionally,\nautonomous systems deployed in unconstrained real-world scenarios must be able\nof dealing with novel situations and object that have never been seen before.\nIn this article, we tackle the problem of open-world panoptic segmentation,\ni.e., the task of discovering new semantic categories and new object instances\nat test time, while enforcing consistency among the categories that we\nincrementally discover. We propose Con2MAV, an approach for open-world panoptic\nsegmentation that extends our previous work, ContMAV, which was developed for\nopen-world semantic segmentation. Through extensive experiments across multiple\ndatasets, we show that our model achieves state-of-the-art results on\nopen-world segmentation tasks, while still performing competitively on the\nknown categories. We will open-source our implementation upon acceptance.\nAdditionally, we propose PANIC (Panoptic ANomalies In Context), a benchmark for\nevaluating open-world panoptic segmentation in autonomous driving scenarios.\nThis dataset, recorded with a multi-modal sensor suite mounted on a car,\nprovides high-quality, pixel-wise annotations of anomalous objects at both\nsemantic and instance level. Our dataset contains 800 images, with more than 50\nunknown classes, i.e., classes that do not appear in the training set, and 4000\nobject instances, making it an extremely challenging dataset for open-world\nsegmentation tasks in the autonomous driving scenario. We provide competitions\nfor multiple open-world tasks on a hidden test set. Our dataset and\ncompetitions are available at https://www.ipb.uni-bonn.de/data/panic.\n","authors":["Matteo Sodano","Federico Magistri","Jens Behley","Cyrill Stachniss"],"pdf_url":"https://arxiv.org/pdf/2412.12740v1.pdf","comment":"Submitted to PAMI"},{"id":"http://arxiv.org/abs/2412.12716v1","updated":"2024-12-17T09:30:31Z","published":"2024-12-17T09:30:31Z","title":"Unsupervised UAV 3D Trajectories Estimation with Sparse Point Clouds","summary":"  Compact UAV systems, while advancing delivery and surveillance, pose\nsignificant security challenges due to their small size, which hinders\ndetection by traditional methods. This paper presents a cost-effective,\nunsupervised UAV detection method using spatial-temporal sequence processing to\nfuse multiple LiDAR scans for accurate UAV tracking in real-world scenarios.\nOur approach segments point clouds into foreground and background, analyzes\nspatial-temporal data, and employs a scoring mechanism to enhance detection\naccuracy. Tested on a public dataset, our solution placed 4th in the CVPR 2024\nUG2+ Challenge, demonstrating its practical effectiveness. We plan to\nopen-source all designs, code, and sample data for the research community\ngithub.com/lianghanfang/UnLiDAR-UAV-Est.\n","authors":["Hanfang Liang","Yizhuo Yang","Jinming Hu","Jianfei Yang","Fen Liu","Shenghai Yuan"],"pdf_url":"https://arxiv.org/pdf/2412.12716v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12698v1","updated":"2024-12-17T09:16:28Z","published":"2024-12-17T09:16:28Z","title":"Audio Array-Based 3D UAV Trajectory Estimation with LiDAR\n  Pseudo-Labeling","summary":"  As small unmanned aerial vehicles (UAVs) become increasingly prevalent, there\nis growing concern regarding their impact on public safety and privacy,\nhighlighting the need for advanced tracking and trajectory estimation\nsolutions. In response, this paper introduces a novel framework that utilizes\naudio array for 3D UAV trajectory estimation. Our approach incorporates a\nself-supervised learning model, starting with the conversion of audio data into\nmel-spectrograms, which are analyzed through an encoder to extract crucial\ntemporal and spectral information. Simultaneously, UAV trajectories are\nestimated using LiDAR point clouds via unsupervised methods. These LiDAR-based\nestimations act as pseudo labels, enabling the training of an Audio Perception\nNetwork without requiring labeled data. In this architecture, the LiDAR-based\nsystem operates as the Teacher Network, guiding the Audio Perception Network,\nwhich serves as the Student Network. Once trained, the model can independently\npredict 3D trajectories using only audio signals, with no need for LiDAR data\nor external ground truth during deployment. To further enhance precision, we\napply Gaussian Process modeling for improved spatiotemporal tracking. Our\nmethod delivers top-tier performance on the MMAUD dataset, establishing a new\nbenchmark in trajectory estimation using self-supervised learning techniques\nwithout reliance on ground truth annotations.\n","authors":["Allen Lei","Tianchen Deng","Han Wang","Jianfei Yang","Shenghai Yuan"],"pdf_url":"https://arxiv.org/pdf/2412.12698v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12650v1","updated":"2024-12-17T08:19:40Z","published":"2024-12-17T08:19:40Z","title":"Neural-Network-Driven Reward Prediction as a Heuristic: Advancing\n  Q-Learning for Mobile Robot Path Planning","summary":"  Q-learning is a widely used reinforcement learning technique for solving path\nplanning problems. It primarily involves the interaction between an agent and\nits environment, enabling the agent to learn an optimal strategy that maximizes\ncumulative rewards. Although many studies have reported the effectiveness of\nQ-learning, it still faces slow convergence issues in practical applications.\nTo address this issue, we propose the NDR-QL method, which utilizes neural\nnetwork outputs as heuristic information to accelerate the convergence process\nof Q-learning. Specifically, we improved the dual-output neural network model\nby introducing a start-end channel separation mechanism and enhancing the\nfeature fusion process. After training, the proposed NDR model can output a\nnarrowly focused optimal probability distribution, referred to as the\nguideline, and a broadly distributed suboptimal distribution, referred to as\nthe region. Subsequently, based on the guideline prediction, we calculate the\ncontinuous reward function for the Q-learning method, and based on the region\nprediction, we initialize the Q-table with a bias. We conducted training,\nvalidation, and path planning simulation experiments on public datasets. The\nresults indicate that the NDR model outperforms previous methods by up to 5\\%\nin prediction accuracy. Furthermore, the proposed NDR-QL method improves the\nconvergence speed of the baseline Q-learning method by 90\\% and also surpasses\nthe previously improved Q-learning methods in path quality metrics.\n","authors":["Yiming Ji","Kaijie Yun","Yang Liu","Zongwu Xie","Hong Liu"],"pdf_url":"https://arxiv.org/pdf/2412.12650v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18393v2","updated":"2024-12-17T08:10:04Z","published":"2024-02-28T15:13:33Z","title":"Decictor: Towards Evaluating the Robustness of Decision-Making in\n  Autonomous Driving Systems","summary":"  Autonomous Driving System (ADS) testing is crucial in ADS development, with\nthe current primary focus being on safety. However, the evaluation of\nnon-safety-critical performance, particularly the ADS's ability to make optimal\ndecisions and produce optimal paths for autonomous vehicles (AVs), is also\nvital to ensure the intelligence and reduce risks of AVs. Currently, there is\nlittle work dedicated to assessing the robustness of ADSs' path-planning\ndecisions (PPDs), i.e., whether an ADS can maintain the optimal PPD after an\ninsignificant change in the environment. The key challenges include the lack of\nclear oracles for assessing PPD optimality and the difficulty in searching for\nscenarios that lead to non-optimal PPDs. To fill this gap, in this paper, we\nfocus on evaluating the robustness of ADSs' PPDs and propose the first method,\nDecictor, for generating non-optimal decision scenarios (NoDSs), where the ADS\ndoes not plan optimal paths for AVs. Decictor comprises three main components:\nNon-invasive Mutation, Consistency Check, and Feedback. To overcome the oracle\nchallenge, Non-invasive Mutation is devised to implement conservative\nmodifications, ensuring the preservation of the original optimal path in the\nmutated scenarios. Subsequently, the Consistency Check is applied to determine\nthe presence of non-optimal PPDs by comparing the driving paths in the original\nand mutated scenarios. To deal with the challenge of large environment space,\nwe design Feedback metrics that integrate spatial and temporal dimensions of\nthe AV's movement. These metrics are crucial for effectively steering the\ngeneration of NoDSs. We evaluate Decictor on Baidu Apollo, an open-source and\nproduction-grade ADS. The experimental results validate the effectiveness of\nDecictor in detecting non-optimal PPDs of ADSs.\n","authors":["Mingfei Cheng","Yuan Zhou","Xiaofei Xie","Junjie Wang","Guozhu Meng","Kairui Yang"],"pdf_url":"https://arxiv.org/pdf/2402.18393v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12602v1","updated":"2024-12-17T06:59:29Z","published":"2024-12-17T06:59:29Z","title":"Don't Yell at Your Robot: Physical Correction as the Collaborative\n  Interface for Language Model Powered Robots","summary":"  We present a novel approach for enhancing human-robot collaboration using\nphysical interactions for real-time error correction of large language model\n(LLM) powered robots. Unlike other methods that rely on verbal or text\ncommands, the robot leverages an LLM to proactively executes 6 DoF linear\nDynamical System (DS) commands using a description of the scene in natural\nlanguage. During motion, a human can provide physical corrections, used to\nre-estimate the desired intention, also parameterized by linear DS. This\ncorrected DS can be converted to natural language and used as part of the\nprompt to improve future LLM interactions. We provide proof-of-concept result\nin a hybrid real+sim experiment, showcasing physical interaction as a new\npossibility for LLM powered human-robot interface.\n","authors":["Chuye Zhang","Yifei Simon Shao","Harshil Parekh","Junyao Shi","Pratik Chaudhari","Vijay Kumar","Nadia Figueroa"],"pdf_url":"https://arxiv.org/pdf/2412.12602v1.pdf","comment":"7 pages, 3 figures; Generative Modeling meets HRI - RSS'24 Workshop"},{"id":"http://arxiv.org/abs/2411.14358v2","updated":"2024-12-17T06:40:05Z","published":"2024-11-21T17:58:07Z","title":"InCrowd-VI: A Realistic Visual-Inertial Dataset for Evaluating SLAM in\n  Indoor Pedestrian-Rich Spaces for Human Navigation","summary":"  Simultaneous localization and mapping (SLAM) techniques can be used to\nnavigate the visually impaired, but the development of robust SLAM solutions\nfor crowded spaces is limited by the lack of realistic datasets. To address\nthis, we introduce InCrowd-VI, a novel visual-inertial dataset specifically\ndesigned for human navigation in indoor pedestrian-rich environments. Recorded\nusing Meta Aria Project glasses, it captures realistic scenarios without\nenvironmental control. InCrowd-VI features 58 sequences totaling a 5 km\ntrajectory length and 1.5 hours of recording time, including RGB, stereo\nimages, and IMU measurements. The dataset captures important challenges such as\npedestrian occlusions, varying crowd densities, complex layouts, and lighting\nchanges. Ground-truth trajectories, accurate to approximately 2 cm, are\nprovided in the dataset, originating from the Meta Aria project machine\nperception SLAM service. In addition, a semi-dense 3D point cloud of scenes is\nprovided for each sequence. The evaluation of state-of-the-art visual odometry\n(VO) and SLAM algorithms on InCrowd-VI revealed severe performance limitations\nin these realistic scenarios. Under challenging conditions, systems exceeded\nthe required localization accuracy of 0.5 meters and the 1\\% drift threshold,\nwith classical methods showing drift up to 5-10\\%. While deep learning-based\napproaches maintained high pose estimation coverage (>90\\%), they failed to\nachieve real-time processing speeds necessary for walking pace navigation.\nThese results demonstrate the need and value of a new dataset to advance SLAM\nresearch for visually impaired navigation in complex indoor environments. The\ndataset and associated tools are publicly available at\nhttps://incrowd-vi.cloudlab.zhaw.ch/.\n","authors":["Marziyeh Bamdad","Hans-Peter Hutter","Alireza Darvishy"],"pdf_url":"https://arxiv.org/pdf/2411.14358v2.pdf","comment":"24 pages, 8 figures, 6 tables"},{"id":"http://arxiv.org/abs/2311.16900v2","updated":"2024-12-17T06:06:02Z","published":"2023-11-28T15:58:13Z","title":"Lane-Keeping Control of Autonomous Vehicles Through a Soft-Constrained\n  Iterative LQR","summary":"  The accurate prediction of smooth steering inputs is crucial for automotive\napplications because control actions with jitter might cause the vehicle system\nto become unstable. To address this problem in automobile lane-keeping control\nwithout the use of additional smoothing algorithms, we developed a novel\nsoft-constrained iterative linear quadratic regulator (soft-CILQR) algorithm by\nintegrating CILQR algorithm and a model predictive control (MPC) constraint\nrelaxation method. We incorporated slack variables into the state and control\nbarrier functions of the soft-CILQR solver to soften the constraints in the\noptimization process such that control input stabilization can be achieved in a\ncomputationally simple manner. Two types of automotive lane-keeping experiments\n(numerical simulations and experiments involving challenging vision-based\nmaneuvers) were conducted with a linear system dynamics model to test the\nperformance of the proposed soft-CILQR algorithm, and its performance was\ncompared with that of the CILQR algorithm. In the numerical simulations, the\nsoft-CILQR and CILQR solvers managed to drive the system toward the reference\nstate asymptotically; however, the soft-CILQR solver obtained smooth steering\ninput trajectories more easily than did the CILQR solver under conditions\ninvolving additive disturbances. The results of the vision-based experiments in\nwhich an ego vehicle drove in perturbed TORCS environments with various road\nfriction settings were consistent with those of the numerical tests. The\nproposed soft-CILQR algorithm achieved an average runtime of 2.55 ms and is\nthus applicable for real-time autonomous driving scenarios.\n","authors":["Der-Hau Lee"],"pdf_url":"https://arxiv.org/pdf/2311.16900v2.pdf","comment":"17 figures, 13 pages"},{"id":"http://arxiv.org/abs/2409.09777v4","updated":"2024-12-17T05:14:03Z","published":"2024-09-15T15:55:24Z","title":"DiFSD: Ego-Centric Fully Sparse Paradigm with Uncertainty Denoising and\n  Iterative Refinement for Efficient End-to-End Self-Driving","summary":"  Current end-to-end autonomous driving methods resort to unifying modular\ndesigns for various tasks (e.g. perception, prediction and planning). Although\noptimized in a planning-oriented spirit with a fully differentiable framework,\nexisting end-to-end driving systems without ego-centric designs still suffer\nfrom unsatisfactory performance and inferior efficiency, owing to the\nrasterized scene representation learning and redundant information\ntransmission. In this paper, we revisit the human driving behavior and propose\nan ego-centric fully sparse paradigm, named DiFSD, for end-to-end self-driving.\nSpecifically, DiFSD mainly consists of sparse perception, hierarchical\ninteraction and iterative motion planner. The sparse perception module performs\ndetection, tracking and online mapping based on sparse representation of the\ndriving scene. The hierarchical interaction module aims to select the Closest\nIn-Path Vehicle / Stationary (CIPV / CIPS) from coarse to fine, benefiting from\nan additional geometric prior. As for the iterative motion planner, both\nselected interactive agents and ego-vehicle are considered for joint motion\nprediction, where the output multi-modal ego-trajectories are optimized in an\niterative fashion. Besides, both position-level motion diffusion and\ntrajectory-level planning denoising are introduced for uncertainty modeling,\nthus facilitating the training stability and convergence of the whole\nframework. Extensive experiments conducted on nuScenes and Bench2Drive datasets\ndemonstrate the superior planning performance and great efficiency of DiFSD.\n","authors":["Haisheng Su","Wei Wu","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2409.09777v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12542v1","updated":"2024-12-17T05:09:36Z","published":"2024-12-17T05:09:36Z","title":"Bots against Bias: Critical Next Steps for Human-Robot Interaction","summary":"  We humans are biased - and our robotic creations are biased, too. Bias is a\nnatural phenomenon that drives our perceptions and behavior, including when it\ncomes to socially expressive robots that have humanlike features. Recognizing\nthat we embed bias, knowingly or not, within the design of such robots is\ncrucial to studying its implications for people in modern societies. In this\nchapter, I consider the multifaceted question of bias in the context of\nhumanoid, AI-enabled, and expressive social robots: Where does bias arise, what\ndoes it look like, and what can (or should) we do about it. I offer\nobservations on human-robot interaction (HRI) along two parallel tracks: (1)\nrobots designed in bias-conscious ways and (2) robots that may help us tackle\nbias in the human world. I outline a curated selection of cases for each track\ndrawn from the latest HRI research and positioned against social, legal, and\nethical factors. I also propose a set of critical next steps to tackle the\nchallenges and opportunities on bias within HRI research and practice.\n","authors":["Katie Seaborn"],"pdf_url":"https://arxiv.org/pdf/2412.12542v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.10426v4","updated":"2024-12-17T03:39:17Z","published":"2023-09-19T08:40:46Z","title":"Multi-Object Graph Affordance Network: Goal-Oriented Planning through\n  Learned Compound Object Affordances","summary":"  Learning object affordances is an effective tool in the field of robot\nlearning. While the data-driven models investigate affordances of single or\npaired objects, there is a gap in the exploration of affordances of compound\nobjects composed of an arbitrary number of objects. We propose the Multi-Object\nGraph Affordance Network which models complex compound object affordances by\nlearning the outcomes of robot actions that facilitate interactions between an\nobject and a compound. Given the depth images of the objects, the object\nfeatures are extracted via convolution operations and encoded in the nodes of\ngraph neural networks. Graph convolution operations are used to encode the\nstate of the compounds, which are used as input to decoders to predict the\noutcome of the object-compound interactions. After learning the compound object\naffordances, given different tasks, the learned outcome predictors are used to\nplan sequences of stack actions that involve stacking objects on top of each\nother, inserting smaller objects into larger containers and passing through\nring-like objects through poles. We showed that our system successfully modeled\nthe affordances of compound objects that include concave and convex objects, in\nboth simulated and real-world environments. We benchmarked our system with a\nbaseline model to highlight its advantages.\n","authors":["Tuba Girgin","Emre Ugur"],"pdf_url":"https://arxiv.org/pdf/2309.10426v4.pdf","comment":"This work has been accepted by the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2412.12448v1","updated":"2024-12-17T01:24:02Z","published":"2024-12-17T01:24:02Z","title":"Task-Parameter Nexus: Task-Specific Parameter Learning for Model-Based\n  Control","summary":"  This paper presents the Task-Parameter Nexus (TPN), a learning-based approach\nfor online determination of the (near-)optimal control parameters of\nmodel-based controllers (MBCs) for tracking tasks. In TPN, a deep neural\nnetwork is introduced to predict the control parameters for any given tracking\ntask at runtime, especially when optimal parameters for new tasks are not\nimmediately available. To train this network, we constructed a trajectory bank\nwith various speeds and curvatures that represent different motion\ncharacteristics. Then, for each trajectory in the bank, we auto-tune the\noptimal control parameters offline and use them as the corresponding ground\ntruth. With this dataset, the TPN is trained by supervised learning. We\nevaluated the TPN on the quadrotor platform. In simulation experiments, it is\nshown that the TPN can predict near-optimal control parameters for a spectrum\nof tracking tasks, demonstrating its robust generalization capabilities to\nunseen tasks.\n","authors":["Sheng Cheng","Ran Tao","Yuliang Gu","Shenlong Wang","Xiaofeng Wang","Naira Hovakimyan"],"pdf_url":"https://arxiv.org/pdf/2412.12448v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12442v1","updated":"2024-12-17T01:10:18Z","published":"2024-12-17T01:10:18Z","title":"Multi-Task Reinforcement Learning for Quadrotors","summary":"  Reinforcement learning (RL) has shown great effectiveness in quadrotor\ncontrol, enabling specialized policies to develop even human-champion-level\nperformance in single-task scenarios. However, these specialized policies often\nstruggle with novel tasks, requiring a complete retraining of the policy from\nscratch. To address this limitation, this paper presents a novel multi-task\nreinforcement learning (MTRL) framework tailored for quadrotor control,\nleveraging the shared physical dynamics of the platform to enhance sample\nefficiency and task performance. By employing a multi-critic architecture and\nshared task encoders, our framework facilitates knowledge transfer across\ntasks, enabling a single policy to execute diverse maneuvers, including\nhigh-speed stabilization, velocity tracking, and autonomous racing. Our\nexperimental results, validated both in simulation and real-world scenarios,\ndemonstrate that our framework outperforms baseline approaches in terms of\nsample efficiency and overall task performance.\n","authors":["Jiaxu Xing","Ismail Geles","Yunlong Song","Elie Aljalbout","Davide Scaramuzza"],"pdf_url":"https://arxiv.org/pdf/2412.12442v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12437v1","updated":"2024-12-17T01:05:15Z","published":"2024-12-17T01:05:15Z","title":"Swarm Intelligence in Collision-free Formation Control for Multi-UAV\n  Systems with 3D Obstacle Avoidance Maneuvers","summary":"  Recent advances in multi-agent systems manipulation have demonstrated a\nrising demand for the implementation of multi-UAV systems in urban areas which\nare always subjected to the presence of static and dynamic obstacles. The focus\nof the presented research is on the introduction of a nature-inspired\ncollision-free control for a multi-UAV system considering obstacle avoidance\nmaneuvers. Inspired by the collective behavior of tilapia fish and pigeon, the\npresented framework in this study uses a centralized controller for the optimal\nformation control/recovery, which is defined by probabilistic Lloyd's\nalgorithm, while it uses a distributed controller for the intervehicle\ncollision and obstacle avoidance. Further, the presented framework has been\nextended to the 3D space with 3D maneuvers. Finally, the presented framework\nhas been applied to a multi-UAV system in 2D and 3D scenarios, and obtained\nresults demonstrated the validity of the presented method in the presence of\nbuildings and different types of obstacles.\n","authors":["Reza Ahmadvand","Sarah Sharif","Yaser Banad"],"pdf_url":"https://arxiv.org/pdf/2412.12437v1.pdf","comment":"7 Figures, 2 Tables"},{"id":"http://arxiv.org/abs/2410.06325v2","updated":"2024-12-17T00:32:17Z","published":"2024-10-08T20:02:02Z","title":"Meta-Learning Augmented MPC for Disturbance-Aware Motion Planning and\n  Control of Quadrotors","summary":"  A major challenge in autonomous flights is unknown disturbances, which can\njeopardize safety and lead to collisions, especially in obstacle-rich\nenvironments. This paper presents a disturbance-aware motion planning and\ncontrol framework designed for autonomous aerial flights. The framework is\ncomposed of two key components: a disturbance-aware motion planner and a\ntracking controller. The disturbance-aware motion planner consists of a\npredictive control scheme and a learned model of disturbances that is adapted\nonline. The tracking controller is designed using contraction control methods\nto provide safety bounds on the quadrotor behaviour in the vicinity of the\nobstacles with respect to the disturbance-aware motion plan. Finally, the\nalgorithm is tested in simulation scenarios with a quadrotor facing strong\ncrosswind and ground-induced disturbances.\n","authors":["Dženan Lapandić","Fengze Xie","Christos K. Verginis","Soon-Jo Chung","Dimos V. Dimarogonas","Bo Wahlberg"],"pdf_url":"https://arxiv.org/pdf/2410.06325v2.pdf","comment":"6 pages, 3 figures, accepted for publication in L-CSS"},{"id":"http://arxiv.org/abs/2412.12427v1","updated":"2024-12-17T00:31:11Z","published":"2024-12-17T00:31:11Z","title":"Ultra-wideband Time Difference of Arrival Indoor Localization: From\n  Sensor Placement to System Evaluation","summary":"  Wireless indoor localization has attracted significant research interest due\nto its high accuracy, low cost, lightweight design, and low power consumption.\nSpecifically, ultra-wideband (UWB) time difference of arrival (TDOA)-based\nlocalization has emerged as a scalable positioning solution for mobile robots,\nconsumer electronics, and wearable devices, featuring good accuracy and\nreliability. While UWB TDOA-based localization systems rely on the deployment\nof UWB radio sensors as positioning landmarks, existing works often assume\nthese placements are predetermined or study the sensor placement problem alone\nwithout evaluating it in practical scenarios. In this article, we bridge this\ngap by approaching the UWB TDOA localization from a system-level perspective,\nintegrating sensor placement as a key component and conducting practical\nevaluation in real-world scenarios. Through extensive real-world experiments,\nwe demonstrate the accuracy and robustness of our localization system,\ncomparing its performance to the theoretical lower bounds. Using a challenging\nmulti-room environment as a case study, we illustrate the full system\nconstruction process, from sensor placement optimization to real-world\ndeployment. Our evaluation, comprising a cumulative total of 39 minutes of\nreal-world experiments involving up to five agents and covering 2608 meters\nacross four distinct scenarios, provides valuable insights and guidelines for\nconstructing UWB TDOA localization systems.\n","authors":["Wenda Zhao","Abhishek Goudar","Mingliang Tang","Angela P. Schoellig"],"pdf_url":"https://arxiv.org/pdf/2412.12427v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23022v2","updated":"2024-12-17T22:29:46Z","published":"2024-10-30T13:52:43Z","title":"Online Intrinsic Rewards for Decision Making Agents from Large Language\n  Model Feedback","summary":"  Automatically synthesizing dense rewards from natural language descriptions\nis a promising paradigm in reinforcement learning (RL), with applications to\nsparse reward problems, open-ended exploration, and hierarchical skill design.\nRecent works have made promising steps by exploiting the prior knowledge of\nlarge language models (LLMs). However, these approaches suffer from important\nlimitations: they are either not scalable to problems requiring billions of\nenvironment samples, due to requiring LLM annotations for each observation, or\nthey require a diverse offline dataset, which may not exist or be impossible to\ncollect. In this work, we address these limitations through a combination of\nalgorithmic and systems-level contributions. We propose \\oni, a distributed\narchitecture that simultaneously learns an RL policy and an intrinsic reward\nfunction using LLM feedback. Our approach annotates the agent's collected\nexperience via an asynchronous LLM server, which is then distilled into an\nintrinsic reward model. We explore a range of algorithmic choices for reward\nmodeling with varying complexity, including hashing, classification, and\nranking models. By studying their relative tradeoffs, we shed light on\nquestions regarding intrinsic reward design for sparse reward problems. Our\napproach achieves state-of-the-art performance across a range of challenging,\nsparse reward tasks from the NetHack Learning Environment in a simple unified\nprocess, solely using the agent's gathered experience, without requiring\nexternal datasets. We make our code available at\n\\url{https://github.com/facebookresearch/oni}.\n","authors":["Qinqing Zheng","Mikael Henaff","Amy Zhang","Aditya Grover","Brandon Amos"],"pdf_url":"https://arxiv.org/pdf/2410.23022v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13973v3","updated":"2024-12-17T22:20:22Z","published":"2024-10-17T18:57:15Z","title":"MarineFormer: A Spatio-Temporal Attention Model for USV Navigation in\n  Dynamic Marine Environments","summary":"  Navigating autonomously in marine environments including dynamic and static\nobstacles, and strong flow disturbances, such as in high-flow rivers, poses\nsignificant challenges for USVs. To address these challenges, we propose a\nnovel methodology that leverages two types of attention: spatial attention,\nwhich learns to integrate diverse environmental factors and sensory information\ninto navigation decisions, and temporal attention within a transformer\nframework to account for the dynamic, continuously changing nature of the\nenvironment. We devise MarineFormer, a Trans${\\bf \\text{former}}$-based\nnavigation policy for dynamic ${\\bf \\text{Marine}}$ environments, trained\nend-to-end through reinforcement learning (RL). At its core, MarineFormer uses\ngraph attention to capture spatial information and a transformer architecture\nto process temporal sequences in an environment that simulates a 2D turbulent\nmarine condition involving multiple static and dynamic obstacles. We\nextensively evaluate the performance of the proposed method versus the\nstate-of-the-art methods, as well as other classical planners. Our approach\noutperforms the state-of-the-art by nearly $20\\%$ in episode completion success\nrate and additionally enhances the USV's path length efficiency.\n","authors":["Ehsan Kazemi","Iman Soltani"],"pdf_url":"https://arxiv.org/pdf/2410.13973v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13359v1","updated":"2024-12-17T22:17:42Z","published":"2024-12-17T22:17:42Z","title":"Multi-Agent Motion Planning For Differential Drive Robots Through\n  Stationary State Search","summary":"  Multi-Agent Motion Planning (MAMP) finds various applications in fields such\nas traffic management, airport operations, and warehouse automation. In many of\nthese environments, differential drive robots are commonly used. These robots\nhave a kinodynamic model that allows only in-place rotation and movement along\ntheir current orientation, subject to speed and acceleration limits. However,\nexisting Multi-Agent Path Finding (MAPF)-based methods often use simplified\nmodels for robot kinodynamics, which limits their practicality and realism. In\nthis paper, we introduce a three-level framework called MASS to address these\nchallenges. MASS combines MAPF-based methods with our proposed stationary state\nsearch planner to generate high-quality kinodynamically-feasible plans. We\nfurther extend MASS using an adaptive window mechanism to address the lifelong\nMAMP problem. Empirically, we tested our methods on the single-shot grid map\ndomain and the lifelong warehouse domain. Our method shows up to 400%\nimprovements in terms of throughput compared to existing methods.\n","authors":["Jingtian Yan","Jiaoyang Li"],"pdf_url":"https://arxiv.org/pdf/2412.13359v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11013v2","updated":"2024-12-17T22:14:33Z","published":"2024-10-14T19:05:38Z","title":"Incorporating Task Progress Knowledge for Subgoal Generation in Robotic\n  Manipulation through Image Edits","summary":"  Understanding the progress of a task allows humans to not only track what has\nbeen done but also to better plan for future goals. We demonstrate TaKSIE, a\nnovel framework that incorporates task progress knowledge into visual subgoal\ngeneration for robotic manipulation tasks. We jointly train a recurrent network\nwith a latent diffusion model to generate the next visual subgoal based on the\nrobot's current observation and the input language command. At execution time,\nthe robot leverages a visual progress representation to monitor the task\nprogress and adaptively samples the next visual subgoal from the model to guide\nthe manipulation policy. We train and validate our model in simulated and\nreal-world robotic tasks, achieving state-of-the-art performance on the CALVIN\nmanipulation benchmark. We find that the inclusion of task progress knowledge\ncan improve the robustness of trained policy for different initial robot poses\nor various movement speeds during demonstrations. The project website can be\nfound at https://live-robotics-uva.github.io/TaKSIE/ .\n","authors":["Xuhui Kang","Yen-Ling Kuo"],"pdf_url":"https://arxiv.org/pdf/2410.11013v2.pdf","comment":"WACV2025, 12 pages, 11 figures"},{"id":"http://arxiv.org/abs/2403.15621v3","updated":"2024-12-17T21:33:26Z","published":"2024-03-22T21:16:25Z","title":"Multi-Robot Task Allocation using Global Games with Negative Feedback:\n  The Colony Maintenance Problem","summary":"  In this article we address the multi-robot task allocation problem, where\nrobots must cooperatively assign themselves to accomplish a set of tasks. We\nconsider the colony maintenance problem as an example, where a team of robots\nare tasked with continuously maintaining the energy supply of a central colony.\nWe model this as a global game, where each robot measures the energy level of\nthe colony, and the current number of assigned robots, to determine whether or\nnot to forage for energy sources. The key to our approach is introducing a\nnegative feedback term into the robots' utility, which also eliminates the\ntrivial solution where foraging or not foraging are strictly dominant\nstrategies. We compare our approach qualitatively to existing an global games\napproach, where a positive positive feedback term admits threshold-based\ndecision making that encourages many robots to forage. We discuss how positive\nfeedback can lead to a cascading failure when robots are removed from the\nsystem, and we demonstrate the resilience of our approach in simulation.\n","authors":["Logan E. Beaver"],"pdf_url":"https://arxiv.org/pdf/2403.15621v3.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2412.13305v1","updated":"2024-12-17T20:17:16Z","published":"2024-12-17T20:17:16Z","title":"Scene Modeling of Autonomous Vehicles Avoiding Stationary and Moving\n  Vehicles on Narrow Roads","summary":"  Navigating narrow roads with oncoming vehicles is a significant challenge\nthat has garnered considerable public interest. These scenarios often involve\nsections that cannot accommodate two moving vehicles simultaneously due to the\npresence of stationary vehicles or limited road width. Autonomous vehicles must\ntherefore profoundly comprehend their surroundings to identify passable areas\nand execute sophisticated maneuvers. To address this issue, this paper presents\na comprehensive model for such an intricate scenario. The primary contribution\nis the principle of road width occupancy minimization, which models the narrow\nroad problem and identifies candidate meeting gaps. Additionally, the concept\nof homology classes is introduced to help initialize and optimize candidate\ntrajectories, while evaluation strategies are developed to select the optimal\ngap and most efficient trajectory. Qualitative and quantitative simulations\ndemonstrate that the proposed approach, SM-NR, achieves high scene pass rates,\nefficient movement, and robust decisions. Experiments conducted in tiny gap\nscenarios and conflict scenarios reveal that the autonomous vehicle can\nrobustly select meeting gaps and trajectories, compromising flexibly for safety\nwhile advancing bravely for efficiency.\n","authors":["Qianyi Zhang","Jinzheng Guang","Zhenzhong Cao","Jingtai Liu"],"pdf_url":"https://arxiv.org/pdf/2412.13305v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05910v2","updated":"2024-12-17T20:14:23Z","published":"2024-07-08T13:15:11Z","title":"Enhancing Vision-Language Models with Scene Graphs for Traffic Accident\n  Understanding","summary":"  Recognizing a traffic accident is an essential part of any autonomous driving\nor road monitoring system. An accident can appear in a wide variety of forms,\nand understanding what type of accident is taking place may be useful to\nprevent it from reoccurring. This work focuses on classification of traffic\nscenes into specific accident types. We approach the problem by representing a\ntraffic scene as a graph, where objects such as cars can be represented as\nnodes, and relative distances and directions between them as edges. This\nrepresentation of a traffic scene is referred to as a scene graph, and can be\nused as input for an accident classifier. Better results are obtained with a\nclassifier that fuses the scene graph input with visual and textual\nrepresentations. This work introduces a multi-stage, multimodal pipeline that\npre-processes videos of traffic accidents, encodes them as scene graphs, and\naligns this representation with vision and language modalities before executing\nthe classification task. When trained on 4 classes, our method achieves a\nbalanced accuracy score of 57.77% on an (unbalanced) subset of the popular\nDetection of Traffic Anomaly (DoTA) benchmark, representing an increase of\nclose to 5 percentage points from the case where scene graph information is not\ntaken into account.\n","authors":["Aaron Lohner","Francesco Compagno","Jonathan Francis","Alessandro Oltramari"],"pdf_url":"https://arxiv.org/pdf/2407.05910v2.pdf","comment":"Won the 'Best Paper Runner-up Award' at the 2024 IEEE International\n  Automated Vehicle Validation Conference (IAVVC 2024). Also accepted at the\n  1st Workshop on Semantic Reasoning and Goal Understanding in Robotics, at the\n  Robotics Science and Systems Conference (RSS SemRob 2024)"},{"id":"http://arxiv.org/abs/2412.13238v1","updated":"2024-12-17T16:45:27Z","published":"2024-12-17T16:45:27Z","title":"SafeDrive: Knowledge- and Data-Driven Risk-Sensitive Decision-Making for\n  Autonomous Vehicles with Large Language Models","summary":"  Recent advancements in autonomous vehicles (AVs) use Large Language Models\n(LLMs) to perform well in normal driving scenarios. However, ensuring safety in\ndynamic, high-risk environments and managing safety-critical long-tail events\nremain significant challenges. To address these issues, we propose SafeDrive, a\nknowledge- and data-driven risk-sensitive decision-making framework to enhance\nAV safety and adaptability. The proposed framework introduces a modular system\ncomprising: (1) a Risk Module for quantifying multi-factor coupled risks\ninvolving driver, vehicle, and road interactions; (2) a Memory Module for\nstoring and retrieving typical scenarios to improve adaptability; (3) a\nLLM-powered Reasoning Module for context-aware safety decision-making; and (4)\na Reflection Module for refining decisions through iterative learning. By\nintegrating knowledge-driven insights with adaptive learning mechanisms, the\nframework ensures robust decision-making under uncertain conditions. Extensive\nevaluations on real-world traffic datasets, including highways (HighD),\nintersections (InD), and roundabouts (RounD), validate the framework's ability\nto enhance decision-making safety (achieving a 100% safety rate), replicate\nhuman-like driving behaviors (with decision alignment exceeding 85%), and adapt\neffectively to unpredictable scenarios. SafeDrive establishes a novel paradigm\nfor integrating knowledge- and data-driven methods, highlighting significant\npotential to improve safety and adaptability of autonomous driving in high-risk\ntraffic scenarios.\n","authors":["Zhiyuan Zhou","Heye Huang","Boqi Li","Shiyue Zhao","Yao Mu"],"pdf_url":"https://arxiv.org/pdf/2412.13238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13231v1","updated":"2024-12-17T13:42:49Z","published":"2024-12-17T13:42:49Z","title":"C2F-TP: A Coarse-to-Fine Denoising Framework for Uncertainty-Aware\n  Trajectory Prediction","summary":"  Accurately predicting the trajectory of vehicles is critically important for\nensuring safety and reliability in autonomous driving. Although considerable\nresearch efforts have been made recently, the inherent trajectory uncertainty\ncaused by various factors including the dynamic driving intends and the diverse\ndriving scenarios still poses significant challenges to accurate trajectory\nprediction. To address this issue, we propose C2F-TP, a coarse-to-fine\ndenoising framework for uncertainty-aware vehicle trajectory prediction. C2F-TP\nfeatures an innovative two-stage coarse-to-fine prediction process.\nSpecifically, in the spatial-temporal interaction stage, we propose a\nspatial-temporal interaction module to capture the inter-vehicle interactions\nand learn a multimodal trajectory distribution, from which a certain number of\nnoisy trajectories are sampled. Next, in the trajectory refinement stage, we\ndesign a conditional denoising model to reduce the uncertainty of the sampled\ntrajectories through a step-wise denoising operation. Extensive experiments are\nconducted on two real datasets NGSIM and highD that are widely adopted in\ntrajectory prediction. The result demonstrates the effectiveness of our\nproposal.\n","authors":["Zichen Wang","Hao Miao","Senzhang Wang","Renzhi Wang","Jianxin Wang","Jian Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.13231v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13224v1","updated":"2024-12-17T04:13:06Z","published":"2024-12-17T04:13:06Z","title":"Physics-model-guided Worst-case Sampling for Safe Reinforcement Learning","summary":"  Real-world accidents in learning-enabled CPS frequently occur in challenging\ncorner cases. During the training of deep reinforcement learning (DRL) policy,\nthe standard setup for training conditions is either fixed at a single initial\ncondition or uniformly sampled from the admissible state space. This setup\noften overlooks the challenging but safety-critical corner cases. To bridge\nthis gap, this paper proposes a physics-model-guided worst-case sampling\nstrategy for training safe policies that can handle safety-critical cases\ntoward guaranteed safety. Furthermore, we integrate the proposed worst-case\nsampling strategy into the physics-regulated deep reinforcement learning\n(Phy-DRL) framework to build a more data-efficient and safe learning algorithm\nfor safety-critical CPS. We validate the proposed training strategy with\nPhy-DRL through extensive experiments on a simulated cart-pole system, a 2D\nquadrotor, a simulated and a real quadruped robot, showing remarkably improved\nsampling efficiency to learn more robust safe policies.\n","authors":["Hongpeng Cao","Yanbing Mao","Lui Sha","Marco Caccamo"],"pdf_url":"https://arxiv.org/pdf/2412.13224v1.pdf","comment":"under review"}]},"2024-12-18T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2412.14173v1","updated":"2024-12-18T18:59:59Z","published":"2024-12-18T18:59:59Z","title":"AniDoc: Animation Creation Made Easier","summary":"  The production of 2D animation follows an industry-standard workflow,\nencompassing four essential stages: character design, keyframe animation,\nin-betweening, and coloring. Our research focuses on reducing the labor costs\nin the above process by harnessing the potential of increasingly powerful\ngenerative AI. Using video diffusion models as the foundation, AniDoc emerges\nas a video line art colorization tool, which automatically converts sketch\nsequences into colored animations following the reference character\nspecification. Our model exploits correspondence matching as an explicit\nguidance, yielding strong robustness to the variations (e.g., posture) between\nthe reference character and each line art frame. In addition, our model could\neven automate the in-betweening process, such that users can easily create a\ntemporally consistent animation by simply providing a character image as well\nas the start and end sketches. Our code is available at:\nhttps://yihao-meng.github.io/AniDoc_demo.\n","authors":["Yihao Meng","Hao Ouyang","Hanlin Wang","Qiuyu Wang","Wen Wang","Ka Leong Cheng","Zhiheng Liu","Yujun Shen","Huamin Qu"],"pdf_url":"https://arxiv.org/pdf/2412.14173v1.pdf","comment":"Project page and code: https://yihao-meng.github.io/AniDoc_demo"},{"id":"http://arxiv.org/abs/2412.14172v1","updated":"2024-12-18T18:59:56Z","published":"2024-12-18T18:59:56Z","title":"Learning from Massive Human Videos for Universal Humanoid Pose Control","summary":"  Scalable learning of humanoid robots is crucial for their deployment in\nreal-world applications. While traditional approaches primarily rely on\nreinforcement learning or teleoperation to achieve whole-body control, they are\noften limited by the diversity of simulated environments and the high costs of\ndemonstration collection. In contrast, human videos are ubiquitous and present\nan untapped source of semantic and motion information that could significantly\nenhance the generalization capabilities of humanoid robots. This paper\nintroduces Humanoid-X, a large-scale dataset of over 20 million humanoid robot\nposes with corresponding text-based motion descriptions, designed to leverage\nthis abundant data. Humanoid-X is curated through a comprehensive pipeline:\ndata mining from the Internet, video caption generation, motion retargeting of\nhumans to humanoid robots, and policy learning for real-world deployment. With\nHumanoid-X, we further train a large humanoid model, UH-1, which takes text\ninstructions as input and outputs corresponding actions to control a humanoid\nrobot. Extensive simulated and real-world experiments validate that our\nscalable training approach leads to superior generalization in text-based\nhumanoid control, marking a significant step toward adaptable, real-world-ready\nhumanoid robots.\n","authors":["Jiageng Mao","Siheng Zhao","Siqi Song","Tianheng Shi","Junjie Ye","Mingtong Zhang","Haoran Geng","Jitendra Malik","Vitor Guizilini","Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14172v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14171v1","updated":"2024-12-18T18:59:54Z","published":"2024-12-18T18:59:54Z","title":"Thinking in Space: How Multimodal Large Language Models See, Remember,\n  and Recall Spaces","summary":"  Humans possess the visual-spatial intelligence to remember spaces from\nsequential visual observations. However, can Multimodal Large Language Models\n(MLLMs) trained on million-scale video datasets also ``think in space'' from\nvideos? We present a novel video-based visual-spatial intelligence benchmark\n(VSI-Bench) of over 5,000 question-answer pairs, and find that MLLMs exhibit\ncompetitive - though subhuman - visual-spatial intelligence. We probe models to\nexpress how they think in space both linguistically and visually and find that\nwhile spatial reasoning capabilities remain the primary bottleneck for MLLMs to\nreach higher benchmark performance, local world models and spatial awareness do\nemerge within these models. Notably, prevailing linguistic reasoning techniques\n(e.g., chain-of-thought, self-consistency, tree-of-thoughts) fail to improve\nperformance, whereas explicitly generating cognitive maps during\nquestion-answering enhances MLLMs' spatial distance ability.\n","authors":["Jihan Yang","Shusheng Yang","Anjali W. Gupta","Rilyn Han","Li Fei-Fei","Saining Xie"],"pdf_url":"https://arxiv.org/pdf/2412.14171v1.pdf","comment":"Project page:\n  https://vision-x-nyu.github.io/thinking-in-space.github.io/"},{"id":"http://arxiv.org/abs/2412.14169v1","updated":"2024-12-18T18:59:53Z","published":"2024-12-18T18:59:53Z","title":"Autoregressive Video Generation without Vector Quantization","summary":"  This paper presents a novel approach that enables autoregressive video\ngeneration with high efficiency. We propose to reformulate the video generation\nproblem as a non-quantized autoregressive modeling of temporal frame-by-frame\nprediction and spatial set-by-set prediction. Unlike raster-scan prediction in\nprior autoregressive models or joint distribution modeling of fixed-length\ntokens in diffusion models, our approach maintains the causal property of\nGPT-style models for flexible in-context capabilities, while leveraging\nbidirectional modeling within individual frames for efficiency. With the\nproposed approach, we train a novel video autoregressive model without vector\nquantization, termed NOVA. Our results demonstrate that NOVA surpasses prior\nautoregressive video models in data efficiency, inference speed, visual\nfidelity, and video fluency, even with a much smaller model capacity, i.e.,\n0.6B parameters. NOVA also outperforms state-of-the-art image diffusion models\nin text-to-image generation tasks, with a significantly lower training cost.\nAdditionally, NOVA generalizes well across extended video durations and enables\ndiverse zero-shot applications in one unified model. Code and models are\npublicly available at https://github.com/baaivision/NOVA.\n","authors":["Haoge Deng","Ting Pan","Haiwen Diao","Zhengxiong Luo","Yufeng Cui","Huchuan Lu","Shiguang Shan","Yonggang Qi","Xinlong Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14169v1.pdf","comment":"22 pages, 16 figures"},{"id":"http://arxiv.org/abs/2412.14170v1","updated":"2024-12-18T18:59:53Z","published":"2024-12-18T18:59:53Z","title":"E-CAR: Efficient Continuous Autoregressive Image Generation via\n  Multistage Modeling","summary":"  Recent advances in autoregressive (AR) models with continuous tokens for\nimage generation show promising results by eliminating the need for discrete\ntokenization. However, these models face efficiency challenges due to their\nsequential token generation nature and reliance on computationally intensive\ndiffusion-based sampling. We present ECAR (Efficient Continuous Auto-Regressive\nImage Generation via Multistage Modeling), an approach that addresses these\nlimitations through two intertwined innovations: (1) a stage-wise continuous\ntoken generation strategy that reduces computational complexity and provides\nprogressively refined token maps as hierarchical conditions, and (2) a\nmultistage flow-based distribution modeling method that transforms only\npartial-denoised distributions at each stage comparing to complete denoising in\nnormal diffusion models. Holistically, ECAR operates by generating tokens at\nincreasing resolutions while simultaneously denoising the image at each stage.\nThis design not only reduces token-to-image transformation cost by a factor of\nthe stage number but also enables parallel processing at the token level. Our\napproach not only enhances computational efficiency but also aligns naturally\nwith image generation principles by operating in continuous token space and\nfollowing a hierarchical generation process from coarse to fine details.\nExperimental results demonstrate that ECAR achieves comparable image quality to\nDiT Peebles & Xie [2023] while requiring 10$\\times$ FLOPs reduction and\n5$\\times$ speedup to generate a 256$\\times$256 image.\n","authors":["Zhihang Yuan","Yuzhang Shang","Hanling Zhang","Tongcheng Fang","Rui Xie","Bingxin Xu","Yan Yan","Shengen Yan","Guohao Dai","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14170v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14168v1","updated":"2024-12-18T18:59:50Z","published":"2024-12-18T18:59:50Z","title":"FashionComposer: Compositional Fashion Image Generation","summary":"  We present FashionComposer for compositional fashion image generation. Unlike\nprevious methods, FashionComposer is highly flexible. It takes multi-modal\ninput (i.e., text prompt, parametric human model, garment image, and face\nimage) and supports personalizing the appearance, pose, and figure of the human\nand assigning multiple garments in one pass. To achieve this, we first develop\na universal framework capable of handling diverse input modalities. We\nconstruct scaled training data to enhance the model's robust compositional\ncapabilities. To accommodate multiple reference images (garments and faces)\nseamlessly, we organize these references in a single image as an \"asset\nlibrary\" and employ a reference UNet to extract appearance features. To inject\nthe appearance features into the correct pixels in the generated result, we\npropose subject-binding attention. It binds the appearance features from\ndifferent \"assets\" with the corresponding text features. In this way, the model\ncould understand each asset according to their semantics, supporting arbitrary\nnumbers and types of reference images. As a comprehensive solution,\nFashionComposer also supports many other applications like human album\ngeneration, diverse virtual try-on tasks, etc.\n","authors":["Sihui Ji","Yiyang Wang","Xi Chen","Xiaogang Xu","Hao Luo","Hengshuang Zhao"],"pdf_url":"https://arxiv.org/pdf/2412.14168v1.pdf","comment":"https://sihuiji.github.io/FashionComposer-Page"},{"id":"http://arxiv.org/abs/2412.14167v1","updated":"2024-12-18T18:59:49Z","published":"2024-12-18T18:59:49Z","title":"VideoDPO: Omni-Preference Alignment for Video Diffusion Generation","summary":"  Recent progress in generative diffusion models has greatly advanced\ntext-to-video generation. While text-to-video models trained on large-scale,\ndiverse datasets can produce varied outputs, these generations often deviate\nfrom user preferences, highlighting the need for preference alignment on\npre-trained models. Although Direct Preference Optimization (DPO) has\ndemonstrated significant improvements in language and image generation, we\npioneer its adaptation to video diffusion models and propose a VideoDPO\npipeline by making several key adjustments. Unlike previous image alignment\nmethods that focus solely on either (i) visual quality or (ii) semantic\nalignment between text and videos, we comprehensively consider both dimensions\nand construct a preference score accordingly, which we term the OmniScore. We\ndesign a pipeline to automatically collect preference pair data based on the\nproposed OmniScore and discover that re-weighting these pairs based on the\nscore significantly impacts overall preference alignment. Our experiments\ndemonstrate substantial improvements in both visual quality and semantic\nalignment, ensuring that no preference aspect is neglected. Code and data will\nbe shared at https://videodpo.github.io/.\n","authors":["Runtao Liu","Haoyu Wu","Zheng Ziqiang","Chen Wei","Yingqing He","Renjie Pi","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2412.14167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14166v1","updated":"2024-12-18T18:59:38Z","published":"2024-12-18T18:59:38Z","title":"MegaSynth: Scaling Up 3D Scene Reconstruction with Synthesized Data","summary":"  We propose scaling up 3D scene reconstruction by training with synthesized\ndata. At the core of our work is MegaSynth, a procedurally generated 3D dataset\ncomprising 700K scenes - over 50 times larger than the prior real dataset DL3DV\n- dramatically scaling the training data. To enable scalable data generation,\nour key idea is eliminating semantic information, removing the need to model\ncomplex semantic priors such as object affordances and scene composition.\nInstead, we model scenes with basic spatial structures and geometry primitives,\noffering scalability. Besides, we control data complexity to facilitate\ntraining while loosely aligning it with real-world data distribution to benefit\nreal-world generalization. We explore training LRMs with both MegaSynth and\navailable real data. Experiment results show that joint training or\npre-training with MegaSynth improves reconstruction quality by 1.2 to 1.8 dB\nPSNR across diverse image domains. Moreover, models trained solely on MegaSynth\nperform comparably to those trained on real data, underscoring the low-level\nnature of 3D reconstruction. Additionally, we provide an in-depth analysis of\nMegaSynth's properties for enhancing model capability, training stability, and\ngeneralization.\n","authors":["Hanwen Jiang","Zexiang Xu","Desai Xie","Ziwen Chen","Haian Jin","Fujun Luan","Zhixin Shu","Kai Zhang","Sai Bi","Xin Sun","Jiuxiang Gu","Qixing Huang","Georgios Pavlakos","Hao Tan"],"pdf_url":"https://arxiv.org/pdf/2412.14166v1.pdf","comment":"Project page: https://hwjiang1510.github.io/MegaSynth/"},{"id":"http://arxiv.org/abs/2412.14164v1","updated":"2024-12-18T18:58:50Z","published":"2024-12-18T18:58:50Z","title":"MetaMorph: Multimodal Understanding and Generation via Instruction\n  Tuning","summary":"  In this work, we propose Visual-Predictive Instruction Tuning (VPiT) - a\nsimple and effective extension to visual instruction tuning that enables a\npretrained LLM to quickly morph into an unified autoregressive model capable of\ngenerating both text and visual tokens. VPiT teaches an LLM to predict discrete\ntext tokens and continuous visual tokens from any input sequence of image and\ntext data curated in an instruction-following format. Our empirical\ninvestigation reveals several intriguing properties of VPiT: (1) visual\ngeneration ability emerges as a natural byproduct of improved visual\nunderstanding, and can be unlocked efficiently with a small amount of\ngeneration data; (2) while we find understanding and generation to be mutually\nbeneficial, understanding data contributes to both capabilities more\neffectively than generation data. Building upon these findings, we train our\nMetaMorph model and achieve competitive performance on both visual\nunderstanding and generation. In visual generation, MetaMorph can leverage the\nworld knowledge and reasoning abilities gained from LLM pretraining, and\novercome common failure modes exhibited by other generation models. Our results\nsuggest that LLMs may have strong \"prior\" vision capabilities that can be\nefficiently adapted to both visual understanding and generation with a\nrelatively simple instruction tuning process.\n","authors":["Shengbang Tong","David Fan","Jiachen Zhu","Yunyang Xiong","Xinlei Chen","Koustuv Sinha","Michael Rabbat","Yann LeCun","Saining Xie","Zhuang Liu"],"pdf_url":"https://arxiv.org/pdf/2412.14164v1.pdf","comment":"Project page at tsb0601.github.io/metamorph"},{"id":"http://arxiv.org/abs/2412.14158v1","updated":"2024-12-18T18:53:22Z","published":"2024-12-18T18:53:22Z","title":"AKiRa: Augmentation Kit on Rays for optical video generation","summary":"  Recent advances in text-conditioned video diffusion have greatly improved\nvideo quality. However, these methods offer limited or sometimes no control to\nusers on camera aspects, including dynamic camera motion, zoom, distorted lens\nand focus shifts. These motion and optical aspects are crucial for adding\ncontrollability and cinematic elements to generation frameworks, ultimately\nresulting in visual content that draws focus, enhances mood, and guides\nemotions according to filmmakers' controls. In this paper, we aim to close the\ngap between controllable video generation and camera optics. To achieve this,\nwe propose AKiRa (Augmentation Kit on Rays), a novel augmentation framework\nthat builds and trains a camera adapter with a complex camera model over an\nexisting video generation backbone. It enables fine-tuned control over camera\nmotion as well as complex optical parameters (focal length, distortion,\naperture) to achieve cinematic effects such as zoom, fisheye effect, and bokeh.\nExtensive experiments demonstrate AKiRa's effectiveness in combining and\ncomposing camera optics while outperforming all state-of-the-art methods. This\nwork sets a new landmark in controlled and optically enhanced video generation,\npaving the way for future optical video generation methods.\n","authors":["Xi Wang","Robin Courant","Marc Christie","Vicky Kalogeiton"],"pdf_url":"https://arxiv.org/pdf/2412.14158v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14148v1","updated":"2024-12-18T18:45:35Z","published":"2024-12-18T18:45:35Z","title":"MCMat: Multiview-Consistent and Physically Accurate PBR Material\n  Generation","summary":"  Existing 2D methods utilize UNet-based diffusion models to generate\nmulti-view physically-based rendering (PBR) maps but struggle with multi-view\ninconsistency, while some 3D methods directly generate UV maps, encountering\ngeneralization issues due to the limited 3D data. To address these problems, we\npropose a two-stage approach, including multi-view generation and UV materials\nrefinement. In the generation stage, we adopt a Diffusion Transformer (DiT)\nmodel to generate PBR materials, where both the specially designed multi-branch\nDiT and reference-based DiT blocks adopt a global attention mechanism to\npromote feature interaction and fusion between different views, thereby\nimproving multi-view consistency. In addition, we adopt a PBR-based diffusion\nloss to ensure that the generated materials align with realistic physical\nprinciples. In the refinement stage, we propose a material-refined DiT that\nperforms inpainting in empty areas and enhances details in UV space. Except for\nthe normal condition, this refinement also takes the material map from the\ngeneration stage as an additional condition to reduce the learning difficulty\nand improve generalization. Extensive experiments show that our method achieves\nstate-of-the-art performance in texturing 3D objects with PBR materials and\nprovides significant advantages for graphics relighting applications. Project\nPage: https://lingtengqiu.github.io/2024/MCMat/\n","authors":["Shenhao Zhu","Lingteng Qiu","Xiaodong Gu","Zhengyi Zhao","Chao Xu","Yuxiao He","Zhe Li","Xiaoguang Han","Yao Yao","Xun Cao","Siyu Zhu","Weihao Yuan","Zilong Dong","Hao Zhu"],"pdf_url":"https://arxiv.org/pdf/2412.14148v1.pdf","comment":"Project Page: https://lingtengqiu.github.io/2024/MCMat/"},{"id":"http://arxiv.org/abs/2412.14145v1","updated":"2024-12-18T18:43:21Z","published":"2024-12-18T18:43:21Z","title":"Incorporating Feature Pyramid Tokenization and Open Vocabulary Semantic\n  Segmentation","summary":"  The visual understanding are often approached from 3 granular levels: image,\npatch and pixel. Visual Tokenization, trained by self-supervised reconstructive\nlearning, compresses visual data by codebook in patch-level with marginal\ninformation loss, but the visual tokens does not have semantic meaning. Open\nVocabulary semantic segmentation benefits from the evolving Vision-Language\nmodels (VLMs) with strong image zero-shot capability, but transferring\nimage-level to pixel-level understanding remains an imminent challenge. In this\npaper, we treat segmentation as tokenizing pixels and study a united perceptual\nand semantic token compression for all granular understanding and consequently\nfacilitate open vocabulary semantic segmentation. Referring to the cognitive\nprocess of pretrained VLM where the low-level features are progressively\ncomposed to high-level semantics, we propose Feature Pyramid Tokenization (PAT)\nto cluster and represent multi-resolution feature by learnable codebooks and\nthen decode them by joint learning pixel reconstruction and semantic\nsegmentation. We design loosely coupled pixel and semantic learning branches.\nThe pixel branch simulates bottom-up composition and top-down visualization of\ncodebook tokens, while the semantic branch collectively fuse hierarchical\ncodebooks as auxiliary segmentation guidance. Our experiments show that PAT\nenhances the semantic intuition of VLM feature pyramid, improves performance\nover the baseline segmentation model and achieves competitive performance on\nopen vocabulary semantic segmentation benchmark. Our model is\nparameter-efficient for VLM integration and flexible for the independent\ntokenization. We hope to give inspiration not only on improving segmentation\nbut also on semantic visual token utilization.\n","authors":["Jianyu Zhang","Li Zhang","Shijian Li"],"pdf_url":"https://arxiv.org/pdf/2412.14145v1.pdf","comment":"6 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.14123v1","updated":"2024-12-18T18:11:53Z","published":"2024-12-18T18:11:53Z","title":"AnySat: An Earth Observation Model for Any Resolutions, Scales, and\n  Modalities","summary":"  Geospatial models must adapt to the diversity of Earth observation data in\nterms of resolutions, scales, and modalities. However, existing approaches\nexpect fixed input configurations, which limits their practical applicability.\nWe propose AnySat, a multimodal model based on joint embedding predictive\narchitecture (JEPA) and resolution-adaptive spatial encoders, allowing us to\ntrain a single model on highly heterogeneous data in a self-supervised manner.\nTo demonstrate the advantages of this unified approach, we compile GeoPlex, a\ncollection of $5$ multimodal datasets with varying characteristics and $11$\ndistinct sensors. We then train a single powerful model on these diverse\ndatasets simultaneously. Once fine-tuned, we achieve better or near\nstate-of-the-art results on the datasets of GeoPlex and $4$ additional ones for\n$5$ environment monitoring tasks: land cover mapping, tree species\nidentification, crop type classification, change detection, and flood\nsegmentation. The code and models are available at\nhttps://github.com/gastruc/AnySat.\n","authors":["Guillaume Astruc","Nicolas Gonthier","Clement Mallet","Loic Landrieu"],"pdf_url":"https://arxiv.org/pdf/2412.14123v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10178v2","updated":"2024-12-18T18:05:43Z","published":"2024-12-13T14:50:26Z","title":"SwiftTry: Fast and Consistent Video Virtual Try-On with Diffusion Models","summary":"  Given an input video of a person and a new garment, the objective of this\npaper is to synthesize a new video where the person is wearing the specified\ngarment while maintaining spatiotemporal consistency. Although significant\nadvances have been made in image-based virtual try-on, extending these\nsuccesses to video often leads to frame-to-frame inconsistencies. Some\napproaches have attempted to address this by increasing the overlap of frames\nacross multiple video chunks, but this comes at a steep computational cost due\nto the repeated processing of the same frames, especially for long video\nsequences. To tackle these challenges, we reconceptualize video virtual try-on\nas a conditional video inpainting task, with garments serving as input\nconditions. Specifically, our approach enhances image diffusion models by\nincorporating temporal attention layers to improve temporal coherence. To\nreduce computational overhead, we propose ShiftCaching, a novel technique that\nmaintains temporal consistency while minimizing redundant computations.\nFurthermore, we introduce the TikTokDress dataset, a new video try-on dataset\nfeaturing more complex backgrounds, challenging movements, and higher\nresolution compared to existing public datasets. Extensive experiments\ndemonstrate that our approach outperforms current baselines, particularly in\nterms of video consistency and inference speed. The project page is available\nat https://swift-try.github.io/.\n","authors":["Hung Nguyen","Quang Qui-Vinh Nguyen","Khoi Nguyen","Rang Nguyen"],"pdf_url":"https://arxiv.org/pdf/2412.10178v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14118v1","updated":"2024-12-18T18:04:12Z","published":"2024-12-18T18:04:12Z","title":"GaraMoSt: Parallel Multi-Granularity Motion and Structural Modeling for\n  Efficient Multi-Frame Interpolation in DSA Images","summary":"  The rapid and accurate direct multi-frame interpolation method for Digital\nSubtraction Angiography (DSA) images is crucial for reducing radiation and\nproviding real-time assistance to physicians for precise diagnostics and\ntreatment. DSA images contain complex vascular structures and various motions.\nApplying natural scene Video Frame Interpolation (VFI) methods results in\nmotion artifacts, structural dissipation, and blurriness. Recently, MoSt-DSA\nhas specifically addressed these issues for the first time and achieved SOTA\nresults. However, MoSt-DSA's focus on real-time performance leads to\ninsufficient suppression of high-frequency noise and incomplete filtering of\nlow-frequency noise in the generated images. To address these issues within the\nsame computational time scale, we propose GaraMoSt. Specifically, we optimize\nthe network pipeline with a parallel design and propose a module named MG-MSFE.\nMG-MSFE extracts frame-relative motion and structural features at various\ngranularities in a fully convolutional parallel manner and supports\nindependent, flexible adjustment of context-aware granularity at different\nscales, thus enhancing computational efficiency and accuracy. Extensive\nexperiments demonstrate that GaraMoSt achieves the SOTA performance in\naccuracy, robustness, visual effects, and noise suppression, comprehensively\nsurpassing MoSt-DSA and other natural scene VFI methods. The code and models\nare available at https://github.com/ZyoungXu/GaraMoSt.\n","authors":["Ziyang Xu","Huangxuan Zhao","Wenyu Liu","Xinggang Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14118v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.14111v1","updated":"2024-12-18T17:58:16Z","published":"2024-12-18T17:58:16Z","title":"Event-based Photometric Bundle Adjustment","summary":"  We tackle the problem of bundle adjustment (i.e., simultaneous refinement of\ncamera poses and scene map) for a purely rotating event camera. Starting from\nfirst principles, we formulate the problem as a classical non-linear least\nsquares optimization. The photometric error is defined using the event\ngeneration model directly in the camera rotations and the semi-dense scene\nbrightness that triggers the events. We leverage the sparsity of event data to\ndesign a tractable Levenberg-Marquardt solver that handles the very large\nnumber of variables involved. To the best of our knowledge, our method, which\nwe call Event-based Photometric Bundle Adjustment (EPBA), is the first\nevent-only photometric bundle adjustment method that works on the brightness\nmap directly and exploits the space-time characteristics of event data, without\nhaving to convert events into image-like representations. Comprehensive\nexperiments on both synthetic and real-world datasets demonstrate EPBA's\neffectiveness in decreasing the photometric error (by up to 90%), yielding\nresults of unparalleled quality. The refined maps reveal details that were\nhidden using prior state-of-the-art rotation-only estimation methods. The\nexperiments on modern high-resolution event cameras show the applicability of\nEPBA to panoramic imaging in various scenarios (without map initialization, at\nmultiple resolutions, and in combination with other methods, such as IMU dead\nreckoning or previous event-based rotation estimation methods). We make the\nsource code publicly available. https://github.com/tub-rip/epba\n","authors":["Shuang Guo","Guillermo Gallego"],"pdf_url":"https://arxiv.org/pdf/2412.14111v1.pdf","comment":"21 pages, 19 figures, 10 tables. Project page:\n  https://github.com/tub-rip/epba"},{"id":"http://arxiv.org/abs/2412.14103v1","updated":"2024-12-18T17:50:15Z","published":"2024-12-18T17:50:15Z","title":"Foundation Models Meet Low-Cost Sensors: Test-Time Adaptation for\n  Rescaling Disparity for Zero-Shot Metric Depth Estimation","summary":"  The recent development of foundation models for monocular depth estimation\nsuch as Depth Anything paved the way to zero-shot monocular depth estimation.\nSince it returns an affine-invariant disparity map, the favored technique to\nrecover the metric depth consists in fine-tuning the model. However, this stage\nis costly to perform because of the training but also due to the creation of\nthe dataset. It must contain images captured by the camera that will be used at\ntest time and the corresponding ground truth. Moreover, the fine-tuning may\nalso degrade the generalizing capacity of the original model. Instead, we\npropose in this paper a new method to rescale Depth Anything predictions using\n3D points provided by low-cost sensors or techniques such as low-resolution\nLiDAR, stereo camera, structure-from-motion where poses are given by an IMU.\nThus, this approach avoids fine-tuning and preserves the generalizing power of\nthe original depth estimation model while being robust to the noise of the\nsensor or of the depth model. Our experiments highlight improvements relative\nto other metric depth estimation methods and competitive results compared to\nfine-tuned approaches. Code available at\nhttps://gitlab.ensta.fr/ssh/monocular-depth-rescaling.\n","authors":["Rémi Marsal","Alexandre Chapoutot","Philippe Xu","David Filliat"],"pdf_url":"https://arxiv.org/pdf/2412.14103v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10604v2","updated":"2024-12-18T17:49:32Z","published":"2024-12-13T23:15:35Z","title":"EvalGIM: A Library for Evaluating Generative Image Models","summary":"  As the use of text-to-image generative models increases, so does the adoption\nof automatic benchmarking methods used in their evaluation. However, while\nmetrics and datasets abound, there are few unified benchmarking libraries that\nprovide a framework for performing evaluations across many datasets and\nmetrics. Furthermore, the rapid introduction of increasingly robust\nbenchmarking methods requires that evaluation libraries remain flexible to new\ndatasets and metrics. Finally, there remains a gap in synthesizing evaluations\nin order to deliver actionable takeaways about model performance. To enable\nunified, flexible, and actionable evaluations, we introduce EvalGIM (pronounced\n''EvalGym''), a library for evaluating generative image models. EvalGIM\ncontains broad support for datasets and metrics used to measure quality,\ndiversity, and consistency of text-to-image generative models. In addition,\nEvalGIM is designed with flexibility for user customization as a top priority\nand contains a structure that allows plug-and-play additions of new datasets\nand metrics. To enable actionable evaluation insights, we introduce\n''Evaluation Exercises'' that highlight takeaways for specific evaluation\nquestions. The Evaluation Exercises contain easy-to-use and reproducible\nimplementations of two state-of-the-art evaluation methods of text-to-image\ngenerative models: consistency-diversity-realism Pareto Fronts and\ndisaggregated measurements of performance disparities across groups. EvalGIM\nalso contains Evaluation Exercises that introduce two new analysis methods for\ntext-to-image generative models: robustness analyses of model rankings and\nbalanced evaluations across different prompt styles. We encourage text-to-image\nmodel exploration with EvalGIM and invite contributions at\nhttps://github.com/facebookresearch/EvalGIM/.\n","authors":["Melissa Hall","Oscar Mañas","Reyhane Askari-Hemmat","Mark Ibrahim","Candace Ross","Pietro Astolfi","Tariq Berrada Ifriqi","Marton Havasi","Yohann Benchetrit","Karen Ullrich","Carolina Braga","Abhishek Charnalia","Maeve Ryan","Mike Rabbat","Michal Drozdzal","Jakob Verbeek","Adriana Romero-Soriano"],"pdf_url":"https://arxiv.org/pdf/2412.10604v2.pdf","comment":"For code, see https://github.com/facebookresearch/EvalGIM/tree/main"},{"id":"http://arxiv.org/abs/2412.14100v1","updated":"2024-12-18T17:48:32Z","published":"2024-12-18T17:48:32Z","title":"Parameter-efficient Fine-tuning for improved Convolutional Baseline for\n  Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset","summary":"  Automating brain tumor segmentation using deep learning methods is an ongoing\nchallenge in medical imaging. Multiple lingering issues exist including\ndomain-shift and applications in low-resource settings which brings a unique\nset of challenges including scarcity of data. As a step towards solving these\nspecific problems, we propose Convolutional adapter-inspired\nParameter-efficient Fine-tuning (PEFT) of MedNeXt architecture. To validate our\nidea, we show our method performs comparable to full fine-tuning with the added\nbenefit of reduced training compute using BraTS-2021 as pre-training dataset\nand BraTS-Africa as the fine-tuning dataset. BraTS-Africa consists of a small\ndataset (60 train / 35 validation) from the Sub-Saharan African population with\nmarked shift in the MRI quality compared to BraTS-2021 (1251 train samples). We\nfirst show that models trained on BraTS-2021 dataset do not generalize well to\nBraTS-Africa as shown by 20% reduction in mean dice on BraTS-Africa validation\nsamples. Then, we show that PEFT can leverage both the BraTS-2021 and\nBraTS-Africa dataset to obtain mean dice of 0.8 compared to 0.72 when trained\nonly on BraTS-Africa. Finally, We show that PEFT (0.80 mean dice) results in\ncomparable performance to full fine-tuning (0.77 mean dice) which may show PEFT\nto be better on average but the boxplots show that full finetuning results is\nmuch lesser variance in performance. Nevertheless, on disaggregation of the\ndice metrics, we find that the model has tendency to oversegment as shown by\nhigh specificity (0.99) compared to relatively low sensitivity(0.75). The\nsource code is available at\nhttps://github.com/CAMERA-MRI/SPARK2024/tree/main/PEFT_MedNeXt\n","authors":["Bijay Adhikari","Pratibha Kulung","Jakesh Bohaju","Laxmi Kanta Poudel","Confidence Raymond","Dong Zhang","Udunna C Anazodo","Bishesh Khanal","Mahesh Shakya"],"pdf_url":"https://arxiv.org/pdf/2412.14100v1.pdf","comment":"Accepted to \"The International Brain Tumor Segmentation (BraTS)\n  challenge organized at MICCAI 2024 conference\""},{"id":"http://arxiv.org/abs/2412.14097v1","updated":"2024-12-18T17:47:46Z","published":"2024-12-18T17:47:46Z","title":"Adaptive Concept Bottleneck for Foundation Models Under Distribution\n  Shifts","summary":"  Advancements in foundation models (FMs) have led to a paradigm shift in\nmachine learning. The rich, expressive feature representations from these\npre-trained, large-scale FMs are leveraged for multiple downstream tasks,\nusually via lightweight fine-tuning of a shallow fully-connected network\nfollowing the representation. However, the non-interpretable, black-box nature\nof this prediction pipeline can be a challenge, especially in critical domains\nsuch as healthcare, finance, and security. In this paper, we explore the\npotential of Concept Bottleneck Models (CBMs) for transforming complex,\nnon-interpretable foundation models into interpretable decision-making\npipelines using high-level concept vectors. Specifically, we focus on the\ntest-time deployment of such an interpretable CBM pipeline \"in the wild\", where\nthe input distribution often shifts from the original training distribution. We\nfirst identify the potential failure modes of such a pipeline under different\ntypes of distribution shifts. Then we propose an adaptive concept bottleneck\nframework to address these failure modes, that dynamically adapts the\nconcept-vector bank and the prediction layer based solely on unlabeled data\nfrom the target domain, without access to the source (training) dataset.\nEmpirical evaluations with various real-world distribution shifts show that our\nadaptation method produces concept-based interpretations better aligned with\nthe test data and boosts post-deployment accuracy by up to 28%, aligning the\nCBM performance with that of non-interpretable classification.\n","authors":["Jihye Choi","Jayaram Raghuram","Yixuan Li","Somesh Jha"],"pdf_url":"https://arxiv.org/pdf/2412.14097v1.pdf","comment":"The preliminary version of the work appeared in the ICML 2024\n  Workshop on Foundation Models in the Wild"},{"id":"http://arxiv.org/abs/2412.14088v1","updated":"2024-12-18T17:34:52Z","published":"2024-12-18T17:34:52Z","title":"Joint Perception and Prediction for Autonomous Driving: A Survey","summary":"  Perception and prediction modules are critical components of autonomous\ndriving systems, enabling vehicles to navigate safely through complex\nenvironments. The perception module is responsible for perceiving the\nenvironment, including static and dynamic objects, while the prediction module\nis responsible for predicting the future behavior of these objects. These\nmodules are typically divided into three tasks: object detection, object\ntracking, and motion prediction. Traditionally, these tasks are developed and\noptimized independently, with outputs passed sequentially from one to the next.\nHowever, this approach has significant limitations: computational resources are\nnot shared across tasks, the lack of joint optimization can amplify errors as\nthey propagate throughout the pipeline, and uncertainty is rarely propagated\nbetween modules, resulting in significant information loss. To address these\nchallenges, the joint perception and prediction paradigm has emerged,\nintegrating perception and prediction into a unified model through multi-task\nlearning. This strategy not only overcomes the limitations of previous methods,\nbut also enables the three tasks to have direct access to raw sensor data,\nallowing richer and more nuanced environmental interpretations. This paper\npresents the first comprehensive survey of joint perception and prediction for\nautonomous driving. We propose a taxonomy that categorizes approaches based on\ninput representation, scene context modeling, and output representation,\nhighlighting their contributions and limitations. Additionally, we present a\nqualitative analysis and quantitative comparison of existing methods. Finally,\nwe discuss future research directions based on identified gaps in the\nstate-of-the-art.\n","authors":["Lucas Dal'Col","Miguel Oliveira","Vítor Santos"],"pdf_url":"https://arxiv.org/pdf/2412.14088v1.pdf","comment":"24 pages, 5 sections, 7 figures, 7 tables. This work has been\n  submitted to the IEEE Transactions on Intelligent Transportation Systems for\n  possible publication"},{"id":"http://arxiv.org/abs/2412.14058v1","updated":"2024-12-18T17:07:20Z","published":"2024-12-18T17:07:20Z","title":"Towards Generalist Robot Policies: What Matters in Building\n  Vision-Language-Action Models","summary":"  Foundation Vision Language Models (VLMs) exhibit strong capabilities in\nmulti-modal representation learning, comprehension, and reasoning. By injecting\naction components into the VLMs, Vision-Language-Action Models (VLAs) can be\nnaturally formed and also show promising performance. Existing work has\ndemonstrated the effectiveness and generalization of VLAs in multiple scenarios\nand tasks. Nevertheless, the transfer from VLMs to VLAs is not trivial since\nexisting VLAs differ in their backbones, action-prediction formulations, data\ndistributions, and training recipes. This leads to a missing piece for a\nsystematic understanding of the design choices of VLAs. In this work, we\ndisclose the key factors that significantly influence the performance of VLA\nand focus on answering three essential design choices: which backbone to\nselect, how to formulate the VLA architectures, and when to add\ncross-embodiment data. The obtained results convince us firmly to explain why\nwe need VLA and develop a new family of VLAs, RoboVLMs, which require very few\nmanual designs and achieve a new state-of-the-art performance in three\nsimulation tasks and real-world experiments. Through our extensive experiments,\nwhich include over 8 VLM backbones, 4 policy architectures, and over 600\ndistinct designed experiments, we provide a detailed guidebook for the future\ndesign of VLAs. In addition to the study, the highly flexible RoboVLMs\nframework, which supports easy integrations of new VLMs and free combinations\nof various design choices, is made public to facilitate future research. We\nopen-source all details, including codes, models, datasets, and toolkits, along\nwith detailed training and evaluation recipes at: robovlms.github.io.\n","authors":["Xinghang Li","Peiyan Li","Minghuan Liu","Dong Wang","Jirong Liu","Bingyi Kang","Xiao Ma","Tao Kong","Hanbo Zhang","Huaping Liu"],"pdf_url":"https://arxiv.org/pdf/2412.14058v1.pdf","comment":"Project page: robovlms.github.io"},{"id":"http://arxiv.org/abs/2412.14056v1","updated":"2024-12-18T17:06:21Z","published":"2024-12-18T17:06:21Z","title":"A Review of Multimodal Explainable Artificial Intelligence: Past,\n  Present and Future","summary":"  Artificial intelligence (AI) has rapidly developed through advancements in\ncomputational power and the growth of massive datasets. However, this progress\nhas also heightened challenges in interpreting the \"black-box\" nature of AI\nmodels. To address these concerns, eXplainable AI (XAI) has emerged with a\nfocus on transparency and interpretability to enhance human understanding and\ntrust in AI decision-making processes. In the context of multimodal data fusion\nand complex reasoning scenarios, the proposal of Multimodal eXplainable AI\n(MXAI) integrates multiple modalities for prediction and explanation tasks.\nMeanwhile, the advent of Large Language Models (LLMs) has led to remarkable\nbreakthroughs in natural language processing, yet their complexity has further\nexacerbated the issue of MXAI. To gain key insights into the development of\nMXAI methods and provide crucial guidance for building more transparent, fair,\nand trustworthy AI systems, we review the MXAI methods from a historical\nperspective and categorize them across four eras: traditional machine learning,\ndeep learning, discriminative foundation models, and generative LLMs. We also\nreview evaluation metrics and datasets used in MXAI research, concluding with a\ndiscussion of future challenges and directions. A project related to this\nreview has been created at https://github.com/ShilinSun/mxai_review.\n","authors":["Shilin Sun","Wenbin An","Feng Tian","Fang Nan","Qidong Liu","Jun Liu","Nazaraf Shah","Ping Chen"],"pdf_url":"https://arxiv.org/pdf/2412.14056v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2412.14042v1","updated":"2024-12-18T16:55:42Z","published":"2024-12-18T16:55:42Z","title":"CAD-Recode: Reverse Engineering CAD Code from Point Clouds","summary":"  Computer-Aided Design (CAD) models are typically constructed by sequentially\ndrawing parametric sketches and applying CAD operations to obtain a 3D model.\nThe problem of 3D CAD reverse engineering consists of reconstructing the sketch\nand CAD operation sequences from 3D representations such as point clouds. In\nthis paper, we address this challenge through novel contributions across three\nlevels: CAD sequence representation, network design, and dataset. In\nparticular, we represent CAD sketch-extrude sequences as Python code. The\nproposed CAD-Recode translates a point cloud into Python code that, when\nexecuted, reconstructs the CAD model. Taking advantage of the exposure of\npre-trained Large Language Models (LLMs) to Python code, we leverage a\nrelatively small LLM as a decoder for CAD-Recode and combine it with a\nlightweight point cloud projector. CAD-Recode is trained solely on a proposed\nsynthetic dataset of one million diverse CAD sequences. CAD-Recode\nsignificantly outperforms existing methods across three datasets while\nrequiring fewer input points. Notably, it achieves 10 times lower mean Chamfer\ndistance than state-of-the-art methods on DeepCAD and Fusion360 datasets.\nFurthermore, we show that our CAD Python code output is interpretable by\noff-the-shelf LLMs, enabling CAD editing and CAD-specific question answering\nfrom point clouds.\n","authors":["Danila Rukhovich","Elona Dupont","Dimitrios Mallis","Kseniya Cherenkova","Anis Kacem","Djamila Aouada"],"pdf_url":"https://arxiv.org/pdf/2412.14042v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13372v2","updated":"2024-12-18T16:51:18Z","published":"2024-07-18T10:26:53Z","title":"Restore Anything Model via Efficient Degradation Adaptation","summary":"  With the proliferation of mobile devices, the need for an efficient model to\nrestore any degraded image has become increasingly significant and impactful.\nTraditional approaches typically involve training dedicated models for each\nspecific degradation, resulting in inefficiency and redundancy. More recent\nsolutions either introduce additional modules to learn visual prompts\nsignificantly increasing model size or incorporate cross-modal transfer from\nlarge language models trained on vast datasets, adding complexity to the system\narchitecture. In contrast, our approach, termed RAM, takes a unified path that\nleverages inherent similarities across various degradations to enable both\nefficient and comprehensive restoration through a joint embedding mechanism\nwithout scaling up the model or relying on large multimodal models.\nSpecifically, we examine the sub-latent space of each input, identifying key\ncomponents and reweighting them in a gated manner. This intrinsic degradation\nawareness is further combined with contextualized attention in an X-shaped\nframework, enhancing local-global interactions. Extensive benchmarking in an\nall-in-one restoration setting confirms RAM's SOTA performance, reducing model\ncomplexity by approximately 82% in trainable parameters and 85% in FLOPs. Our\ncode and models will be publicly available.\n","authors":["Bin Ren","Eduard Zamfir","Zongwei Wu","Yawei Li","Yidi Li","Danda Pani Paudel","Radu Timofte","Ming-Hsuan Yang","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2407.13372v2.pdf","comment":"Efficient Any Image Restoration"},{"id":"http://arxiv.org/abs/2412.14018v1","updated":"2024-12-18T16:34:51Z","published":"2024-12-18T16:34:51Z","title":"SurgSora: Decoupled RGBD-Flow Diffusion Model for Controllable Surgical\n  Video Generation","summary":"  Medical video generation has transformative potential for enhancing surgical\nunderstanding and pathology insights through precise and controllable visual\nrepresentations. However, current models face limitations in controllability\nand authenticity. To bridge this gap, we propose SurgSora, a\nmotion-controllable surgical video generation framework that uses a single\ninput frame and user-controllable motion cues. SurgSora consists of three key\nmodules: the Dual Semantic Injector (DSI), which extracts object-relevant RGB\nand depth features from the input frame and integrates them with segmentation\ncues to capture detailed spatial features of complex anatomical structures; the\nDecoupled Flow Mapper (DFM), which fuses optical flow with semantic-RGB-D\nfeatures at multiple scales to enhance temporal understanding and object\nspatial dynamics; and the Trajectory Controller (TC), which allows users to\nspecify motion directions and estimates sparse optical flow, guiding the video\ngeneration process. The fused features are used as conditions for a frozen\nStable Diffusion model to produce realistic, temporally coherent surgical\nvideos. Extensive evaluations demonstrate that SurgSora outperforms\nstate-of-the-art methods in controllability and authenticity, showing its\npotential to advance surgical video generation for medical education, training,\nand research.\n","authors":["Tong Chen","Shuya Yang","Junyi Wang","Long Bai","Hongliang Ren","Luping Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.14018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14015v1","updated":"2024-12-18T16:32:12Z","published":"2024-12-18T16:32:12Z","title":"Prompting Depth Anything for 4K Resolution Accurate Metric Depth\n  Estimation","summary":"  Prompts play a critical role in unleashing the power of language and vision\nfoundation models for specific tasks. For the first time, we introduce\nprompting into depth foundation models, creating a new paradigm for metric\ndepth estimation termed Prompt Depth Anything. Specifically, we use a low-cost\nLiDAR as the prompt to guide the Depth Anything model for accurate metric depth\noutput, achieving up to 4K resolution. Our approach centers on a concise prompt\nfusion design that integrates the LiDAR at multiple scales within the depth\ndecoder. To address training challenges posed by limited datasets containing\nboth LiDAR depth and precise GT depth, we propose a scalable data pipeline that\nincludes synthetic data LiDAR simulation and real data pseudo GT depth\ngeneration. Our approach sets new state-of-the-arts on the ARKitScenes and\nScanNet++ datasets and benefits downstream applications, including 3D\nreconstruction and generalized robotic grasping.\n","authors":["Haotong Lin","Sida Peng","Jingxiao Chen","Songyou Peng","Jiaming Sun","Minghuan Liu","Hujun Bao","Jiashi Feng","Xiaowei Zhou","Bingyi Kang"],"pdf_url":"https://arxiv.org/pdf/2412.14015v1.pdf","comment":"Project page: https://PromptDA.github.io/"},{"id":"http://arxiv.org/abs/2412.14006v1","updated":"2024-12-18T16:20:40Z","published":"2024-12-18T16:20:40Z","title":"InstructSeg: Unifying Instructed Visual Segmentation with Multi-modal\n  Large Language Models","summary":"  Boosted by Multi-modal Large Language Models (MLLMs), text-guided universal\nsegmentation models for the image and video domains have made rapid progress\nrecently. However, these methods are often developed separately for specific\ndomains, overlooking the similarities in task settings and solutions across\nthese two areas. In this paper, we define the union of referring segmentation\nand reasoning segmentation at both the image and video levels as Instructed\nVisual Segmentation (IVS). Correspondingly, we propose InstructSeg, an\nend-to-end segmentation pipeline equipped with MLLMs for IVS. Specifically, we\nemploy an object-aware video perceiver to extract temporal and object\ninformation from reference frames, facilitating comprehensive video\nunderstanding. Additionally, we introduce vision-guided multi-granularity text\nfusion to better integrate global and detailed text information with\nfine-grained visual guidance. By leveraging multi-task and end-to-end training,\nInstructSeg demonstrates superior performance across diverse image and video\nsegmentation tasks, surpassing both segmentation specialists and MLLM-based\nmethods with a single model. Our code is available at\nhttps://github.com/congvvc/InstructSeg.\n","authors":["Cong Wei","Yujie Zhong","Haoxian Tan","Yingsen Zeng","Yong Liu","Zheng Zhao","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2412.14006v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14005v1","updated":"2024-12-18T16:20:21Z","published":"2024-12-18T16:20:21Z","title":"Real-Time Position-Aware View Synthesis from Single-View Input","summary":"  Recent advancements in view synthesis have significantly enhanced immersive\nexperiences across various computer graphics and multimedia applications,\nincluding telepresence, and entertainment. By enabling the generation of new\nperspectives from a single input view, view synthesis allows users to better\nperceive and interact with their environment. However, many state-of-the-art\nmethods, while achieving high visual quality, face limitations in real-time\nperformance, which makes them less suitable for live applications where low\nlatency is critical. In this paper, we present a lightweight, position-aware\nnetwork designed for real-time view synthesis from a single input image and a\ntarget camera pose. The proposed framework consists of a Position Aware\nEmbedding, modeled with a multi-layer perceptron, which efficiently maps\npositional information from the target pose to generate high dimensional\nfeature maps. These feature maps, along with the input image, are fed into a\nRendering Network that merges features from dual encoder branches to resolve\nboth high level semantics and low level details, producing a realistic new view\nof the scene. Experimental results demonstrate that our method achieves\nsuperior efficiency and visual quality compared to existing approaches,\nparticularly in handling complex translational movements without explicit\ngeometric operations like warping. This work marks a step toward enabling\nreal-time view synthesis from a single image for live and interactive\napplications.\n","authors":["Manu Gond","Emin Zerman","Sebastian Knorr","Mårten Sjöström"],"pdf_url":"https://arxiv.org/pdf/2412.14005v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13983v1","updated":"2024-12-18T16:05:40Z","published":"2024-12-18T16:05:40Z","title":"GraphAvatar: Compact Head Avatars with GNN-Generated 3D Gaussians","summary":"  Rendering photorealistic head avatars from arbitrary viewpoints is crucial\nfor various applications like virtual reality. Although previous methods based\non Neural Radiance Fields (NeRF) can achieve impressive results, they lack\nfidelity and efficiency. Recent methods using 3D Gaussian Splatting (3DGS) have\nimproved rendering quality and real-time performance but still require\nsignificant storage overhead. In this paper, we introduce a method called\nGraphAvatar that utilizes Graph Neural Networks (GNN) to generate 3D Gaussians\nfor the head avatar. Specifically, GraphAvatar trains a geometric GNN and an\nappearance GNN to generate the attributes of the 3D Gaussians from the tracked\nmesh. Therefore, our method can store the GNN models instead of the 3D\nGaussians, significantly reducing the storage overhead to just 10MB. To reduce\nthe impact of face-tracking errors, we also present a novel graph-guided\noptimization module to refine face-tracking parameters during training.\nFinally, we introduce a 3D-aware enhancer for post-processing to enhance the\nrendering quality. We conduct comprehensive experiments to demonstrate the\nadvantages of GraphAvatar, surpassing existing methods in visual fidelity and\nstorage consumption. The ablation study sheds light on the trade-offs between\nrendering quality and model size. The code will be released at:\nhttps://github.com/ucwxb/GraphAvatar\n","authors":["Xiaobao Wei","Peng Chen","Ming Lu","Hui Chen","Feng Tian"],"pdf_url":"https://arxiv.org/pdf/2412.13983v1.pdf","comment":"accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2410.13016v3","updated":"2024-12-18T16:01:44Z","published":"2024-10-16T20:18:21Z","title":"Interpreting and Analysing CLIP's Zero-Shot Image Classification via\n  Mutual Knowledge","summary":"  Contrastive Language-Image Pretraining (CLIP) performs zero-shot image\nclassification by mapping images and textual class representation into a shared\nembedding space, then retrieving the class closest to the image. This work\nprovides a new approach for interpreting CLIP models for image classification\nfrom the lens of mutual knowledge between the two modalities. Specifically, we\nask: what concepts do both vision and language CLIP encoders learn in common\nthat influence the joint embedding space, causing points to be closer or\nfurther apart? We answer this question via an approach of textual concept-based\nexplanations, showing their effectiveness, and perform an analysis encompassing\na pool of 13 CLIP models varying in architecture, size and pretraining\ndatasets. We explore those different aspects in relation to mutual knowledge,\nand analyze zero-shot predictions. Our approach demonstrates an effective and\nhuman-friendly way of understanding zero-shot classification decisions with\nCLIP.\n","authors":["Fawaz Sammani","Nikos Deligiannis"],"pdf_url":"https://arxiv.org/pdf/2410.13016v3.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2412.11657v2","updated":"2024-12-18T15:56:51Z","published":"2024-12-16T11:00:02Z","title":"CNNtention: Can CNNs do better with Attention?","summary":"  Convolutional Neural Networks (CNNs) have been the standard for image\nclassification tasks for a long time, but more recently attention-based\nmechanisms have gained traction. This project aims to compare traditional CNNs\nwith attention-augmented CNNs across an image classification task. By\nevaluating and comparing their performance, accuracy and computational\nefficiency, the project will highlight benefits and trade-off of the localized\nfeature extraction of traditional CNNs and the global context capture in\nattention-augmented CNNs. By doing this, we can reveal further insights into\ntheir respective strengths and weaknesses, guide the selection of models based\non specific application needs and ultimately, enhance understanding of these\narchitectures in the deep learning community.\n  This was our final project for CS7643 Deep Learning course at Georgia Tech.\n","authors":["Julian Glattki","Nikhil Kapila","Tejas Rathi"],"pdf_url":"https://arxiv.org/pdf/2412.11657v2.pdf","comment":"10 pages, 11 figures"},{"id":"http://arxiv.org/abs/2412.03401v2","updated":"2024-12-18T15:47:57Z","published":"2024-12-04T15:32:37Z","title":"Benchmarking Pretrained Attention-based Models for Real-Time Recognition\n  in Robot-Assisted Esophagectomy","summary":"  Esophageal cancer is among the most common types of cancer worldwide. It is\ntraditionally treated using open esophagectomy, but in recent years,\nrobot-assisted minimally invasive esophagectomy (RAMIE) has emerged as a\npromising alternative. However, robot-assisted surgery can be challenging for\nnovice surgeons, as they often suffer from a loss of spatial orientation.\nComputer-aided anatomy recognition holds promise for improving surgical\nnavigation, but research in this area remains limited. In this study, we\ndeveloped a comprehensive dataset for semantic segmentation in RAMIE, featuring\nthe largest collection of vital anatomical structures and surgical instruments\nto date. Handling this diverse set of classes presents challenges, including\nclass imbalance and the recognition of complex structures such as nerves. This\nstudy aims to understand the challenges and limitations of current\nstate-of-the-art algorithms on this novel dataset and problem. Therefore, we\nbenchmarked eight real-time deep learning models using two pretraining\ndatasets. We assessed both traditional and attention-based networks,\nhypothesizing that attention-based networks better capture global patterns and\naddress challenges such as occlusion caused by blood or other tissues. The\nbenchmark includes our RAMIE dataset and the publicly available CholecSeg8k\ndataset, enabling a thorough assessment of surgical segmentation tasks. Our\nfindings indicate that pretraining on ADE20k, a dataset for semantic\nsegmentation, is more effective than pretraining on ImageNet. Furthermore,\nattention-based models outperform traditional convolutional neural networks,\nwith SegNeXt and Mask2Former achieving higher Dice scores, and Mask2Former\nadditionally excelling in average symmetric surface distance.\n","authors":["Ronald L. P. D. de Jong","Yasmina al Khalil","Tim J. M. Jaspers","Romy C. van Jaarsveld","Gino M. Kuiper","Yiping Li","Richard van Hillegersberg","Jelle P. Ruurda","Marcel Breeuwer","Fons van der Sommen"],"pdf_url":"https://arxiv.org/pdf/2412.03401v2.pdf","comment":"Accepted for presentation at the SPIE Medical Imaging Conference,\n  2025"},{"id":"http://arxiv.org/abs/2412.13949v1","updated":"2024-12-18T15:29:30Z","published":"2024-12-18T15:29:30Z","title":"Cracking the Code of Hallucination in LVLMs with Vision-aware Head\n  Divergence","summary":"  Large vision-language models (LVLMs) have made substantial progress in\nintegrating large language models (LLMs) with visual inputs, enabling advanced\nmultimodal reasoning. Despite their success, a persistent challenge is\nhallucination-where generated text fails to accurately reflect visual\ncontent-undermining both accuracy and reliability. Existing methods focus on\nalignment training or decoding refinements but primarily address symptoms at\nthe generation stage without probing the underlying causes. In this work, we\ninvestigate the internal mechanisms driving hallucination in LVLMs, with an\nemphasis on the multi-head attention module. Specifically, we introduce\nVision-aware Head Divergence (VHD), a metric that quantifies the sensitivity of\nattention head outputs to visual context. Based on this, our findings reveal\nthe presence of vision-aware attention heads that are more attuned to visual\ninformation; however, the model's overreliance on its prior language patterns\nis closely related to hallucinations. Building on these insights, we propose\nVision-aware Head Reinforcement (VHR), a training-free approach to mitigate\nhallucination by enhancing the role of vision-aware attention heads. Extensive\nexperiments demonstrate that our method achieves superior performance compared\nto state-of-the-art approaches in mitigating hallucinations, while maintaining\nhigh efficiency with negligible additional time overhead.\n","authors":["Jinghan He","Kuan Zhu","Haiyun Guo","Junfeng Fang","Zhenglin Hua","Yuheng Jia","Ming Tang","Tat-Seng Chua","Jinqiao Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13947v1","updated":"2024-12-18T15:28:08Z","published":"2024-12-18T15:28:08Z","title":"Real Classification by Description: Extending CLIP's Limits of Part\n  Attributes Recognition","summary":"  In this study, we define and tackle zero shot \"real\" classification by\ndescription, a novel task that evaluates the ability of Vision-Language Models\n(VLMs) like CLIP to classify objects based solely on descriptive attributes,\nexcluding object class names. This approach highlights the current limitations\nof VLMs in understanding intricate object descriptions, pushing these models\nbeyond mere object recognition. To facilitate this exploration, we introduce a\nnew challenge and release description data for six popular fine-grained\nbenchmarks, which omit object names to encourage genuine zero-shot learning\nwithin the research community. Additionally, we propose a method to enhance\nCLIP's attribute detection capabilities through targeted training using\nImageNet21k's diverse object categories, paired with rich attribute\ndescriptions generated by large language models. Furthermore, we introduce a\nmodified CLIP architecture that leverages multiple resolutions to improve the\ndetection of fine-grained part attributes. Through these efforts, we broaden\nthe understanding of part-attribute recognition in CLIP, improving its\nperformance in fine-grained classification tasks across six popular benchmarks,\nas well as in the PACO dataset, a widely used benchmark for object-attribute\nrecognition. Code is available at:\nhttps://github.com/ethanbar11/grounding_ge_public.\n","authors":["Ethan Baron","Idan Tankel","Peter Tu","Guy Ben-Yosef"],"pdf_url":"https://arxiv.org/pdf/2412.13947v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13943v1","updated":"2024-12-18T15:25:36Z","published":"2024-12-18T15:25:36Z","title":"On Explaining Knowledge Distillation: Measuring and Visualising the\n  Knowledge Transfer Process","summary":"  Knowledge distillation (KD) remains challenging due to the opaque nature of\nthe knowledge transfer process from a Teacher to a Student, making it difficult\nto address certain issues related to KD. To address this, we proposed UniCAM, a\nnovel gradient-based visual explanation method, which effectively interprets\nthe knowledge learned during KD. Our experimental results demonstrate that with\nthe guidance of the Teacher's knowledge, the Student model becomes more\nefficient, learning more relevant features while discarding those that are not\nrelevant. We refer to the features learned with the Teacher's guidance as\ndistilled features and the features irrelevant to the task and ignored by the\nStudent as residual features. Distilled features focus on key aspects of the\ninput, such as textures and parts of objects. In contrast, residual features\ndemonstrate more diffused attention, often targeting irrelevant areas,\nincluding the backgrounds of the target objects. In addition, we proposed two\nnovel metrics: the feature similarity score (FSS) and the relevance score (RS),\nwhich quantify the relevance of the distilled knowledge. Experiments on the\nCIFAR10, ASIRRA, and Plant Disease datasets demonstrate that UniCAM and the two\nmetrics offer valuable insights to explain the KD process.\n","authors":["Gereziher Adhane","Mohammad Mahdi Dehshibi","Dennis Vetter","David Masip","Gemma Roig"],"pdf_url":"https://arxiv.org/pdf/2412.13943v1.pdf","comment":"Accepted to 2025 IEEE/CVF Winter Conference on Applications of\n  Computer Vision (WACV'25). Includes 5 pages of supplementary material"},{"id":"http://arxiv.org/abs/2412.13187v2","updated":"2024-12-18T15:19:55Z","published":"2024-12-17T18:58:33Z","title":"HandsOnVLM: Vision-Language Models for Hand-Object Interaction\n  Prediction","summary":"  How can we predict future interaction trajectories of human hands in a scene\ngiven high-level colloquial task specifications in the form of natural\nlanguage? In this paper, we extend the classic hand trajectory prediction task\nto two tasks involving explicit or implicit language queries. Our proposed\ntasks require extensive understanding of human daily activities and reasoning\nabilities about what should be happening next given cues from the current\nscene. We also develop new benchmarks to evaluate the proposed two tasks,\nVanilla Hand Prediction (VHP) and Reasoning-Based Hand Prediction (RBHP). We\nenable solving these tasks by integrating high-level world knowledge and\nreasoning capabilities of Vision-Language Models (VLMs) with the\nauto-regressive nature of low-level ego-centric hand trajectories. Our model,\nHandsOnVLM is a novel VLM that can generate textual responses and produce\nfuture hand trajectories through natural-language conversations. Our\nexperiments show that HandsOnVLM outperforms existing task-specific methods and\nother VLM baselines on proposed tasks, and demonstrates its ability to\neffectively utilize world knowledge for reasoning about low-level human hand\ntrajectories based on the provided context. Our website contains code and\ndetailed video results https://www.chenbao.tech/handsonvlm/\n","authors":["Chen Bao","Jiarui Xu","Xiaolong Wang","Abhinav Gupta","Homanga Bharadhwaj"],"pdf_url":"https://arxiv.org/pdf/2412.13187v2.pdf","comment":"Preprint. Under Review"},{"id":"http://arxiv.org/abs/2403.13352v4","updated":"2024-12-18T15:14:48Z","published":"2024-03-20T07:31:07Z","title":"AGFSync: Leveraging AI-Generated Feedback for Preference Optimization in\n  Text-to-Image Generation","summary":"  Text-to-Image (T2I) diffusion models have achieved remarkable success in\nimage generation. Despite their progress, challenges remain in both\nprompt-following ability, image quality and lack of high-quality datasets,\nwhich are essential for refining these models. As acquiring labeled data is\ncostly, we introduce AGFSync, a framework that enhances T2I diffusion models\nthrough Direct Preference Optimization (DPO) in a fully AI-driven approach.\nAGFSync utilizes Vision-Language Models (VLM) to assess image quality across\nstyle, coherence, and aesthetics, generating feedback data within an AI-driven\nloop. By applying AGFSync to leading T2I models such as SD v1.4, v1.5, and\nSDXL-base, our extensive experiments on the TIFA dataset demonstrate notable\nimprovements in VQA scores, aesthetic evaluations, and performance on the HPSv2\nbenchmark, consistently outperforming the base models. AGFSync's method of\nrefining T2I diffusion models paves the way for scalable alignment techniques.\nOur code and dataset are publicly available at\nhttps://anjingkun.github.io/AGFSync.\n","authors":["Jingkun An","Yinghao Zhu","Zongjian Li","Enshen Zhou","Haoran Feng","Xijie Huang","Bohua Chen","Yemin Shi","Chengwei Pan"],"pdf_url":"https://arxiv.org/pdf/2403.13352v4.pdf","comment":"Accepted by AAAI-2025"},{"id":"http://arxiv.org/abs/2307.16879v2","updated":"2024-12-18T15:07:39Z","published":"2023-07-31T17:45:16Z","title":"Image Synthesis under Limited Data: A Survey and Taxonomy","summary":"  Deep generative models, which target reproducing the given data distribution\nto produce novel samples, have made unprecedented advancements in recent years.\nTheir technical breakthroughs have enabled unparalleled quality in the\nsynthesis of visual content. However, one critical prerequisite for their\ntremendous success is the availability of a sufficient number of training\nsamples, which requires massive computation resources. When trained on limited\ndata, generative models tend to suffer from severe performance deterioration\ndue to overfitting and memorization. Accordingly, researchers have devoted\nconsiderable attention to develop novel models that are capable of generating\nplausible and diverse images from limited training data recently. Despite\nnumerous efforts to enhance training stability and synthesis quality in the\nlimited data scenarios, there is a lack of a systematic survey that provides 1)\na clear problem definition, critical challenges, and taxonomy of various tasks;\n2) an in-depth analysis on the pros, cons, and remain limitations of existing\nliterature; as well as 3) a thorough discussion on the potential applications\nand future directions in the field of image synthesis under limited data. In\norder to fill this gap and provide a informative introduction to researchers\nwho are new to this topic, this survey offers a comprehensive review and a\nnovel taxonomy on the development of image synthesis under limited data. In\nparticular, it covers the problem definition, requirements, main solutions,\npopular benchmarks, and remain challenges in a comprehensive and all-around\nmanner.\n","authors":["Mengping Yang","Zhe Wang"],"pdf_url":"https://arxiv.org/pdf/2307.16879v2.pdf","comment":"230 references, 25 pages. GitHub:\n  https://github.com/kobeshegu/awesome-few-shot-generation"},{"id":"http://arxiv.org/abs/2412.13916v1","updated":"2024-12-18T14:56:03Z","published":"2024-12-18T14:56:03Z","title":"Retrieval Augmented Image Harmonization","summary":"  When embedding objects (foreground) into images (background), considering the\ninfluence of photography conditions like illumination, it is usually necessary\nto perform image harmonization to make the foreground object coordinate with\nthe background image in terms of brightness, color, and etc. Although existing\nimage harmonization methods have made continuous efforts toward visually\npleasing results, they are still plagued by two main issues. Firstly, the image\nharmonization becomes highly ill-posed when there are no contents similar to\nthe foreground object in the background, making the harmonization results\nunreliable. Secondly, even when similar contents are available, the\nharmonization process is often interfered with by irrelevant areas, mainly\nattributed to an insufficient understanding of image contents and inaccurate\nattention. As a remedy, we present a retrieval-augmented image harmonization\n(Raiha) framework, which seeks proper reference images to reduce the\nill-posedness and restricts the attention to better utilize the useful\ninformation. Specifically, an efficient retrieval method is designed to find\nreference images that contain similar objects as the foreground while the\nillumination is consistent with the background. For training the Raiha\nframework to effectively utilize the reference information, a data augmentation\nstrategy is delicately designed by leveraging existing non-reference image\nharmonization datasets. Besides, the image content priors are introduced to\nensure reasonable attention. With the presented Raiha framework, the image\nharmonization performance is greatly boosted under both non-reference and\nretrieval-augmented settings. The source code and pre-trained models will be\npublicly available.\n","authors":["Haolin Wang","Ming Liu","Zifei Yan","Chao Zhou","Longan Xiao","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2412.13916v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2412.13913v1","updated":"2024-12-18T14:53:38Z","published":"2024-12-18T14:53:38Z","title":"A Black-Box Evaluation Framework for Semantic Robustness in Bird's Eye\n  View Detection","summary":"  Camera-based Bird's Eye View (BEV) perception models receive increasing\nattention for their crucial role in autonomous driving, a domain where concerns\nabout the robustness and reliability of deep learning have been raised. While\nonly a few works have investigated the effects of randomly generated semantic\nperturbations, aka natural corruptions, on the multi-view BEV detection task,\nwe develop a black-box robustness evaluation framework that adversarially\noptimises three common semantic perturbations: geometric transformation, colour\nshifting, and motion blur, to deceive BEV models, serving as the first approach\nin this emerging field. To address the challenge posed by optimising the\nsemantic perturbation, we design a smoothed, distance-based surrogate function\nto replace the mAP metric and introduce SimpleDIRECT, a deterministic\noptimisation algorithm that utilises observed slopes to guide the optimisation\nprocess. By comparing with randomised perturbation and two optimisation\nbaselines, we demonstrate the effectiveness of the proposed framework.\nAdditionally, we provide a benchmark on the semantic robustness of ten recent\nBEV models. The results reveal that PolarFormer, which emphasises geometric\ninformation from multi-view images, exhibits the highest robustness, whereas\nBEVDet is fully compromised, with its precision reduced to zero.\n","authors":["Fu Wang","Yanghao Zhang","Xiangyu Yin","Guangliang Cheng","Zeyu Fu","Xiaowei Huang","Wenjie Ruan"],"pdf_url":"https://arxiv.org/pdf/2412.13913v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13908v1","updated":"2024-12-18T14:51:25Z","published":"2024-12-18T14:51:25Z","title":"Memorizing SAM: 3D Medical Segment Anything Model with Memorizing\n  Transformer","summary":"  Segment Anything Models (SAMs) have gained increasing attention in medical\nimage analysis due to their zero-shot generalization capability in segmenting\nobjects of unseen classes and domains when provided with appropriate user\nprompts. Addressing this performance gap is important to fully leverage the\npre-trained weights of SAMs, particularly in the domain of volumetric medical\nimage segmentation, where accuracy is important but well-annotated 3D medical\ndata for fine-tuning is limited. In this work, we investigate whether\nintroducing the memory mechanism as a plug-in, specifically the ability to\nmemorize and recall internal representations of past inputs, can improve the\nperformance of SAM with limited computation cost. To this end, we propose\nMemorizing SAM, a novel 3D SAM architecture incorporating a memory Transformer\nas a plug-in. Unlike conventional memorizing Transformers that save the\ninternal representation during training or inference, our Memorizing SAM\nutilizes existing highly accurate internal representation as the memory source\nto ensure the quality of memory. We evaluate the performance of Memorizing SAM\nin 33 categories from the TotalSegmentator dataset, which indicates that\nMemorizing SAM can outperform state-of-the-art 3D SAM variant i.e., FastSAM3D\nwith an average Dice increase of 11.36% at the cost of only 4.38 millisecond\nincrease in inference time. The source code is publicly available at\nhttps://github.com/swedfr/memorizingSAM\n","authors":["Xinyuan Shao","Yiqing Shen","Mathias Unberath"],"pdf_url":"https://arxiv.org/pdf/2412.13908v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.06740v2","updated":"2024-12-18T14:41:53Z","published":"2022-05-13T16:19:21Z","title":"Towards Deployable OCR models for Indic languages","summary":"  Recognition of text on word or line images, without the need for sub-word\nsegmentation has become the mainstream of research and development of text\nrecognition for Indian languages. Modelling unsegmented sequences using\nConnectionist Temporal Classification (CTC) is the most commonly used approach\nfor segmentation-free OCR. In this work we present a comprehensive empirical\nstudy of various neural network models that uses CTC for transcribing step-wise\npredictions in the neural network output to a Unicode sequence. The study is\nconducted for 13 Indian languages, using an internal dataset that has around\n1000 pages per language. We study the choice of line vs word as the recognition\nunit, and use of synthetic data to train the models. We compare our models with\npopular publicly available OCR tools for end-to-end document image recognition.\nOur end-to-end pipeline that employ our recognition models and existing text\nsegmentation tools outperform these public OCR tools for 8 out of the 13\nlanguages. We also introduce a new public dataset called Mozhi for word and\nline recognition in Indian language. The dataset contains more than 1.2 million\nannotated word images (120 thousand text lines) across 13 Indian languages. Our\ncode, trained models and the Mozhi dataset will be made available at\nhttp://cvit.iiit.ac.in/research/projects/cvit-projects/\n","authors":["Minesh Mathew","Ajoy Mondal","CV Jawahar"],"pdf_url":"https://arxiv.org/pdf/2205.06740v2.pdf","comment":"presented at ICPR 2024;\n  https://link.springer.com/chapter/10.1007/978-3-031-78495-8_11"},{"id":"http://arxiv.org/abs/2412.13897v1","updated":"2024-12-18T14:39:43Z","published":"2024-12-18T14:39:43Z","title":"Data-Efficient Inference of Neural Fluid Fields via SciML Foundation\n  Model","summary":"  Recent developments in 3D vision have enabled successful progress in\ninferring neural fluid fields and realistic rendering of fluid dynamics.\nHowever, these methods require real-world flow captures, which demand dense\nvideo sequences and specialized lab setups, making the process costly and\nchallenging. Scientific machine learning (SciML) foundation models, which are\npretrained on extensive simulations of partial differential equations (PDEs),\nencode rich multiphysics knowledge and thus provide promising sources of domain\npriors for inferring fluid fields. Nevertheless, their potential to advance\nreal-world vision problems remains largely underexplored, raising questions\nabout the transferability and practical utility of these foundation models. In\nthis work, we demonstrate that SciML foundation model can significantly improve\nthe data efficiency of inferring real-world 3D fluid dynamics with improved\ngeneralization. At the core of our method is leveraging the strong forecasting\ncapabilities and meaningful representations of SciML foundation models. We\nequip neural fluid fields with a novel collaborative training approach that\nutilizes augmented views and fluid features extracted by our foundation model.\nOur method demonstrates significant improvements in both quantitative metrics\nand visual quality, showcasing the practical applicability of SciML foundation\nmodels in real-world fluid dynamics.\n","authors":["Yuqiu Liu","Jingxuan Xu","Mauricio Soroco","Yunchao Wei","Wuyang Chen"],"pdf_url":"https://arxiv.org/pdf/2412.13897v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13884v1","updated":"2024-12-18T14:23:54Z","published":"2024-12-18T14:23:54Z","title":"Navigating limitations with precision: A fine-grained ensemble approach\n  to wrist pathology recognition on a limited x-ray dataset","summary":"  The exploration of automated wrist fracture recognition has gained\nconsiderable research attention in recent years. In practical medical\nscenarios, physicians and surgeons may lack the specialized expertise required\nfor accurate X-ray interpretation, highlighting the need for machine vision to\nenhance diagnostic accuracy. However, conventional recognition techniques face\nchallenges in discerning subtle differences in X-rays when classifying wrist\npathologies, as many of these pathologies, such as fractures, can be small and\nhard to distinguish. This study tackles wrist pathology recognition as a\nfine-grained visual recognition (FGVR) problem, utilizing a limited,\ncustom-curated dataset that mirrors real-world medical constraints, relying\nsolely on image-level annotations. We introduce a specialized FGVR-based\nensemble approach to identify discriminative regions within X-rays. We employ\nan Explainable AI (XAI) technique called Grad-CAM to pinpoint these regions.\nOur ensemble approach outperformed many conventional SOTA and FGVR techniques,\nunderscoring the effectiveness of our strategy in enhancing accuracy in wrist\npathology recognition.\n","authors":["Ammar Ahmed","Ali Shariq Imran","Mohib Ullah","Zenun Kastrati","Sher Muhammad Daudpota"],"pdf_url":"https://arxiv.org/pdf/2412.13884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13875v1","updated":"2024-12-18T14:16:40Z","published":"2024-12-18T14:16:40Z","title":"Denoising Nearest Neighbor Graph via Continuous CRF for Visual\n  Re-ranking without Fine-tuning","summary":"  Visual re-ranking using Nearest Neighbor graph~(NN graph) has been adapted to\nyield high retrieval accuracy, since it is beneficial to exploring an\nhigh-dimensional manifold and applicable without additional fine-tuning. The\nquality of visual re-ranking using NN graph, however, is limited to that of\nconnectivity, i.e., edges of the NN graph. Some edges can be misconnected with\nnegative images. This is known as a noisy edge problem, resulting in a\ndegradation of the retrieval quality. To address this, we propose a\ncomplementary denoising method based on Continuous Conditional Random Field\n(C-CRF) that uses a statistical distance of our similarity-based distribution.\nThis method employs the concept of cliques to make the process computationally\nfeasible. We demonstrate the complementarity of our method through its\napplication to three visual re-ranking methods, observing quality boosts in\nlandmark retrieval and person re-identification (re-ID).\n","authors":["Jaeyoon Kim","Yoonki Cho","Taeyong Kim","Sung-Eui Yoon"],"pdf_url":"https://arxiv.org/pdf/2412.13875v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13871v1","updated":"2024-12-18T14:07:46Z","published":"2024-12-18T14:07:46Z","title":"LLaVA-UHD v2: an MLLM Integrating High-Resolution Feature Pyramid via\n  Hierarchical Window Transformer","summary":"  In multimodal large language models (MLLMs), vision transformers (ViTs) are\nwidely employed for visual encoding. However, their performance in solving\nuniversal MLLM tasks is not satisfactory. We attribute it to a lack of\ninformation from diverse visual levels, impeding alignment with the various\nsemantic granularity required for language generation. To address this issue,\nwe present LLaVA-UHD v2, an advanced MLLM centered around a Hierarchical window\ntransformer that enables capturing diverse visual granularity by constructing\nand integrating a high-resolution feature pyramid. As a vision-language\nprojector, Hiwin transformer comprises two primary modules: (i) an inverse\nfeature pyramid, constructed by a ViT-derived feature up-sampling process\nutilizing high-frequency details from an image pyramid, and (ii) hierarchical\nwindow attention, focusing on a set of key sampling features within cross-scale\nwindows to condense multi-level feature maps. Extensive experiments demonstrate\nthat LLaVA-UHD v2 achieves superior performance over existing MLLMs on popular\nbenchmarks. Notably, our design brings an average boost of 3.7% across 14\nbenchmarks compared with the baseline method, 9.3% on DocVQA for instance. We\nmake all the data, model checkpoint, and code publicly available to facilitate\nfuture research.\n","authors":["Yipeng Zhang","Yifan Liu","Zonghao Guo","Yidan Zhang","Xuesong Yang","Chi Chen","Jun Song","Bo Zheng","Yuan Yao","Zhiyuan Liu","Tat-Seng Chua","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2412.13871v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20008v2","updated":"2024-12-18T13:54:02Z","published":"2024-05-30T12:45:34Z","title":"Sharing Key Semantics in Transformer Makes Efficient Image Restoration","summary":"  Image Restoration (IR), a classic low-level vision task, has witnessed\nsignificant advancements through deep models that effectively model global\ninformation. Notably, the emergence of Vision Transformers (ViTs) has further\npropelled these advancements. When computing, the self-attention mechanism, a\ncornerstone of ViTs, tends to encompass all global cues, even those from\nsemantically unrelated objects or regions. This inclusivity introduces\ncomputational inefficiencies, particularly noticeable with high input\nresolution, as it requires processing irrelevant information, thereby impeding\nefficiency. Additionally, for IR, it is commonly noted that small segments of a\ndegraded image, particularly those closely aligned semantically, provide\nparticularly relevant information to aid in the restoration process, as they\ncontribute essential contextual cues crucial for accurate reconstruction. To\naddress these challenges, we propose boosting IR's performance by sharing the\nkey semantics via Transformer for IR (\\ie, SemanIR) in this paper.\nSpecifically, SemanIR initially constructs a sparse yet comprehensive\nkey-semantic dictionary within each transformer stage by establishing essential\nsemantic connections for every degraded patch. Subsequently, this dictionary is\nshared across all subsequent transformer blocks within the same stage. This\nstrategy optimizes attention calculation within each block by focusing\nexclusively on semantically related components stored in the key-semantic\ndictionary. As a result, attention calculation achieves linear computational\ncomplexity within each window. Extensive experiments across 6 IR tasks confirm\nthe proposed SemanIR's state-of-the-art performance, quantitatively and\nqualitatively showcasing advancements. The visual results, code, and trained\nmodels are available at https://github.com/Amazingren/SemanIR.\n","authors":["Bin Ren","Yawei Li","Jingyun Liang","Rakesh Ranjan","Mengyuan Liu","Rita Cucchiara","Luc Van Gool","Ming-Hsuan Yang","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2405.20008v2.pdf","comment":"Accepted by NeurIPS2024"},{"id":"http://arxiv.org/abs/2412.13859v1","updated":"2024-12-18T13:53:16Z","published":"2024-12-18T13:53:16Z","title":"Zero-Shot Prompting and Few-Shot Fine-Tuning: Revisiting Document Image\n  Classification Using Large Language Models","summary":"  Classifying scanned documents is a challenging problem that involves image,\nlayout, and text analysis for document understanding. Nevertheless, for certain\nbenchmark datasets, notably RVL-CDIP, the state of the art is closing in to\nnear-perfect performance when considering hundreds of thousands of training\nsamples. With the advent of large language models (LLMs), which are excellent\nfew-shot learners, the question arises to what extent the document\nclassification problem can be addressed with only a few training samples, or\neven none at all. In this paper, we investigate this question in the context of\nzero-shot prompting and few-shot model fine-tuning, with the aim of reducing\nthe need for human-annotated training samples as much as possible.\n","authors":["Anna Scius-Bertrand","Michael Jungo","Lars Vögtlin","Jean-Marc Spat","Andreas Fischer"],"pdf_url":"https://arxiv.org/pdf/2412.13859v1.pdf","comment":"ICPR 2024"},{"id":"http://arxiv.org/abs/2304.06376v2","updated":"2024-12-18T13:53:04Z","published":"2023-04-13T10:01:29Z","title":"Signal Reconstruction from Samples at Unknown Locations with Application\n  to 2D Unknown View Tomography","summary":"  It is well known that a band-limited signal can be reconstructed from its\nuniformly spaced samples if the sampling rate is sufficiently high. More\nrecently, it has been proved that one can reconstruct a 1D band-limited signal\neven if the exact sample locations are unknown, but given a uniform\ndistribution of the sample locations and their ordering in 1D. In this work, we\nextend the analytical error bounds in such scenarios for quasi-bandlimited\n(QBL) signals, and for the case of arbitrary but known sampling distributions.\nWe also prove that such reconstruction methods are resilient to a certain\nproportion of errors in the specification of the sample location ordering. We\nthen express the problem of tomographic reconstruction of 2D images from 1D\nRadon projections under unknown angles (2D UVT) with known angle distribution,\nas a special case for reconstruction of QBL signals from samples at unknown\nlocations with known distribution. Building upon our theoretical background, we\npresent asymptotic bounds for 2D QBL image reconstruction from 1D Radon\nprojections in the unknown angles setting, and present an extensive set of\nsimulations to verify these bounds in varied parameter regimes. To the best of\nour knowledge, this is the first piece of work to perform such an analysis for\n2D UVT and explicitly relate it to advances in sampling theory, even though the\nassociated reconstruction algorithms have been known for a long time.\n","authors":["Sheel Shah","Kaishva Shah","Karthik S. Gurumoorthy","Ajit Rajwade"],"pdf_url":"https://arxiv.org/pdf/2304.06376v2.pdf","comment":"This is a preprint of a paper accepted to Signal Processing\n  (Elsevier)"},{"id":"http://arxiv.org/abs/2412.13857v1","updated":"2024-12-18T13:52:42Z","published":"2024-12-18T13:52:42Z","title":"Diagnosising Helicobacter pylori using AutoEncoders and Limited\n  Annotations through Anomalous Staining Patterns in IHC Whole Slide Images","summary":"  Purpose: This work addresses the detection of Helicobacter pylori (H. pylori)\nin histological images with immunohistochemical staining. This analysis is a\ntime demanding task, currently done by an expert pathologist that visually\ninspects the samples. Given the effort required to localise the pathogen in\nimages, a limited number of annotations might be available in an initial\nsetting. Our goal is to design an approach that, using a limited set of\nannotations, is capable of obtaining results good enough to be used as a\nsupport tool. Methods: We propose to use autoencoders to learn the latent\npatterns of healthy patches and formulate a specific measure of the\nreconstruction error of the image in HSV space. ROC analysis is used to set the\noptimal threshold of this measure and the percentage of positive patches in a\nsample that determines the presence of H. pylori. Results: Our method has been\ntested on an own database of 245 Whole Slide Images (WSI) having 117 cases\nwithout H. pylori and different density of the bacteria in the remaining ones.\nThe database has 1211 annotated patches, with only 163 positive patches. This\ndataset of positive annotations was used to train a baseline thresholding and\nan SVM using the features of a pre-trained RedNet18 and ViT models. A 10-fold\ncross-validation shows that our method has better performance with 91%\naccuracy, 86% sensitivity, 96% specificity and 0.97 AUC in the diagnosis of H.\npylori. Conclusion: Unlike classification approaches, our shallow autoencoder\nwith threshold adaptation for the detection of anomalous staining is able to\nachieve competitive results with a limited set of annotated data. This initial\napproach is good enough to be used as a guide for fast annotation of infected\npatches.\n","authors":["Pau Cano","Eva Musulen","Debora Gil"],"pdf_url":"https://arxiv.org/pdf/2412.13857v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13856v1","updated":"2024-12-18T13:52:20Z","published":"2024-12-18T13:52:20Z","title":"A Systematic Analysis of Input Modalities for Fracture Classification of\n  the Paediatric Wrist","summary":"  Fractures, particularly in the distal forearm, are among the most common\ninjuries in children and adolescents, with approximately 800 000 cases treated\nannually in Germany. The AO/OTA system provides a structured fracture type\nclassification, which serves as the foundation for treatment decisions.\nAlthough accurately classifying fractures can be challenging, current deep\nlearning models have demonstrated performance comparable to that of experienced\nradiologists. While most existing approaches rely solely on radiographs, the\npotential impact of incorporating other additional modalities, such as\nautomatic bone segmentation, fracture location, and radiology reports, remains\nunderexplored. In this work, we systematically analyse the contribution of\nthese three additional information types, finding that combining them with\nradiographs increases the AUROC from 91.71 to 93.25. Our code is available on\nGitHub.\n","authors":["Ron Keuth","Maren Balks","Sebastian Tschauner","Ludger Tüshaus","Mattias Heinrich"],"pdf_url":"https://arxiv.org/pdf/2412.13856v1.pdf","comment":"Code available on\n  https://github.com/multimodallearning/AO_Classification"},{"id":"http://arxiv.org/abs/2404.09507v2","updated":"2024-12-18T13:50:13Z","published":"2024-04-15T06:58:09Z","title":"Clothes-Changing Person Re-Identification with Feasibility-Aware\n  Intermediary Matching","summary":"  Current clothes-changing person re-identification (re-id) approaches usually\nperform retrieval based on clothes-irrelevant features, while neglecting the\npotential of clothes-relevant features. However, we observe that relying solely\non clothes-irrelevant features for clothes-changing re-id is limited, since\nthey often lack adequate identity information and suffer from large intra-class\nvariations. On the contrary, clothes-relevant features can be used to discover\nsame-clothes intermediaries that possess informative identity clues. Based on\nthis observation, we propose a Feasibility-Aware Intermediary Matching (FAIM)\nframework to additionally utilize clothes-relevant features for retrieval.\nFirstly, an Intermediary Matching (IM) module is designed to perform an\nintermediary-assisted matching process. This process involves using\nclothes-relevant features to find informative intermediates, and then using\nclothes-irrelevant features of these intermediates to complete the matching.\nSecondly, in order to reduce the negative effect of low-quality intermediaries,\nan Intermediary-Based Feasibility Weighting (IBFW) module is designed to\nevaluate the feasibility of intermediary matching process by assessing the\nquality of intermediaries. Extensive experiments demonstrate that our method\noutperforms state-of-the-art methods on several widely-used clothes-changing\nre-id benchmarks.\n","authors":["Jiahe Zhao","Ruibing Hou","Hong Chang","Xinqian Gu","Bingpeng Ma","Shiguang Shan","Xilin Chen"],"pdf_url":"https://arxiv.org/pdf/2404.09507v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.01476v3","updated":"2024-12-18T13:47:38Z","published":"2024-06-03T16:05:25Z","title":"DreamPhysics: Learning Physics-Based 3D Dynamics with Video Diffusion\n  Priors","summary":"  Dynamic 3D interaction has been attracting a lot of attention recently.\nHowever, creating such 4D content remains challenging. One solution is to\nanimate 3D scenes with physics-based simulation, which requires manually\nassigning precise physical properties to the object or the simulated results\nwould become unnatural. Another solution is to learn the deformation of 3D\nobjects with the distillation of video generative models, which, however, tends\nto produce 3D videos with small and discontinuous motions due to the\ninappropriate extraction and application of physics priors. In this work, to\ncombine the strengths and complementing shortcomings of the above two\nsolutions, we propose to learn the physical properties of a material field with\nvideo diffusion priors, and then utilize a physics-based Material-Point-Method\n(MPM) simulator to generate 4D content with realistic motions. In particular,\nwe propose motion distillation sampling to emphasize video motion information\nduring distillation. In addition, to facilitate the optimization, we further\npropose a KAN-based material field with frame boosting. Experimental results\ndemonstrate that our method enjoys more realistic motions than\nstate-of-the-arts do.\n","authors":["Tianyu Huang","Haoze Zhang","Yihan Zeng","Zhilu Zhang","Hui Li","Wangmeng Zuo","Rynson W. H. Lau"],"pdf_url":"https://arxiv.org/pdf/2406.01476v3.pdf","comment":"Accepted by AAAI 2025. Codes are released at:\n  https://github.com/tyhuang0428/DreamPhysics"},{"id":"http://arxiv.org/abs/2412.13848v1","updated":"2024-12-18T13:42:06Z","published":"2024-12-18T13:42:06Z","title":"MobiFuse: A High-Precision On-device Depth Perception System with\n  Multi-Data Fusion","summary":"  We present MobiFuse, a high-precision depth perception system on mobile\ndevices that combines dual RGB and Time-of-Flight (ToF) cameras. To achieve\nthis, we leverage physical principles from various environmental factors to\npropose the Depth Error Indication (DEI) modality, characterizing the depth\nerror of ToF and stereo-matching. Furthermore, we employ a progressive fusion\nstrategy, merging geometric features from ToF and stereo depth maps with depth\nerror features from the DEI modality to create precise depth maps.\nAdditionally, we create a new ToF-Stereo depth dataset, RealToF, to train and\nvalidate our model. Our experiments demonstrate that MobiFuse excels over\nbaselines by significantly reducing depth measurement errors by up to 77.7%. It\nalso showcases strong generalization across diverse datasets and proves\neffectiveness in two downstream tasks: 3D reconstruction and 3D segmentation.\nThe demo video of MobiFuse in real-life scenarios is available at the\nde-identified YouTube link(https://youtu.be/jy-Sp7T1LVs).\n","authors":["Jinrui Zhang","Deyu Zhang","Tingting Long","Wenxin Chen","Ju Ren","Yunxin Liu","Yudong Zhao","Yaoxue Zhang","Youngki Lee"],"pdf_url":"https://arxiv.org/pdf/2412.13848v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13845v1","updated":"2024-12-18T13:38:06Z","published":"2024-12-18T13:38:06Z","title":"Do Language Models Understand Time?","summary":"  Large language models (LLMs) have revolutionized video-based computer vision\napplications, including action recognition, anomaly detection, and video\nsummarization. Videos inherently pose unique challenges, combining spatial\ncomplexity with temporal dynamics that are absent in static images or textual\ndata. Current approaches to video understanding with LLMs often rely on\npretrained video encoders to extract spatiotemporal features and text encoders\nto capture semantic meaning. These representations are integrated within LLM\nframeworks, enabling multimodal reasoning across diverse video tasks. However,\nthe critical question persists: Can LLMs truly understand the concept of time,\nand how effectively can they reason about temporal relationships in videos?\nThis work critically examines the role of LLMs in video processing, with a\nspecific focus on their temporal reasoning capabilities. We identify key\nlimitations in the interaction between LLMs and pretrained encoders, revealing\ngaps in their ability to model long-term dependencies and abstract temporal\nconcepts such as causality and event progression. Furthermore, we analyze\nchallenges posed by existing video datasets, including biases, lack of temporal\nannotations, and domain-specific limitations that constrain the temporal\nunderstanding of LLMs. To address these gaps, we explore promising future\ndirections, including the co-evolution of LLMs and encoders, the development of\nenriched datasets with explicit temporal labels, and innovative architectures\nfor integrating spatial, temporal, and semantic reasoning. By addressing these\nchallenges, we aim to advance the temporal comprehension of LLMs, unlocking\ntheir full potential in video analysis and beyond.\n","authors":["Xi Ding","Lei Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13845v1.pdf","comment":"Research report"},{"id":"http://arxiv.org/abs/2412.12525v2","updated":"2024-12-18T13:37:48Z","published":"2024-12-17T04:33:31Z","title":"CREST: An Efficient Conjointly-trained Spike-driven Framework for\n  Event-based Object Detection Exploiting Spatiotemporal Dynamics","summary":"  Event-based cameras feature high temporal resolution, wide dynamic range, and\nlow power consumption, which is ideal for high-speed and low-light object\ndetection. Spiking neural networks (SNNs) are promising for event-based object\nrecognition and detection due to their spiking nature but lack efficient\ntraining methods, leading to gradient vanishing and high computational\ncomplexity, especially in deep SNNs. Additionally, existing SNN frameworks\noften fail to effectively handle multi-scale spatiotemporal features, leading\nto increased data redundancy and reduced accuracy. To address these issues, we\npropose CREST, a novel conjointly-trained spike-driven framework to exploit\nspatiotemporal dynamics in event-based object detection. We introduce the\nconjoint learning rule to accelerate SNN learning and alleviate gradient\nvanishing. It also supports dual operation modes for efficient and flexible\nimplementation on different hardware types. Additionally, CREST features a\nfully spike-driven framework with a multi-scale spatiotemporal event integrator\n(MESTOR) and a spatiotemporal-IoU (ST-IoU) loss. Our approach achieves superior\nobject recognition & detection performance and up to 100X energy efficiency\ncompared with state-of-the-art SNN algorithms on three datasets, providing an\nefficient solution for event-based object detection algorithms suitable for SNN\nhardware implementation.\n","authors":["Ruixin Mao","Aoyu Shen","Lin Tang","Jun Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.12525v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2407.21534v4","updated":"2024-12-18T13:12:29Z","published":"2024-07-31T11:40:29Z","title":"ControlMLLM: Training-Free Visual Prompt Learning for Multimodal Large\n  Language Models","summary":"  In this work, we propose a training-free method to inject visual referring\ninto Multimodal Large Language Models (MLLMs) through learnable visual token\noptimization. We observe the relationship between text prompt tokens and visual\ntokens in MLLMs, where attention layers model the connection between them. Our\napproach involves adjusting visual tokens from the MLP output during inference,\ncontrolling which text prompt tokens attend to which visual tokens. We optimize\na learnable visual token based on an energy function, enhancing the strength of\nreferential regions in the attention map. This enables detailed region\ndescription and reasoning without the need for substantial training costs or\nmodel retraining. Our method offers a promising direction for integrating\nreferential abilities into MLLMs. Our method support referring with box, mask,\nscribble and point. The results demonstrate that our method exhibits\ncontrollability and interpretability.\n","authors":["Mingrui Wu","Xinyue Cai","Jiayi Ji","Jiale Li","Oucheng Huang","Gen Luo","Hao Fei","Guannan Jiang","Xiaoshuai Sun","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2407.21534v4.pdf","comment":"Accepted to NeurIPS 2024;\n  Code:https://github.com/mrwu-mac/ControlMLLM"},{"id":"http://arxiv.org/abs/2412.13823v1","updated":"2024-12-18T13:11:58Z","published":"2024-12-18T13:11:58Z","title":"Prompt Categories Cluster for Weakly Supervised Semantic Segmentation","summary":"  Weakly Supervised Semantic Segmentation (WSSS), which leverages image-level\nlabels, has garnered significant attention due to its cost-effectiveness. The\nprevious methods mainly strengthen the inter-class differences to avoid class\nsemantic ambiguity which may lead to erroneous activation. However, they\noverlook the positive function of some shared information between similar\nclasses. Categories within the same cluster share some similar features.\nAllowing the model to recognize these features can further relieve the semantic\nambiguity between these classes. To effectively identify and utilize this\nshared information, in this paper, we introduce a novel WSSS framework called\nPrompt Categories Clustering (PCC). Specifically, we explore the ability of\nLarge Language Models (LLMs) to derive category clusters through prompts. These\nclusters effectively represent the intrinsic relationships between categories.\nBy integrating this relational information into the training network, our model\nis able to better learn the hidden connections between categories. Experimental\nresults demonstrate the effectiveness of our approach, showing its ability to\nenhance performance on the PASCAL VOC 2012 dataset and surpass existing\nstate-of-the-art methods in WSSS.\n","authors":["Wangyu Wu","Xianglin Qiu","Siqi Song","Xiaowei Huang","Fei Ma","Jimin Xiao"],"pdf_url":"https://arxiv.org/pdf/2412.13823v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17686v2","updated":"2024-12-18T13:09:20Z","published":"2024-09-26T09:51:11Z","title":"MoGenTS: Motion Generation based on Spatial-Temporal Joint Modeling","summary":"  Motion generation from discrete quantization offers many advantages over\ncontinuous regression, but at the cost of inevitable approximation errors.\nPrevious methods usually quantize the entire body pose into one code, which not\nonly faces the difficulty in encoding all joints within one vector but also\nloses the spatial relationship between different joints. Differently, in this\nwork we quantize each individual joint into one vector, which i) simplifies the\nquantization process as the complexity associated with a single joint is\nmarkedly lower than that of the entire pose; ii) maintains a spatial-temporal\nstructure that preserves both the spatial relationships among joints and the\ntemporal movement patterns; iii) yields a 2D token map, which enables the\napplication of various 2D operations widely used in 2D images. Grounded in the\n2D motion quantization, we build a spatial-temporal modeling framework, where\n2D joint VQVAE, temporal-spatial 2D masking technique, and spatial-temporal 2D\nattention are proposed to take advantage of spatial-temporal signals among the\n2D tokens. Extensive experiments demonstrate that our method significantly\noutperforms previous methods across different datasets, with a 26.6% decrease\nof FID on HumanML3D and a 29.9% decrease on KIT-ML. Project page:\nhttps://aigc3d.github.io/mogents.\n","authors":["Weihao Yuan","Weichao Shen","Yisheng He","Yuan Dong","Xiaodong Gu","Zilong Dong","Liefeng Bo","Qixing Huang"],"pdf_url":"https://arxiv.org/pdf/2409.17686v2.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.15105v2","updated":"2024-12-18T13:08:47Z","published":"2024-10-19T13:37:24Z","title":"Standardizing Generative Face Video Compression using Supplemental\n  Enhancement Information","summary":"  This paper proposes a Generative Face Video Compression (GFVC) approach using\nSupplemental Enhancement Information (SEI), where a series of compact spatial\nand temporal representations of a face video signal (i.e., 2D/3D key-points,\nfacial semantics and compact features) can be coded using SEI message and\ninserted into the coded video bitstream. At the time of writing, the proposed\nGFVC approach using SEI messages has been adopted into the official working\ndraft of Versatile Supplemental Enhancement Information (VSEI) standard by the\nJoint Video Experts Team (JVET) of ISO/IEC JTC 1/SC 29 and ITU-T SG16, which\nwill be standardized as a new version for \"ITU-T H.274 | ISO/IEC 23002-7\". To\nthe best of the authors' knowledge, the JVET work on the proposed SEI-based\nGFVC approach is the first standardization activity for generative video\ncompression. The proposed SEI approach has not only advanced the reconstruction\nquality of early-day Model-Based Coding (MBC) via the state-of-the-art\ngenerative technique, but also established a new SEI definition for future GFVC\napplications and deployment. Experimental results illustrate that the proposed\nSEI-based GFVC approach can achieve remarkable rate-distortion performance\ncompared with the latest Versatile Video Coding (VVC) standard, whilst also\npotentially enabling a wide variety of functionalities including user-specified\nanimation/filtering and metaverse-related applications.\n","authors":["Bolin Chen","Yan Ye","Jie Chen","Ru-Ling Liao","Shanzhi Yin","Shiqi Wang","Kaifa Yang","Yue Li","Yiling Xu","Ye-Kui Wang","Shiv Gehlot","Guan-Ming Su","Peng Yin","Sean McCarthy","Gary J. Sullivan"],"pdf_url":"https://arxiv.org/pdf/2410.15105v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13817v1","updated":"2024-12-18T13:04:30Z","published":"2024-12-18T13:04:30Z","title":"Nullu: Mitigating Object Hallucinations in Large Vision-Language Models\n  via HalluSpace Projection","summary":"  Recent studies have shown that large vision-language models (LVLMs) often\nsuffer from the issue of object hallucinations (OH). To mitigate this issue, we\nintroduce an efficient method that edits the model weights based on an unsafe\nsubspace, which we call HalluSpace in this paper. With truthful and\nhallucinated text prompts accompanying the visual content as inputs, the\nHalluSpace can be identified by extracting the hallucinated embedding features\nand removing the truthful representations in LVLMs. By orthogonalizing the\nmodel weights, input features will be projected into the Null space of the\nHalluSpace to reduce OH, based on which we name our method Nullu. We reveal\nthat HalluSpaces generally contain statistical bias and unimodal priors of the\nlarge language models (LLMs) applied to build LVLMs, which have been shown as\nessential causes of OH in previous studies. Therefore, null space projection\nsuppresses the LLMs' priors to filter out the hallucinated features, resulting\nin contextually accurate outputs. Experiments show that our method can\neffectively mitigate OH across different LVLM families without extra inference\ncosts and also show strong performance in general LVLM benchmarks. Code is\nreleased at \\url{https://github.com/Ziwei-Zheng/Nullu}.\n","authors":["Le Yang","Ziwei Zheng","Boxu Chen","Zhengyu Zhao","Chenhao Lin","Chao Shen"],"pdf_url":"https://arxiv.org/pdf/2412.13817v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2412.13815v1","updated":"2024-12-18T13:03:00Z","published":"2024-12-18T13:03:00Z","title":"Object Style Diffusion for Generalized Object Detection in Urban Scene","summary":"  Object detection is a critical task in computer vision, with applications in\nvarious domains such as autonomous driving and urban scene monitoring. However,\ndeep learning-based approaches often demand large volumes of annotated data,\nwhich are costly and difficult to acquire, particularly in complex and\nunpredictable real-world environments. This dependency significantly hampers\nthe generalization capability of existing object detection techniques. To\naddress this issue, we introduce a novel single-domain object detection\ngeneralization method, named GoDiff, which leverages a pre-trained model to\nenhance generalization in unseen domains. Central to our approach is the Pseudo\nTarget Data Generation (PTDG) module, which employs a latent diffusion model to\ngenerate pseudo-target domain data that preserves source domain characteristics\nwhile introducing stylistic variations. By integrating this pseudo data with\nsource domain data, we diversify the training dataset. Furthermore, we\nintroduce a cross-style instance normalization technique to blend style\nfeatures from different domains generated by the PTDG module, thereby\nincreasing the detector's robustness. Experimental results demonstrate that our\nmethod not only enhances the generalization ability of existing detectors but\nalso functions as a plug-and-play enhancement for other single-domain\ngeneralization methods, achieving state-of-the-art performance in autonomous\ndriving scenarios.\n","authors":["Hao Li","Xiangyuan Yang","Mengzhu Wang","Long Lan","Ke Liang","Xinwang Liu","Kenli Li"],"pdf_url":"https://arxiv.org/pdf/2412.13815v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12888v2","updated":"2024-12-18T13:01:11Z","published":"2024-12-17T13:12:31Z","title":"ArtAug: Enhancing Text-to-Image Generation through\n  Synthesis-Understanding Interaction","summary":"  The emergence of diffusion models has significantly advanced image synthesis.\nThe recent studies of model interaction and self-corrective reasoning approach\nin large language models offer new insights for enhancing text-to-image models.\nInspired by these studies, we propose a novel method called ArtAug for\nenhancing text-to-image models in this paper. To the best of our knowledge,\nArtAug is the first one that improves image synthesis models via model\ninteractions with understanding models. In the interactions, we leverage human\npreferences implicitly learned by image understanding models to provide\nfine-grained suggestions for image synthesis models. The interactions can\nmodify the image content to make it aesthetically pleasing, such as adjusting\nexposure, changing shooting angles, and adding atmospheric effects. The\nenhancements brought by the interaction are iteratively fused into the\nsynthesis model itself through an additional enhancement module. This enables\nthe synthesis model to directly produce aesthetically pleasing images without\nany extra computational cost. In the experiments, we train the ArtAug\nenhancement module on existing text-to-image models. Various evaluation metrics\nconsistently demonstrate that ArtAug enhances the generative capabilities of\ntext-to-image models without incurring additional computational costs. The\nsource code and models will be released publicly.\n","authors":["Zhongjie Duan","Qianyi Zhao","Cen Chen","Daoyuan Chen","Wenmeng Zhou","Yaliang Li","Yingda Chen"],"pdf_url":"https://arxiv.org/pdf/2412.12888v2.pdf","comment":"18 pages, 8 figures"},{"id":"http://arxiv.org/abs/2412.13811v1","updated":"2024-12-18T12:58:38Z","published":"2024-12-18T12:58:38Z","title":"Spatial Brain Tumor Concentration Estimation for Individualized\n  Radiotherapy Planning","summary":"  Biophysical modeling of brain tumors has emerged as a promising strategy for\npersonalizing radiotherapy planning by estimating the otherwise hidden\ndistribution of tumor cells within the brain. However, many existing\nstate-of-the-art methods are computationally intensive, limiting their\nwidespread translation into clinical practice. In this work, we propose an\nefficient and direct method that utilizes soft physical constraints to estimate\nthe tumor cell concentration from preoperative MRI of brain tumor patients. Our\napproach optimizes a 3D tumor concentration field by simultaneously minimizing\nthe difference between the observed MRI and a physically informed loss\nfunction. Compared to existing state-of-the-art techniques, our method\nsignificantly improves predicting tumor recurrence on two public datasets with\na total of 192 patients while maintaining a clinically viable runtime of under\none minute - a substantial reduction from the 30 minutes required by the\ncurrent best approach. Furthermore, we showcase the generalizability of our\nframework by incorporating additional imaging information and physical\nconstraints, highlighting its potential to translate to various medical\ndiffusion phenomena with imperfect data.\n","authors":["Jonas Weidner","Michal Balcerak","Ivan Ezhov","André Datchev","Laurin Lux","Lucas Zimmerand Daniel Rueckert","Björn Menze","Benedikt Wiestler"],"pdf_url":"https://arxiv.org/pdf/2412.13811v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13810v1","updated":"2024-12-18T12:57:56Z","published":"2024-12-18T12:57:56Z","title":"CAD-Assistant: Tool-Augmented VLLMs as Generic CAD Task Solvers?","summary":"  We propose CAD-Assistant, a general-purpose CAD agent for AI-assisted design.\nOur approach is based on a powerful Vision and Large Language Model (VLLM) as a\nplanner and a tool-augmentation paradigm using CAD-specific modules.\nCAD-Assistant addresses multimodal user queries by generating actions that are\niteratively executed on a Python interpreter equipped with the FreeCAD\nsoftware, accessed via its Python API. Our framework is able to assess the\nimpact of generated CAD commands on geometry and adapts subsequent actions\nbased on the evolving state of the CAD design. We consider a wide range of\nCAD-specific tools including Python libraries, modules of the FreeCAD Python\nAPI, helpful routines, rendering functions and other specialized modules. We\nevaluate our method on multiple CAD benchmarks and qualitatively demonstrate\nthe potential of tool-augmented VLLMs as generic CAD task solvers across\ndiverse CAD workflows.\n","authors":["Dimitrios Mallis","Ahmet Serdar Karadeniz","Sebastian Cavada","Danila Rukhovich","Niki Foteinopoulou","Kseniya Cherenkova","Anis Kacem","Djamila Aouada"],"pdf_url":"https://arxiv.org/pdf/2412.13810v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04377v2","updated":"2024-12-18T12:55:49Z","published":"2024-12-05T17:52:35Z","title":"A Hitchhiker's Guide to Understanding Performances of Two-Class\n  Classifiers","summary":"  Properly understanding the performances of classifiers is essential in\nvarious scenarios. However, the literature often relies only on one or two\nstandard scores to compare classifiers, which fails to capture the nuances of\napplication-specific requirements, potentially leading to suboptimal classifier\nselection. Recently, a paper on the foundations of the theory of\nperformance-based ranking introduced a tool, called the Tile, that organizes an\ninfinity of ranking scores into a 2D map. Thanks to the Tile, it is now\npossible to evaluate and compare classifiers efficiently, displaying all\npossible application-specific preferences instead of having to rely on a pair\nof scores. In this paper, we provide a first hitchhiker's guide for\nunderstanding the performances of two-class classifiers by presenting four\nscenarios, each showcasing a different user profile: a theoretical analyst, a\nmethod designer, a benchmarker, and an application developer. Particularly, we\nshow that we can provide different interpretative flavors that are adapted to\nthe user's needs by mapping different values on the Tile. As an illustration,\nwe leverage the newly introduced Tile tool and the different flavors to rank\nand analyze the performances of 74 state-of-the-art semantic segmentation\nmodels in two-class classification through the eyes of the four user profiles.\nThrough these user profiles, we demonstrate that the Tile effectively captures\nthe behavior of classifiers in a single visualization, while accommodating an\ninfinite number of ranking scores.\n","authors":["Anaïs Halin","Sébastien Piérard","Anthony Cioppa","Marc Van Droogenbroeck"],"pdf_url":"https://arxiv.org/pdf/2412.04377v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04309v2","updated":"2024-12-18T12:50:29Z","published":"2024-12-05T16:27:59Z","title":"The Tile: A 2D Map of Ranking Scores for Two-Class Classification","summary":"  In the computer vision and machine learning communities, as well as in many\nother research domains, rigorous evaluation of any new method, including\nclassifiers, is essential. One key component of the evaluation process is the\nability to compare and rank methods. However, ranking classifiers and\naccurately comparing their performances, especially when taking\napplication-specific preferences into account, remains challenging. For\ninstance, commonly used evaluation tools like Receiver Operating Characteristic\n(ROC) and Precision/Recall (PR) spaces display performances based on two\nscores. Hence, they are inherently limited in their ability to compare\nclassifiers across a broader range of scores and lack the capability to\nestablish a clear ranking among classifiers. In this paper, we present a novel\nversatile tool, named the Tile, that organizes an infinity of ranking scores in\na single 2D map for two-class classifiers, including common evaluation scores\nsuch as the accuracy, the true positive rate, the positive predictive value,\nJaccard's coefficient, and all F-beta scores. Furthermore, we study the\nproperties of the underlying ranking scores, such as the influence of the\npriors or the correspondences with the ROC space, and depict how to\ncharacterize any other score by comparing them to the Tile. Overall, we\ndemonstrate that the Tile is a powerful tool that effectively captures all the\nrankings in a single visualization and allows interpreting them.\n","authors":["Sébastien Piérard","Anaïs Halin","Anthony Cioppa","Adrien Deliège","Marc Van Droogenbroeck"],"pdf_url":"https://arxiv.org/pdf/2412.04309v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13803v1","updated":"2024-12-18T12:50:11Z","published":"2024-12-18T12:50:11Z","title":"M$^3$-VOS: Multi-Phase, Multi-Transition, and Multi-Scenery Video Object\n  Segmentation","summary":"  Intelligent robots need to interact with diverse objects across various\nenvironments. The appearance and state of objects frequently undergo complex\ntransformations depending on the object properties, e.g., phase transitions.\nHowever, in the vision community, segmenting dynamic objects with phase\ntransitions is overlooked. In light of this, we introduce the concept of phase\nin segmentation, which categorizes real-world objects based on their visual\ncharacteristics and potential morphological and appearance changes. Then, we\npresent a new benchmark, Multi-Phase, Multi-Transition, and Multi-Scenery Video\nObject Segmentation (M3-VOS), to verify the ability of models to understand\nobject phases, which consists of 479 high-resolution videos spanning over 10\ndistinct everyday scenarios. It provides dense instance mask annotations that\ncapture both object phases and their transitions. We evaluate state-of-the-art\nmethods on M3-VOS, yielding several key insights. Notably, current appearance\nbased approaches show significant room for improvement when handling objects\nwith phase transitions. The inherent changes in disorder suggest that the\npredictive performance of the forward entropy-increasing process can be\nimproved through a reverse entropy-reducing process. These findings lead us to\npropose ReVOS, a new plug-and-play model that improves its performance by\nreversal refinement. Our data and code will be publicly available\n","authors":["Zixuan Chen","Jiaxin Li","Liming Tan","Yejie Guo","Junxuan Liang","Cewu Lu","Yonglu Li"],"pdf_url":"https://arxiv.org/pdf/2412.13803v1.pdf","comment":"18 pages, 12 figures"},{"id":"http://arxiv.org/abs/2412.04227v2","updated":"2024-12-18T12:45:58Z","published":"2024-12-05T15:05:25Z","title":"Foundations of the Theory of Performance-Based Ranking","summary":"  Ranking entities such as algorithms, devices, methods, or models based on\ntheir performances, while accounting for application-specific preferences, is a\nchallenge. To address this challenge, we establish the foundations of a\nuniversal theory for performance-based ranking. First, we introduce a rigorous\nframework built on top of both the probability and order theories. Our new\nframework encompasses the elements necessary to (1) manipulate performances as\nmathematical objects, (2) express which performances are worse than or\nequivalent to others, (3) model tasks through a variable called satisfaction,\n(4) consider properties of the evaluation, (5) define scores, and (6) specify\napplication-specific preferences through a variable called importance. On top\nof this framework, we propose the first axiomatic definition of performance\norderings and performance-based rankings. Then, we introduce a universal\nparametric family of scores, called ranking scores, that can be used to\nestablish rankings satisfying our axioms, while considering\napplication-specific preferences. Finally, we show, in the case of two-class\nclassification, that the family of ranking scores encompasses well-known\nperformance scores, including the accuracy, the true positive rate (recall,\nsensitivity), the true negative rate (specificity), the positive predictive\nvalue (precision), and F1. However, we also show that some other scores\ncommonly used to compare classifiers are unsuitable to derive performance\norderings satisfying the axioms. Therefore, this paper provides the computer\nvision and machine learning communities with a rigorous framework for\nevaluating and ranking entities.\n","authors":["Sébastien Piérard","Anaïs Halin","Anthony Cioppa","Adrien Deliège","Marc Van Droogenbroeck"],"pdf_url":"https://arxiv.org/pdf/2412.04227v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.00705v2","updated":"2024-12-18T12:26:03Z","published":"2024-12-01T07:02:36Z","title":"Photoacoustic Iterative Optimization Algorithm with Shape Prior\n  Regularization","summary":"  Photoacoustic imaging (PAI) suffers from inherent limitations that can\ndegrade the quality of reconstructed results, such as noise, artifacts and\nincomplete data acquisition caused by sparse sampling or partial array\ndetection. In this study, we proposed a new optimization method for both\ntwo-dimensional (2D) and three-dimensional (3D) PAI reconstruction results,\ncalled the regularized iteration method with shape prior. The shape prior is a\nprobability matrix derived from the reconstruction results of multiple sets of\nrandom partial array signals in a computational imaging system using any\nreconstruction algorithm, such as Delay-and-Sum (DAS) and Back-Projection (BP).\nIn the probability matrix, high-probability locations indicate high consistency\namong multiple reconstruction results at those positions, suggesting a high\nlikelihood of representing the true imaging results. In contrast,\nlow-probability locations indicate higher randomness, leaning more towards\nnoise or artifacts. As a shape prior, this probability matrix guides the\niteration and regularization of the entire array signal reconstruction results\nusing the original reconstruction algorithm (the same algorithm for processing\nrandom partial array signals). The method takes advantage of the property that\nthe similarity of the object to be imitated is higher than that of noise or\nartifact in the results reconstructed by multiple sets of random partial array\nsignals of the entire imaging system. The probability matrix is taken as a\nprerequisite for improving the original reconstruction results, and the\noptimizer is used to further iterate the imaging results to remove noise and\nartifacts and improve the imaging fidelity. Especially in the case involving\nsparse view which brings more artifacts, the effect is remarkable. Simulation\nand real experiments have both demonstrated the superiority of this method.\n","authors":["Yu Zhang","Shuang Li","Yibing Wang","Yu Sun","Wenyi Xiang"],"pdf_url":"https://arxiv.org/pdf/2412.00705v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13772v1","updated":"2024-12-18T12:10:33Z","published":"2024-12-18T12:10:33Z","title":"An Efficient Occupancy World Model via Decoupled Dynamic Flow and\n  Image-assisted Training","summary":"  The field of autonomous driving is experiencing a surge of interest in world\nmodels, which aim to predict potential future scenarios based on historical\nobservations. In this paper, we introduce DFIT-OccWorld, an efficient 3D\noccupancy world model that leverages decoupled dynamic flow and image-assisted\ntraining strategy, substantially improving 4D scene forecasting performance. To\nsimplify the training process, we discard the previous two-stage training\nstrategy and innovatively reformulate the occupancy forecasting problem as a\ndecoupled voxels warping process. Our model forecasts future dynamic voxels by\nwarping existing observations using voxel flow, whereas static voxels are\neasily obtained through pose transformation. Moreover, our method incorporates\nan image-assisted training paradigm to enhance prediction reliability.\nSpecifically, differentiable volume rendering is adopted to generate rendered\ndepth maps through predicted future volumes, which are adopted in render-based\nphotometric consistency. Experiments demonstrate the effectiveness of our\napproach, showcasing its state-of-the-art performance on the nuScenes and\nOpenScene benchmarks for 4D occupancy forecasting, end-to-end motion planning\nand point cloud forecasting. Concretely, it achieves state-of-the-art\nperformances compared to existing 3D world models while incurring substantially\nlower computational costs.\n","authors":["Haiming Zhang","Ying Xue","Xu Yan","Jiacheng Zhang","Weichao Qiu","Dongfeng Bai","Bingbing Liu","Shuguang Cui","Zhen Li"],"pdf_url":"https://arxiv.org/pdf/2412.13772v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09954v2","updated":"2024-12-18T11:51:45Z","published":"2024-12-13T08:24:12Z","title":"A2RNet: Adversarial Attack Resilient Network for Robust Infrared and\n  Visible Image Fusion","summary":"  Infrared and visible image fusion (IVIF) is a crucial technique for enhancing\nvisual performance by integrating unique information from different modalities\ninto one fused image. Exiting methods pay more attention to conducting fusion\nwith undisturbed data, while overlooking the impact of deliberate interference\non the effectiveness of fusion results. To investigate the robustness of fusion\nmodels, in this paper, we propose a novel adversarial attack resilient network,\ncalled $\\textrm{A}^{\\textrm{2}}$RNet. Specifically, we develop an adversarial\nparadigm with an anti-attack loss function to implement adversarial attacks and\ntraining. It is constructed based on the intrinsic nature of IVIF and provide a\nrobust foundation for future research advancements. We adopt a Unet as the\npipeline with a transformer-based defensive refinement module (DRM) under this\nparadigm, which guarantees fused image quality in a robust coarse-to-fine\nmanner. Compared to previous works, our method mitigates the adverse effects of\nadversarial perturbations, consistently maintaining high-fidelity fusion\nresults. Furthermore, the performance of downstream tasks can also be well\nmaintained under adversarial attacks. Code is available at\nhttps://github.com/lok-18/A2RNet.\n","authors":["Jiawei Li","Hongwei Yu","Jiansheng Chen","Xinlong Ding","Jinlong Wang","Jinyuan Liu","Bochao Zou","Huimin Ma"],"pdf_url":"https://arxiv.org/pdf/2412.09954v2.pdf","comment":"9 pages, 8 figures, The 39th Annual AAAI Conference on Artificial\n  Intelligence"},{"id":"http://arxiv.org/abs/2412.13753v1","updated":"2024-12-18T11:43:41Z","published":"2024-12-18T11:43:41Z","title":"Mesoscopic Insights: Orchestrating Multi-scale & Hybrid Architecture for\n  Image Manipulation Localization","summary":"  The mesoscopic level serves as a bridge between the macroscopic and\nmicroscopic worlds, addressing gaps overlooked by both. Image manipulation\nlocalization (IML), a crucial technique to pursue truth from fake images, has\nlong relied on low-level (microscopic-level) traces. However, in practice, most\ntampering aims to deceive the audience by altering image semantics. As a\nresult, manipulation commonly occurs at the object level (macroscopic level),\nwhich is equally important as microscopic traces. Therefore, integrating these\ntwo levels into the mesoscopic level presents a new perspective for IML\nresearch. Inspired by this, our paper explores how to simultaneously construct\nmesoscopic representations of micro and macro information for IML and\nintroduces the Mesorch architecture to orchestrate both. Specifically, this\narchitecture i) combines Transformers and CNNs in parallel, with Transformers\nextracting macro information and CNNs capturing micro details, and ii) explores\nacross different scales, assessing micro and macro information seamlessly.\nAdditionally, based on the Mesorch architecture, the paper introduces two\nbaseline models aimed at solving IML tasks through mesoscopic representation.\nExtensive experiments across four datasets have demonstrated that our models\nsurpass the current state-of-the-art in terms of performance, computational\ncomplexity, and robustness.\n","authors":["Xuekang Zhu","Xiaochen Ma","Lei Su","Zhuohang Jiang","Bo Du","Xiwen Wang","Zeyu Lei","Wentao Feng","Chi-Man Pun","Jizhe Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.13753v1.pdf","comment":"AAAI 2025. Code:\n  $\\href{https://github.com/scu-zjz/Mesorch}{this~url}$"},{"id":"http://arxiv.org/abs/2412.13749v1","updated":"2024-12-18T11:33:16Z","published":"2024-12-18T11:33:16Z","title":"Multi-Exposure Image Fusion via Distilled 3D LUT Grid with Editable Mode","summary":"  With the rising imaging resolution of handheld devices, existing\nmulti-exposure image fusion algorithms struggle to generate a high dynamic\nrange image with ultra-high resolution in real-time. Apart from that, there is\na trend to design a manageable and editable algorithm as the different needs of\nreal application scenarios. To tackle these issues, we introduce 3D LUT\ntechnology, which can enhance images with ultra-high-definition (UHD)\nresolution in real time on resource-constrained devices. However, since the\nfusion of information from multiple images with different exposure rates is\nuncertain, and this uncertainty significantly trials the generalization power\nof the 3D LUT grid. To address this issue and ensure a robust learning space\nfor the model, we propose using a teacher-student network to model the\nuncertainty on the 3D LUT grid.Furthermore, we provide an editable mode for the\nmulti-exposure image fusion algorithm by using the implicit representation\nfunction to match the requirements in different scenarios. Extensive\nexperiments demonstrate that our proposed method is highly competitive in\nefficiency and accuracy.\n","authors":["Xin Su","Zhuoran Zheng"],"pdf_url":"https://arxiv.org/pdf/2412.13749v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13742v1","updated":"2024-12-18T11:19:23Z","published":"2024-12-18T11:19:23Z","title":"Learnable Prompting SAM-induced Knowledge Distillation for\n  Semi-supervised Medical Image Segmentation","summary":"  The limited availability of labeled data has driven advancements in\nsemi-supervised learning for medical image segmentation. Modern large-scale\nmodels tailored for general segmentation, such as the Segment Anything Model\n(SAM), have revealed robust generalization capabilities. However, applying\nthese models directly to medical image segmentation still exposes performance\ndegradation. In this paper, we propose a learnable prompting SAM-induced\nKnowledge distillation framework (KnowSAM) for semi-supervised medical image\nsegmentation. Firstly, we propose a Multi-view Co-training (MC) strategy that\nemploys two distinct sub-networks to employ a co-teaching paradigm, resulting\nin more robust outcomes. Secondly, we present a Learnable Prompt Strategy (LPS)\nto dynamically produce dense prompts and integrate an adapter to fine-tune SAM\nspecifically for medical image segmentation tasks. Moreover, we propose\nSAM-induced Knowledge Distillation (SKD) to transfer useful knowledge from SAM\nto two sub-networks, enabling them to learn from SAM's predictions and\nalleviate the effects of incorrect pseudo-labels during training. Notably, the\npredictions generated by our subnets are used to produce mask prompts for SAM,\nfacilitating effective inter-module information exchange. Extensive\nexperimental results on various medical segmentation tasks demonstrate that our\nmodel outperforms the state-of-the-art semi-supervised segmentation approaches.\nCrucially, our SAM distillation framework can be seamlessly integrated into\nother semi-supervised segmentation methods to enhance performance. The code\nwill be released upon acceptance of this manuscript at:\nhttps://github.com/taozh2017/KnowSAM\n","authors":["Kaiwen Huang","Tao Zhou","Huazhu Fu","Yizhe Zhang","Yi Zhou","Chen Gong","Dong Liang"],"pdf_url":"https://arxiv.org/pdf/2412.13742v1.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2412.13736v1","updated":"2024-12-18T11:14:02Z","published":"2024-12-18T11:14:02Z","title":"MedCoT: Medical Chain of Thought via Hierarchical Expert","summary":"  Artificial intelligence has advanced in Medical Visual Question Answering\n(Med-VQA), but prevalent research tends to focus on the accuracy of the\nanswers, often overlooking the reasoning paths and interpretability, which are\ncrucial in clinical settings. Besides, current Med-VQA algorithms, typically\nreliant on singular models, lack the robustness needed for real-world medical\ndiagnostics which usually require collaborative expert evaluation. To address\nthese shortcomings, this paper presents MedCoT, a novel hierarchical expert\nverification reasoning chain method designed to enhance interpretability and\naccuracy in biomedical imaging inquiries. MedCoT is predicated on two\nprinciples: The necessity for explicit reasoning paths in Med-VQA and the\nrequirement for multi-expert review to formulate accurate conclusions. The\nmethodology involves an Initial Specialist proposing diagnostic rationales,\nfollowed by a Follow-up Specialist who validates these rationales, and finally,\na consensus is reached through a vote among a sparse Mixture of Experts within\nthe locally deployed Diagnostic Specialist, which then provides the definitive\ndiagnosis. Experimental evaluations on four standard Med-VQA datasets\ndemonstrate that MedCoT surpasses existing state-of-the-art approaches,\nproviding significant improvements in performance and interpretability.\n","authors":["Jiaxiang Liu","Yuan Wang","Jiawei Du","Joey Tianyi Zhou","Zuozhu Liu"],"pdf_url":"https://arxiv.org/pdf/2412.13736v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13735v1","updated":"2024-12-18T11:14:01Z","published":"2024-12-18T11:14:01Z","title":"3D Registration in 30 Years: A Survey","summary":"  3D point cloud registration is a fundamental problem in computer vision,\ncomputer graphics, robotics, remote sensing, and etc. Over the last thirty\nyears, we have witnessed the amazing advancement in this area with numerous\nkinds of solutions. Although a handful of relevant surveys have been conducted,\ntheir coverage is still limited. In this work, we present a comprehensive\nsurvey on 3D point cloud registration, covering a set of sub-areas such as\npairwise coarse registration, pairwise fine registration, multi-view\nregistration, cross-scale registration, and multi-instance registration. The\ndatasets, evaluation metrics, method taxonomy, discussions of the merits and\ndemerits, insightful thoughts of future directions are comprehensively\npresented in this survey. The regularly updated project page of the survey is\navailable at https://github.com/Amyyyy11/3D-Registration-in-30-Years-A-Survey.\n","authors":["Jiaqi Yang","Chu'ai Zhang","Zhengbao Wang","Xinyue Cao","Xuan Ouyang","Xiyu Zhang","Zhenxuan Zeng","Zhao Zeng","Borui Lu","Zhiyi Xia","Qian Zhang","Yulan Guo","Yanning Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.13735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09892v2","updated":"2024-12-18T11:12:49Z","published":"2024-12-13T06:14:57Z","title":"VQTalker: Towards Multilingual Talking Avatars through Facial Motion\n  Tokenization","summary":"  We present VQTalker, a Vector Quantization-based framework for multilingual\ntalking head generation that addresses the challenges of lip synchronization\nand natural motion across diverse languages. Our approach is grounded in the\nphonetic principle that human speech comprises a finite set of distinct sound\nunits (phonemes) and corresponding visual articulations (visemes), which often\nshare commonalities across languages. We introduce a facial motion tokenizer\nbased on Group Residual Finite Scalar Quantization (GRFSQ), which creates a\ndiscretized representation of facial features. This method enables\ncomprehensive capture of facial movements while improving generalization to\nmultiple languages, even with limited training data. Building on this quantized\nrepresentation, we implement a coarse-to-fine motion generation process that\nprogressively refines facial animations. Extensive experiments demonstrate that\nVQTalker achieves state-of-the-art performance in both video-driven and\nspeech-driven scenarios, particularly in multilingual settings. Notably, our\nmethod achieves high-quality results at a resolution of 512*512 pixels while\nmaintaining a lower bitrate of approximately 11 kbps. Our work opens new\npossibilities for cross-lingual talking face generation. Synthetic results can\nbe viewed at https://x-lance.github.io/VQTalker.\n","authors":["Tao Liu","Ziyang Ma","Qi Chen","Feilong Chen","Shuai Fan","Xie Chen","Kai Yu"],"pdf_url":"https://arxiv.org/pdf/2412.09892v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2412.13734v1","updated":"2024-12-18T11:12:10Z","published":"2024-12-18T11:12:10Z","title":"Text2Relight: Creative Portrait Relighting with Text Guidance","summary":"  We present a lighting-aware image editing pipeline that, given a portrait\nimage and a text prompt, performs single image relighting. Our model modifies\nthe lighting and color of both the foreground and background to align with the\nprovided text description. The unbounded nature in creativeness of a text\nallows us to describe the lighting of a scene with any sensory features\nincluding temperature, emotion, smell, time, and so on. However, the modeling\nof such mapping between the unbounded text and lighting is extremely\nchallenging due to the lack of dataset where there exists no scalable data that\nprovides large pairs of text and relighting, and therefore, current text-driven\nimage editing models does not generalize to lighting-specific use cases. We\novercome this problem by introducing a novel data synthesis pipeline: First,\ndiverse and creative text prompts that describe the scenes with various\nlighting are automatically generated under a crafted hierarchy using a large\nlanguage model (*e.g.,* ChatGPT). A text-guided image generation model creates\na lighting image that best matches the text. As a condition of the lighting\nimages, we perform image-based relighting for both foreground and background\nusing a single portrait image or a set of OLAT (One-Light-at-A-Time) images\ncaptured from lightstage system. Particularly for the background relighting, we\nrepresent the lighting image as a set of point lights and transfer them to\nother background images. A generative diffusion model learns the synthesized\nlarge-scale data with auxiliary task augmentation (*e.g.,* portrait delighting\nand light positioning) to correlate the latent text and lighting distribution\nfor text-guided portrait relighting.\n","authors":["Junuk Cha","Mengwei Ren","Krishna Kumar Singh","He Zhang","Yannick Hold-Geoffroy","Seunghyun Yoon","HyunJoon Jung","Jae Shin Yoon","Seungryul Baek"],"pdf_url":"https://arxiv.org/pdf/2412.13734v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13732v1","updated":"2024-12-18T11:10:18Z","published":"2024-12-18T11:10:18Z","title":"Modelling Multi-modal Cross-interaction for ML-FSIC Based on Local\n  Feature Selection","summary":"  The aim of multi-label few-shot image classification (ML-FSIC) is to assign\nsemantic labels to images, in settings where only a small number of training\nexamples are available for each label. A key feature of the multi-label setting\nis that images often have several labels, which typically refer to objects\nappearing in different regions of the image. When estimating label prototypes,\nin a metric-based setting, it is thus important to determine which regions are\nrelevant for which labels, but the limited amount of training data and the\nnoisy nature of local features make this highly challenging. As a solution, we\npropose a strategy in which label prototypes are gradually refined. First, we\ninitialize the prototypes using word embeddings, which allows us to leverage\nprior knowledge about the meaning of the labels. Second, taking advantage of\nthese initial prototypes, we then use a Loss Change Measurement~(LCM) strategy\nto select the local features from the training images (i.e.\\ the support set)\nthat are most likely to be representative of a given label. Third, we construct\nthe final prototype of the label by aggregating these representative local\nfeatures using a multi-modal cross-interaction mechanism, which again relies on\nthe initial word embedding-based prototypes. Experiments on COCO, PASCAL VOC,\nNUS-WIDE, and iMaterialist show that our model substantially improves the\ncurrent state-of-the-art.\n","authors":["Kun Yan","Zied Bouraoui","Fangyun Wei","Chang Xu","Ping Wang","Shoaib Jameel","Steven Schockaert"],"pdf_url":"https://arxiv.org/pdf/2412.13732v1.pdf","comment":"Accepted in Transactions on Multimedia Computing Communications and\n  Applications"},{"id":"http://arxiv.org/abs/2412.13726v1","updated":"2024-12-18T11:05:56Z","published":"2024-12-18T11:05:56Z","title":"Unified Understanding of Environment, Task, and Human for Human-Robot\n  Interaction in Real-World Environments","summary":"  To facilitate human--robot interaction (HRI) tasks in real-world scenarios,\nservice robots must adapt to dynamic environments and understand the required\ntasks while effectively communicating with humans. To accomplish HRI in\npractice, we propose a novel indoor dynamic map, task understanding system, and\nresponse generation system. The indoor dynamic map optimizes robot behavior by\nmanaging an occupancy grid map and dynamic information, such as furniture and\nhumans, in separate layers. The task understanding system targets tasks that\nrequire multiple actions, such as serving ordered items. Task representations\nthat predefine the flow of necessary actions are applied to achieve highly\naccurate understanding. The response generation system is executed in parallel\nwith task understanding to facilitate smooth HRI by informing humans of the\nsubsequent actions of the robot. In this study, we focused on waiter duties in\na restaurant setting as a representative application of HRI in a dynamic\nenvironment. We developed an HRI system that could perform tasks such as\nserving food and cleaning up while communicating with customers. In experiments\nconducted in a simulated restaurant environment, the proposed HRI system\nsuccessfully communicated with customers and served ordered food with 90\\%\naccuracy. In a questionnaire administered after the experiment, the HRI system\nof the robot received 4.2 points out of 5. These outcomes indicated the\neffectiveness of the proposed method and HRI system in executing waiter tasks\nin real-world environments.\n","authors":["Yuga Yano","Akinobu Mizutani","Yukiya Fukuda","Daiju Kanaoka","Tomohiro Ono","Hakaru Tamukoh"],"pdf_url":"https://arxiv.org/pdf/2412.13726v1.pdf","comment":"2024 33rd IEEE International Conference on Robot and Human\n  Interactive Communication (RO-MAN)"},{"id":"http://arxiv.org/abs/2408.10360v2","updated":"2024-12-18T11:02:07Z","published":"2024-08-19T18:56:24Z","title":"HaSPeR: An Image Repository for Hand Shadow Puppet Recognition","summary":"  Hand shadow puppetry, also known as shadowgraphy or ombromanie, is a form of\ntheatrical art and storytelling where hand shadows are projected onto flat\nsurfaces to create illusions of living creatures. The skilled performers create\nthese silhouettes by hand positioning, finger movements, and dexterous gestures\nto resemble shadows of animals and objects. Due to the lack of practitioners\nand a seismic shift in people's entertainment standards, this art form is on\nthe verge of extinction. To facilitate its preservation and proliferate it to a\nwider audience, we introduce ${\\rm H{\\small A}SP{\\small E}R}$, a novel dataset\nconsisting of 15,000 images of hand shadow puppets across 15 classes extracted\nfrom both professional and amateur hand shadow puppeteer clips. We provide a\ndetailed statistical analysis of the dataset and employ a range of pretrained\nimage classification models to establish baselines. Our findings show a\nsubstantial performance superiority of skip-connected convolutional models over\nattention-based transformer architectures. We also find that lightweight\nmodels, such as MobileNetV2, suited for mobile applications and embedded\ndevices, perform comparatively well. We surmise that such low-latency\narchitectures can be useful in developing ombromanie teaching tools, and we\ncreate a prototype application to explore this surmission. Keeping the\nbest-performing model ResNet34 under the limelight, we conduct comprehensive\nfeature-spatial, explainability, and error analyses to gain insights into its\ndecision-making process. To the best of our knowledge, this is the first\ndocumented dataset and research endeavor to preserve this dying art for future\ngenerations, with computer vision approaches. Our code and data will be\npublicly available.\n","authors":["Syed Rifat Raiyan","Zibran Zarif Amio","Sabbir Ahmed"],"pdf_url":"https://arxiv.org/pdf/2408.10360v2.pdf","comment":"Submitted to IEEE Transactions on Artificial Intelligence (IEEE TAI),\n  13 pages, 105 figures, 2 tables"},{"id":"http://arxiv.org/abs/2412.13717v1","updated":"2024-12-18T10:55:58Z","published":"2024-12-18T10:55:58Z","title":"Towards Automatic Evaluation for Image Transcreation","summary":"  Beyond conventional paradigms of translating speech and text, recently, there\nhas been interest in automated transcreation of images to facilitate\nlocalization of visual content across different cultures. Attempts to define\nthis as a formal Machine Learning (ML) problem have been impeded by the lack of\nautomatic evaluation mechanisms, with previous work relying solely on human\nevaluation. In this paper, we seek to close this gap by proposing a suite of\nautomatic evaluation metrics inspired by machine translation (MT) metrics,\ncategorized into: a) Object-based, b) Embedding-based, and c) VLM-based.\nDrawing on theories from translation studies and real-world transcreation\npractices, we identify three critical dimensions of image transcreation:\ncultural relevance, semantic equivalence and visual similarity, and design our\nmetrics to evaluate systems along these axes. Our results show that proprietary\nVLMs best identify cultural relevance and semantic equivalence, while\nvision-encoder representations are adept at measuring visual similarity.\nMeta-evaluation across 7 countries shows our metrics agree strongly with human\nratings, with average segment-level correlations ranging from 0.55-0.87.\nFinally, through a discussion of the merits and demerits of each metric, we\noffer a robust framework for automated image transcreation evaluation, grounded\nin both theoretical foundations and practical application. Our code can be\nfound here: https://github.com/simran-khanuja/automatic-eval-transcreation\n","authors":["Simran Khanuja","Vivek Iyer","Claire He","Graham Neubig"],"pdf_url":"https://arxiv.org/pdf/2412.13717v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13709v1","updated":"2024-12-18T10:51:59Z","published":"2024-12-18T10:51:59Z","title":"Physics-Based Adversarial Attack on Near-Infrared Human Detector for\n  Nighttime Surveillance Camera Systems","summary":"  Many surveillance cameras switch between daytime and nighttime modes based on\nilluminance levels. During the day, the camera records ordinary RGB images\nthrough an enabled IR-cut filter. At night, the filter is disabled to capture\nnear-infrared (NIR) light emitted from NIR LEDs typically mounted around the\nlens. While RGB-based AI algorithm vulnerabilities have been widely reported,\nthe vulnerabilities of NIR-based AI have rarely been investigated. In this\npaper, we identify fundamental vulnerabilities in NIR-based image understanding\ncaused by color and texture loss due to the intrinsic characteristics of\nclothes' reflectance and cameras' spectral sensitivity in the NIR range. We\nfurther show that the nearly co-located configuration of illuminants and\ncameras in existing surveillance systems facilitates concealing and fully\npassive attacks in the physical world. Specifically, we demonstrate how\nretro-reflective and insulation plastic tapes can manipulate the intensity\ndistribution of NIR images. We showcase an attack on the YOLO-based human\ndetector using binary patterns designed in the digital space (via black-box\nquery and searching) and then physically realized using tapes pasted onto\nclothes. Our attack highlights significant reliability concerns for nighttime\nsurveillance systems, which are intended to enhance security. Codes Available:\nhttps://github.com/MyNiuuu/AdvNIR\n","authors":["Muyao Niu","Zhuoxiao Li","Yifan Zhan","Huy H. Nguyen","Isao Echizen","Yinqiang Zheng"],"pdf_url":"https://arxiv.org/pdf/2412.13709v1.pdf","comment":"Appeared in ACM MM 2023"},{"id":"http://arxiv.org/abs/2412.13708v1","updated":"2024-12-18T10:51:31Z","published":"2024-12-18T10:51:31Z","title":"JoVALE: Detecting Human Actions in Video Using Audiovisual and Language\n  Contexts","summary":"  Video Action Detection (VAD) involves localizing and categorizing action\ninstances in videos. Videos inherently contain various information sources,\nincluding audio, visual cues, and surrounding scene contexts. Effectively\nleveraging this multi-modal information for VAD is challenging, as the model\nmust accurately focus on action-relevant cues. In this study, we introduce a\nnovel multi-modal VAD architecture called the Joint Actor-centric Visual,\nAudio, Language Encoder (JoVALE). JoVALE is the first VAD method to integrate\naudio and visual features with scene descriptive context derived from large\nimage captioning models. The core principle of JoVALE is the actor-centric\naggregation of audio, visual, and scene descriptive contexts, where\naction-related cues from each modality are identified and adaptively combined.\nWe propose a specialized module called the Actor-centric Multi-modal Fusion\nNetwork, designed to capture the joint interactions among actors and\nmulti-modal contexts through Transformer architecture. Our evaluation conducted\non three popular VAD benchmarks, AVA, UCF101-24, and JHMDB51-21, demonstrates\nthat incorporating multi-modal information leads to significant performance\ngains. JoVALE achieves state-of-the-art performances. The code will be\navailable at \\texttt{https://github.com/taeiin/AAAI2025-JoVALE}.\n","authors":["Taein Son","Soo Won Seo","Jisong Kim","Seok Hwan Lee","Jun Won Choi"],"pdf_url":"https://arxiv.org/pdf/2412.13708v1.pdf","comment":"Accepted to AAAI Conference on Artificial Intelligence 2025, 9 pages,\n  5 figures"},{"id":"http://arxiv.org/abs/2412.13705v1","updated":"2024-12-18T10:49:41Z","published":"2024-12-18T10:49:41Z","title":"Mitigating Adversarial Attacks in LLMs through Defensive Suffix\n  Generation","summary":"  Large language models (LLMs) have exhibited outstanding performance in\nnatural language processing tasks. However, these models remain susceptible to\nadversarial attacks in which slight input perturbations can lead to harmful or\nmisleading outputs. A gradient-based defensive suffix generation algorithm is\ndesigned to bolster the robustness of LLMs. By appending carefully optimized\ndefensive suffixes to input prompts, the algorithm mitigates adversarial\ninfluences while preserving the models' utility. To enhance adversarial\nunderstanding, a novel total loss function ($L_{\\text{total}}$) combining\ndefensive loss ($L_{\\text{def}}$) and adversarial loss ($L_{\\text{adv}}$)\ngenerates defensive suffixes more effectively. Experimental evaluations\nconducted on open-source LLMs such as Gemma-7B, mistral-7B, Llama2-7B, and\nLlama2-13B show that the proposed method reduces attack success rates (ASR) by\nan average of 11\\% compared to models without defensive suffixes. Additionally,\nthe perplexity score of Gemma-7B decreased from 6.57 to 3.93 when applying the\ndefensive suffix generated by openELM-270M. Furthermore, TruthfulQA evaluations\ndemonstrate consistent improvements with Truthfulness scores increasing by up\nto 10\\% across tested configurations. This approach significantly enhances the\nsecurity of LLMs in critical applications without requiring extensive\nretraining.\n","authors":["Minkyoung Kim","Yunha Kim","Hyeram Seo","Heejung Choi","Jiye Han","Gaeun Kee","Soyoung Ko","HyoJe Jung","Byeolhee Kim","Young-Hak Kim","Sanghyun Park","Tae Joon Jun"],"pdf_url":"https://arxiv.org/pdf/2412.13705v1.pdf","comment":"9 pages, 2 figures"},{"id":"http://arxiv.org/abs/2412.13703v1","updated":"2024-12-18T10:46:04Z","published":"2024-12-18T10:46:04Z","title":"MBInception: A new Multi-Block Inception Model for Enhancing Image\n  Processing Efficiency","summary":"  Deep learning models, specifically convolutional neural networks, have\ntransformed the landscape of image classification by autonomously extracting\nfeatures directly from raw pixel data. This article introduces an innovative\nimage classification model that employs three consecutive inception blocks\nwithin a convolutional neural networks framework, providing a comprehensive\ncomparative analysis with well-established architectures such as Visual\nGeometry Group, Residual Network, and MobileNet. Through the utilization of\nbenchmark datasets, including Canadian Institute for Advanced Researc, Modified\nNational Institute of Standards and Technology database, and Fashion Modified\nNational Institute of Standards and Technology database, we assess the\nperformance of our proposed model in comparison to these benchmarks. The\noutcomes reveal that our novel model consistently outperforms its counterparts\nacross diverse datasets, underscoring its effectiveness and potential for\nadvancing the current state-of-the-art in image classification. Evaluation\nmetrics further emphasize that the proposed model surpasses the other compared\narchitectures, thereby enhancing the efficiency of image classification on\nstandard datasets.\n","authors":["Fatemeh Froughirad","Reza Bakhoda Eshtivani","Hamed Khajavi","Amir Rastgoo"],"pdf_url":"https://arxiv.org/pdf/2412.13703v1.pdf","comment":"26 pages, 10 figures"},{"id":"http://arxiv.org/abs/2406.02347v3","updated":"2024-12-18T10:45:06Z","published":"2024-06-04T14:23:27Z","title":"Flash Diffusion: Accelerating Any Conditional Diffusion Model for Few\n  Steps Image Generation","summary":"  In this paper, we propose an efficient, fast, and versatile distillation\nmethod to accelerate the generation of pre-trained diffusion models: Flash\nDiffusion. The method reaches state-of-the-art performances in terms of FID and\nCLIP-Score for few steps image generation on the COCO2014 and COCO2017\ndatasets, while requiring only several GPU hours of training and fewer\ntrainable parameters than existing methods. In addition to its efficiency, the\nversatility of the method is also exposed across several tasks such as\ntext-to-image, inpainting, face-swapping, super-resolution and using different\nbackbones such as UNet-based denoisers (SD1.5, SDXL) or DiT (Pixart-$\\alpha$),\nas well as adapters. In all cases, the method allowed to reduce drastically the\nnumber of sampling steps while maintaining very high-quality image generation.\nThe official implementation is available at\nhttps://github.com/gojasper/flash-diffusion.\n","authors":["Clément Chadebec","Onur Tasar","Eyal Benaroche","Benjamin Aubin"],"pdf_url":"https://arxiv.org/pdf/2406.02347v3.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2412.13695v1","updated":"2024-12-18T10:36:46Z","published":"2024-12-18T10:36:46Z","title":"Optical aberrations in autonomous driving: Physics-informed\n  parameterized temperature scaling for neural network uncertainty calibration","summary":"  'A trustworthy representation of uncertainty is desirable and should be\nconsidered as a key feature of any machine learning method' (Huellermeier and\nWaegeman, 2021). This conclusion of Huellermeier et al. underpins the\nimportance of calibrated uncertainties. Since AI-based algorithms are heavily\nimpacted by dataset shifts, the automotive industry needs to safeguard its\nsystem against all possible contingencies. One important but often neglected\ndataset shift is caused by optical aberrations induced by the windshield. For\nthe verification of the perception system performance, requirements on the AI\nperformance need to be translated into optical metrics by a bijective mapping\n(Braun, 2023). Given this bijective mapping it is evident that the optical\nsystem characteristics add additional information about the magnitude of the\ndataset shift. As a consequence, we propose to incorporate a physical inductive\nbias into the neural network calibration architecture to enhance the robustness\nand the trustworthiness of the AI target application, which we demonstrate by\nusing a semantic segmentation task as an example. By utilizing the Zernike\ncoefficient vector of the optical system as a physical prior we can\nsignificantly reduce the mean expected calibration error in case of optical\naberrations. As a result, we pave the way for a trustworthy uncertainty\nrepresentation and for a holistic verification strategy of the perception\nchain.\n","authors":["Dominik Werner Wolf","Alexander Braun","Markus Ulrich"],"pdf_url":"https://arxiv.org/pdf/2412.13695v1.pdf","comment":"Under review at the International Journal of Computer Vision (IJCV)"},{"id":"http://arxiv.org/abs/2410.23318v2","updated":"2024-12-18T10:32:31Z","published":"2024-10-29T21:38:54Z","title":"Denoising Diffusion Probabilistic Models for Magnetic Resonance\n  Fingerprinting","summary":"  Magnetic Resonance Fingerprinting (MRF) is a time-efficient approach to\nquantitative MRI, enabling the mapping of multiple tissue properties from a\nsingle, accelerated scan. However, achieving accurate reconstructions remains\nchallenging, particularly in highly accelerated and undersampled acquisitions,\nwhich are crucial for reducing scan times. While deep learning techniques have\nadvanced image reconstruction, the recent introduction of diffusion models\noffers new possibilities for imaging tasks, though their application in the\nmedical field is still emerging. Notably, diffusion models have not yet been\nexplored for the MRF problem. In this work, we propose for the first time a\nconditional diffusion probabilistic model for MRF image reconstruction.\nQualitative and quantitative comparisons on in-vivo brain scan data demonstrate\nthat the proposed approach can outperform established deep learning and\ncompressed sensing algorithms for MRF reconstruction. Extensive ablation\nstudies also explore strategies to improve computational efficiency of our\napproach.\n","authors":["Perla Mayo","Carolin M. Pirkl","Alin Achim","Bjoern H. Menze","Mohammad Golbabaee"],"pdf_url":"https://arxiv.org/pdf/2410.23318v2.pdf","comment":"13 pages, 5 figures, 3 tables, 2 algorithms"},{"id":"http://arxiv.org/abs/2412.13684v1","updated":"2024-12-18T10:19:12Z","published":"2024-12-18T10:19:12Z","title":"MMO-IG: Multi-Class and Multi-Scale Object Image Generation for Remote\n  Sensing","summary":"  The rapid advancement of deep generative models (DGMs) has significantly\nadvanced research in computer vision, providing a cost-effective alternative to\nacquiring vast quantities of expensive imagery. However, existing methods\npredominantly focus on synthesizing remote sensing (RS) images aligned with\nreal images in a global layout view, which limits their applicability in RS\nimage object detection (RSIOD) research. To address these challenges, we\npropose a multi-class and multi-scale object image generator based on DGMs,\ntermed MMO-IG, designed to generate RS images with supervised object labels\nfrom global and local aspects simultaneously. Specifically, from the local\nview, MMO-IG encodes various RS instances using an iso-spacing instance map\n(ISIM). During the generation process, it decodes each instance region with\niso-spacing value in ISIM-corresponding to both background and foreground\ninstances-to produce RS images through the denoising process of diffusion\nmodels. Considering the complex interdependencies among MMOs, we construct a\nspatial-cross dependency knowledge graph (SCDKG). This ensures a realistic and\nreliable multidirectional distribution among MMOs for region embedding, thereby\nreducing the discrepancy between source and target domains. Besides, we propose\na structured object distribution instruction (SODI) to guide the generation of\nsynthesized RS image content from a global aspect with SCDKG-based ISIM\ntogether. Extensive experimental results demonstrate that our MMO-IG exhibits\nsuperior generation capabilities for RS images with dense MMO-supervised\nlabels, and RS detectors pre-trained with MMO-IG show excellent performance on\nreal-world datasets.\n","authors":["Chuang Yang","Bingxuan Zhao","Qing Zhou","Qi Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13684v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.14164v4","updated":"2024-12-18T10:16:59Z","published":"2022-10-19T21:52:01Z","title":"Understanding Key Point Cloud Features for Development Three-dimensional\n  Adversarial Attacks","summary":"  Adversarial attacks pose serious challenges for deep neural network\n(DNN)-based analysis of various input signals. In the case of three-dimensional\npoint clouds, methods have been developed to identify points that play a key\nrole in network decision, and these become crucial in generating existing\nadversarial attacks. For example, a saliency map approach is a popular method\nfor identifying adversarial drop points, whose removal would significantly\nimpact the network decision. This paper seeks to enhance the understanding of\nthree-dimensional adversarial attacks by exploring which point cloud features\nare most important for predicting adversarial points. Specifically, Fourteen\nkey point cloud features such as edge intensity and distance from the centroid\nare defined, and multiple linear regression is employed to assess their\npredictive power for adversarial points. Based on critical feature selection\ninsights, a new attack method has been developed to evaluate whether the\nselected features can generate an attack successfully. Unlike traditional\nattack methods that rely on model-specific vulnerabilities, this approach\nfocuses on the intrinsic characteristics of the point clouds themselves. It is\ndemonstrated that these features can predict adversarial points across four\ndifferent DNN architectures, Point Network (PointNet), PointNet++, Dynamic\nGraph Convolutional Neural Networks (DGCNN), and Point Convolutional Network\n(PointConv) outperforming random guessing and achieving results comparable to\nsaliency map-based attacks. This study has important engineering applications,\nsuch as enhancing the security and robustness of three-dimensional point\ncloud-based systems in fields like robotics and autonomous driving.\n","authors":["Hanieh Naderi","Chinthaka Dinesh","Ivan V. Bajic","Shohreh Kasaei"],"pdf_url":"https://arxiv.org/pdf/2210.14164v4.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.07616v2","updated":"2024-12-18T10:02:35Z","published":"2024-12-10T15:54:53Z","title":"PVP: Polar Representation Boost for 3D Semantic Occupancy Prediction","summary":"  Recently, polar coordinate-based representations have shown promise for 3D\nperceptual tasks. Compared to Cartesian methods, polar grids provide a viable\nalternative, offering better detail preservation in nearby spaces while\ncovering larger areas. However, they face feature distortion due to non-uniform\ndivision. To address these issues, we introduce the Polar Voxel Occupancy\nPredictor (PVP), a novel 3D multi-modal predictor that operates in polar\ncoordinates. PVP features two key design elements to overcome distortion: a\nGlobal Represent Propagation (GRP) module that integrates global spatial data\ninto 3D volumes, and a Plane Decomposed Convolution (PD-Conv) that simplifies\n3D distortions into 2D convolutions. These innovations enable PVP to outperform\nexisting methods, achieving significant improvements in mIoU and IoU metrics on\nthe OpenOccupancy dataset.\n","authors":["Yujing Xue","Jiaxiang Liu","Jiawei Du","Joey Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.07616v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12628v2","updated":"2024-12-18T09:58:32Z","published":"2024-12-17T07:43:36Z","title":"Dense Audio-Visual Event Localization under Cross-Modal Consistency and\n  Multi-Temporal Granularity Collaboration","summary":"  In the field of audio-visual learning, most research tasks focus exclusively\non short videos. This paper focuses on the more practical Dense Audio-Visual\nEvent Localization (DAVEL) task, advancing audio-visual scene understanding for\nlonger, untrimmed videos. This task seeks to identify and temporally pinpoint\nall events simultaneously occurring in both audio and visual streams.\nTypically, each video encompasses dense events of multiple classes, which may\noverlap on the timeline, each exhibiting varied durations. Given these\nchallenges, effectively exploiting the audio-visual relations and the temporal\nfeatures encoded at various granularities becomes crucial. To address these\nchallenges, we introduce a novel CCNet, comprising two core modules: the\nCross-Modal Consistency Collaboration (CMCC) and the Multi-Temporal Granularity\nCollaboration (MTGC). Specifically, the CMCC module contains two branches: a\ncross-modal interaction branch and a temporal consistency-gated branch. The\nformer branch facilitates the aggregation of consistent event semantics across\nmodalities through the encoding of audio-visual relations, while the latter\nbranch guides one modality's focus to pivotal event-relevant temporal areas as\ndiscerned in the other modality. The MTGC module includes a coarse-to-fine\ncollaboration block and a fine-to-coarse collaboration block, providing\nbidirectional support among coarse- and fine-grained temporal features.\nExtensive experiments on the UnAV-100 dataset validate our module design,\nresulting in a new state-of-the-art performance in dense audio-visual event\nlocalization. The code is available at\nhttps://github.com/zzhhfut/CCNet-AAAI2025.\n","authors":["Ziheng Zhou","Jinxing Zhou","Wei Qian","Shengeng Tang","Xiaojun Chang","Dan Guo"],"pdf_url":"https://arxiv.org/pdf/2412.12628v2.pdf","comment":"Accepted by AAAI 2025. Project page:\n  https://github.com/zzhhfut/CCNet-AAAI2025. Jinxing Zhou and Dan Guo are the\n  corresponding authors"},{"id":"http://arxiv.org/abs/2408.11481v2","updated":"2024-12-18T09:55:40Z","published":"2024-08-21T09:49:32Z","title":"VE-Bench: Subjective-Aligned Benchmark Suite for Text-Driven Video\n  Editing Quality Assessment","summary":"  Text-driven video editing has recently experienced rapid development. Despite\nthis, evaluating edited videos remains a considerable challenge. Current\nmetrics tend to fail to align with human perceptions, and effective\nquantitative metrics for video editing are still notably absent. To address\nthis, we introduce VE-Bench, a benchmark suite tailored to the assessment of\ntext-driven video editing. This suite includes VE-Bench DB, a video quality\nassessment (VQA) database for video editing. VE-Bench DB encompasses a diverse\nset of source videos featuring various motions and subjects, along with\nmultiple distinct editing prompts, editing results from 8 different models, and\nthe corresponding Mean Opinion Scores (MOS) from 24 human annotators. Based on\nVE-Bench DB, we further propose VE-Bench QA, a quantitative human-aligned\nmeasurement for the text-driven video editing task. In addition to the\naesthetic, distortion, and other visual quality indicators that traditional VQA\nmethods emphasize, VE-Bench QA focuses on the text-video alignment and the\nrelevance modeling between source and edited videos. It proposes a new\nassessment network for video editing that attains superior performance in\nalignment with human preferences. To the best of our knowledge, VE-Bench\nintroduces the first quality assessment dataset for video editing and an\neffective subjective-aligned quantitative metric for this domain. All data and\ncode will be publicly available at https://github.com/littlespray/VE-Bench.\n","authors":["Shangkun Sun","Xiaoyu Liang","Songlin Fan","Wenxu Gao","Wei Gao"],"pdf_url":"https://arxiv.org/pdf/2408.11481v2.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2409.17920v2","updated":"2024-12-18T09:55:15Z","published":"2024-09-26T15:04:13Z","title":"Resolving Multi-Condition Confusion for Finetuning-Free Personalized\n  Image Generation","summary":"  Personalized text-to-image generation methods can generate customized images\nbased on the reference images, which have garnered wide research interest.\nRecent methods propose a finetuning-free approach with a decoupled\ncross-attention mechanism to generate personalized images requiring no\ntest-time finetuning. However, when multiple reference images are provided, the\ncurrent decoupled cross-attention mechanism encounters the object confusion\nproblem and fails to map each reference image to its corresponding object,\nthereby seriously limiting its scope of application. To address the object\nconfusion problem, in this work we investigate the relevance of different\npositions of the latent image features to the target object in diffusion model,\nand accordingly propose a weighted-merge method to merge multiple reference\nimage features into the corresponding objects. Next, we integrate this\nweighted-merge method into existing pre-trained models and continue to train\nthe model on a multi-object dataset constructed from the open-sourced SA-1B\ndataset. To mitigate object confusion and reduce training costs, we propose an\nobject quality score to estimate the image quality for the selection of\nhigh-quality training samples. Furthermore, our weighted-merge training\nframework can be employed on single-object generation when a single object has\nmultiple reference images. The experiments verify that our method achieves\nsuperior performance to the state-of-the-arts on the Concept101 dataset and\nDreamBooth dataset of multi-object personalized image generation, and\nremarkably improves the performance on single-object personalized image\ngeneration. Our code is available at https://github.com/hqhQAQ/MIP-Adapter.\n","authors":["Qihan Huang","Siming Fu","Jinlong Liu","Hao Jiang","Yipeng Yu","Jie Song"],"pdf_url":"https://arxiv.org/pdf/2409.17920v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15024v2","updated":"2024-12-18T09:47:25Z","published":"2024-11-22T15:55:19Z","title":"DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models","summary":"  Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.\n","authors":["Keda Tao","Can Qin","Haoxuan You","Yang Sui","Huan Wang"],"pdf_url":"https://arxiv.org/pdf/2411.15024v2.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.13168v2","updated":"2024-12-18T09:47:15Z","published":"2024-12-17T18:45:53Z","title":"Lifting Scheme-Based Implicit Disentanglement of Emotion-Related Facial\n  Dynamics in the Wild","summary":"  In-the-wild dynamic facial expression recognition (DFER) encounters a\nsignificant challenge in recognizing emotion-related expressions, which are\noften temporally and spatially diluted by emotion-irrelevant expressions and\nglobal context. Most prior DFER methods directly utilize coupled spatiotemporal\nrepresentations that may incorporate weakly relevant features with\nemotion-irrelevant context bias. Several DFER methods highlight dynamic\ninformation for DFER, but following explicit guidance that may be vulnerable to\nirrelevant motion. In this paper, we propose a novel Implicit Facial Dynamics\nDisentanglement framework (IFDD). Through expanding wavelet lifting scheme to\nfully learnable framework, IFDD disentangles emotion-related dynamic\ninformation from emotion-irrelevant global context in an implicit manner, i.e.,\nwithout exploit operations and external guidance. The disentanglement process\ncontains two stages. The first is Inter-frame Static-dynamic Splitting Module\n(ISSM) for rough disentanglement estimation, which explores inter-frame\ncorrelation to generate content-aware splitting indexes on-the-fly. We utilize\nthese indexes to split frame features into two groups, one with greater global\nsimilarity, and the other with more unique dynamic features. The second stage\nis Lifting-based Aggregation-Disentanglement Module (LADM) for further\nrefinement. LADM first aggregates two groups of features from ISSM to obtain\nfine-grained global context features by an updater, and then disentangles\nemotion-related facial dynamic features from the global context by a predictor.\nExtensive experiments on in-the-wild datasets have demonstrated that IFDD\noutperforms prior supervised DFER methods with higher recognition accuracy and\ncomparable efficiency. Code is available at\nhttps://github.com/CyberPegasus/IFDD.\n","authors":["Xingjian Wang","Li Chai"],"pdf_url":"https://arxiv.org/pdf/2412.13168v2.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2211.05781v3","updated":"2024-12-18T09:45:17Z","published":"2022-11-10T18:59:43Z","title":"Demystify Transformers & Convolutions in Modern Image Deep Networks","summary":"  Vision transformers have gained popularity recently, leading to the\ndevelopment of new vision backbones with improved features and consistent\nperformance gains. However, these advancements are not solely attributable to\nnovel feature transformation designs; certain benefits also arise from advanced\nnetwork-level and block-level architectures. This paper aims to identify the\nreal gains of popular convolution and attention operators through a detailed\nstudy. We find that the key difference among these feature transformation\nmodules, such as attention or convolution, lies in their spatial feature\naggregation approach, known as the \"spatial token mixer\" (STM). To facilitate\nan impartial comparison, we introduce a unified architecture to neutralize the\nimpact of divergent network-level and block-level designs. Subsequently,\nvarious STMs are integrated into this unified framework for comprehensive\ncomparative analysis. Our experiments on various tasks and an analysis of\ninductive bias show a significant performance boost due to advanced\nnetwork-level and block-level designs, but performance differences persist\namong different STMs. Our detailed analysis also reveals various findings about\ndifferent STMs, including effective receptive fields, invariance, and\nadversarial robustness tests.\n","authors":["Xiaowei Hu","Min Shi","Weiyun Wang","Sitong Wu","Linjie Xing","Wenhai Wang","Xizhou Zhu","Lewei Lu","Jie Zhou","Xiaogang Wang","Yu Qiao","Jifeng Dai"],"pdf_url":"https://arxiv.org/pdf/2211.05781v3.pdf","comment":"This paper was accepted to IEEE Transactions on Pattern Analysis and\n  Machine Intelligence (IEEE TPAMI). All models and codes used in this study\n  are publicly available at https://github.com/OpenGVLab/STM-Evaluation"},{"id":"http://arxiv.org/abs/2412.13662v1","updated":"2024-12-18T09:39:12Z","published":"2024-12-18T09:39:12Z","title":"When Should We Prefer State-to-Visual DAgger Over Visual Reinforcement\n  Learning?","summary":"  Learning policies from high-dimensional visual inputs, such as pixels and\npoint clouds, is crucial in various applications. Visual reinforcement learning\nis a promising approach that directly trains policies from visual observations,\nalthough it faces challenges in sample efficiency and computational costs. This\nstudy conducts an empirical comparison of State-to-Visual DAgger, a two-stage\nframework that initially trains a state policy before adopting online imitation\nto learn a visual policy, and Visual RL across a diverse set of tasks. We\nevaluate both methods across 16 tasks from three benchmarks, focusing on their\nasymptotic performance, sample efficiency, and computational costs.\nSurprisingly, our findings reveal that State-to-Visual DAgger does not\nuniversally outperform Visual RL but shows significant advantages in\nchallenging tasks, offering more consistent performance. In contrast, its\nbenefits in sample efficiency are less pronounced, although it often reduces\nthe overall wall-clock time required for training. Based on our findings, we\nprovide recommendations for practitioners and hope that our results contribute\nvaluable perspectives for future research in visual policy learning.\n","authors":["Tongzhou Mu","Zhaoyang Li","Stanisław Wiktor Strzelecki","Xiu Yuan","Yunchao Yao","Litian Liang","Hao Su"],"pdf_url":"https://arxiv.org/pdf/2412.13662v1.pdf","comment":"Accepted by The 39th Annual AAAI Conference on Artificial\n  Intelligence (AAAI 2025)"},{"id":"http://arxiv.org/abs/2412.08200v2","updated":"2024-12-18T09:36:47Z","published":"2024-12-11T08:43:52Z","title":"GN-FR:Generalizable Neural Radiance Fields for Flare Removal","summary":"  Flare, an optical phenomenon resulting from unwanted scattering and\nreflections within a lens system, presents a significant challenge in imaging.\nThe diverse patterns of flares, such as halos, streaks, color bleeding, and\nhaze, complicate the flare removal process. Existing traditional and\nlearning-based methods have exhibited limited efficacy due to their reliance on\nsingle-image approaches, where flare removal is highly ill-posed. We address\nthis by framing flare removal as a multi-view image problem, taking advantage\nof the view-dependent nature of flare artifacts. This approach leverages\ninformation from neighboring views to recover details obscured by flare in\nindividual images. Our proposed framework, GN-FR (Generalizable Neural Radiance\nFields for Flare Removal), can render flare-free views from a sparse set of\ninput images affected by lens flare and generalizes across different scenes in\nan unsupervised manner. GN-FR incorporates several modules within the\nGeneralizable NeRF Transformer (GNT) framework: Flare-occupancy Mask Generation\n(FMG), View Sampler (VS), and Point Sampler (PS). To overcome the\nimpracticality of capturing both flare-corrupted and flare-free data, we\nintroduce a masking loss function that utilizes mask information in an\nunsupervised setting. Additionally, we present a 3D multi-view flare dataset,\ncomprising 17 real flare scenes with 782 images, 80 real flare patterns, and\ntheir corresponding annotated flare-occupancy masks. To our knowledge, this is\nthe first work to address flare removal within a Neural Radiance Fields (NeRF)\nframework.\n","authors":["Gopi Raju Matta","Rahul Siddartha","Rongali Simhachala Venkata Girish","Sumit Sharma","Kaushik Mitra"],"pdf_url":"https://arxiv.org/pdf/2412.08200v2.pdf","comment":"Accepted for publication at BMVC-24"},{"id":"http://arxiv.org/abs/2412.13656v1","updated":"2024-12-18T09:34:59Z","published":"2024-12-18T09:34:59Z","title":"GLCF: A Global-Local Multimodal Coherence Analysis Framework for Talking\n  Face Generation Detection","summary":"  Talking face generation (TFG) allows for producing lifelike talking videos of\nany character using only facial images and accompanying text. Abuse of this\ntechnology could pose significant risks to society, creating the urgent need\nfor research into corresponding detection methods. However, research in this\nfield has been hindered by the lack of public datasets. In this paper, we\nconstruct the first large-scale multi-scenario talking face dataset (MSTF),\nwhich contains 22 audio and video forgery techniques, filling the gap of\ndatasets in this field. The dataset covers 11 generation scenarios and more\nthan 20 semantic scenarios, closer to the practical application scenario of\nTFG. Besides, we also propose a TFG detection framework, which leverages the\nanalysis of both global and local coherence in the multimodal content of TFG\nvideos. Therefore, a region-focused smoothness detection module (RSFDM) and a\ndiscrepancy capture-time frame aggregation module (DCTAM) are introduced to\nevaluate the global temporal coherence of TFG videos, aggregating multi-grained\nspatial information. Additionally, a visual-audio fusion module (V-AFM) is\ndesigned to evaluate audiovisual coherence within a localized temporal\nperspective. Comprehensive experiments demonstrate the reasonableness and\nchallenges of our datasets, while also indicating the superiority of our\nproposed method compared to the state-of-the-art deepfake detection approaches.\n","authors":["Xiaocan Chen","Qilin Yin","Jiarui Liu","Wei Lu","Xiangyang Luo","Jiantao Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.13656v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13655v1","updated":"2024-12-18T09:34:32Z","published":"2024-12-18T09:34:32Z","title":"VIIS: Visible and Infrared Information Synthesis for Severe Low-light\n  Image Enhancement","summary":"  Images captured in severe low-light circumstances often suffer from\nsignificant information absence. Existing singular modality image enhancement\nmethods struggle to restore image regions lacking valid information. By\nleveraging light-impervious infrared images, visible and infrared image fusion\nmethods have the potential to reveal information hidden in darkness. However,\nthey primarily emphasize inter-modal complementation but neglect intra-modal\nenhancement, limiting the perceptual quality of output images. To address these\nlimitations, we propose a novel task, dubbed visible and infrared information\nsynthesis (VIIS), which aims to achieve both information enhancement and fusion\nof the two modalities. Given the difficulty in obtaining ground truth in the\nVIIS task, we design an information synthesis pretext task (ISPT) based on\nimage augmentation. We employ a diffusion model as the framework and design a\nsparse attention-based dual-modalities residual (SADMR) conditioning mechanism\nto enhance information interaction between the two modalities. This mechanism\nenables features with prior knowledge from both modalities to adaptively and\niteratively attend to each modality's information during the denoising process.\nOur extensive experiments demonstrate that our model qualitatively and\nquantitatively outperforms not only the state-of-the-art methods in relevant\nfields but also the newly designed baselines capable of both information\nenhancement and fusion. The code is available at\nhttps://github.com/Chenz418/VIIS.\n","authors":["Chen Zhao","Mengyuan Yu","Fan Yang","Peiguang Jing"],"pdf_url":"https://arxiv.org/pdf/2412.13655v1.pdf","comment":"Accepted to WACV 2025"},{"id":"http://arxiv.org/abs/2306.09801v3","updated":"2024-12-18T09:34:18Z","published":"2023-06-16T12:22:19Z","title":"Semantics-Aware Next-best-view Planning for Efficient Search and\n  Detection of Task-relevant Plant Parts","summary":"  Searching and detecting the task-relevant parts of plants is important to\nautomate harvesting and de-leafing of tomato plants using robots. This is\nchallenging due to high levels of occlusion in tomato plants. Active vision is\na promising approach in which the robot strategically plans its camera\nviewpoints to overcome occlusion and improve perception accuracy. However,\ncurrent active-vision algorithms cannot differentiate between relevant and\nirrelevant plant parts and spend time on perceiving irrelevant plant parts.\nThis work proposed a semantics-aware active-vision strategy that uses semantic\ninformation to identify the relevant plant parts and prioritise them during\nview planning. The proposed strategy was evaluated on the task of searching and\ndetecting the relevant plant parts using simulation and real-world experiments.\nIn simulation experiments, the semantics-aware strategy proposed could search\nand detect 81.8% of the relevant plant parts using nine viewpoints. It was\nsignificantly faster and detected more plant parts than predefined, random, and\nvolumetric active-vision strategies that do not use semantic information. The\nstrategy proposed was also robust to uncertainty in plant and plant-part\npositions, plant complexity, and different viewpoint-sampling strategies. In\nreal-world experiments, the semantics-aware strategy could search and detect\n82.7% of the relevant plant parts using seven viewpoints, under complex\ngreenhouse conditions with natural variation and occlusion, natural\nillumination, sensor noise, and uncertainty in camera poses. The results of\nthis work clearly indicate the advantage of using semantics-aware active vision\nfor targeted perception of plant parts and its applicability in the real world.\nIt can significantly improve the efficiency of automated harvesting and\nde-leafing in tomato crop production.\n","authors":["Akshay K. Burusa","Joost Scholten","David Rapado Rincon","Xin Wang","Eldert J. van Henten","Gert Kootstra"],"pdf_url":"https://arxiv.org/pdf/2306.09801v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13654v1","updated":"2024-12-18T09:33:20Z","published":"2024-12-18T09:33:20Z","title":"GAGS: Granularity-Aware Feature Distillation for Language Gaussian\n  Splatting","summary":"  3D open-vocabulary scene understanding, which accurately perceives complex\nsemantic properties of objects in space, has gained significant attention in\nrecent years. In this paper, we propose GAGS, a framework that distills 2D CLIP\nfeatures into 3D Gaussian splatting, enabling open-vocabulary queries for\nrenderings on arbitrary viewpoints. The main challenge of distilling 2D\nfeatures for 3D fields lies in the multiview inconsistency of extracted 2D\nfeatures, which provides unstable supervision for the 3D feature field. GAGS\naddresses this challenge with two novel strategies. First, GAGS associates the\nprompt point density of SAM with the camera distances, which significantly\nimproves the multiview consistency of segmentation results. Second, GAGS\nfurther decodes a granularity factor to guide the distillation process and this\ngranularity factor can be learned in a unsupervised manner to only select the\nmultiview consistent 2D features in the distillation process. Experimental\nresults on two datasets demonstrate significant performance and stability\nimprovements of GAGS in visual grounding and semantic segmentation, with an\ninference speed 2$\\times$ faster than baseline methods. The code and additional\nresults are available at https://pz0826.github.io/GAGS-Webpage/ .\n","authors":["Yuning Peng","Haiping Wang","Yuan Liu","Chenglu Wen","Zhen Dong","Bisheng Yang"],"pdf_url":"https://arxiv.org/pdf/2412.13654v1.pdf","comment":"Project page: https://pz0826.github.io/GAGS-Webpage/"},{"id":"http://arxiv.org/abs/2412.13652v1","updated":"2024-12-18T09:31:06Z","published":"2024-12-18T09:31:06Z","title":"RelationField: Relate Anything in Radiance Fields","summary":"  Neural radiance fields are an emerging 3D scene representation and recently\neven been extended to learn features for scene understanding by distilling\nopen-vocabulary features from vision-language models. However, current method\nprimarily focus on object-centric representations, supporting object\nsegmentation or detection, while understanding semantic relationships between\nobjects remains largely unexplored. To address this gap, we propose\nRelationField, the first method to extract inter-object relationships directly\nfrom neural radiance fields. RelationField represents relationships between\nobjects as pairs of rays within a neural radiance field, effectively extending\nits formulation to include implicit relationship queries. To teach\nRelationField complex, open-vocabulary relationships, relationship knowledge is\ndistilled from multi-modal LLMs. To evaluate RelationField, we solve\nopen-vocabulary 3D scene graph generation tasks and relationship-guided\ninstance segmentation, achieving state-of-the-art performance in both tasks.\nSee the project website at https://relationfield.github.io.\n","authors":["Sebastian Koch","Johanna Wald","Mirco Colosi","Narunas Vaskevicius","Pedro Hermosilla","Federico Tombari","Timo Ropinski"],"pdf_url":"https://arxiv.org/pdf/2412.13652v1.pdf","comment":"Project page: https://relationfield.github.io"},{"id":"http://arxiv.org/abs/2407.19323v4","updated":"2024-12-18T09:28:37Z","published":"2024-07-27T19:00:44Z","title":"MSP-MVS: Multi-Granularity Segmentation Prior Guided Multi-View Stereo","summary":"  Recently, patch deformation-based methods have demonstrated significant\nstrength in multi-view stereo by adaptively expanding the reception field of\npatches to help reconstruct textureless areas. However, such methods mainly\nconcentrate on searching for pixels without matching ambiguity (i.e., reliable\npixels) when constructing deformed patches, while neglecting the deformation\ninstability caused by unexpected edge-skipping, resulting in potential matching\ndistortions. Addressing this, we propose MSP-MVS, a method introducing\nmulti-granularity segmentation prior for edge-confined patch deformation.\nSpecifically, to avoid unexpected edge-skipping, we first aggregate and further\nrefine multi-granularity depth edges gained from Semantic-SAM as prior to guide\npatch deformation within depth-continuous (i.e., homogeneous) areas. Moreover,\nto address attention imbalance caused by edge-confined patch deformation, we\nimplement adaptive equidistribution and disassemble-clustering of correlative\nreliable pixels (i.e., anchors), thereby promoting attention-consistent patch\ndeformation. Finally, to prevent deformed patches from falling into\nlocal-minimum matching costs caused by the fixed sampling pattern, we introduce\ndisparity-sampling synergistic 3D optimization to help identify global-minimum\nmatching costs. Evaluations on ETH3D and Tanks & Temples benchmarks prove our\nmethod obtains state-of-the-art performance with remarkable generalization.\n","authors":["Zhenlong Yuan","Cong Liu","Fei Shen","Zhaoxin Li","Jinguo Luo","Tianlu Mao","Zhaoqi Wang"],"pdf_url":"https://arxiv.org/pdf/2407.19323v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13647v1","updated":"2024-12-18T09:23:12Z","published":"2024-12-18T09:23:12Z","title":"G-VEval: A Versatile Metric for Evaluating Image and Video Captions\n  Using GPT-4o","summary":"  Evaluation metric of visual captioning is important yet not thoroughly\nexplored. Traditional metrics like BLEU, METEOR, CIDEr, and ROUGE often miss\nsemantic depth, while trained metrics such as CLIP-Score, PAC-S, and Polos are\nlimited in zero-shot scenarios. Advanced Language Model-based metrics also\nstruggle with aligning to nuanced human preferences. To address these issues,\nwe introduce G-VEval, a novel metric inspired by G-Eval and powered by the new\nGPT-4o. G-VEval uses chain-of-thought reasoning in large multimodal models and\nsupports three modes: reference-free, reference-only, and combined,\naccommodating both video and image inputs. We also propose MSVD-Eval, a new\ndataset for video captioning evaluation, to establish a more transparent and\nconsistent framework for both human experts and evaluation metrics. It is\ndesigned to address the lack of clear criteria in existing datasets by\nintroducing distinct dimensions of Accuracy, Completeness, Conciseness, and\nRelevance (ACCR). Extensive results show that G-VEval outperforms existing\nmethods in correlation with human annotations, as measured by Kendall tau-b and\nKendall tau-c. This provides a flexible solution for diverse captioning tasks\nand suggests a straightforward yet effective approach for large language models\nto understand video content, paving the way for advancements in automated\ncaptioning. Codes are available at https://github.com/ztangaj/gveval\n","authors":["Tony Cheng Tong","Sirui He","Zhiwen Shao","Dit-Yan Yeung"],"pdf_url":"https://arxiv.org/pdf/2412.13647v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.09365v3","updated":"2024-12-18T09:11:06Z","published":"2024-05-15T14:17:44Z","title":"SARATR-X: Towards Building A Foundation Model for SAR Target Recognition","summary":"  Despite the remarkable progress in synthetic aperture radar automatic target\nrecognition (SAR ATR), recent efforts have concentrated on detecting and\nclassifying a specific category, e.g., vehicles, ships, airplanes, or\nbuildings. One of the fundamental limitations of the top-performing SAR ATR\nmethods is that the learning paradigm is supervised, task-specific,\nlimited-category, closed-world learning, which depends on massive amounts of\naccurately annotated samples that are expensively labeled by expert SAR\nanalysts and have limited generalization capability and scalability. In this\nwork, we make the first attempt towards building a foundation model for SAR\nATR, termed SARATR-X. SARATR-X learns generalizable representations via\nself-supervised learning (SSL) and provides a cornerstone for label-efficient\nmodel adaptation to generic SAR target detection and classification tasks.\nSpecifically, SARATR-X is trained on 0.18 M unlabelled SAR target samples,\nwhich are curated by combining contemporary benchmarks and constitute the\nlargest publicly available dataset till now. Considering the characteristics of\nSAR images, a backbone tailored for SAR ATR is carefully designed, and a\ntwo-step SSL method endowed with multi-scale gradient features was applied to\nensure the feature diversity and model scalability of SARATR-X. The\ncapabilities of SARATR-X are evaluated on classification under few-shot and\nrobustness settings and detection across various categories and scenes, and\nimpressive performance is achieved, often competitive with or even superior to\nprior fully supervised, semi-supervised, or self-supervised algorithms. Our\nSARATR-X and the curated dataset are released at\nhttps://github.com/waterdisappear/SARATR-X to foster research into foundation\nmodels for SAR image interpretation.\n","authors":["Weijie Li","Wei Yang","Yuenan Hou","Li Liu","Yongxiang Liu","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2405.09365v3.pdf","comment":"20 pages, 9 figures"},{"id":"http://arxiv.org/abs/2412.13636v1","updated":"2024-12-18T09:09:41Z","published":"2024-12-18T09:09:41Z","title":"Consistency of Compositional Generalization across Multiple Levels","summary":"  Compositional generalization is the capability of a model to understand novel\ncompositions composed of seen concepts. There are multiple levels of novel\ncompositions including phrase-phrase level, phrase-word level, and word-word\nlevel. Existing methods achieve promising compositional generalization, but the\nconsistency of compositional generalization across multiple levels of novel\ncompositions remains unexplored. The consistency refers to that a model should\ngeneralize to a phrase-phrase level novel composition, and\nphrase-word/word-word level novel compositions that can be derived from it\nsimultaneously. In this paper, we propose a meta-learning based framework, for\nachieving consistent compositional generalization across multiple levels. The\nbasic idea is to progressively learn compositions from simple to complex for\nconsistency. Specifically, we divide the original training set into multiple\nvalidation sets based on compositional complexity, and introduce multiple\nmeta-weight-nets to generate sample weights for samples in different validation\nsets. To fit the validation sets in order of increasing compositional\ncomplexity, we optimize the parameters of each meta-weight-net independently\nand sequentially in a multilevel optimization manner. We build a GQA-CCG\ndataset to quantitatively evaluate the consistency. Experimental results on\nvisual question answering and temporal video grounding, demonstrate the\neffectiveness of the proposed framework. We release GQA-CCG at\nhttps://github.com/NeverMoreLCH/CCG.\n","authors":["Chuanhao Li","Zhen Li","Chenchen Jing","Xiaomeng Fan","Wenbo Ye","Yuwei Wu","Yunde Jia"],"pdf_url":"https://arxiv.org/pdf/2412.13636v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.13635v1","updated":"2024-12-18T09:09:39Z","published":"2024-12-18T09:09:39Z","title":"Self-control: A Better Conditional Mechanism for Masked Autoregressive\n  Model","summary":"  Autoregressive conditional image generation algorithms are capable of\ngenerating photorealistic images that are consistent with given textual or\nimage conditions, and have great potential for a wide range of applications.\nNevertheless, the majority of popular autoregressive image generation methods\nrely heavily on vector quantization, and the inherent discrete characteristic\nof codebook presents a considerable challenge to achieving high-quality image\ngeneration. To address this limitation, this paper introduces a novel\nconditional introduction network for continuous masked autoregressive models.\nThe proposed self-control network serves to mitigate the negative impact of\nvector quantization on the quality of the generated images, while\nsimultaneously enhancing the conditional control during the generation process.\nIn particular, the self-control network is constructed upon a continuous mask\nautoregressive generative model, which incorporates multimodal conditional\ninformation, including text and images, into a unified autoregressive sequence\nin a serial manner. Through a self-attention mechanism, the network is capable\nof generating images that are controllable based on specific conditions. The\nself-control network discards the conventional cross-attention-based\nconditional fusion mechanism and effectively unifies the conditional and\ngenerative information within the same space, thereby facilitating more\nseamless learning and fusion of multimodal features.\n","authors":["Qiaoying Qu","Shiyu Shen"],"pdf_url":"https://arxiv.org/pdf/2412.13635v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10843v3","updated":"2024-12-18T08:54:03Z","published":"2024-08-19T11:42:54Z","title":"Detecting Wildfires on UAVs with Real-time Segmentation Trained by\n  Larger Teacher Models","summary":"  Early detection of wildfires is essential to prevent large-scale fires\nresulting in extensive environmental, structural, and societal damage. Uncrewed\naerial vehicles (UAVs) can cover large remote areas effectively with quick\ndeployment requiring minimal infrastructure and equipping them with small\ncameras and computers enables autonomous real-time detection. In remote areas,\nhowever, detection methods are limited to onboard computation due to the lack\nof high-bandwidth mobile networks. For accurate camera-based localisation,\nsegmentation of the detected smoke is essential but training data for deep\nlearning-based wildfire smoke segmentation is limited. This study shows how\nsmall specialised segmentation models can be trained using only bounding box\nlabels, leveraging zero-shot foundation model supervision. The method offers\nthe advantages of needing only fairly easily obtainable bounding box labels and\nrequiring training solely for the smaller student network. The proposed method\nachieved 63.3% mIoU on a manually annotated and diverse wildfire dataset. The\nused model can perform in real-time at ~25 fps with a UAV-carried NVIDIA Jetson\nOrin NX computer while reliably recognising smoke, as demonstrated at\nreal-world forest burning events. Code is available at:\nhttps://gitlab.com/fgi_nls/public/wildfire-real-time-segmentation\n","authors":["Julius Pesonen","Teemu Hakala","Väinö Karjalainen","Niko Koivumäki","Lauri Markelin","Anna-Maria Raita-Hakola","Juha Suomalainen","Ilkka Pölönen","Eija Honkavaara"],"pdf_url":"https://arxiv.org/pdf/2408.10843v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13615v1","updated":"2024-12-18T08:53:52Z","published":"2024-12-18T08:53:52Z","title":"MambaLCT: Boosting Tracking via Long-term Context State Space Model","summary":"  Effectively constructing context information with long-term dependencies from\nvideo sequences is crucial for object tracking. However, the context length\nconstructed by existing work is limited, only considering object information\nfrom adjacent frames or video clips, leading to insufficient utilization of\ncontextual information. To address this issue, we propose MambaLCT, which\nconstructs and utilizes target variation cues from the first frame to the\ncurrent frame for robust tracking. First, a novel unidirectional Context Mamba\nmodule is designed to scan frame features along the temporal dimension,\ngathering target change cues throughout the entire sequence. Specifically,\ntarget-related information in frame features is compressed into a hidden state\nspace through selective scanning mechanism. The target information across the\nentire video is continuously aggregated into target variation cues. Next, we\ninject the target change cues into the attention mechanism, providing temporal\ninformation for modeling the relationship between the template and search\nframes. The advantage of MambaLCT is its ability to continuously extend the\nlength of the context, capturing complete target change cues, which enhances\nthe stability and robustness of the tracker. Extensive experiments show that\nlong-term context information enhances the model's ability to perceive targets\nin complex scenarios. MambaLCT achieves new SOTA performance on six benchmarks\nwhile maintaining real-time running speeds.\n","authors":["Xiaohai Li","Bineng Zhong","Qihua Liang","Guorong Li","Zhiyi Mo","Shuxiang Song"],"pdf_url":"https://arxiv.org/pdf/2412.13615v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01988v2","updated":"2024-12-18T08:51:39Z","published":"2024-11-04T11:20:17Z","title":"QCS:Feature Refining from Quadruplet Cross Similarity for Facial\n  Expression Recognition","summary":"  Facial expression recognition faces challenges where labeled significant\nfeatures in datasets are mixed with unlabeled redundant ones. In this paper, we\nintroduce Cross Similarity Attention (CSA) to mine richer intrinsic information\nfrom image pairs, overcoming a limitation when the Scaled Dot-Product Attention\nof ViT is directly applied to calculate the similarity between two different\nimages. Based on CSA, we simultaneously minimize intra-class differences and\nmaximize inter-class differences at the fine-grained feature level through\ninteractions among multiple branches. Contrastive residual distillation is\nutilized to transfer the information learned in the cross module back to the\nbase network. We ingeniously design a four-branch centrally symmetric network,\nnamed Quadruplet Cross Similarity (QCS), which alleviates gradient conflicts\narising from the cross module and achieves balanced and stable training. It can\nadaptively extract discriminative features while isolating redundant ones. The\ncross-attention modules exist during training, and only one base branch is\nretained during inference, resulting in no increase in inference time. Our\nproposed method achieves state-of-the-art performance on several FER datasets.\n","authors":["Chengpeng Wang","Li Chen","Lili Wang","Zhaofan Li","Xuebin Lv"],"pdf_url":"https://arxiv.org/pdf/2411.01988v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11223v2","updated":"2024-12-18T08:49:16Z","published":"2024-11-18T01:25:58Z","title":"Efficient Transfer Learning for Video-language Foundation Models","summary":"  Pre-trained vision-language models provide a robust foundation for efficient\ntransfer learning across various downstream tasks. In the field of video action\nrecognition, mainstream approaches often introduce additional parameter modules\nto capture temporal information. While the increased model capacity brought by\nthese additional parameters helps better fit the video-specific inductive\nbiases, existing methods require learning a large number of parameters and are\nprone to catastrophic forgetting of the original generalizable knowledge. In\nthis paper, we propose a simple yet effective Multi-modal Spatio-Temporal\nAdapter (MSTA) to improve the alignment between representations in the text and\nvision branches, achieving a balance between general knowledge and\ntask-specific knowledge. Furthermore, to mitigate over-fitting and enhance\ngeneralizability, we introduce a spatio-temporal description-guided consistency\nconstraint. This constraint involves feeding template inputs (i.e., ``a video\nof $\\{\\textbf{cls}\\}$'') into the trainable language branch, while\nLLM-generated spatio-temporal descriptions are input into the pre-trained\nlanguage branch, enforcing consistency between the outputs of the two branches.\nThis mechanism prevents over-fitting to downstream tasks and improves the\ndistinguishability of the trainable branch within the spatio-temporal semantic\nspace. We evaluate the effectiveness of our approach across four tasks:\nzero-shot transfer, few-shot learning, base-to-novel generalization, and\nfully-supervised learning. Compared to many state-of-the-art methods, our MSTA\nachieves outstanding performance across all evaluations, while using only 2-7\\%\nof the trainable parameters in the original model. Code will be avaliable at\nhttps://github.com/chenhaoxing/ETL4Video.\n","authors":["Haoxing Chen","Zizheng Huang","Yan Hong","Yanshuo Wang","Zhongcai Lyu","Zhuoer Xu","Jun Lan","Zhangxuan Gu"],"pdf_url":"https://arxiv.org/pdf/2411.11223v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13614v1","updated":"2024-12-18T08:49:01Z","published":"2024-12-18T08:49:01Z","title":"Reverse Region-to-Entity Annotation for Pixel-Level Visual Entity\n  Linking","summary":"  Visual Entity Linking (VEL) is a crucial task for achieving fine-grained\nvisual understanding, matching objects within images (visual mentions) to\nentities in a knowledge base. Previous VEL tasks rely on textual inputs, but\nwriting queries for complex scenes can be challenging. Visual inputs like\nclicks or bounding boxes offer a more convenient alternative. Therefore, we\npropose a new task, Pixel-Level Visual Entity Linking (PL-VEL), which uses\npixel masks from visual inputs to refer to objects, supplementing reference\nmethods for VEL. To facilitate research on this task, we have constructed the\nMaskOVEN-Wiki dataset through an entirely automatic reverse region-entity\nannotation framework. This dataset contains over 5 million annotations aligning\npixel-level regions with entity-level labels, which will advance visual\nunderstanding towards fine-grained. Moreover, as pixel masks correspond to\nsemantic regions in an image, we enhance previous patch-interacted attention\nwith region-interacted attention by a visual semantic tokenization approach.\nManual evaluation results indicate that the reverse annotation framework\nachieved a 94.8% annotation success rate. Experimental results show that models\ntrained on this dataset improved accuracy by 18 points compared to zero-shot\nmodels. Additionally, the semantic tokenization method achieved a 5-point\naccuracy improvement over the trained baseline.\n","authors":["Zhengfei Xu","Sijia Zhao","Yanchao Hao","Xiaolong Liu","Lili Li","Yuyang Yin","Bo Li","Xi Chen","Xin Xin"],"pdf_url":"https://arxiv.org/pdf/2412.13614v1.pdf","comment":"AAAI 2025;Dataset are released at\n  https://github.com/NP-NET-research/PL-VEL"},{"id":"http://arxiv.org/abs/2405.16751v2","updated":"2024-12-18T08:38:06Z","published":"2024-05-27T01:47:14Z","title":"REVECA: Adaptive Planning and Trajectory-based Validation in Cooperative\n  Language Agents using Information Relevance and Relative Proximity","summary":"  We address the challenge of multi-agent cooperation, where agents achieve a\ncommon goal by cooperating with decentralized agents under complex partial\nobservations. Existing cooperative agent systems often struggle with\nefficiently processing continuously accumulating information, managing globally\nsuboptimal planning due to lack of consideration of collaborators, and\naddressing false planning caused by environmental changes introduced by other\ncollaborators. To overcome these challenges, we propose the RElevance,\nProximity, and Validation-Enhanced Cooperative Language Agent (REVECA), a novel\ncognitive architecture powered by GPT-4o-mini. REVECA enables efficient memory\nmanagement, optimal planning, and cost-effective prevention of false planning\nby leveraging Relevance Estimation, Adaptive Planning, and Trajectory-based\nValidation. Extensive experimental results demonstrate REVECA's superiority\nover existing methods across various benchmarks, while a user study reveals its\npotential for achieving trustworthy human-AI cooperation.\n","authors":["SeungWon Seo","SeongRae Noh","Junhyeok Lee","SooBin Lim","Won Hee Lee","HyeongYeop Kang"],"pdf_url":"https://arxiv.org/pdf/2405.16751v2.pdf","comment":"v2 is the AAAI'25 camera-ready version, including the appendix, which\n  has been enhanced based on the reviewers' comments"},{"id":"http://arxiv.org/abs/2412.13611v1","updated":"2024-12-18T08:37:22Z","published":"2024-12-18T08:37:22Z","title":"Robust Tracking via Mamba-based Context-aware Token Learning","summary":"  How to make a good trade-off between performance and computational cost is\ncrucial for a tracker. However, current famous methods typically focus on\ncomplicated and time-consuming learning that combining temporal and appearance\ninformation by input more and more images (or features). Consequently, these\nmethods not only increase the model's computational source and learning burden\nbut also introduce much useless and potentially interfering information. To\nalleviate the above issues, we propose a simple yet robust tracker that\nseparates temporal information learning from appearance modeling and extracts\ntemporal relations from a set of representative tokens rather than several\nimages (or features). Specifically, we introduce one track token for each frame\nto collect the target's appearance information in the backbone. Then, we design\na mamba-based Temporal Module for track tokens to be aware of context by\ninteracting with other track tokens within a sliding window. This module\nconsists of a mamba layer with autoregressive characteristic and a\ncross-attention layer with strong global perception ability, ensuring\nsufficient interaction for track tokens to perceive the appearance changes and\nmovement trends of the target. Finally, track tokens serve as a guidance to\nadjust the appearance feature for the final prediction in the head. Experiments\nshow our method is effective and achieves competitive performance on multiple\nbenchmarks at a real-time speed. Code and trained models will be available at\nhttps://github.com/GXNU-ZhongLab/TemTrack.\n","authors":["Jinxia Xie","Bineng Zhong","Qihua Liang","Ning Li","Zhiyi Mo","Shuxiang Song"],"pdf_url":"https://arxiv.org/pdf/2412.13611v1.pdf","comment":"AAAI2025"},{"id":"http://arxiv.org/abs/2412.13610v1","updated":"2024-12-18T08:37:13Z","published":"2024-12-18T08:37:13Z","title":"Faster and Stronger: When ANN-SNN Conversion Meets Parallel Spiking\n  Calculation","summary":"  Spiking Neural Network (SNN), as a brain-inspired and energy-efficient\nnetwork, is currently facing the pivotal challenge of exploring a suitable and\nefficient learning framework. The predominant training methodologies, namely\nSpatial-Temporal Back-propagation (STBP) and ANN-SNN Conversion, are encumbered\nby substantial training overhead or pronounced inference latency, which impedes\nthe advancement of SNNs in scaling to larger networks and navigating intricate\napplication domains. In this work, we propose a novel parallel conversion\nlearning framework, which establishes a mathematical mapping relationship\nbetween each time-step of the parallel spiking neurons and the cumulative spike\nfiring rate. We theoretically validate the lossless and sorting properties of\nthe conversion process, as well as pointing out the optimal shifting distance\nfor each step. Furthermore, by integrating the above framework with the\ndistribution-aware error calibration technique, we can achieve efficient\nconversion towards more general activation functions or training-free\ncircumstance. Extensive experiments have confirmed the significant performance\nadvantages of our method for various conversion cases under ultra-low time\nlatency. To our best knowledge, this is the first work which jointly utilizes\nparallel spiking calculation and ANN-SNN Conversion, providing a highly\npromising approach for SNN supervised training.\n","authors":["Zecheng Hao","Zhaofei Yu","Tiejun Huang"],"pdf_url":"https://arxiv.org/pdf/2412.13610v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13609v1","updated":"2024-12-18T08:36:35Z","published":"2024-12-18T08:36:35Z","title":"Sign-IDD: Iconicity Disentangled Diffusion for Sign Language Production","summary":"  Sign Language Production (SLP) aims to generate semantically consistent sign\nvideos from textual statements, where the conversion from textual glosses to\nsign poses (G2P) is a crucial step. Existing G2P methods typically treat sign\nposes as discrete three-dimensional coordinates and directly fit them, which\noverlooks the relative positional relationships among joints. To this end, we\nprovide a new perspective, constraining joint associations and gesture details\nby modeling the limb bones to improve the accuracy and naturalness of the\ngenerated poses. In this work, we propose a pioneering iconicity disentangled\ndiffusion framework, termed Sign-IDD, specifically designed for SLP. Sign-IDD\nincorporates a novel Iconicity Disentanglement (ID) module to bridge the gap\nbetween relative positions among joints. The ID module disentangles the\nconventional 3D joint representation into a 4D bone representation, comprising\nthe 3D spatial direction vector and 1D spatial distance vector between adjacent\njoints. Additionally, an Attribute Controllable Diffusion (ACD) module is\nintroduced to further constrain joint associations, in which the attribute\nseparation layer aims to separate the bone direction and length attributes, and\nthe attribute control layer is designed to guide the pose generation by\nleveraging the above attributes. The ACD module utilizes the gloss embeddings\nas semantic conditions and finally generates sign poses from noise embeddings.\nExtensive experiments on PHOENIX14T and USTC-CSL datasets validate the\neffectiveness of our method. The code is available at:\nhttps://github.com/NaVi-start/Sign-IDD.\n","authors":["Shengeng Tang","Jiayi He","Dan Guo","Yanyan Wei","Feng Li","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2412.13609v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2412.13601v1","updated":"2024-12-18T08:31:34Z","published":"2024-12-18T08:31:34Z","title":"Hybrid CNN-LSTM based Indoor Pedestrian Localization with CSI\n  Fingerprint Maps","summary":"  The paper presents a novel Wi-Fi fingerprinting system that uses Channel\nState Information (CSI) data for fine-grained pedestrian localization. The\nproposed system exploits the frequency diversity and spatial diversity of the\nfeatures extracted from CSI data to generate a 2D+channel image termed as a CSI\nFingerprint Map. We then use this CSI Fingerprint Map representation of CSI\ndata to generate a pedestrian trajectory hypothesis using a hybrid architecture\nthat combines a Convolutional Neural Network and a Long Short-Term Memory\nRecurrent Neural Network model. The proposed architecture exploits the temporal\nand spatial relationship information among the CSI data observations gathered\nat neighboring locations. A particle filter is then employed to separate out\nthe most likely hypothesis matching a human walk model. The experimental\nperformance of our method is compared to existing deep learning localization\nmethods such ConFi, DeepFi and to a self-developed temporal-feature based LSTM\nbased location classifier. The experimental results show marked improvement\nwith an average RMSE of 0.36 m in a moderately dynamic and 0.17 m in a static\nenvironment. Our method is essentially a proof of concept that with (1) sparse\navailability of observations, (2) limited infrastructure requirements, (3)\nmoderate level of short-term and long-term noise in the training and testing\nenvironment, reliable fine-grained Wi-Fi based pedestrian localization is a\npotential option.\n","authors":["Muhammad Emad-ud-din"],"pdf_url":"https://arxiv.org/pdf/2412.13601v1.pdf","comment":"12 pages, 14 figures and 3 tables"},{"id":"http://arxiv.org/abs/2412.13599v1","updated":"2024-12-18T08:31:26Z","published":"2024-12-18T08:31:26Z","title":"Unlocking the Potential of Weakly Labeled Data: A Co-Evolutionary\n  Learning Framework for Abnormality Detection and Report Generation","summary":"  Anatomical abnormality detection and report generation of chest X-ray (CXR)\nare two essential tasks in clinical practice. The former aims at localizing and\ncharacterizing cardiopulmonary radiological findings in CXRs, while the latter\nsummarizes the findings in a detailed report for further diagnosis and\ntreatment. Existing methods often focused on either task separately, ignoring\ntheir correlation. This work proposes a co-evolutionary abnormality detection\nand report generation (CoE-DG) framework. The framework utilizes both fully\nlabeled (with bounding box annotations and clinical reports) and weakly labeled\n(with reports only) data to achieve mutual promotion between the abnormality\ndetection and report generation tasks. Specifically, we introduce a\nbi-directional information interaction strategy with generator-guided\ninformation propagation (GIP) and detector-guided information propagation\n(DIP). For semi-supervised abnormality detection, GIP takes the informative\nfeature extracted by the generator as an auxiliary input to the detector and\nuses the generator's prediction to refine the detector's pseudo labels. We\nfurther propose an intra-image-modal self-adaptive non-maximum suppression\nmodule (SA-NMS). This module dynamically rectifies pseudo detection labels\ngenerated by the teacher detection model with high-confidence predictions by\nthe student.Inversely, for report generation, DIP takes the abnormalities'\ncategories and locations predicted by the detector as input and guidance for\nthe generator to improve the generated reports.\n","authors":["Jinghan Sun","Dong Wei","Zhe Xu","Donghuan Lu","Hong Liu","Hong Wang","Sotirios A. Tsaftaris","Steven McDonagh","Yefeng Zheng","Liansheng Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13599v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.04363v2","updated":"2024-12-18T08:30:59Z","published":"2024-04-05T19:16:30Z","title":"Idea23D: Collaborative LMM Agents Enable 3D Model Generation from\n  Interleaved Multimodal Inputs","summary":"  With the success of 2D diffusion models, 2D AIGC content has already\ntransformed our lives. Recently, this success has been extended to 3D AIGC,\nwith state-of-the-art methods generating textured 3D models from single images\nor text. However, we argue that current 3D AIGC methods still do not fully\nunleash human creativity. We often imagine 3D content made from multimodal\ninputs, such as what it would look like if my pet bunny were eating a doughnut\non the table. In this paper, we explore a novel 3D AIGC approach: generating 3D\ncontent from IDEAs. An IDEA is a multimodal input composed of text, image, and\n3D models. To our knowledge, this challenging and exciting 3D AIGC setting has\nnot been studied before. We propose the new framework Idea23D, which combines\nthree agents based on large multimodal models (LMMs) and existing algorithmic\ntools. These three LMM-based agents are tasked with prompt generation, model\nselection, and feedback reflection. They collaborate and critique each other in\na fully automated loop, without human intervention. The framework then\ngenerates a text prompt to create 3D models that align closely with the input\nIDEAs. We demonstrate impressive 3D AIGC results that surpass previous methods.\nTo comprehensively assess the 3D AIGC capabilities of Idea23D, we introduce the\nEval3DAIGC-198 dataset, containing 198 multimodal inputs for 3D generation\ntasks. This dataset evaluates the alignment between generated 3D content and\ninput IDEAs. Our user study and quantitative results show that Idea23D\nsignificantly improves the success rate and accuracy of 3D generation, with\nexcellent compatibility across various LMM, Text-to-Image, and Image-to-3D\nmodels. Code and dataset are available at \\url{https://idea23d.github.io/}.\n","authors":["Junhao Chen","Xiang Li","Xiaojun Ye","Chao Li","Zhaoxin Fan","Hao Zhao"],"pdf_url":"https://arxiv.org/pdf/2404.04363v2.pdf","comment":"Accepted by COLING 2025 (The 31st International Conference on\n  Computational Linguistics) Project Page: https://idea23d.github.io/ Code:\n  https://github.com/yisuanwang/Idea23D"},{"id":"http://arxiv.org/abs/2412.10824v2","updated":"2024-12-18T08:25:55Z","published":"2024-12-14T13:05:05Z","title":"Diffusion Model from Scratch","summary":"  Diffusion generative models are currently the most popular generative models.\nHowever, their underlying modeling process is quite complex, and starting\ndirectly with the seminal paper Denoising Diffusion Probability Model (DDPM)\ncan be challenging. This paper aims to assist readers in building a\nfoundational understanding of generative models by tracing the evolution from\nVAEs to DDPM through detailed mathematical derivations and a problem-oriented\nanalytical approach. It also explores the core ideas and improvement strategies\nof current mainstream methodologies, providing guidance for undergraduate and\ngraduate students interested in learning about diffusion models.\n","authors":["Wang Zhen","Dong Yunyun"],"pdf_url":"https://arxiv.org/pdf/2412.10824v2.pdf","comment":"There were problems with the typography of our illustrations, and\n  there were problems with the derivation of the 200-step formula"},{"id":"http://arxiv.org/abs/2412.13594v1","updated":"2024-12-18T08:18:03Z","published":"2024-12-18T08:18:03Z","title":"Generalizable Sensor-Based Activity Recognition via Categorical Concept\n  Invariant Learning","summary":"  Human Activity Recognition (HAR) aims to recognize activities by training\nmodels on massive sensor data. In real-world deployment, a crucial aspect of\nHAR that has been largely overlooked is that the test sets may have different\ndistributions from training sets due to inter-subject variability including\nage, gender, behavioral habits, etc., which leads to poor generalization\nperformance. One promising solution is to learn domain-invariant\nrepresentations to enable a model to generalize on an unseen distribution.\nHowever, most existing methods only consider the feature-invariance of the\npenultimate layer for domain-invariant learning, which leads to suboptimal\nresults. In this paper, we propose a Categorical Concept Invariant Learning\n(CCIL) framework for generalizable activity recognition, which introduces a\nconcept matrix to regularize the model in the training stage by simultaneously\nconcentrating on feature-invariance and logit-invariance. Our key idea is that\nthe concept matrix for samples belonging to the same activity category should\nbe similar. Extensive experiments on four public HAR benchmarks demonstrate\nthat our CCIL substantially outperforms the state-of-the-art approaches under\ncross-person, cross-dataset, cross-position, and one-person-to-another\nsettings.\n","authors":["Di Xiong","Shuoyuan Wang","Lei Zhang","Wenbo Huang","Chaolei Han"],"pdf_url":"https://arxiv.org/pdf/2412.13594v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.11077v2","updated":"2024-12-18T07:59:03Z","published":"2024-12-15T06:22:20Z","title":"Reason-before-Retrieve: One-Stage Reflective Chain-of-Thoughts for\n  Training-Free Zero-Shot Composed Image Retrieval","summary":"  Composed Image Retrieval (CIR) aims to retrieve target images that closely\nresemble a reference image while integrating user-specified textual\nmodifications, thereby capturing user intent more precisely. Existing\ntraining-free zero-shot CIR (ZS-CIR) methods often employ a two-stage process:\nthey first generate a caption for the reference image and then use Large\nLanguage Models for reasoning to obtain a target description. However, these\nmethods suffer from missing critical visual details and limited reasoning\ncapabilities, leading to suboptimal retrieval performance. To address these\nchallenges, we propose a novel, training-free one-stage method, One-Stage\nReflective Chain-of-Thought Reasoning for ZS-CIR (OSrCIR), which employs\nMultimodal Large Language Models to retain essential visual information in a\nsingle-stage reasoning process, eliminating the information loss seen in\ntwo-stage methods. Our Reflective Chain-of-Thought framework further improves\ninterpretative accuracy by aligning manipulation intent with contextual cues\nfrom reference images. OSrCIR achieves performance gains of 1.80% to 6.44% over\nexisting training-free methods across multiple tasks, setting new\nstate-of-the-art results in ZS-CIR and enhancing its utility in vision-language\napplications. Our code will be available at\nhttps://github.com/Pter61/osrcir2024/.\n","authors":["Yuanmin Tang","Xiaoting Qin","Jue Zhang","Jing Yu","Gaopeng Gou","Gang Xiong","Qingwei Ling","Saravan Rajmohan","Dongmei Zhang","Qi Wu"],"pdf_url":"https://arxiv.org/pdf/2412.11077v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12974v2","updated":"2024-12-18T07:52:14Z","published":"2024-12-17T14:56:59Z","title":"Attentive Eraser: Unleashing Diffusion Model's Object Removal Potential\n  via Self-Attention Redirection Guidance","summary":"  Recently, diffusion models have emerged as promising newcomers in the field\nof generative models, shining brightly in image generation. However, when\nemployed for object removal tasks, they still encounter issues such as\ngenerating random artifacts and the incapacity to repaint foreground object\nareas with appropriate content after removal. To tackle these problems, we\npropose Attentive Eraser, a tuning-free method to empower pre-trained diffusion\nmodels for stable and effective object removal. Firstly, in light of the\nobservation that the self-attention maps influence the structure and shape\ndetails of the generated images, we propose Attention Activation and\nSuppression (ASS), which re-engineers the self-attention mechanism within the\npre-trained diffusion models based on the given mask, thereby prioritizing the\nbackground over the foreground object during the reverse generation process.\nMoreover, we introduce Self-Attention Redirection Guidance (SARG), which\nutilizes the self-attention redirected by ASS to guide the generation process,\neffectively removing foreground objects within the mask while simultaneously\ngenerating content that is both plausible and coherent. Experiments demonstrate\nthe stability and effectiveness of Attentive Eraser in object removal across a\nvariety of pre-trained diffusion models, outperforming even training-based\nmethods. Furthermore, Attentive Eraser can be implemented in various diffusion\nmodel architectures and checkpoints, enabling excellent scalability. Code is\navailable at https://github.com/Anonym0u3/AttentiveEraser.\n","authors":["Wenhao Sun","Benlei Cui","Xue-Mei Dong","Jingqun Tang"],"pdf_url":"https://arxiv.org/pdf/2412.12974v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.13577v1","updated":"2024-12-18T07:51:35Z","published":"2024-12-18T07:51:35Z","title":"Bridge then Begin Anew: Generating Target-relevant Intermediate Model\n  for Source-free Visual Emotion Adaptation","summary":"  Visual emotion recognition (VER), which aims at understanding humans'\nemotional reactions toward different visual stimuli, has attracted increasing\nattention. Given the subjective and ambiguous characteristics of emotion,\nannotating a reliable large-scale dataset is hard. For reducing reliance on\ndata labeling, domain adaptation offers an alternative solution by adapting\nmodels trained on labeled source data to unlabeled target data. Conventional\ndomain adaptation methods require access to source data. However, due to\nprivacy concerns, source emotional data may be inaccessible. To address this\nissue, we propose an unexplored task: source-free domain adaptation (SFDA) for\nVER, which does not have access to source data during the adaptation process.\nTo achieve this, we propose a novel framework termed Bridge then Begin Anew\n(BBA), which consists of two steps: domain-bridged model generation (DMG) and\ntarget-related model adaptation (TMA). First, the DMG bridges cross-domain gaps\nby generating an intermediate model, avoiding direct alignment between two VER\ndatasets with significant differences. Then, the TMA begins training the target\nmodel anew to fit the target structure, avoiding the influence of\nsource-specific knowledge. Extensive experiments are conducted on six SFDA\nsettings for VER. The results demonstrate the effectiveness of BBA, which\nachieves remarkable performance gains compared with state-of-the-art SFDA\nmethods and outperforms representative unsupervised domain adaptation\napproaches.\n","authors":["Jiankun Zhu","Sicheng Zhao","Jing Jiang","Wenbo Tang","Zhaopan Xu","Tingting Han","Pengfei Xu","Hongxun Yao"],"pdf_url":"https://arxiv.org/pdf/2412.13577v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.13573v1","updated":"2024-12-18T07:45:30Z","published":"2024-12-18T07:45:30Z","title":"Seeking Consistent Flat Minima for Better Domain Generalization via\n  Refining Loss Landscapes","summary":"  Domain generalization aims to learn a model from multiple training domains\nand generalize it to unseen test domains. Recent theory has shown that seeking\nthe deep models, whose parameters lie in the flat minima of the loss landscape,\ncan significantly reduce the out-of-domain generalization error. However,\nexisting methods often neglect the consistency of loss landscapes in different\ndomains, resulting in models that are not simultaneously in the optimal flat\nminima in all domains, which limits their generalization ability. To address\nthis issue, this paper proposes an iterative Self-Feedback Training (SFT)\nframework to seek consistent flat minima that are shared across different\ndomains by progressively refining loss landscapes during training. It\nalternatively generates a feedback signal by measuring the inconsistency of\nloss landscapes in different domains and refines these loss landscapes for\ngreater consistency using this feedback signal. Benefiting from the consistency\nof the flat minima within these refined loss landscapes, our SFT helps achieve\nbetter out-of-domain generalization. Extensive experiments on DomainBed\ndemonstrate superior performances of SFT when compared to state-of-the-art\nsharpness-aware methods and other prevalent DG baselines. On average across\nfive DG benchmarks, SFT surpasses the sharpness-aware minimization by 2.6% with\nResNet-50 and 1.5% with ViT-B/16, respectively. The code will be available\nsoon.\n","authors":["Aodi Li","Liansheng Zhuang","Xiao Long","Minghong Yao","Shafei Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08584v2","updated":"2024-12-18T07:45:11Z","published":"2024-10-11T07:24:21Z","title":"ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification","summary":"  The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs through a dynamic ratio\nallocation strategy of important tokens. This ratio is adaptively determined\nbased on the layer-specific distribution of attention scores, rather than fixed\nhyper-parameters, thereby improving efficiency for less complex tasks while\nmaintaining high performance for more challenging ones. Then we select\nimportant tokens based on their normalized attention scores and perform sparse\nattention mechanism solely on those important tokens, reducing the latency in\nthe prefill phase. Tokens deemed less important will be discarded to reduce KV\ncache size, alleviating the memory bottleneck in the decoding phase. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.3$\\times$ and improve decoding throughput by 2.8$\\times$, with a minimal\naccuracy reduction of only 0.5\\% on VQAv2 benchmark over LLaVA-Next-13B model,\neffectively enhancing the generation efficiency of LVLMs.\n","authors":["Yefei He","Feng Chen","Jing Liu","Wenqi Shao","Hong Zhou","Kaipeng Zhang","Bohan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2410.08584v2.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2411.18290v2","updated":"2024-12-18T07:40:45Z","published":"2024-11-27T12:28:46Z","title":"Leveraging Semantic Asymmetry for Precise Gross Tumor Volume\n  Segmentation of Nasopharyngeal Carcinoma in Planning CT","summary":"  In the radiation therapy of nasopharyngeal carcinoma (NPC), clinicians\ntypically delineate the gross tumor volume (GTV) using non-contrast planning\ncomputed tomography to ensure accurate radiation dose delivery. However, the\nlow contrast between tumors and adjacent normal tissues necessitates that\nradiation oncologists manually delineate the tumors, often relying on\ndiagnostic MRI for guidance. % In this study, we propose a novel approach to\ndirectly segment NPC gross tumors on non-contrast planning CT images,\ncircumventing potential registration errors when aligning MRI or MRI-derived\ntumor masks to planning CT. To address the low contrast issues between tumors\nand adjacent normal structures in planning CT, we introduce a 3D Semantic\nAsymmetry Tumor segmentation (SATs) method. Specifically, we posit that a\nhealthy nasopharyngeal region is characteristically bilaterally symmetric,\nwhereas the emergence of nasopharyngeal carcinoma disrupts this symmetry. Then,\nwe propose a Siamese contrastive learning segmentation framework that minimizes\nthe voxel-wise distance between original and flipped areas without tumor and\nencourages a larger distance between original and flipped areas with tumor.\nThus, our approach enhances the sensitivity of features to semantic\nasymmetries. % Extensive experiments demonstrate that the proposed SATs\nachieves the leading NPC GTV segmentation performance in both internal and\nexternal testing, \\emph{e.g.}, with at least 2\\% absolute Dice score\nimprovement and 12\\% average distance error reduction when compared to other\nstate-of-the-art methods in the external testing.\n","authors":["Zi Li","Ying Chen","Zeli Chen","Yanzhou Su","Tai Ma","Tony C. W. Mok","Yan-Jie Zhou","Yunhai Bai","Zhinlin Zheng","Le Lu","Yirui Wang","Jia Ge","Xianghua Ye","Senxiang Yan","Dakai Jin"],"pdf_url":"https://arxiv.org/pdf/2411.18290v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13569v1","updated":"2024-12-18T07:35:42Z","published":"2024-12-18T07:35:42Z","title":"Multi-View Pedestrian Occupancy Prediction with a Novel Synthetic\n  Dataset","summary":"  We address an advanced challenge of predicting pedestrian occupancy as an\nextension of multi-view pedestrian detection in urban traffic. To support this,\nwe have created a new synthetic dataset called MVP-Occ, designed for dense\npedestrian scenarios in large-scale scenes. Our dataset provides detailed\nrepresentations of pedestrians using voxel structures, accompanied by rich\nsemantic scene understanding labels, facilitating visual navigation and\ninsights into pedestrian spatial information. Furthermore, we present a robust\nbaseline model, termed OmniOcc, capable of predicting both the voxel occupancy\nstate and panoptic labels for the entire scene from multi-view images. Through\nin-depth analysis, we identify and evaluate the key elements of our proposed\nmodel, highlighting their specific contributions and importance.\n","authors":["Sithu Aung","Min-Cheol Sagong","Junghyun Cho"],"pdf_url":"https://arxiv.org/pdf/2412.13569v1.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2412.13565v1","updated":"2024-12-18T07:33:22Z","published":"2024-12-18T07:33:22Z","title":"CA-Edit: Causality-Aware Condition Adapter for High-Fidelity Local\n  Facial Attribute Editing","summary":"  For efficient and high-fidelity local facial attribute editing, most existing\nediting methods either require additional fine-tuning for different editing\neffects or tend to affect beyond the editing regions. Alternatively, inpainting\nmethods can edit the target image region while preserving external areas.\nHowever, current inpainting methods still suffer from the generation\nmisalignment with facial attributes description and the loss of facial skin\ndetails. To address these challenges, (i) a novel data utilization strategy is\nintroduced to construct datasets consisting of attribute-text-image triples\nfrom a data-driven perspective, (ii) a Causality-Aware Condition Adapter is\nproposed to enhance the contextual causality modeling of specific details,\nwhich encodes the skin details from the original image while preventing\nconflicts between these cues and textual conditions. In addition, a Skin\nTransition Frequency Guidance technique is introduced for the local modeling of\ncontextual causality via sampling guidance driven by low-frequency alignment.\nExtensive quantitative and qualitative experiments demonstrate the\neffectiveness of our method in boosting both fidelity and editability for\nlocalized attribute editing. The code is available at\nhttps://github.com/connorxian/CA-Edit.\n","authors":["Xiaole Xian","Xilin He","Zenghao Niu","Junliang Zhang","Weicheng Xie","Siyang Song","Zitong Yu","Linlin Shen"],"pdf_url":"https://arxiv.org/pdf/2412.13565v1.pdf","comment":"accepted by aaai"},{"id":"http://arxiv.org/abs/2412.04062v2","updated":"2024-12-18T07:28:52Z","published":"2024-12-05T10:57:08Z","title":"ZipAR: Accelerating Auto-regressive Image Generation through Spatial\n  Locality","summary":"  In this paper, we propose ZipAR, a training-free, plug-and-play parallel\ndecoding framework for accelerating auto-regressive (AR) visual generation. The\nmotivation stems from the observation that images exhibit local structures, and\nspatially distant regions tend to have minimal interdependence. Given a\npartially decoded set of visual tokens, in addition to the original next-token\nprediction scheme in the row dimension, the tokens corresponding to spatially\nadjacent regions in the column dimension can be decoded in parallel, enabling\nthe ``next-set prediction'' paradigm. By decoding multiple tokens\nsimultaneously in a single forward pass, the number of forward passes required\nto generate an image is significantly reduced, resulting in a substantial\nimprovement in generation efficiency. Experiments demonstrate that ZipAR can\nreduce the number of model forward passes by up to 91% on the Emu3-Gen model\nwithout requiring any additional retraining. Code is available here:\nhttps://github.com/ThisisBillhe/ZipAR.\n","authors":["Yefei He","Feng Chen","Yuanyu He","Shaoxuan He","Hong Zhou","Kaipeng Zhang","Bohan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2412.04062v2.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2412.08941v2","updated":"2024-12-18T07:26:20Z","published":"2024-12-12T05:08:05Z","title":"Optimized Gradient Clipping for Noisy Label Learning","summary":"  Previous research has shown that constraining the gradient of loss function\nwith respect to model-predicted probabilities can enhance the model robustness\nagainst noisy labels. These methods typically specify a fixed optimal threshold\nfor gradient clipping through validation data to obtain the desired robustness\nagainst noise. However, this common practice overlooks the dynamic distribution\nof gradients from both clean and noisy-labeled samples at different stages of\ntraining, significantly limiting the model capability to adapt to the variable\nnature of gradients throughout the training process. To address this issue, we\npropose a simple yet effective approach called Optimized Gradient Clipping\n(OGC), which dynamically adjusts the clipping threshold based on the ratio of\nnoise gradients to clean gradients after clipping, estimated by modeling the\ndistributions of clean and noisy samples. This approach allows us to modify the\nclipping threshold at each training step, effectively controlling the influence\nof noise gradients. Additionally, we provide statistical analysis to certify\nthe noise-tolerance ability of OGC. Our extensive experiments across various\ntypes of label noise, including symmetric, asymmetric, instance-dependent, and\nreal-world noise, demonstrate the effectiveness of our approach.\n","authors":["Xichen Ye","Yifan Wu","Weizhong Zhang","Xiaoqiang Li","Yifan Chen","Cheng Jin"],"pdf_url":"https://arxiv.org/pdf/2412.08941v2.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2406.16993v2","updated":"2024-12-18T07:26:10Z","published":"2024-06-24T08:01:05Z","title":"Are Vision xLSTM Embedded UNet More Reliable in Medical 3D Image\n  Segmentation?","summary":"  The development of efficient segmentation strategies for medical images has\nevolved from its initial dependence on Convolutional Neural Networks (CNNs) to\nthe current investigation of hybrid models that combine CNNs with Vision\nTransformers. There is an increasing focus on creating architectures that are\nboth high-performance and computationally efficient, able to be deployed on\nremote systems with limited resources. Although transformers can capture global\ndependencies in the input space, they face challenges from the corresponding\nhigh computational and storage expenses involved. This paper investigates the\nintegration of CNNs with Vision Extended Long Short-Term Memory (Vision-xLSTM)s\nby introducing the novel {\\it \\textbf{U-VixLSTM}}.\n  The Vision-xLSTM blocks capture temporal and global relationships within the\npatches, as extracted from the CNN feature maps. The convolutional feature\nreconstruction path upsamples the output volume from the Vision-xLSTM blocks,\nto produce the segmentation output. Our primary objective is to propose that\nVision-xLSTM forms an appropriate backbone for medical image segmentation,\noffering excellent performance with reduced computational costs. The U-VixLSTM\nexhibits superior performance, compared to the state-of-the-art networks in the\npublicly available Synapse, ISIC and ACDC datasets. Code provided:\nhttps://github.com/duttapallabi2907/U-VixLSTM\n","authors":["Pallabi Dutta","Soham Bose","Swalpa Kumar Roy","Sushmita Mitra"],"pdf_url":"https://arxiv.org/pdf/2406.16993v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13558v1","updated":"2024-12-18T07:19:48Z","published":"2024-12-18T07:19:48Z","title":"Read Like a Radiologist: Efficient Vision-Language Model for 3D Medical\n  Imaging Interpretation","summary":"  Recent medical vision-language models (VLMs) have shown promise in 2D medical\nimage interpretation. However extending them to 3D medical imaging has been\nchallenging due to computational complexities and data scarcity. Although a few\nrecent VLMs specified for 3D medical imaging have emerged, all are limited to\nlearning volumetric representation of a 3D medical image as a set of\nsub-volumetric features. Such process introduces overly correlated\nrepresentations along the z-axis that neglect slice-specific clinical details,\nparticularly for 3D medical images where adjacent slices have low redundancy.\nTo address this limitation, we introduce MS-VLM that mimic radiologists'\nworkflow in 3D medical image interpretation. Specifically, radiologists analyze\n3D medical images by examining individual slices sequentially and synthesizing\ninformation across slices and views. Likewise, MS-VLM leverages self-supervised\n2D transformer encoders to learn a volumetric representation that capture\ninter-slice dependencies from a sequence of slice-specific features. Unbound by\nsub-volumetric patchification, MS-VLM is capable of obtaining useful volumetric\nrepresentations from 3D medical images with any slice length and from multiple\nimages acquired from different planes and phases. We evaluate MS-VLM on\npublicly available chest CT dataset CT-RATE and in-house rectal MRI dataset. In\nboth scenarios, MS-VLM surpasses existing methods in radiology report\ngeneration, producing more coherent and clinically relevant reports. These\nfindings highlight the potential of MS-VLM to advance 3D medical image\ninterpretation and improve the robustness of medical VLMs.\n","authors":["Changsun Lee","Sangjoon Park","Cheong-Il Shin","Woo Hee Choi","Hyun Jeong Park","Jeong Eun Lee","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2412.13558v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10050v2","updated":"2024-12-18T07:08:26Z","published":"2024-12-13T11:22:01Z","title":"ManipGPT: Is Affordance Segmentation by Large Vision Models Enough for\n  Articulated Object Manipulation?","summary":"  Visual actionable affordance has emerged as a transformative approach in\nrobotics, focusing on perceiving interaction areas prior to manipulation.\nTraditional methods rely on pixel sampling to identify successful interaction\nsamples or processing pointclouds for affordance mapping. However, these\napproaches are computationally intensive and struggle to adapt to diverse and\ndynamic environments. This paper introduces ManipGPT, a framework designed to\npredict optimal interaction areas for articulated objects using a large\npre-trained vision transformer (ViT). We created a dataset of 9.9k simulated\nand real images to bridge the sim-to-real gap and enhance real-world\napplicability. By fine-tuning the vision transformer on this small dataset, we\nsignificantly improved part-level affordance segmentation, adapting the model's\nin-context segmentation capabilities to robot manipulation scenarios. This\nenables effective manipulation across simulated and real-world environments by\ngenerating part-level affordance masks, paired with an impedance adaptation\npolicy, sufficiently eliminating the need for complex datasets or perception\nsystems.\n","authors":["Taewhan Kim","Hojin Bae","Zeming Li","Xiaoqi Li","Iaroslav Ponomarenko","Ruihai Wu","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2412.10050v2.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.06268v2","updated":"2024-12-18T07:08:01Z","published":"2024-12-09T07:39:39Z","title":"Open-Vocabulary High-Resolution 3D (OVHR3D) Data Segmentation and\n  Annotation Framework","summary":"  In the domain of the U.S. Army modeling and simulation, the availability of\nhigh quality annotated 3D data is pivotal to creating virtual environments for\ntraining and simulations. Traditional methodologies for 3D semantic and\ninstance segmentation, such as KpConv, RandLA, Mask3D, etc., are designed to\ntrain on extensive labeled datasets to obtain satisfactory performance in\npractical tasks. This requirement presents a significant challenge, given the\ninherent scarcity of manually annotated 3D datasets, particularly for the\nmilitary use cases. Recognizing this gap, our previous research leverages the\nOne World Terrain data repository manually annotated databases, as showcased at\nIITSEC 2019 and 2021, to enrich the training dataset for deep learning models.\nHowever, collecting and annotating large scale 3D data for specific tasks\nremains costly and inefficient. To this end, the objective of this research is\nto design and develop a comprehensive and efficient framework for 3D\nsegmentation tasks to assist in 3D data annotation. This framework integrates\nGrounding DINO and Segment anything Model, augmented by an enhancement in 2D\nimage rendering via 3D mesh. Furthermore, the authors have also developed a\nuser friendly interface that facilitates the 3D annotation process, offering\nintuitive visualization of rendered images and the 3D point cloud.\n","authors":["Jiuyi Xu","Meida Chen","Andrew Feng","Zifan Yu","Yangming Shi"],"pdf_url":"https://arxiv.org/pdf/2412.06268v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13552v1","updated":"2024-12-18T07:02:01Z","published":"2024-12-18T07:02:01Z","title":"DragScene: Interactive 3D Scene Editing with Single-view Drag\n  Instructions","summary":"  3D editing has shown remarkable capability in editing scenes based on various\ninstructions. However, existing methods struggle with achieving intuitive,\nlocalized editing, such as selectively making flowers blossom. Drag-style\nediting has shown exceptional capability to edit images with direct\nmanipulation instead of ambiguous text commands. Nevertheless, extending\ndrag-based editing to 3D scenes presents substantial challenges due to\nmulti-view inconsistency. To this end, we introduce DragScene, a framework that\nintegrates drag-style editing with diverse 3D representations. First, latent\noptimization is performed on a reference view to generate 2D edits based on\nuser instructions. Subsequently, coarse 3D clues are reconstructed from the\nreference view using a point-based representation to capture the geometric\ndetails of the edits. The latent representation of the edited view is then\nmapped to these 3D clues, guiding the latent optimization of other views. This\nprocess ensures that edits are propagated seamlessly across multiple views,\nmaintaining multi-view consistency. Finally, the target 3D scene is\nreconstructed from the edited multi-view images. Extensive experiments\ndemonstrate that DragScene facilitates precise and flexible drag-style editing\nof 3D scenes, supporting broad applicability across diverse 3D representations.\n","authors":["Chenghao Gu","Zhenzhe Li","Zhengqi Zhang","Yunpeng Bai","Shuzhao Xie","Zhi Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13552v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18853v2","updated":"2024-12-18T06:55:31Z","published":"2024-02-29T05:00:30Z","title":"Rethinking Multi-domain Generalization with A General Learning Objective","summary":"  Multi-domain generalization (mDG) is universally aimed to minimize the\ndiscrepancy between training and testing distributions to enhance\nmarginal-to-label distribution mapping. However, existing mDG literature lacks\na general learning objective paradigm and often imposes constraints on static\ntarget marginal distributions. In this paper, we propose to leverage a\n$Y$-mapping to relax the constraint. We rethink the learning objective for mDG\nand design a new \\textbf{general learning objective} to interpret and analyze\nmost existing mDG wisdom. This general objective is bifurcated into two\nsynergistic amis: learning domain-independent conditional features and\nmaximizing a posterior. Explorations also extend to two effective\nregularization terms that incorporate prior information and suppress invalid\ncausality, alleviating the issues that come with relaxed constraints. We\ntheoretically contribute an upper bound for the domain alignment of\ndomain-independent conditional features, disclosing that many previous mDG\nendeavors actually \\textbf{optimize partially the objective} and thus lead to\nlimited performance. As such, our study distills a general learning objective\ninto four practical components, providing a general, robust, and flexible\nmechanism to handle complex domain shifts. Extensive empirical results indicate\nthat the proposed objective with $Y$-mapping leads to substantially better mDG\nperformance in various downstream tasks, including regression, segmentation,\nand classification.\n","authors":["Zhaorui Tan","Xi Yang","Kaizhu Huang"],"pdf_url":"https://arxiv.org/pdf/2402.18853v2.pdf","comment":"Accepted by CVPR24"},{"id":"http://arxiv.org/abs/2412.13547v1","updated":"2024-12-18T06:46:40Z","published":"2024-12-18T06:46:40Z","title":"Turbo-GS: Accelerating 3D Gaussian Fitting for High-Quality Radiance\n  Fields","summary":"  Novel-view synthesis is an important problem in computer vision with\napplications in 3D reconstruction, mixed reality, and robotics. Recent methods\nlike 3D Gaussian Splatting (3DGS) have become the preferred method for this\ntask, providing high-quality novel views in real time. However, the training\ntime of a 3DGS model is slow, often taking 30 minutes for a scene with 200\nviews. In contrast, our goal is to reduce the optimization time by training for\nfewer steps while maintaining high rendering quality. Specifically, we combine\nthe guidance from both the position error and the appearance error to achieve a\nmore effective densification. To balance the rate between adding new Gaussians\nand fitting old Gaussians, we develop a convergence-aware budget control\nmechanism. Moreover, to make the densification process more reliable, we\nselectively add new Gaussians from mostly visited regions. With these designs,\nwe reduce the Gaussian optimization steps to one-third of the previous approach\nwhile achieving a comparable or even better novel view rendering quality. To\nfurther facilitate the rapid fitting of 4K resolution images, we introduce a\ndilation-based rendering technique. Our method, Turbo-GS, speeds up\noptimization for typical scenes and scales well to high-resolution (4K)\nscenarios on standard datasets. Through extensive experiments, we show that our\nmethod is significantly faster in optimization than other methods while\nretaining quality. Project page: https://ivl.cs.brown.edu/research/turbo-gs.\n","authors":["Tao Lu","Ankit Dhiman","R Srinath","Emre Arslan","Angela Xing","Yuanbo Xiangli","R Venkatesh Babu","Srinath Sridhar"],"pdf_url":"https://arxiv.org/pdf/2412.13547v1.pdf","comment":"Project page: https://ivl.cs.brown.edu/research/turbo-gs"},{"id":"http://arxiv.org/abs/2404.17805v2","updated":"2024-12-18T06:46:25Z","published":"2024-04-27T07:05:41Z","title":"From Optimization to Generalization: Fair Federated Learning against\n  Quality Shift via Inter-Client Sharpness Matching","summary":"  Due to escalating privacy concerns, federated learning has been recognized as\na vital approach for training deep neural networks with decentralized medical\ndata. In practice, it is challenging to ensure consistent imaging quality\nacross various institutions, often attributed to equipment malfunctions\naffecting a minority of clients. This imbalance in image quality can cause the\nfederated model to develop an inherent bias towards higher-quality images, thus\nposing a severe fairness issue. In this study, we pioneer the identification\nand formulation of this new fairness challenge within the context of the\nimaging quality shift. Traditional methods for promoting fairness in federated\nlearning predominantly focus on balancing empirical risks across diverse client\ndistributions. This strategy primarily facilitates fair optimization across\ndifferent training data distributions, yet neglects the crucial aspect of\ngeneralization. To address this, we introduce a solution termed Federated\nlearning with Inter-client Sharpness Matching (FedISM). FedISM enhances both\nlocal training and global aggregation by incorporating sharpness-awareness,\naiming to harmonize the sharpness levels across clients for fair\ngeneralization. Our empirical evaluations, conducted using the widely-used ICH\nand ISIC 2019 datasets, establish FedISM's superiority over current\nstate-of-the-art federated learning methods in promoting fairness. Code is\navailable at https://github.com/wnn2000/FFL4MIA.\n","authors":["Nannan Wu","Zhuo Kuang","Zengqiang Yan","Li Yu"],"pdf_url":"https://arxiv.org/pdf/2404.17805v2.pdf","comment":"This paper is accepted at IJCAI'24 (Main Track)"},{"id":"http://arxiv.org/abs/2412.13543v1","updated":"2024-12-18T06:43:06Z","published":"2024-12-18T06:43:06Z","title":"Query-centric Audio-Visual Cognition Network for Moment Retrieval,\n  Segmentation and Step-Captioning","summary":"  Video has emerged as a favored multimedia format on the internet. To better\ngain video contents, a new topic HIREST is presented, including video\nretrieval, moment retrieval, moment segmentation, and step-captioning. The\npioneering work chooses the pre-trained CLIP-based model for video retrieval,\nand leverages it as a feature extractor for other three challenging tasks\nsolved in a multi-task learning paradigm. Nevertheless, this work struggles to\nlearn the comprehensive cognition of user-preferred content, due to\ndisregarding the hierarchies and association relations across modalities. In\nthis paper, guided by the shallow-to-deep principle, we propose a query-centric\naudio-visual cognition (QUAG) network to construct a reliable multi-modal\nrepresentation for moment retrieval, segmentation and step-captioning.\nSpecifically, we first design the modality-synergistic perception to obtain\nrich audio-visual content, by modeling global contrastive alignment and local\nfine-grained interaction between visual and audio modalities. Then, we devise\nthe query-centric cognition that uses the deep-level query to perform the\ntemporal-channel filtration on the shallow-level audio-visual representation.\nThis can cognize user-preferred content and thus attain a query-centric\naudio-visual representation for three tasks. Extensive experiments show QUAG\nachieves the SOTA results on HIREST. Further, we test QUAG on the query-based\nvideo summarization task and verify its good generalization.\n","authors":["Yunbin Tu","Liang Li","Li Su","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2412.13543v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.13541v1","updated":"2024-12-18T06:40:53Z","published":"2024-12-18T06:40:53Z","title":"Spatio-Temporal Fuzzy-oriented Multi-Modal Meta-Learning for\n  Fine-grained Emotion Recognition","summary":"  Fine-grained emotion recognition (FER) plays a vital role in various fields,\nsuch as disease diagnosis, personalized recommendations, and multimedia mining.\nHowever, existing FER methods face three key challenges in real-world\napplications: (i) they rely on large amounts of continuously annotated data to\nensure accuracy since emotions are complex and ambiguous in reality, which is\ncostly and time-consuming; (ii) they cannot capture the temporal heterogeneity\ncaused by changing emotion patterns, because they usually assume that the\ntemporal correlation within sampling periods is the same; (iii) they do not\nconsider the spatial heterogeneity of different FER scenarios, that is, the\ndistribution of emotion information in different data may have bias or\ninterference. To address these challenges, we propose a Spatio-Temporal\nFuzzy-oriented Multi-modal Meta-learning framework (ST-F2M). Specifically,\nST-F2M first divides the multi-modal videos into multiple views, and each view\ncorresponds to one modality of one emotion. Multiple randomly selected views\nfor the same emotion form a meta-training task. Next, ST-F2M uses an integrated\nmodule with spatial and temporal convolutions to encode the data of each task,\nreflecting the spatial and temporal heterogeneity. Then it adds fuzzy semantic\ninformation to each task based on generalized fuzzy rules, which helps handle\nthe complexity and ambiguity of emotions. Finally, ST-F2M learns\nemotion-related general meta-knowledge through meta-recurrent neural networks\nto achieve fast and robust fine-grained emotion recognition. Extensive\nexperiments show that ST-F2M outperforms various state-of-the-art methods in\nterms of accuracy and model efficiency. In addition, we construct ablation\nstudies and further analysis to explore why ST-F2M performs well.\n","authors":["Jingyao Wang","Yuxuan Yang","Wenwen Qiang","Changwen Zheng","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2412.13541v1.pdf","comment":"13 pages, Submitted to TMM in 30-May-2024"},{"id":"http://arxiv.org/abs/2412.13540v1","updated":"2024-12-18T06:35:18Z","published":"2024-12-18T06:35:18Z","title":"Benchmarking and Improving Large Vision-Language Models for Fundamental\n  Visual Graph Understanding and Reasoning","summary":"  Large Vision-Language Models (LVLMs) have demonstrated remarkable performance\nacross diverse tasks. Despite great success, recent studies show that LVLMs\nencounter substantial limitations when engaging with visual graphs. To study\nthe reason behind these limitations, we propose VGCure, a comprehensive\nbenchmark covering 22 tasks for examining the fundamental graph understanding\nand reasoning capacities of LVLMs. Extensive evaluations conducted on 14 LVLMs\nreveal that LVLMs are weak in basic graph understanding and reasoning tasks,\nparticularly those concerning relational or structurally complex information.\nBased on this observation, we propose a structure-aware fine-tuning framework\nto enhance LVLMs with structure learning abilities through 3 self-supervised\nlearning tasks. Experiments validate the effectiveness of our method in\nimproving LVLMs' zero-shot performance on fundamental graph learning tasks, as\nwell as enhancing the robustness of LVLMs against complex visual graphs.\n","authors":["Yingjie Zhu","Xuefeng Bai","Kehai Chen","Yang Xiang","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.13540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.06742v3","updated":"2024-12-18T06:33:44Z","published":"2024-08-13T09:03:00Z","title":"Long-Tailed Out-of-Distribution Detection: Prioritizing Attention to\n  Tail","summary":"  Current out-of-distribution (OOD) detection methods typically assume balanced\nin-distribution (ID) data, while most real-world data follow a long-tailed\ndistribution. Previous approaches to long-tailed OOD detection often involve\nbalancing the ID data by reducing the semantics of head classes. However, this\nreduction can severely affect the classification accuracy of ID data. The main\nchallenge of this task lies in the severe lack of features for tail classes,\nleading to confusion with OOD data. To tackle this issue, we introduce a novel\nPrioritizing Attention to Tail (PATT) method using augmentation instead of\nreduction. Our main intuition involves using a mixture of von Mises-Fisher\n(vMF) distributions to model the ID data and a temperature scaling module to\nboost the confidence of ID data. This enables us to generate infinite\ncontrastive pairs, implicitly enhancing the semantics of ID classes while\npromoting differentiation between ID and OOD data. To further strengthen the\ndetection of OOD data without compromising the classification performance of ID\ndata, we propose feature calibration during the inference phase. By extracting\nan attention weight from the training set that prioritizes the tail classes and\nreduces the confidence in OOD data, we improve the OOD detection capability.\nExtensive experiments verified that our method outperforms the current\nstate-of-the-art methods on various benchmarks.\n","authors":["Yina He","Lei Peng","Yongcun Zhang","Juanjuan Weng","Zhiming Luo","Shaozi Li"],"pdf_url":"https://arxiv.org/pdf/2408.06742v3.pdf","comment":"Accepted by AAAI'25. Extended version with full appendix, 13 pages"},{"id":"http://arxiv.org/abs/2412.13533v1","updated":"2024-12-18T06:19:03Z","published":"2024-12-18T06:19:03Z","title":"Language-guided Medical Image Segmentation with Target-informed\n  Multi-level Contrastive Alignments","summary":"  Medical image segmentation is crucial in modern medical image analysis, which\ncan aid into diagnosis of various disease conditions. Recently, language-guided\nsegmentation methods have shown promising results in automating image\nsegmentation where text reports are incorporated as guidance. These text\nreports, containing image impressions and insights given by clinicians,\nprovides auxiliary guidance. However, these methods neglect the inherent\npattern gaps between the two distinct modalities, which leads to sub-optimal\nimage-text feature fusion without proper cross-modality feature alignments.\nContrastive alignments are widely used to associate image-text semantics in\nrepresentation learning; however, it has not been exploited to bridge the\npattern gaps in language-guided segmentation that relies on subtle low level\nimage details to represent diseases. Existing contrastive alignment methods\ntypically algin high-level global image semantics without involving low-level,\nlocalized target information, and therefore fails to explore fine-grained text\nguidance for language-guided segmentation. In this study, we propose a\nlanguage-guided segmentation network with Target-informed Multi-level\nContrastive Alignments (TMCA). TMCA enables target-informed cross-modality\nalignments and fine-grained text guidance to bridge the pattern gaps in\nlanguage-guided segmentation. Specifically, we introduce: 1) a target-sensitive\nsemantic distance module that enables granular image-text alignment modelling,\nand 2) a multi-level alignment strategy that directs text guidance on low-level\nimage features. In addition, a language-guided target enhancement module is\nproposed to leverage the aligned text to redirect attention to focus on\ncritical localized image features. Extensive experiments on 4 image-text\ndatasets, involving 3 medical imaging modalities, demonstrated that our TMCA\nachieved superior performances.\n","authors":["Mingjian Li","Mingyuan Meng","Shuchang Ye","David Dagan Feng","Lei Bi","Jinman Kim"],"pdf_url":"https://arxiv.org/pdf/2412.13533v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13525v1","updated":"2024-12-18T05:52:16Z","published":"2024-12-18T05:52:16Z","title":"Hybrid Data-Free Knowledge Distillation","summary":"  Data-free knowledge distillation aims to learn a compact student network from\na pre-trained large teacher network without using the original training data of\nthe teacher network. Existing collection-based and generation-based methods\ntrain student networks by collecting massive real examples and generating\nsynthetic examples, respectively. However, they inevitably become weak in\npractical scenarios due to the difficulties in gathering or emulating\nsufficient real-world data. To solve this problem, we propose a novel method\ncalled \\textbf{H}ybr\\textbf{i}d \\textbf{D}ata-\\textbf{F}ree\n\\textbf{D}istillation (HiDFD), which leverages only a small amount of collected\ndata as well as generates sufficient examples for training student networks.\nOur HiDFD comprises two primary modules, \\textit{i.e.}, the teacher-guided\ngeneration and student distillation. The teacher-guided generation module\nguides a Generative Adversarial Network (GAN) by the teacher network to produce\nhigh-quality synthetic examples from very few real-world collected examples.\nSpecifically, we design a feature integration mechanism to prevent the GAN from\noverfitting and facilitate the reliable representation learning from the\nteacher network. Meanwhile, we drive a category frequency smoothing technique\nvia the teacher network to balance the generative training of each category. In\nthe student distillation module, we explore a data inflation strategy to\nproperly utilize a blend of real and synthetic data to train the student\nnetwork via a classifier-sharing-based feature alignment technique. Intensive\nexperiments across multiple benchmarks demonstrate that our HiDFD can achieve\nstate-of-the-art performance using 120 times less collected data than existing\nmethods. Code is available at https://github.com/tangjialiang97/HiDFD.\n","authors":["Jialiang Tang","Shuo Chen","Chen Gong"],"pdf_url":"https://arxiv.org/pdf/2412.13525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13161v2","updated":"2024-12-18T05:51:58Z","published":"2024-12-17T18:39:10Z","title":"BanglishRev: A Large-Scale Bangla-English and Code-mixed Dataset of\n  Product Reviews in E-Commerce","summary":"  This work presents the BanglishRev Dataset, the largest e-commerce product\nreview dataset to date for reviews written in Bengali, English, a mixture of\nboth and Banglish, Bengali words written with English alphabets. The dataset\ncomprises of 1.74 million written reviews from 3.2 million ratings information\ncollected from a total of 128k products being sold in online e-commerce\nplatforms targeting the Bengali population. It includes an extensive array of\nrelated metadata for each of the reviews including the rating given by the\nreviewer, date the review was posted and date of purchase, number of likes,\ndislikes, response from the seller, images associated with the review etc. With\nsentiment analysis being the most prominent usage of review datasets,\nexperimentation with a binary sentiment analysis model with the review rating\nserving as an indicator of positive or negative sentiment was conducted to\nevaluate the effectiveness of the large amount of data presented in BanglishRev\nfor sentiment analysis tasks. A BanglishBERT model is trained on the data from\nBanglishRev with reviews being considered labeled positive if the rating is\ngreater than 3 and negative if the rating is less than or equal to 3. The model\nis evaluated by being testing against a previously published manually annotated\ndataset for e-commerce reviews written in a mixture of Bangla, English and\nBanglish. The experimental model achieved an exceptional accuracy of 94\\% and\nF1 score of 0.94, demonstrating the dataset's efficacy for sentiment analysis.\nSome of the intriguing patterns and observations seen within the dataset and\nfuture research directions where the dataset can be utilized is also discussed\nand explored. The dataset can be accessed through\nhttps://huggingface.co/datasets/BanglishRev/bangla-english-and-code-mixed-ecommerce-review-dataset.\n","authors":["Mohammad Nazmush Shamael","Sabila Nawshin","Swakkhar Shatabda","Salekul Islam"],"pdf_url":"https://arxiv.org/pdf/2412.13161v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08475v2","updated":"2024-12-18T05:47:35Z","published":"2024-09-13T02:02:07Z","title":"RT-DETRv3: Real-time End-to-End Object Detection with Hierarchical Dense\n  Positive Supervision","summary":"  RT-DETR is the first real-time end-to-end transformer-based object detector.\nIts efficiency comes from the framework design and the Hungarian matching.\nHowever, compared to dense supervision detectors like the YOLO series, the\nHungarian matching provides much sparser supervision, leading to insufficient\nmodel training and difficult to achieve optimal results. To address these\nissues, we proposed a hierarchical dense positive supervision method based on\nRT-DETR, named RT-DETRv3. Firstly, we introduce a CNN-based auxiliary branch\nthat provides dense supervision that collaborates with the original decoder to\nenhance the encoder feature representation. Secondly, to address insufficient\ndecoder training, we propose a novel learning strategy involving self-attention\nperturbation. This strategy diversifies label assignment for positive samples\nacross multiple query groups, thereby enriching positive supervisions.\nAdditionally, we introduce a shared-weight decoder branch for dense positive\nsupervision to ensure more high-quality queries matching each ground truth.\nNotably, all aforementioned modules are training-only. We conduct extensive\nexperiments to demonstrate the effectiveness of our approach on COCO val2017.\nRT-DETRv3 significantly outperforms existing real-time detectors, including the\nRT-DETR series and the YOLO series. For example, RT-DETRv3-R18 achieves 48.1%\nAP (+1.6%/+1.4%) compared to RT-DETR-R18/RT-DETRv2-R18, while maintaining the\nsame latency. Furthermore, RT-DETRv3-R101 can attain an impressive 54.6% AP\noutperforming YOLOv10-X. The code will be released at\nhttps://github.com/clxia12/RT-DETRv3.\n","authors":["Shuo Wang","Chunlong Xia","Feng Lv","Yifeng Shi"],"pdf_url":"https://arxiv.org/pdf/2409.08475v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13510v1","updated":"2024-12-18T05:19:09Z","published":"2024-12-18T05:19:09Z","title":"Dynamic Adapter with Semantics Disentangling for Cross-lingual\n  Cross-modal Retrieval","summary":"  Existing cross-modal retrieval methods typically rely on large-scale\nvision-language pair data. This makes it challenging to efficiently develop a\ncross-modal retrieval model for under-resourced languages of interest.\nTherefore, Cross-lingual Cross-modal Retrieval (CCR), which aims to align\nvision and the low-resource language (the target language) without using any\nhuman-labeled target-language data, has gained increasing attention. As a\ngeneral parameter-efficient way, a common solution is to utilize adapter\nmodules to transfer the vision-language alignment ability of Vision-Language\nPretraining (VLP) models from a source language to a target language. However,\nthese adapters are usually static once learned, making it difficult to adapt to\ntarget-language captions with varied expressions. To alleviate it, we propose\nDynamic Adapter with Semantics Disentangling (DASD), whose parameters are\ndynamically generated conditioned on the characteristics of the input captions.\nConsidering that the semantics and expression styles of the input caption\nlargely influence how to encode it, we propose a semantic disentangling module\nto extract the semantic-related and semantic-agnostic features from the input,\nensuring that generated adapters are well-suited to the characteristics of\ninput caption. Extensive experiments on two image-text datasets and one\nvideo-text dataset demonstrate the effectiveness of our model for cross-lingual\ncross-modal retrieval, as well as its good compatibility with various VLP\nmodels.\n","authors":["Rui Cai","Zhiyu Dong","Jianfeng Dong","Xun Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13510v1.pdf","comment":"Accepted by the 39th AAAI Conference on Artificial Intelligence\n  (AAAI-25)"},{"id":"http://arxiv.org/abs/2412.12772v2","updated":"2024-12-18T05:17:53Z","published":"2024-12-17T10:33:36Z","title":"Optimize the Unseen -- Fast NeRF Cleanup with Free Space Prior","summary":"  Neural Radiance Fields (NeRF) have advanced photorealistic novel view\nsynthesis, but their reliance on photometric reconstruction introduces\nartifacts, commonly known as \"floaters\". These artifacts degrade novel view\nquality, especially in areas unseen by the training cameras. We present a fast,\npost-hoc NeRF cleanup method that eliminates such artifacts by enforcing our\nFree Space Prior, effectively minimizing floaters without disrupting the NeRF's\nrepresentation of observed regions. Unlike existing approaches that rely on\neither Maximum Likelihood (ML) estimation to fit the data or a complex, local\ndata-driven prior, our method adopts a Maximum-a-Posteriori (MAP) approach,\nselecting the optimal model parameters under a simple global prior assumption\nthat unseen regions should remain empty. This enables our method to clean\nartifacts in both seen and unseen areas, enhancing novel view quality even in\nchallenging scene regions. Our method is comparable with existing NeRF cleanup\nmodels while being 2.5x faster in inference time, requires no additional memory\nbeyond the original NeRF, and achieves cleanup training in less than 30\nseconds. Our code will be made publically available.\n","authors":["Leo Segre","Shai Avidan"],"pdf_url":"https://arxiv.org/pdf/2412.12772v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13508v1","updated":"2024-12-18T05:14:13Z","published":"2024-12-18T05:14:13Z","title":"Plug-and-Play Tri-Branch Invertible Block for Image Rescaling","summary":"  High-resolution (HR) images are commonly downscaled to low-resolution (LR) to\nreduce bandwidth, followed by upscaling to restore their original details.\nRecent advancements in image rescaling algorithms have employed invertible\nneural networks (INNs) to create a unified framework for downscaling and\nupscaling, ensuring a one-to-one mapping between LR and HR images. Traditional\nmethods, utilizing dual-branch based vanilla invertible blocks, process\nhigh-frequency and low-frequency information separately, often relying on\nspecific distributions to model high-frequency components. However, processing\nthe low-frequency component directly in the RGB domain introduces channel\nredundancy, limiting the efficiency of image reconstruction. To address these\nchallenges, we propose a plug-and-play tri-branch invertible block\n(T-InvBlocks) that decomposes the low-frequency branch into luminance (Y) and\nchrominance (CbCr) components, reducing redundancy and enhancing feature\nprocessing. Additionally, we adopt an all-zero mapping strategy for\nhigh-frequency components during upscaling, focusing essential rescaling\ninformation within the LR image. Our T-InvBlocks can be seamlessly integrated\ninto existing rescaling models, improving performance in both general rescaling\ntasks and scenarios involving lossy compression. Extensive experiments confirm\nthat our method advances the state of the art in HR image reconstruction.\n","authors":["Jingwei Bao","Jinhua Hao","Pengcheng Xu","Ming Sun","Chao Zhou","Shuyuan Zhu"],"pdf_url":"https://arxiv.org/pdf/2412.13508v1.pdf","comment":"Accepted by AAAI 2025. Code is available at\n  https://github.com/Jingwei-Bao/T-InvBlocks"},{"id":"http://arxiv.org/abs/2410.15584v2","updated":"2024-12-18T05:13:44Z","published":"2024-10-21T02:10:49Z","title":"Deep Learning and Machine Learning -- Object Detection and Semantic\n  Segmentation: From Theory to Applications","summary":"  An in-depth exploration of object detection and semantic segmentation is\nprovided, combining theoretical foundations with practical applications.\nState-of-the-art advancements in machine learning and deep learning are\nreviewed, focusing on convolutional neural networks (CNNs), YOLO architectures,\nand transformer-based approaches such as DETR. The integration of artificial\nintelligence (AI) techniques and large language models for enhancing object\ndetection in complex environments is examined. Additionally, a comprehensive\nanalysis of big data processing is presented, with emphasis on model\noptimization and performance evaluation metrics. By bridging the gap between\ntraditional methods and modern deep learning frameworks, valuable insights are\noffered for researchers, data scientists, and engineers aiming to apply\nAI-driven methodologies to large-scale object detection tasks.\n","authors":["Jintao Ren","Ziqian Bi","Qian Niu","Junyu Liu","Benji Peng","Sen Zhang","Xuanhe Pan","Jinlang Wang","Keyu Chen","Caitlyn Heqi Yin","Pohsun Feng","Yizhu Wen","Tianyang Wang","Silin Chen","Ming Li","Jiawei Xu","Ming Liu"],"pdf_url":"https://arxiv.org/pdf/2410.15584v2.pdf","comment":"167 pages"},{"id":"http://arxiv.org/abs/2412.13507v1","updated":"2024-12-18T05:03:18Z","published":"2024-12-18T05:03:18Z","title":"Novel AI Camera Camouflage: Face Cloaking Without Full Disguise","summary":"  This study demonstrates a novel approach to facial camouflage that combines\ntargeted cosmetic perturbations and alpha transparency layer manipulation to\nevade modern facial recognition systems. Unlike previous methods -- such as CV\ndazzle, adversarial patches, and theatrical disguises -- this work achieves\neffective obfuscation through subtle modifications to key-point regions,\nparticularly the brow, nose bridge, and jawline. Empirical testing with Haar\ncascade classifiers and commercial systems like BetaFaceAPI and Microsoft Bing\nVisual Search reveals that vertical perturbations near dense facial key points\nsignificantly disrupt detection without relying on overt disguises.\nAdditionally, leveraging alpha transparency attacks in PNG images creates a\ndual-layer effect: faces remain visible to human observers but disappear in\nmachine-readable RGB layers, rendering them unidentifiable during reverse image\nsearches. The results highlight the potential for creating scalable,\nlow-visibility facial obfuscation strategies that balance effectiveness and\nsubtlety, opening pathways for defeating surveillance while maintaining\nplausible anonymity.\n","authors":["David Noever","Forrest McKee"],"pdf_url":"https://arxiv.org/pdf/2412.13507v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13504v1","updated":"2024-12-18T04:56:29Z","published":"2024-12-18T04:56:29Z","title":"Urban Air Temperature Prediction using Conditional Diffusion Models","summary":"  Urbanization as a global trend has led to many environmental challenges,\nincluding the urban heat island (UHI) effect. The increase in temperature has a\nsignificant impact on the well-being of urban residents. Air temperature\n($T_a$) at 2m above the surface is a key indicator of the UHI effect. How land\nuse land cover (LULC) affects $T_a$ is a critical research question which\nrequires high-resolution (HR) $T_a$ data at neighborhood scale. However,\nweather stations providing $T_a$ measurements are sparsely distributed e.g.\nmore than 10km apart; and numerical models are impractically slow and\ncomputationally expensive. In this work, we propose a novel method to predict\nHR $T_a$ at 100m ground separation distance (gsd) using land surface\ntemperature (LST) and other LULC related features which can be easily obtained\nfrom satellite imagery. Our method leverages diffusion models for the first\ntime to generate accurate and visually realistic HR $T_a$ maps, which\noutperforms prior methods. We pave the way for meteorological research using\ncomputer vision techniques by providing a dataset of an extended spatial and\ntemporal coverage, and a high spatial resolution as a benchmark for future\nresearch. Furthermore, we show that our model can be applied to urban planning\nby simulating the impact of different urban designs on $T_a$.\n","authors":["Siyang Dai","Jun Liu","Ngai-Man Cheung"],"pdf_url":"https://arxiv.org/pdf/2412.13504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13502v1","updated":"2024-12-18T04:50:19Z","published":"2024-12-18T04:50:19Z","title":"Level-Set Parameters: Novel Representation for 3D Shape Analysis","summary":"  3D shape analysis has been largely focused on traditional 3D representations\nof point clouds and meshes, but the discrete nature of these data makes the\nanalysis susceptible to variations in input resolutions. Recent development of\nneural fields brings in level-set parameters from signed distance functions as\na novel, continuous, and numerical representation of 3D shapes, where the shape\nsurfaces are defined as zero-level-sets of those functions. This motivates us\nto extend shape analysis from the traditional 3D data to these novel parameter\ndata. Since the level-set parameters are not Euclidean like point clouds, we\nestablish correlations across different shapes by formulating them as a\npseudo-normal distribution, and learn the distribution prior from the\nrespective dataset. To further explore the level-set parameters with shape\ntransformations, we propose to condition a subset of these parameters on\nrotations and translations, and generate them with a hypernetwork. This\nsimplifies the pose-related shape analysis compared to using traditional data.\nWe demonstrate the promise of the novel representations through applications in\nshape classification (arbitrary poses), retrieval, and 6D object pose\nestimation. Code and data in this research are provided at\nhttps://github.com/EnyaHermite/LevelSetParamData.\n","authors":["Huan Lei","Hongdong Li","Andreas Geiger","Anthony Dick"],"pdf_url":"https://arxiv.org/pdf/2412.13502v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18868v4","updated":"2024-12-18T04:43:59Z","published":"2024-06-27T03:48:57Z","title":"Advancing Cross-domain Discriminability in Continual Learning of\n  Vision-Language Models","summary":"  Continual learning (CL) with Vision-Language Models (VLMs) has overcome the\nconstraints of traditional CL, which only focuses on previously encountered\nclasses. During the CL of VLMs, we need not only to prevent the catastrophic\nforgetting on incrementally learned knowledge but also to preserve the\nzero-shot ability of VLMs. However, existing methods require additional\nreference datasets to maintain such zero-shot ability and rely on\ndomain-identity hints to classify images across different domains. In this\nstudy, we propose Regression-based Analytic Incremental Learning (RAIL), which\nutilizes a recursive ridge regression-based adapter to learn from a sequence of\ndomains in a non-forgetting manner and decouple the cross-domain correlations\nby projecting features to a higher-dimensional space. Cooperating with a\ntraining-free fusion module, RAIL absolutely preserves the VLM's zero-shot\nability on unseen domains without any reference data. Additionally, we\nintroduce Cross-domain Task-Agnostic Incremental Learning (X-TAIL) setting. In\nthis setting, a CL learner is required to incrementally learn from multiple\ndomains and classify test images from both seen and unseen domains without any\ndomain-identity hint. We theoretically prove RAIL's absolute memorization on\nincrementally learned domains. Experiment results affirm RAIL's\nstate-of-the-art performance in both X-TAIL and existing Multi-domain\nTask-Incremental Learning settings. The code is released at\nhttps://github.com/linghan1997/Regression-based-Analytic-Incremental-Learning.\n","authors":["Yicheng Xu","Yuxin Chen","Jiahao Nie","Yusong Wang","Huiping Zhuang","Manabu Okumura"],"pdf_url":"https://arxiv.org/pdf/2406.18868v4.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2412.12716v2","updated":"2024-12-18T04:42:07Z","published":"2024-12-17T09:30:31Z","title":"Unsupervised UAV 3D Trajectories Estimation with Sparse Point Clouds","summary":"  Compact UAV systems, while advancing delivery and surveillance, pose\nsignificant security challenges due to their small size, which hinders\ndetection by traditional methods. This paper presents a cost-effective,\nunsupervised UAV detection method using spatial-temporal sequence processing to\nfuse multiple LiDAR scans for accurate UAV tracking in real-world scenarios.\nOur approach segments point clouds into foreground and background, analyzes\nspatial-temporal data, and employs a scoring mechanism to enhance detection\naccuracy. Tested on a public dataset, our solution placed 4th in the CVPR 2024\nUG2+ Challenge, demonstrating its practical effectiveness. We plan to\nopen-source all designs, code, and sample data for the research community\ngithub.com/lianghanfang/UnLiDAR-UAV-Est.\n","authors":["Hanfang Liang","Yizhuo Yang","Jinming Hu","Jianfei Yang","Fen Liu","Shenghai Yuan"],"pdf_url":"https://arxiv.org/pdf/2412.12716v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.17166v3","updated":"2024-12-18T04:38:38Z","published":"2023-09-29T11:59:57Z","title":"Advances in Kidney Biopsy Lesion Assessment through Dense Instance\n  Segmentation","summary":"  Renal biopsies are the gold standard for the diagnosis of kidney diseases.\nLesion scores made by renal pathologists are semi-quantitative and exhibit high\ninter-observer variability. Automating lesion classification within segmented\nanatomical structures can provide decision support in quantification analysis,\nthereby reducing inter-observer variability. Nevertheless, classifying lesions\nin regions-of-interest (ROIs) is clinically challenging due to (a) a large\namount of densely packed anatomical objects, (b) class imbalance across\ndifferent compartments (at least 3), (c) significant variation in size and\nshape of anatomical objects and (d) the presence of multi-label lesions per\nanatomical structure. Existing models cannot address these complexities in an\nefficient and generic manner. This paper presents an analysis for a\n\\textbf{generalized solution} to datasets from various sources (pathology\ndepartments) with different types of lesions. Our approach utilizes two\nsub-networks: dense instance segmentation and lesion classification. We\nintroduce \\textbf{DiffRegFormer}, an end-to-end dense instance segmentation\nsub-network designed for multi-class, multi-scale objects within ROIs.\nCombining diffusion models, transformers, and RCNNs, DiffRegFormer {is a\ncomputational-friendly framework that can efficiently recognize over 500\nobjects across three anatomical classes, i.e., glomeruli, tubuli, and arteries,\nwithin ROIs.} In a dataset of 303 ROIs from 148 Jones' silver-stained renal\nWhole Slide Images (WSIs), our approach outperforms previous methods, achieving\nan Average Precision of 52.1\\% (detection) and 46.8\\% (segmentation). Moreover,\nour lesion classification sub-network achieves 89.2\\% precision and 64.6\\%\nrecall on 21889 object patches out of the 303 ROIs. Lastly, our model\ndemonstrates direct domain transfer to PAS-stained renal WSIs without\nfine-tuning.\n","authors":["Zhan Xiong","Junling He","Pieter Valkema","Tri Q. Nguyen","Maarten Naesens","Jesper Kers","Fons J. Verbeek"],"pdf_url":"https://arxiv.org/pdf/2309.17166v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13496v1","updated":"2024-12-18T04:34:46Z","published":"2024-12-18T04:34:46Z","title":"QueryCDR: Query-based Controllable Distortion Rectification Network for\n  Fisheye Images","summary":"  Fisheye image rectification aims to correct distortions in images taken with\nfisheye cameras. Although current models show promising results on images with\na similar degree of distortion as the training data, they will produce\nsub-optimal results when the degree of distortion changes and without\nretraining. The lack of generalization ability for dealing with varying degrees\nof distortion limits their practical application. In this paper, we take one\nstep further to enable effective distortion rectification for images with\nvarying degrees of distortion without retraining. We propose a novel\nQuery-based Controllable Distortion Rectification network for fisheye images\n(QueryCDR). In particular, we first present the Distortion-aware Learnable\nQuery Mechanism (DLQM), which defines the latent spatial relationships for\ndifferent distortion degrees as a series of learnable queries. Each query can\nbe learned to obtain position-dependent rectification control conditions,\nproviding control over the rectification process. Then, we propose two kinds of\ncontrollable modulating blocks to enable the control conditions to guide the\nmodulation of the distortion features better. These core components cooperate\nwith each other to effectively boost the generalization ability of the model at\nvarying degrees of distortion. Extensive experiments on fisheye image datasets\nwith different distortion degrees demonstrate our approach achieves\nhigh-quality and controllable distortion rectification.\n","authors":["Pengbo Guo","Chengxu Liu","Xingsong Hou","Xueming Qian"],"pdf_url":"https://arxiv.org/pdf/2412.13496v1.pdf","comment":"ECCV2024"},{"id":"http://arxiv.org/abs/2404.11064v3","updated":"2024-12-18T04:23:29Z","published":"2024-04-17T04:46:27Z","title":"Rethinking 3D Dense Caption and Visual Grounding in A Unified Framework\n  through Prompt-based Localization","summary":"  3D Visual Grounding (3DVG) and 3D Dense Captioning (3DDC) are two crucial\ntasks in various 3D applications, which require both shared and complementary\ninformation in localization and visual-language relationships. Therefore,\nexisting approaches adopt the two-stage \"detect-then-describe/discriminate\"\npipeline, which relies heavily on the performance of the detector, resulting in\nsuboptimal performance. Inspired by DETR, we propose a unified framework,\n3DGCTR, to jointly solve these two distinct but closely related tasks in an\nend-to-end fashion. The key idea is to reconsider the prompt-based localization\nability of the 3DVG model. In this way, the 3DVG model with a well-designed\nprompt as input can assist the 3DDC task by extracting localization information\nfrom the prompt. In terms of implementation, we integrate a Lightweight Caption\nHead into the existing 3DVG network with a Caption Text Prompt as a connection,\neffectively harnessing the existing 3DVG model's inherent localization\ncapacity, thereby boosting 3DDC capability. This integration facilitates\nsimultaneous multi-task training on both tasks, mutually enhancing their\nperformance. Extensive experimental results demonstrate the effectiveness of\nthis approach. Specifically, on the ScanRefer dataset, 3DGCTR surpasses the\nstate-of-the-art 3DDC method by 4.3% in CIDEr@0.5IoU in MLE training and\nimproves upon the SOTA 3DVG method by 3.16% in Acc@0.25IoU. The codes are at\nhttps://github.com/Leon1207/3DGCTR.\n","authors":["Yongdong Luo","Haojia Lin","Xiawu Zheng","Yigeng Jiang","Fei Chao","Jie Hu","Guannan Jiang","Songan Zhang","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2404.11064v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13490v1","updated":"2024-12-18T04:15:32Z","published":"2024-12-18T04:15:32Z","title":"Comparative Analysis of YOLOv9, YOLOv10 and RT-DETR for Real-Time Weed\n  Detection","summary":"  This paper presents a comprehensive evaluation of state-of-the-art object\ndetection models, including YOLOv9, YOLOv10, and RT-DETR, for the task of weed\ndetection in smart-spraying applications focusing on three classes: Sugarbeet,\nMonocot, and Dicot. The performance of these models is compared based on mean\nAverage Precision (mAP) scores and inference times on different GPU devices. We\nconsider various model variations, such as nano, small, medium, large alongside\ndifferent image resolutions (320px, 480px, 640px, 800px, 960px). The results\nhighlight the trade-offs between inference time and detection accuracy,\nproviding valuable insights for selecting the most suitable model for real-time\nweed detection. This study aims to guide the development of efficient and\neffective smart spraying systems, enhancing agricultural productivity through\nprecise weed management.\n","authors":["Ahmet Oğuz Saltık","Alicia Allmendinger","Anthony Stein"],"pdf_url":"https://arxiv.org/pdf/2412.13490v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04766v2","updated":"2024-12-18T04:14:40Z","published":"2024-09-07T08:53:17Z","title":"Cross-Dataset Gaze Estimation by Evidential Inter-intra Fusion","summary":"  Achieving accurate and reliable gaze predictions in complex and diverse\nenvironments remains challenging. Fortunately, it is straightforward to access\ndiverse gaze datasets in real-world applications. We discover that training\nthese datasets jointly can significantly improve the generalization of gaze\nestimation, which is overlooked in previous works. However, due to the inherent\ndistribution shift across different datasets, simply mixing multiple dataset\ndecreases the performance in the original domain despite gaining better\ngeneralization abilities. To address the problem of ``cross-dataset gaze\nestimation'', we propose a novel Evidential Inter-intra Fusion EIF framework,\nfor training a cross-dataset model that performs well across all source and\nunseen domains. Specifically, we build independent single-dataset branches for\nvarious datasets where the data space is partitioned into overlapping subspaces\nwithin each dataset for local regression, and further create a cross-dataset\nbranch to integrate the generalizable features from single-dataset branches.\nFurthermore, evidential regressors based on the Normal and Inverse-Gamma (NIG)\ndistribution are designed to additionally provide uncertainty estimation apart\nfrom predicting gaze. Building upon this foundation, our proposed framework\nachieves both intra-evidential fusion among multiple local regressors within\neach dataset and inter-evidential fusion among multiple branches by Mixture\n\\textbfof Normal Inverse-Gamma (MoNIG distribution. Experiments demonstrate\nthat our method consistently achieves notable improvements in both source\ndomains and unseen domains.\n","authors":["Shijing Wang","Yaping Huang","Jun Xie","Yi Tian","Feng Chen","Zhepeng Wang"],"pdf_url":"https://arxiv.org/pdf/2409.04766v2.pdf","comment":"This paper was previously submitted to ACM MM 2024"},{"id":"http://arxiv.org/abs/2412.13486v1","updated":"2024-12-18T04:01:32Z","published":"2024-12-18T04:01:32Z","title":"T$^3$-S2S: Training-free Triplet Tuning for Sketch to Scene Generation","summary":"  Scene generation is crucial to many computer graphics applications. Recent\nadvances in generative AI have streamlined sketch-to-image workflows, easing\nthe workload for artists and designers in creating scene concept art. However,\nthese methods often struggle for complex scenes with multiple detailed objects,\nsometimes missing small or uncommon instances. In this paper, we propose a\nTraining-free Triplet Tuning for Sketch-to-Scene (T3-S2S) generation after\nreviewing the entire cross-attention mechanism. This scheme revitalizes the\nexisting ControlNet model, enabling effective handling of multi-instance\ngenerations, involving prompt balance, characteristics prominence, and dense\ntuning. Specifically, this approach enhances keyword representation via the\nprompt balance module, reducing the risk of missing critical instances. It also\nincludes a characteristics prominence module that highlights TopK indices in\neach channel, ensuring essential features are better represented based on token\nsketches. Additionally, it employs dense tuning to refine contour details in\nthe attention map, compensating for instance-related regions. Experiments\nvalidate that our triplet tuning approach substantially improves the\nperformance of existing sketch-to-image models. It consistently generates\ndetailed, multi-instance 2D images, closely adhering to the input prompts and\nenhancing visual quality in complex multi-instance scenes. Code is available at\nhttps://github.com/chaos-sun/t3s2s.git.\n","authors":["Zhenhong Sun","Yifu Wang","Yonhon Ng","Yunfei Duan","Daoyi Dong","Hongdong Li","Pan Ji"],"pdf_url":"https://arxiv.org/pdf/2412.13486v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04456v2","updated":"2024-12-18T03:54:25Z","published":"2024-12-05T18:59:00Z","title":"HeatFormer: A Neural Optimizer for Multiview Human Mesh Recovery","summary":"  We introduce a novel method for human shape and pose recovery that can fully\nleverage multiple static views. We target fixed-multiview people monitoring,\nincluding elderly care and safety monitoring, in which calibrated cameras can\nbe installed at the corners of a room or an open space but whose configuration\nmay vary depending on the environment. Our key idea is to formulate it as\nneural optimization. We achieve this with HeatFormer, a neural optimizer that\niteratively refines the SMPL parameters given multiview images, which is\nfundamentally agonistic to the configuration of views. HeatFormer realizes this\nSMPL parameter estimation as heat map generation and alignment with a novel\ntransformer encoder and decoder. We demonstrate the effectiveness of HeatFormer\nincluding its accuracy, robustness to occlusion, and generalizability through\nan extensive set of experiments. We believe HeatFormer can serve a key role in\npassive human behavior modeling.\n","authors":["Yuto Matsubara","Ko Nishino"],"pdf_url":"https://arxiv.org/pdf/2412.04456v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09599v2","updated":"2024-12-18T03:49:22Z","published":"2024-12-12T18:59:00Z","title":"RatBodyFormer: Rodent Body Surface from Keypoints","summary":"  Rat behavior modeling goes to the heart of many scientific studies, yet the\ntextureless body surface evades automatic analysis as it literally has no\nkeypoints that detectors can find. The movement of the body surface, however,\nis a rich source of information for deciphering the rat behavior. We introduce\ntwo key contributions to automatically recover densely 3D sampled rat body\nsurface points, passively. The first is RatDome, a novel multi-camera system\nfor rat behavior capture, and a large-scale dataset captured with it that\nconsists of pairs of 3D keypoints and 3D body surface points. The second is\nRatBodyFormer, a novel network to transform detected keypoints to 3D body\nsurface points. RatBodyFormer is agnostic to the exact locations of the 3D body\nsurface points in the training data and is trained with masked-learning. We\nexperimentally validate our framework with a number of real-world experiments.\nOur results collectively serve as a novel foundation for automated rat behavior\nanalysis and will likely have far-reaching implications for biomedical and\nneuroscientific research.\n","authors":["Ayaka Higami","Karin Oshima","Tomoyo Isoguchi Shiramatsu","Hirokazu Takahashi","Shohei Nobuhara","Ko Nishino"],"pdf_url":"https://arxiv.org/pdf/2412.09599v2.pdf","comment":"https://vision.ist.i.kyoto-u.ac.jp/research/ratbodyformer/"},{"id":"http://arxiv.org/abs/2412.13479v1","updated":"2024-12-18T03:42:42Z","published":"2024-12-18T03:42:42Z","title":"Real-time One-Step Diffusion-based Expressive Portrait Videos Generation","summary":"  Latent diffusion models have made great strides in generating expressive\nportrait videos with accurate lip-sync and natural motion from a single\nreference image and audio input. However, these models are far from real-time,\noften requiring many sampling steps that take minutes to generate even one\nsecond of video-significantly limiting practical use. We introduce OSA-LCM\n(One-Step Avatar Latent Consistency Model), paving the way for real-time\ndiffusion-based avatars. Our method achieves comparable video quality to\nexisting methods but requires only one sampling step, making it more than 10x\nfaster. To accomplish this, we propose a novel avatar discriminator design that\nguides lip-audio consistency and motion expressiveness to enhance video quality\nin limited sampling steps. Additionally, we employ a second-stage training\narchitecture using an editing fine-tuned method (EFT), transforming video\ngeneration into an editing task during training to effectively address the\ntemporal gap challenge in single-step generation. Experiments demonstrate that\nOSA-LCM outperforms existing open-source portrait video generation models while\noperating more efficiently with a single sampling step.\n","authors":["Hanzhong Guo","Hongwei Yi","Daquan Zhou","Alexander William Bergman","Michael Lingelbach","Yizhou Yu"],"pdf_url":"https://arxiv.org/pdf/2412.13479v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2412.13477v1","updated":"2024-12-18T03:41:34Z","published":"2024-12-18T03:41:34Z","title":"Generating Unseen Nonlinear Evolution in Sea Surface Temperature Using a\n  Deep Learning-Based Latent Space Data Assimilation Framework","summary":"  Advances in data assimilation (DA) methods have greatly improved the accuracy\nof Earth system predictions. To fuse multi-source data and reconstruct the\nnonlinear evolution missing from observations, geoscientists are developing\nfuture-oriented DA methods. In this paper, we redesign a purely data-driven\nlatent space DA framework (DeepDA) that employs a generative artificial\nintelligence model to capture the nonlinear evolution in sea surface\ntemperature. Under variational constraints, DeepDA embedded with nonlinear\nfeatures can effectively fuse heterogeneous data. The results show that DeepDA\nremains highly stable in capturing and generating nonlinear evolutions even\nwhen a large amount of observational information is missing. It can be found\nthat when only 10% of the observation information is available, the error\nincrease of DeepDA does not exceed 40%. Furthermore, DeepDA has been shown to\nbe robust in the fusion of real observations and ensemble simulations. In\nparticular, this paper provides a mechanism analysis of the nonlinear evolution\ngenerated by DeepDA from the perspective of physical patterns, which reveals\nthe inherent explainability of our DL model in capturing multi-scale ocean\nsignals.\n","authors":["Qingyu Zheng","Guijun Han","Wei Li","Lige Cao","Gongfu Zhou","Haowen Wu","Qi Shao","Ru Wang","Xiaobo Wu","Xudong Cui","Hong Li","Xuan Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13477v1.pdf","comment":"31 pages, 14 figures"},{"id":"http://arxiv.org/abs/2409.04851v2","updated":"2024-12-18T03:40:35Z","published":"2024-09-07T15:06:30Z","title":"Towards Weather-Robust 3D Human Body Reconstruction: Millimeter-Wave\n  Radar-Based Dataset, Benchmark, and Multi-Modal Fusion","summary":"  3D human reconstruction from RGB images achieves decent results in good\nweather conditions but degrades dramatically in rough weather. Complementarily,\nmmWave radars have been employed to reconstruct 3D human joints and meshes in\nrough weather. However, combining RGB and mmWave signals for weather-robust 3D\nhuman reconstruction is still an open challenge, given the sparse nature of\nmmWave and the vulnerability of RGB images. The limited research about the\nimpact of missing points and sparsity features of mmWave data on reconstruction\nperformance, as well as the lack of available datasets for paired mmWave-RGB\ndata, further complicates the process of fusing the two modalities. To fill\nthese gaps, we build up an automatic 3D body annotation system with multiple\nsensors to collect a large-scale mmWave dataset. The dataset consists of\nsynchronized and calibrated mmWave radar point clouds and RGB(D) images under\ndifferent weather conditions and skeleton/mesh annotations for humans in these\nscenes. With this dataset, we conduct a comprehensive analysis about the\nlimitations of single-modality reconstruction and the impact of missing points\nand sparsity on the reconstruction performance. Based on the guidance of this\nanalysis, we design ImmFusion, the first mmWave-RGB fusion solution to robustly\nreconstruct 3D human bodies in various weather conditions. Specifically, our\nImmFusion consists of image and point backbones for token feature extraction\nand a Transformer module for token fusion. The image and point backbones refine\nglobal and local features from original data, and the Fusion Transformer Module\naims for effective information fusion of two modalities by dynamically\nselecting informative tokens. Extensive experiments demonstrate that ImmFusion\ncan efficiently utilize the information of two modalities to achieve robust 3D\nhuman body reconstruction in various weather environments.\n","authors":["Anjun Chen","Xiangyu Wang","Kun Shi","Yuchi Huo","Jiming Chen","Qi Ye"],"pdf_url":"https://arxiv.org/pdf/2409.04851v2.pdf","comment":"TCSVT 2024, Project Page:\n  https://chen3110.github.io/mmbody/index.html"},{"id":"http://arxiv.org/abs/2412.13469v1","updated":"2024-12-18T03:35:55Z","published":"2024-12-18T03:35:55Z","title":"Enabling Region-Specific Control via Lassos in Point-Based Colorization","summary":"  Point-based interactive colorization techniques allow users to effortlessly\ncolorize grayscale images using user-provided color hints. However, point-based\nmethods often face challenges when different colors are given to semantically\nsimilar areas, leading to color intermingling and unsatisfactory results-an\nissue we refer to as color collapse. The fundamental cause of color collapse is\nthe inadequacy of points for defining the boundaries for each color. To\nmitigate color collapse, we introduce a lasso tool that can control the scope\nof each color hint. Additionally, we design a framework that leverages the\nuser-provided lassos to localize the attention masks. The experimental results\nshow that using a single lasso is as effective as applying 4.18 individual\ncolor hints and can achieve the desired outcomes in 30% less time than using\npoints alone.\n","authors":["Sanghyeon Lee","Jooyeol Yun","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2412.13469v1.pdf","comment":"Accepted to AAAI2025"},{"id":"http://arxiv.org/abs/2412.09982v2","updated":"2024-12-18T03:25:50Z","published":"2024-12-13T09:09:14Z","title":"SplineGS: Robust Motion-Adaptive Spline for Real-Time Dynamic 3D\n  Gaussians from Monocular Video","summary":"  Synthesizing novel views from in-the-wild monocular videos is challenging due\nto scene dynamics and the lack of multi-view cues. To address this, we propose\nSplineGS, a COLMAP-free dynamic 3D Gaussian Splatting (3DGS) framework for\nhigh-quality reconstruction and fast rendering from monocular videos. At its\ncore is a novel Motion-Adaptive Spline (MAS) method, which represents\ncontinuous dynamic 3D Gaussian trajectories using cubic Hermite splines with a\nsmall number of control points. For MAS, we introduce a Motion-Adaptive Control\npoints Pruning (MACP) method to model the deformation of each dynamic 3D\nGaussian across varying motions, progressively pruning control points while\nmaintaining dynamic modeling integrity. Additionally, we present a joint\noptimization strategy for camera parameter estimation and 3D Gaussian\nattributes, leveraging photometric and geometric consistency. This eliminates\nthe need for Structure-from-Motion preprocessing and enhances SplineGS's\nrobustness in real-world conditions. Experiments show that SplineGS\nsignificantly outperforms state-of-the-art methods in novel view synthesis\nquality for dynamic scenes from monocular videos, achieving thousands times\nfaster rendering speed.\n","authors":["Jongmin Park","Minh-Quan Viet Bui","Juan Luis Gonzalez Bello","Jaeho Moon","Jihyong Oh","Munchurl Kim"],"pdf_url":"https://arxiv.org/pdf/2412.09982v2.pdf","comment":"The first two authors contributed equally to this work (equal\n  contribution). The last two authors advised equally to this work. Please\n  visit our project page at this https://kaist-viclab.github.io/splinegs-site/"},{"id":"http://arxiv.org/abs/2311.12059v2","updated":"2024-12-18T03:23:19Z","published":"2023-11-18T16:14:08Z","title":"Mesh Watermark Removal Attack and Mitigation: A Novel Perspective of\n  Function Space","summary":"  Mesh watermark embeds secret messages in 3D meshes and decodes the message\nfrom watermarked meshes for ownership verification. Current watermarking\nmethods directly hide secret messages in vertex and face sets of meshes.\nHowever, mesh is a discrete representation that uses vertex and face sets to\ndescribe a continuous signal, which can be discretized in other discrete\nrepresentations with different vertex and face sets. This raises the question\nof whether the watermark can still be verified on the different discrete\nrepresentations of the watermarked mesh. We conduct this research in an\nattack-then-defense manner by proposing a novel function space mesh watermark\nremoval attack FuncEvade and then mitigating it through function space mesh\nwatermarking FuncMark. In detail, FuncEvade generates a different discrete\nrepresentation of a watermarked mesh by extracting it from the signed distance\nfunction of the watermarked mesh. We observe that the generated mesh can evade\nALL previous watermarking methods. FuncMark mitigates FuncEvade by watermarking\nsigned distance function through message-guided deformation. Such deformation\ncan survive isosurfacing and thus be inherited by the extracted meshes for\nfurther watermark decoding. Extensive experiments demonstrate that FuncEvade\nachieves 100% evasion rate among all previous watermarking methods while\nachieving only 0.3% evasion rate on FuncMark. Besides, our FuncMark performs\nsimilarly on other metrics compared to state-of-the-art mesh watermarking\nmethods.\n","authors":["Xingyu Zhu","Guanhui Ye","Chengdong Dong","Xiapu Luo","Xuetao Wei"],"pdf_url":"https://arxiv.org/pdf/2311.12059v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13463v1","updated":"2024-12-18T03:18:11Z","published":"2024-12-18T03:18:11Z","title":"FlexPose: Pose Distribution Adaptation with Limited Guidance","summary":"  Numerous well-annotated human key-point datasets are publicly available to\ndate. However, annotating human poses for newly collected images is still a\ncostly and time-consuming progress. Pose distributions from different datasets\nshare similar pose hinge-structure priors with different geometric\ntransformations, such as pivot orientation, joint rotation, and bone length\nratio. The difference between Pose distributions is essentially the difference\nbetween the transformation distributions. Inspired by this fact, we propose a\nmethod to calibrate a pre-trained pose generator in which the pose prior has\nalready been learned to an adapted one following a new pose distribution. We\ntreat the representation of human pose joint coordinates as skeleton image and\ntransfer a pre-trained pose annotation generator with only a few annotation\nguidance. By fine-tuning a limited number of linear layers that closely related\nto the pose transformation, the adapted generator is able to produce any number\nof pose annotations that are similar to the target poses. We evaluate our\nproposed method, FlexPose, on several cross-dataset settings both qualitatively\nand quantitatively, which demonstrates that our approach achieves\nstate-of-the-art performance compared to the existing generative-model-based\ntransfer learning methods when given limited annotation guidance.\n","authors":["Zixiao Wang","Junwu Weng","Mengyuan Liu","Bei Yu"],"pdf_url":"https://arxiv.org/pdf/2412.13463v1.pdf","comment":"Accepted by AAAI25, 12 pages, 10 figures"},{"id":"http://arxiv.org/abs/2409.05166v5","updated":"2024-12-18T03:14:55Z","published":"2024-09-08T17:35:48Z","title":"CD-NGP: A Fast Scalable Continual Representation for Dynamic Scenes","summary":"  Current methods for novel view synthesis (NVS) in dynamic scenes encounter\nsignificant challenges in managing memory consumption, model complexity,\ntraining efficiency, and rendering fidelity. Existing offline techniques, while\ndelivering high-quality results, face challenges from substantial memory\ndemands and limited scalability. Conversely, online methods struggle to balance\nrapid convergence with model compactness. To address these issues, we propose\ncontinual dynamic neural graphics primitives (CD-NGP). Our approach leverages a\ncontinual learning framework to reduce memory overhead, and it also integrates\nfeatures from distinct temporal and spatial hash encodings for high rendering\nquality. Meanwhile, our method employs parameter reuse to achieve high\nscalability. Additionally, we introduce a novel dataset featuring multi-view,\nexceptionally long video sequences with substantial rigid and non-rigid motion,\nwhich is seldom possessed by existing datasets. We evaluate the reconstruction\nquality, speed and scalability of our method on both the established public\ndatasets and our exceptionally long video dataset. Notably, our method achieves\nan $85\\%$ reduction in training memory consumption (less than 14GB) compared to\noffline techniques and significantly lowers streaming bandwidth requirements\n(less than 0.4MB/frame) relative to other online alternatives. The experimental\nresults on our long video sequences dataset show the superior scalability and\nreconstruction quality compared to existing state-of-the-art approaches.\n","authors":["Zhenhuan Liu","Shuai Liu","Zhiwei Ning","Jie Yang","Yifan Zuo","Yuming Fang","Wei Liu"],"pdf_url":"https://arxiv.org/pdf/2409.05166v5.pdf","comment":"12+1 pages"},{"id":"http://arxiv.org/abs/2412.13461v1","updated":"2024-12-18T03:14:11Z","published":"2024-12-18T03:14:11Z","title":"Look Inside for More: Internal Spatial Modality Perception for 3D\n  Anomaly Detection","summary":"  3D anomaly detection has recently become a significant focus in computer\nvision. Several advanced methods have achieved satisfying anomaly detection\nperformance. However, they typically concentrate on the external structure of\n3D samples and struggle to leverage the internal information embedded within\nsamples. Inspired by the basic intuition of why not look inside for more, we\nintroduce a straightforward method named Internal Spatial Modality Perception\n(ISMP) to explore the feature representation from internal views fully.\nSpecifically, our proposed ISMP consists of a critical perception module,\nSpatial Insight Engine (SIE), which abstracts complex internal information of\npoint clouds into essential global features. Besides, to better align\nstructural information with point data, we propose an enhanced key point\nfeature extraction module for amplifying spatial structure feature\nrepresentation. Simultaneously, a novel feature filtering module is\nincorporated to reduce noise and redundant features for further aligning\nprecise spatial structure. Extensive experiments validate the effectiveness of\nour proposed method, achieving object-level and pixel-level AUROC improvements\nof 4.2% and 13.1%, respectively, on the Real3D-AD benchmarks. Note that the\nstrong generalization ability of SIE has been theoretically proven and is\nverified in both classification and segmentation tasks.\n","authors":["Hanzhe Liang","Guoyang Xie","Chengbin Hou","Bingshu Wang","Can Gao","Jinbao Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13461v1.pdf","comment":"AAAI2025 Accepted"},{"id":"http://arxiv.org/abs/2412.13026v2","updated":"2024-12-18T03:05:45Z","published":"2024-12-17T15:48:25Z","title":"NAVCON: A Cognitively Inspired and Linguistically Grounded Corpus for\n  Vision and Language Navigation","summary":"  We present NAVCON, a large-scale annotated Vision-Language Navigation (VLN)\ncorpus built on top of two popular datasets (R2R and RxR). The paper introduces\nfour core, cognitively motivated and linguistically grounded, navigation\nconcepts and an algorithm for generating large-scale silver annotations of\nnaturally occurring linguistic realizations of these concepts in navigation\ninstructions. We pair the annotated instructions with video clips of an agent\nacting on these instructions. NAVCON contains 236, 316 concept annotations for\napproximately 30, 0000 instructions and 2.7 million aligned images (from\napproximately 19, 000 instructions) showing what the agent sees when executing\nan instruction. To our knowledge, this is the first comprehensive resource of\nnavigation concepts. We evaluated the quality of the silver annotations by\nconducting human evaluation studies on NAVCON samples. As further validation of\nthe quality and usefulness of the resource, we trained a model for detecting\nnavigation concepts and their linguistic realizations in unseen instructions.\nAdditionally, we show that few-shot learning with GPT-4o performs well on this\ntask using large-scale silver annotations of NAVCON.\n","authors":["Karan Wanchoo","Xiaoye Zuo","Hannah Gonzalez","Soham Dan","Georgios Georgakis","Dan Roth","Kostas Daniilidis","Eleni Miltsakaki"],"pdf_url":"https://arxiv.org/pdf/2412.13026v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11525v2","updated":"2024-12-18T03:00:13Z","published":"2024-12-16T08:00:50Z","title":"Sequence Matters: Harnessing Video Models in 3D Super-Resolution","summary":"  3D super-resolution aims to reconstruct high-fidelity 3D models from\nlow-resolution (LR) multi-view images. Early studies primarily focused on\nsingle-image super-resolution (SISR) models to upsample LR images into\nhigh-resolution images. However, these methods often lack view consistency\nbecause they operate independently on each image. Although various\npost-processing techniques have been extensively explored to mitigate these\ninconsistencies, they have yet to fully resolve the issues. In this paper, we\nperform a comprehensive study of 3D super-resolution by leveraging video\nsuper-resolution (VSR) models. By utilizing VSR models, we ensure a higher\ndegree of spatial consistency and can reference surrounding spatial\ninformation, leading to more accurate and detailed reconstructions. Our\nfindings reveal that VSR models can perform remarkably well even on sequences\nthat lack precise spatial alignment. Given this observation, we propose a\nsimple yet practical approach to align LR images without involving fine-tuning\nor generating 'smooth' trajectory from the trained 3D models over LR images.\nThe experimental results show that the surprisingly simple algorithms can\nachieve the state-of-the-art results of 3D super-resolution tasks on standard\nbenchmark datasets, such as the NeRF-synthetic and MipNeRF-360 datasets.\nProject page: https://ko-lani.github.io/Sequence-Matters\n","authors":["Hyun-kyu Ko","Dongheok Park","Youngin Park","Byeonghyeon Lee","Juhee Han","Eunbyung Park"],"pdf_url":"https://arxiv.org/pdf/2412.11525v2.pdf","comment":"Project page: https://ko-lani.github.io/Sequence-Matters"},{"id":"http://arxiv.org/abs/2412.13454v1","updated":"2024-12-18T02:54:30Z","published":"2024-12-18T02:54:30Z","title":"Pre-training a Density-Aware Pose Transformer for Robust LiDAR-based 3D\n  Human Pose Estimation","summary":"  With the rapid development of autonomous driving, LiDAR-based 3D Human Pose\nEstimation (3D HPE) is becoming a research focus. However, due to the noise and\nsparsity of LiDAR-captured point clouds, robust human pose estimation remains\nchallenging. Most of the existing methods use temporal information, multi-modal\nfusion, or SMPL optimization to correct biased results. In this work, we try to\nobtain sufficient information for 3D HPE only by modeling the intrinsic\nproperties of low-quality point clouds. Hence, a simple yet powerful method is\nproposed, which provides insights both on modeling and augmentation of point\nclouds. Specifically, we first propose a concise and effective density-aware\npose transformer (DAPT) to get stable keypoint representations. By using a set\nof joint anchors and a carefully designed exchange module, valid information is\nextracted from point clouds with different densities. Then 1D heatmaps are\nutilized to represent the precise locations of the keypoints. Secondly, a\ncomprehensive LiDAR human synthesis and augmentation method is proposed to\npre-train the model, enabling it to acquire a better human body prior. We\nincrease the diversity of point clouds by randomly sampling human positions and\norientations and by simulating occlusions through the addition of laser-level\nmasks. Extensive experiments have been conducted on multiple datasets,\nincluding IMU-annotated LidarHuman26M, SLOPER4D, and manually annotated Waymo\nOpen Dataset v2.0 (Waymo), HumanM3. Our method demonstrates SOTA performance in\nall scenarios. In particular, compared with LPFormer on Waymo, we reduce the\naverage MPJPE by $10.0mm$. Compared with PRN on SLOPER4D, we notably reduce the\naverage MPJPE by $20.7mm$.\n","authors":["Xiaoqi An","Lin Zhao","Chen Gong","Jun Li","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2412.13454v1.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2403.15063v2","updated":"2024-12-18T02:51:53Z","published":"2024-03-22T09:40:52Z","title":"Towards a Comprehensive, Efficient and Promptable Anatomic Structure\n  Segmentation Model using 3D Whole-body CT Scans","summary":"  Segment anything model (SAM) demonstrates strong generalization ability on\nnatural image segmentation. However, its direct adaptation in medical image\nsegmentation tasks shows significant performance drops. It also requires an\nexcessive number of prompt points to obtain a reasonable accuracy. Although\nquite a few studies explore adapting SAM into medical image volumes, the\nefficiency of 2D adaptation methods is unsatisfactory and 3D adaptation methods\nare only capable of segmenting specific organs/tumors. In this work, we propose\na comprehensive and scalable 3D SAM model for whole-body CT segmentation, named\nCT-SAM3D. Instead of adapting SAM, we propose a 3D promptable segmentation\nmodel using a (nearly) fully labeled CT dataset. To train CT-SAM3D effectively,\nensuring the model's accurate responses to higher-dimensional spatial prompts\nis crucial, and 3D patch-wise training is required due to GPU memory\nconstraints. Therefore, we propose two key technical developments: 1) a\nprogressively and spatially aligned prompt encoding method to effectively\nencode click prompts in local 3D space; and 2) a cross-patch prompt scheme to\ncapture more 3D spatial context, which is beneficial for reducing the editing\nworkloads when interactively prompting on large organs. CT-SAM3D is trained\nusing a curated dataset of 1204 CT scans containing 107 whole-body anatomies\nand extensively validated using five datasets, achieving significantly better\nresults against all previous SAM-derived models. Code, data, and our 3D\ninteractive segmentation tool with quasi-real-time responses are available at\nhttps://github.com/alibaba-damo-academy/ct-sam3d.\n","authors":["Heng Guo","Jianfeng Zhang","Jiaxing Huang","Tony C. W. Mok","Dazhou Guo","Ke Yan","Le Lu","Dakai Jin","Minfeng Xu"],"pdf_url":"https://arxiv.org/pdf/2403.15063v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.13452v1","updated":"2024-12-18T02:49:20Z","published":"2024-12-18T02:49:20Z","title":"ConDo: Continual Domain Expansion for Absolute Pose Regression","summary":"  Visual localization is a fundamental machine learning problem. Absolute Pose\nRegression (APR) trains a scene-dependent model to efficiently map an input\nimage to the camera pose in a pre-defined scene. However, many applications\nhave continually changing environments, where inference data at novel poses or\nscene conditions (weather, geometry) appear after deployment. Training APR on a\nfixed dataset leads to overfitting, making it fail catastrophically on\nchallenging novel data. This work proposes Continual Domain Expansion (ConDo),\nwhich continually collects unlabeled inference data to update the deployed APR.\nInstead of applying standard unsupervised domain adaptation methods which are\nineffective for APR, ConDo effectively learns from unlabeled data by distilling\nknowledge from scene-agnostic localization methods. By sampling data uniformly\nfrom historical and newly collected data, ConDo can effectively expand the\ngeneralization domain of APR. Large-scale benchmarks with various scene types\nare constructed to evaluate models under practical (long-term) data changes.\nConDo consistently and significantly outperforms baselines across\narchitectures, scene types, and data changes. On challenging scenes (Fig.1), it\nreduces the localization error by >7x (14.8m vs 1.7m). Analysis shows the\nrobustness of ConDo against compute budgets, replay buffer sizes and teacher\nprediction noise. Comparing to model re-training, ConDo achieves similar\nperformance up to 25x faster.\n","authors":["Zijun Li","Zhipeng Cai","Bochun Yang","Xuelun Shen","Siqi Shen","Xiaoliang Fan","Michael Paulitsch","Cheng Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13452v1.pdf","comment":"AAAI2025"},{"id":"http://arxiv.org/abs/2412.06286v2","updated":"2024-12-18T02:44:01Z","published":"2024-12-09T08:16:24Z","title":"No Annotations for Object Detection in Art through Stable Diffusion","summary":"  Object detection in art is a valuable tool for the digital humanities, as it\nallows for faster identification of objects in artistic and historical images\ncompared to humans. However, annotating such images poses significant\nchallenges due to the need for specialized domain expertise. We present NADA\n(no annotations for detection in art), a pipeline that leverages diffusion\nmodels' art-related knowledge for object detection in paintings without the\nneed for full bounding box supervision. Our method, which supports both\nweakly-supervised and zero-shot scenarios and does not require any fine-tuning\nof its pretrained components, consists of a class proposer based on large\nvision-language models and a class-conditioned detector based on Stable\nDiffusion. NADA is evaluated on two artwork datasets, ArtDL 2.0 and IconArt,\noutperforming prior work in weakly-supervised detection, while being the first\nwork for zero-shot object detection in art. Code is available at\nhttps://github.com/patrick-john-ramos/nada\n","authors":["Patrick Ramos","Nicolas Gonthier","Selina Khan","Yuta Nakashima","Noa Garcia"],"pdf_url":"https://arxiv.org/pdf/2412.06286v2.pdf","comment":"8 pages, 6 figures, to be published in WACV 2025"},{"id":"http://arxiv.org/abs/2401.13270v2","updated":"2024-12-18T02:43:40Z","published":"2024-01-24T07:22:05Z","title":"Audio-Infused Automatic Image Colorization by Exploiting Audio Scene\n  Semantics","summary":"  Automatic image colorization is inherently an ill-posed problem with\nuncertainty, which requires an accurate semantic understanding of scenes to\nestimate reasonable colors for grayscale images. Although recent\ninteraction-based methods have achieved impressive performance, it is still a\nvery difficult task to infer realistic and accurate colors for automatic\ncolorization. To reduce the difficulty of semantic understanding of grayscale\nscenes, this paper tries to utilize corresponding audio, which naturally\ncontains extra semantic information about the same scene. Specifically, a novel\nand pluggable audio-infused automatic image colorization (AIAIC) method is\nproposed, which consists of three stages. First, we take color image semantics\nas a bridge and pretrain a colorization network guided by color image\nsemantics. Second, the natural co-occurrence of audio and video is utilized to\nlearn the color semantic correlations between audio and visual scenes. Third,\nthe implicit audio semantic representation is fed into the pretrained network\nto finally realize the audio-guided colorization. The whole process is trained\nin a self-supervised manner without human annotation. Experiments demonstrate\nthat audio guidance can effectively improve the performance of automatic\ncolorization, especially for some scenes that are difficult to understand only\nfrom visual modality.\n","authors":["Pengcheng Zhao","Yanxiang Chen","Yang Zhao","Zhao Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.13270v2.pdf","comment":"Accepted by ICONIP-2024"},{"id":"http://arxiv.org/abs/2412.13443v1","updated":"2024-12-18T02:31:37Z","published":"2024-12-18T02:31:37Z","title":"DarkIR: Robust Low-Light Image Restoration","summary":"  Photography during night or in dark conditions typically suffers from noise,\nlow light and blurring issues due to the dim environment and the common use of\nlong exposure. Although Deblurring and Low-light Image Enhancement (LLIE) are\nrelated under these conditions, most approaches in image restoration solve\nthese tasks separately. In this paper, we present an efficient and robust\nneural network for multi-task low-light image restoration. Instead of following\nthe current tendency of Transformer-based models, we propose new attention\nmechanisms to enhance the receptive field of efficient CNNs. Our method reduces\nthe computational costs in terms of parameters and MAC operations compared to\nprevious methods. Our model, DarkIR, achieves new state-of-the-art results on\nthe popular LOLBlur, LOLv2 and Real-LOLBlur datasets, being able to generalize\non real-world night and dark images. Code and models at\nhttps://github.com/cidautai/DarkIR\n","authors":["Daniel Feijoo","Juan C. Benito","Alvaro Garcia","Marcos V. Conde"],"pdf_url":"https://arxiv.org/pdf/2412.13443v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2412.10967v2","updated":"2024-12-18T02:26:46Z","published":"2024-12-14T20:55:31Z","title":"Biological and Radiological Dictionary of Radiomics Features: Addressing\n  Understandable AI Issues in Personalized Prostate Cancer; Dictionary Version\n  PM1.0","summary":"  We investigate the connection between visual semantic features defined in\nPI-RADS and associated risk factors, moving beyond abnormal imaging findings,\nestablishing a shared framework between medical and AI professionals by\ncreating a standardized dictionary of biological/radiological RFs.\nSubsequently, 6 interpretable and seven complex classifiers, linked with nine\ninterpretable feature selection algorithms (FSA) applied to risk factors, were\nextracted from segmented lesions in T2-weighted imaging (T2WI),\ndiffusion-weighted imaging (DWI), and apparent diffusion coefficient (ADC)\nmultiparametric-prostate MRI sequences to predict the UCLA scores. We then\nutilized the created dictionary to interpret the best-predictive models.\nCombining T2WI, DWI, and ADC with FSAs including ANOVA F-test, Correlation\nCoefficient, and Fisher Score, and utilizing logistic regression, identified\nkey features: The 90th percentile from T2WI, which captures hypo-intensity\nrelated to prostate cancer risk; Variance from T2WI, indicating lesion\nheterogeneity; shape metrics including Least Axis Length and Surface Area to\nVolume ratio from ADC, describing lesion shape and compactness; and Run Entropy\nfrom ADC, reflecting texture consistency. This approach achieved the highest\naverage accuracy of 0.78, significantly outperforming single-sequence methods\n(p-value<0.05). The developed dictionary for Prostate-MRI (PM1.0) serves as a\ncommon language, fosters collaboration between clinical professionals and AI\ndevelopers to advance trustworthy AI solutions that support\nreliable/interpretable clinical decisions.\n","authors":["Mohammad R. Salmanpour","Sajad Amiri","Sara Gharibi","Ahmad Shariftabrizi","Yixi Xu","William B Weeks","Arman Rahmim","Ilker Hacihaliloglu"],"pdf_url":"https://arxiv.org/pdf/2412.10967v2.pdf","comment":"24 pages, 3 Figures, 2 Tables"},{"id":"http://arxiv.org/abs/2412.13441v1","updated":"2024-12-18T02:23:33Z","published":"2024-12-18T02:23:33Z","title":"FlashVTG: Feature Layering and Adaptive Score Handling Network for Video\n  Temporal Grounding","summary":"  Text-guided Video Temporal Grounding (VTG) aims to localize relevant segments\nin untrimmed videos based on textual descriptions, encompassing two subtasks:\nMoment Retrieval (MR) and Highlight Detection (HD). Although previous typical\nmethods have achieved commendable results, it is still challenging to retrieve\nshort video moments. This is primarily due to the reliance on sparse and\nlimited decoder queries, which significantly constrain the accuracy of\npredictions. Furthermore, suboptimal outcomes often arise because previous\nmethods rank predictions based on isolated predictions, neglecting the broader\nvideo context. To tackle these issues, we introduce FlashVTG, a framework\nfeaturing a Temporal Feature Layering (TFL) module and an Adaptive Score\nRefinement (ASR) module. The TFL module replaces the traditional decoder\nstructure to capture nuanced video content variations across multiple temporal\nscales, while the ASR module improves prediction ranking by integrating context\nfrom adjacent moments and multi-temporal-scale features. Extensive experiments\ndemonstrate that FlashVTG achieves state-of-the-art performance on four widely\nadopted datasets in both MR and HD. Specifically, on the QVHighlights dataset,\nit boosts mAP by 5.8% for MR and 3.3% for HD. For short-moment retrieval,\nFlashVTG increases mAP to 125% of previous SOTA performance. All these\nimprovements are made without adding training burdens, underscoring its\neffectiveness. Our code is available at https://github.com/Zhuo-Cao/FlashVTG.\n","authors":["Zhuo Cao","Bingqing Zhang","Heming Du","Xin Yu","Xue Li","Sen Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13441v1.pdf","comment":"Accepted to WACV 2025"},{"id":"http://arxiv.org/abs/2407.19708v3","updated":"2024-12-18T02:10:48Z","published":"2024-07-29T05:19:23Z","title":"ALEN: A Dual-Approach for Uniform and Non-Uniform Low-Light Image\n  Enhancement","summary":"  Low-light image enhancement is an important task in computer vision,\nessential for improving the visibility and quality of images captured in\nnon-optimal lighting conditions. Inadequate illumination can lead to\nsignificant information loss and poor image quality, impacting various\napplications such as surveillance. photography, or even autonomous driving. In\nthis regard, automated methods have been developed to automatically adjust\nillumination in the image for a better visual perception. Current enhancement\ntechniques often use specific datasets to enhance low-light images, but still\npresent challenges when adapting to diverse real-world conditions, where\nillumination degradation may be localized to specific regions. To address this\nchallenge, the Adaptive Light Enhancement Network (ALEN) is introduced, whose\nmain approach is the use of a classification mechanism to determine whether\nlocal or global illumination enhancement is required. Subsequently, estimator\nnetworks adjust illumination based on this classification and simultaneously\nenhance color fidelity. ALEN integrates the Light Classification Network\n(LCNet) for illuminance categorization, complemented by the Single-Channel\nNetwork (SCNet), and Multi-Channel Network (MCNet) for precise estimation of\nillumination and color, respectively. Extensive experiments on publicly\navailable datasets for low-light conditions were carried out to underscore\nALEN's robust generalization capabilities, demonstrating superior performance\nin both quantitative metrics and qualitative assessments when compared to\nrecent state-of-the-art methods. The ALEN not only enhances image quality in\nterms of visual perception but also represents an advancement in high-level\nvision tasks, such as semantic segmentation, as presented in this work. The\ncode of this method is available at https://github.com/xingyumex/ALEN\n","authors":["Ezequiel Perez-Zarate","Oscar Ramos-Soto","Chunxiao Liu","Diego Oliva","Marco Perez-Cisneros"],"pdf_url":"https://arxiv.org/pdf/2407.19708v3.pdf","comment":"Minor updates and corrections"},{"id":"http://arxiv.org/abs/2304.12921v2","updated":"2024-12-18T02:08:15Z","published":"2023-04-24T03:09:25Z","title":"AwesomeMeta+: Bridging the Technical Barriers to Meta-Learning via A\n  Prototyping and Learning System","summary":"  Meta-learning, also known as \"learning to learn\", enables models to acquire\ngreat generalization abilities by learning from various tasks. Recent\nadvancements have made these models applicable across various fields without\ndata constraints, offering new opportunities for general artificial\nintelligence. However, applying these models can be challenging due to their\noften task-specific, standalone nature and the technical barriers involved. To\naddress this challenge, we develop AwesomeMeta+, a prototyping and learning\nsystem that standardizes different components of meta-learning and uses a\nbuilding block metaphor to assist in model construction. AwesomeMeta+ allows\nusers to assemble compatible algorithm modules to meet the application needs in\npractice. To optimize AwesomeMeta+, we provide the interface to 50 researchers\nand refine the design based on their feedback. Through machine-based testing\nand user studies, we demonstrate that AwesomeMeta+ enhances users'\nunderstanding of the related technologies and accelerates their engineering\nprocesses by offering guidance for meta-learning deployments.\n","authors":["Jingyao Wang","Yuxuan Yang","Wenwen Qiang","Changwen Zheng"],"pdf_url":"https://arxiv.org/pdf/2304.12921v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.01284v2","updated":"2024-12-18T01:56:53Z","published":"2024-12-02T08:56:13Z","title":"MFTF: Mask-free Training-free Object Level Layout Control Diffusion\n  Model","summary":"  Text-to-image generation models have revolutionized content creation, but\ndiffusion-based vision-language models still face challenges in precisely\ncontrolling the shape, appearance, and positional placement of objects in\ngenerated images using text guidance alone. Existing global image editing\nmodels rely on additional masks or images as guidance to achieve layout\ncontrol, often requiring retraining of the model. While local object-editing\nmodels allow modifications to object shapes, they lack the capability to\ncontrol object positions. To address these limitations, we propose the\nMask-free Training-free Object-Level Layout Control Diffusion Model (MFTF),\nwhich provides precise control over object positions without requiring\nadditional masks or images. The MFTF model supports both single-object and\nmulti-object positional adjustments, such as translation and rotation, while\nenabling simultaneous layout control and object semantic editing. The MFTF\nmodel employs a parallel denoising process for both the source and target\ndiffusion models. During this process, attention masks are dynamically\ngenerated from the cross-attention layers of the source diffusion model and\napplied to queries from the self-attention layers to isolate objects. These\nqueries, generated in the source diffusion model, are then adjusted according\nto the layout control parameters and re-injected into the self-attention layers\nof the target diffusion model. This approach ensures accurate and precise\npositional control of objects. Project source code available at\nhttps://github.com/syang-genai/MFTF.\n","authors":["Shan Yang"],"pdf_url":"https://arxiv.org/pdf/2412.01284v2.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2412.11067v3","updated":"2024-12-18T01:55:12Z","published":"2024-12-15T05:57:36Z","title":"CFSynthesis: Controllable and Free-view 3D Human Video Synthesis","summary":"  Human video synthesis aims to create lifelike characters in various\nenvironments, with wide applications in VR, storytelling, and content creation.\nWhile 2D diffusion-based methods have made significant progress, they struggle\nto generalize to complex 3D poses and varying scene backgrounds. To address\nthese limitations, we introduce CFSynthesis, a novel framework for generating\nhigh-quality human videos with customizable attributes, including identity,\nmotion, and scene configurations. Our method leverages a texture-SMPL-based\nrepresentation to ensure consistent and stable character appearances across\nfree viewpoints. Additionally, we introduce a novel foreground-background\nseparation strategy that effectively decomposes the scene as foreground and\nbackground, enabling seamless integration of user-defined backgrounds.\nExperimental results on multiple datasets show that CFSynthesis not only\nachieves state-of-the-art performance in complex human animations but also\nadapts effectively to 3D motions in free-view and user-specified scenarios.\n","authors":["Liyuan Cui","Xiaogang Xu","Wenqi Dong","Zesong Yang","Hujun Bao","Zhaopeng Cui"],"pdf_url":"https://arxiv.org/pdf/2412.11067v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13419v1","updated":"2024-12-18T01:31:08Z","published":"2024-12-18T01:31:08Z","title":"Exploring Transformer-Augmented LSTM for Temporal and Spatial Feature\n  Learning in Trajectory Prediction","summary":"  Accurate vehicle trajectory prediction is crucial for ensuring safe and\nefficient autonomous driving. This work explores the integration of Transformer\nbased model with Long Short-Term Memory (LSTM) based technique to enhance\nspatial and temporal feature learning in vehicle trajectory prediction. Here, a\nhybrid model that combines LSTMs for temporal encoding with a Transformer\nencoder for capturing complex interactions between vehicles is proposed.\nSpatial trajectory features of the neighboring vehicles are processed and goes\nthrough a masked scatter mechanism in a grid based environment, which is then\ncombined with temporal trajectory of the vehicles. This combined trajectory\ndata are learned by sequential LSTM encoding and Transformer based attention\nlayers. The proposed model is benchmarked against predecessor LSTM based\nmethods, including STA-LSTM, SA-LSTM, CS-LSTM, and NaiveLSTM. Our results,\nwhile not outperforming it's predecessor, demonstrate the potential of\nintegrating Transformers with LSTM based technique to build interpretable\ntrajectory prediction model. Future work will explore alternative architectures\nusing Transformer applications to further enhance performance. This study\nprovides a promising direction for improving trajectory prediction models by\nleveraging transformer based architectures, paving the way for more robust and\ninterpretable vehicle trajectory prediction system.\n","authors":["Chandra Raskoti","Weizi Li"],"pdf_url":"https://arxiv.org/pdf/2412.13419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11917v2","updated":"2024-12-18T01:15:42Z","published":"2024-12-16T16:01:18Z","title":"Does VLM Classification Benefit from LLM Description Semantics?","summary":"  Accurately describing images with text is a foundation of explainable AI.\nVision-Language Models (VLMs) like CLIP have recently addressed this by\naligning images and texts in a shared embedding space, expressing semantic\nsimilarities between vision and language embeddings. VLM classification can be\nimproved with descriptions generated by Large Language Models (LLMs). However,\nit is difficult to determine the contribution of actual description semantics,\nas the performance gain may also stem from a semantic-agnostic ensembling\neffect, where multiple modified text prompts act as a noisy test-time\naugmentation for the original one. We propose an alternative evaluation\nscenario to decide if a performance boost of LLM-generated descriptions is\ncaused by such a noise augmentation effect or rather by genuine description\nsemantics. The proposed scenario avoids noisy test-time augmentation and\nensures that genuine, distinctive descriptions cause the performance boost.\nFurthermore, we propose a training-free method for selecting discriminative\ndescriptions that work independently of classname-ensembling effects. Our\napproach identifies descriptions that effectively differentiate classes within\na local CLIP label neighborhood, improving classification accuracy across seven\ndatasets. Additionally, we provide insights into the explainability of\ndescription-based image classification with VLMs.\n","authors":["Pingchuan Ma","Lennart Rietdorf","Dmytro Kotovenko","Vincent Tao Hu","Björn Ommer"],"pdf_url":"https://arxiv.org/pdf/2412.11917v2.pdf","comment":"AAAI-25, Code: https://github.com/CompVis/DisCLIP"},{"id":"http://arxiv.org/abs/2412.13401v1","updated":"2024-12-18T00:31:18Z","published":"2024-12-18T00:31:18Z","title":"Zero-Shot Low Light Image Enhancement with Diffusion Prior","summary":"  Balancing aesthetic quality with fidelity when enhancing images from\nchallenging, degraded sources is a core objective in computational photography.\nIn this paper, we address low light image enhancement (LLIE), a task in which\ndark images often contain limited visible information. Diffusion models, known\nfor their powerful image enhancement capacities, are a natural choice for this\nproblem. However, their deep generative priors can also lead to hallucinations,\nintroducing non-existent elements or substantially altering the visual\nsemantics of the original scene. In this work, we introduce a novel zero-shot\nmethod for controlling and refining the generative behavior of diffusion models\nfor dark-to-light image conversion tasks. Our method demonstrates superior\nperformance over existing state-of-the-art methods in the task of low-light\nimage enhancement, as evidenced by both quantitative metrics and qualitative\nanalysis.\n","authors":["Joshua Cho","Sara Aghajanzadeh","Zhen Zhu","D. A. Forsyth"],"pdf_url":"https://arxiv.org/pdf/2412.13401v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01521v2","updated":"2024-12-18T00:26:39Z","published":"2024-07-01T17:59:23Z","title":"Improving Diffusion Inverse Problem Solving with Decoupled Noise\n  Annealing","summary":"  Diffusion models have recently achieved success in solving Bayesian inverse\nproblems with learned data priors. Current methods build on top of the\ndiffusion sampling process, where each denoising step makes small modifications\nto samples from the previous step. However, this process struggles to correct\nerrors from earlier sampling steps, leading to worse performance in complicated\nnonlinear inverse problems, such as phase retrieval. To address this challenge,\nwe propose a new method called Decoupled Annealing Posterior Sampling (DAPS)\nthat relies on a novel noise annealing process. Specifically, we decouple\nconsecutive steps in a diffusion sampling trajectory, allowing them to vary\nconsiderably from one another while ensuring their time-marginals anneal to the\ntrue posterior as we reduce noise levels. This approach enables the exploration\nof a larger solution space, improving the success rate for accurate\nreconstructions. We demonstrate that DAPS significantly improves sample quality\nand stability across multiple image restoration tasks, particularly in\ncomplicated nonlinear inverse problems.\n","authors":["Bingliang Zhang","Wenda Chu","Julius Berner","Chenlin Meng","Anima Anandkumar","Yang Song"],"pdf_url":"https://arxiv.org/pdf/2407.01521v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13394v1","updated":"2024-12-18T00:10:44Z","published":"2024-12-18T00:10:44Z","title":"Distribution Shifts at Scale: Out-of-distribution Detection in Earth\n  Observation","summary":"  Training robust deep learning models is critical in Earth Observation, where\nglobally deployed models often face distribution shifts that degrade\nperformance, especially in low-data regions. Out-of-distribution (OOD)\ndetection addresses this challenge by identifying inputs that differ from\nin-distribution (ID) data. However, existing methods either assume access to\nOOD data or compromise primary task performance, making them unsuitable for\nreal-world deployment. We propose TARDIS, a post-hoc OOD detection method for\nscalable geospatial deployments. The core novelty lies in generating surrogate\nlabels by integrating information from ID data and unknown distributions,\nenabling OOD detection at scale. Our method takes a pre-trained model, ID data,\nand WILD samples, disentangling the latter into surrogate ID and surrogate OOD\nlabels based on internal activations, and fits a binary classifier as an OOD\ndetector. We validate TARDIS on EuroSAT and xBD datasets, across 17\nexperimental setups covering covariate and semantic shifts, showing that it\nperforms close to the theoretical upper bound in assigning surrogate ID and OOD\nsamples in 13 cases. To demonstrate scalability, we deploy TARDIS on the Fields\nof the World dataset, offering actionable insights into pre-trained model\nbehavior for large-scale deployments. The code is publicly available at\nhttps://github.com/microsoft/geospatial-ood-detection.\n","authors":["Burak Ekim","Girmaw Abebe Tadesse","Caleb Robinson","Gilles Hacheme","Michael Schmitt","Rahul Dodhia","Juan M. Lavista Ferres"],"pdf_url":"https://arxiv.org/pdf/2412.13394v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13393v1","updated":"2024-12-18T00:10:00Z","published":"2024-12-18T00:10:00Z","title":"MMHMR: Generative Masked Modeling for Hand Mesh Recovery","summary":"  Reconstructing a 3D hand mesh from a single RGB image is challenging due to\ncomplex articulations, self-occlusions, and depth ambiguities. Traditional\ndiscriminative methods, which learn a deterministic mapping from a 2D image to\na single 3D mesh, often struggle with the inherent ambiguities in 2D-to-3D\nmapping. To address this challenge, we propose MMHMR, a novel generative masked\nmodel for hand mesh recovery that synthesizes plausible 3D hand meshes by\nlearning and sampling from the probabilistic distribution of the ambiguous\n2D-to-3D mapping process. MMHMR consists of two key components: (1) a VQ-MANO,\nwhich encodes 3D hand articulations as discrete pose tokens in a latent space,\nand (2) a Context-Guided Masked Transformer that randomly masks out pose tokens\nand learns their joint distribution, conditioned on corrupted token sequences,\nimage context, and 2D pose cues. This learned distribution facilitates\nconfidence-guided sampling during inference, producing mesh reconstructions\nwith low uncertainty and high precision. Extensive evaluations on benchmark and\nreal-world datasets demonstrate that MMHMR achieves state-of-the-art accuracy,\nrobustness, and realism in 3D hand mesh reconstruction. Project website:\nhttps://m-usamasaleem.github.io/publication/MMHMR/mmhmr.html\n","authors":["Muhammad Usama Saleem","Ekkasit Pinyoanuntapong","Mayur Jagdishbhai Patel","Hongfei Xue","Ahmed Helmy","Srijan Das","Pu Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13393v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13389v1","updated":"2024-12-18T00:06:41Z","published":"2024-12-18T00:06:41Z","title":"Marigold-DC: Zero-Shot Monocular Depth Completion with Guided Diffusion","summary":"  Depth completion upgrades sparse depth measurements into dense depth maps\nguided by a conventional image. Existing methods for this highly ill-posed task\noperate in tightly constrained settings and tend to struggle when applied to\nimages outside the training domain or when the available depth measurements are\nsparse, irregularly distributed, or of varying density. Inspired by recent\nadvances in monocular depth estimation, we reframe depth completion as an\nimage-conditional depth map generation guided by sparse measurements. Our\nmethod, Marigold-DC, builds on a pretrained latent diffusion model for\nmonocular depth estimation and injects the depth observations as test-time\nguidance via an optimization scheme that runs in tandem with the iterative\ninference of denoising diffusion. The method exhibits excellent zero-shot\ngeneralization across a diverse range of environments and handles even\nextremely sparse guidance effectively. Our results suggest that contemporary\nmonocular depth priors greatly robustify depth completion: it may be better to\nview the task as recovering dense depth from (dense) image pixels, guided by\nsparse depth; rather than as inpainting (sparse) depth, guided by an image.\nProject website: https://MarigoldDepthCompletion.github.io/\n","authors":["Massimiliano Viola","Kevin Qu","Nando Metzger","Bingxin Ke","Alexander Becker","Konrad Schindler","Anton Obukhov"],"pdf_url":"https://arxiv.org/pdf/2412.13389v1.pdf","comment":null}],"Robotics":[{"id":"http://arxiv.org/abs/2412.14172v1","updated":"2024-12-18T18:59:56Z","published":"2024-12-18T18:59:56Z","title":"Learning from Massive Human Videos for Universal Humanoid Pose Control","summary":"  Scalable learning of humanoid robots is crucial for their deployment in\nreal-world applications. While traditional approaches primarily rely on\nreinforcement learning or teleoperation to achieve whole-body control, they are\noften limited by the diversity of simulated environments and the high costs of\ndemonstration collection. In contrast, human videos are ubiquitous and present\nan untapped source of semantic and motion information that could significantly\nenhance the generalization capabilities of humanoid robots. This paper\nintroduces Humanoid-X, a large-scale dataset of over 20 million humanoid robot\nposes with corresponding text-based motion descriptions, designed to leverage\nthis abundant data. Humanoid-X is curated through a comprehensive pipeline:\ndata mining from the Internet, video caption generation, motion retargeting of\nhumans to humanoid robots, and policy learning for real-world deployment. With\nHumanoid-X, we further train a large humanoid model, UH-1, which takes text\ninstructions as input and outputs corresponding actions to control a humanoid\nrobot. Extensive simulated and real-world experiments validate that our\nscalable training approach leads to superior generalization in text-based\nhumanoid control, marking a significant step toward adaptable, real-world-ready\nhumanoid robots.\n","authors":["Jiageng Mao","Siheng Zhao","Siqi Song","Tianheng Shi","Junjie Ye","Mingtong Zhang","Haoran Geng","Jitendra Malik","Vitor Guizilini","Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14172v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14111v1","updated":"2024-12-18T17:58:16Z","published":"2024-12-18T17:58:16Z","title":"Event-based Photometric Bundle Adjustment","summary":"  We tackle the problem of bundle adjustment (i.e., simultaneous refinement of\ncamera poses and scene map) for a purely rotating event camera. Starting from\nfirst principles, we formulate the problem as a classical non-linear least\nsquares optimization. The photometric error is defined using the event\ngeneration model directly in the camera rotations and the semi-dense scene\nbrightness that triggers the events. We leverage the sparsity of event data to\ndesign a tractable Levenberg-Marquardt solver that handles the very large\nnumber of variables involved. To the best of our knowledge, our method, which\nwe call Event-based Photometric Bundle Adjustment (EPBA), is the first\nevent-only photometric bundle adjustment method that works on the brightness\nmap directly and exploits the space-time characteristics of event data, without\nhaving to convert events into image-like representations. Comprehensive\nexperiments on both synthetic and real-world datasets demonstrate EPBA's\neffectiveness in decreasing the photometric error (by up to 90%), yielding\nresults of unparalleled quality. The refined maps reveal details that were\nhidden using prior state-of-the-art rotation-only estimation methods. The\nexperiments on modern high-resolution event cameras show the applicability of\nEPBA to panoramic imaging in various scenarios (without map initialization, at\nmultiple resolutions, and in combination with other methods, such as IMU dead\nreckoning or previous event-based rotation estimation methods). We make the\nsource code publicly available. https://github.com/tub-rip/epba\n","authors":["Shuang Guo","Guillermo Gallego"],"pdf_url":"https://arxiv.org/pdf/2412.14111v1.pdf","comment":"21 pages, 19 figures, 10 tables. Project page:\n  https://github.com/tub-rip/epba"},{"id":"http://arxiv.org/abs/2412.14088v1","updated":"2024-12-18T17:34:52Z","published":"2024-12-18T17:34:52Z","title":"Joint Perception and Prediction for Autonomous Driving: A Survey","summary":"  Perception and prediction modules are critical components of autonomous\ndriving systems, enabling vehicles to navigate safely through complex\nenvironments. The perception module is responsible for perceiving the\nenvironment, including static and dynamic objects, while the prediction module\nis responsible for predicting the future behavior of these objects. These\nmodules are typically divided into three tasks: object detection, object\ntracking, and motion prediction. Traditionally, these tasks are developed and\noptimized independently, with outputs passed sequentially from one to the next.\nHowever, this approach has significant limitations: computational resources are\nnot shared across tasks, the lack of joint optimization can amplify errors as\nthey propagate throughout the pipeline, and uncertainty is rarely propagated\nbetween modules, resulting in significant information loss. To address these\nchallenges, the joint perception and prediction paradigm has emerged,\nintegrating perception and prediction into a unified model through multi-task\nlearning. This strategy not only overcomes the limitations of previous methods,\nbut also enables the three tasks to have direct access to raw sensor data,\nallowing richer and more nuanced environmental interpretations. This paper\npresents the first comprehensive survey of joint perception and prediction for\nautonomous driving. We propose a taxonomy that categorizes approaches based on\ninput representation, scene context modeling, and output representation,\nhighlighting their contributions and limitations. Additionally, we present a\nqualitative analysis and quantitative comparison of existing methods. Finally,\nwe discuss future research directions based on identified gaps in the\nstate-of-the-art.\n","authors":["Lucas Dal'Col","Miguel Oliveira","Vítor Santos"],"pdf_url":"https://arxiv.org/pdf/2412.14088v1.pdf","comment":"24 pages, 5 sections, 7 figures, 7 tables. This work has been\n  submitted to the IEEE Transactions on Intelligent Transportation Systems for\n  possible publication"},{"id":"http://arxiv.org/abs/2412.14058v1","updated":"2024-12-18T17:07:20Z","published":"2024-12-18T17:07:20Z","title":"Towards Generalist Robot Policies: What Matters in Building\n  Vision-Language-Action Models","summary":"  Foundation Vision Language Models (VLMs) exhibit strong capabilities in\nmulti-modal representation learning, comprehension, and reasoning. By injecting\naction components into the VLMs, Vision-Language-Action Models (VLAs) can be\nnaturally formed and also show promising performance. Existing work has\ndemonstrated the effectiveness and generalization of VLAs in multiple scenarios\nand tasks. Nevertheless, the transfer from VLMs to VLAs is not trivial since\nexisting VLAs differ in their backbones, action-prediction formulations, data\ndistributions, and training recipes. This leads to a missing piece for a\nsystematic understanding of the design choices of VLAs. In this work, we\ndisclose the key factors that significantly influence the performance of VLA\nand focus on answering three essential design choices: which backbone to\nselect, how to formulate the VLA architectures, and when to add\ncross-embodiment data. The obtained results convince us firmly to explain why\nwe need VLA and develop a new family of VLAs, RoboVLMs, which require very few\nmanual designs and achieve a new state-of-the-art performance in three\nsimulation tasks and real-world experiments. Through our extensive experiments,\nwhich include over 8 VLM backbones, 4 policy architectures, and over 600\ndistinct designed experiments, we provide a detailed guidebook for the future\ndesign of VLAs. In addition to the study, the highly flexible RoboVLMs\nframework, which supports easy integrations of new VLMs and free combinations\nof various design choices, is made public to facilitate future research. We\nopen-source all details, including codes, models, datasets, and toolkits, along\nwith detailed training and evaluation recipes at: robovlms.github.io.\n","authors":["Xinghang Li","Peiyan Li","Minghuan Liu","Dong Wang","Jirong Liu","Bingyi Kang","Xiao Ma","Tao Kong","Hanbo Zhang","Huaping Liu"],"pdf_url":"https://arxiv.org/pdf/2412.14058v1.pdf","comment":"Project page: robovlms.github.io"},{"id":"http://arxiv.org/abs/2402.10088v3","updated":"2024-12-18T17:05:38Z","published":"2024-02-01T15:15:25Z","title":"Deep hybrid models: infer and plan in a dynamic world","summary":"  In order to determine an optimal plan for a complex task, one often deals\nwith dynamic and hierarchical relationships between several entities.\nTraditionally, such problems are tackled with optimal control, which relies on\nthe optimization of cost functions; instead, a recent biologically-motivated\nproposal casts planning and control as an inference process. Active inference\nassumes that action and perception are two complementary aspects of life\nwhereby the role of the former is to fulfill the predictions inferred by the\nlatter. In this study, we present a solution, based on active inference, for\ncomplex control tasks. The proposed architecture exploits hybrid (discrete and\ncontinuous) processing, and it is based on three features: the representation\nof potential body configurations related to the objects of interest; the use of\nhierarchical relationships that enable the agent to flexibly expand its body\nschema for tool use; the definition of potential trajectories related to the\nagent's intentions, used to infer and plan with dynamic elements at different\ntemporal scales. We evaluate this deep hybrid model on a habitual task:\nreaching a moving object after having picked a moving tool. We show that the\nmodel can tackle the presented task under different conditions. This study\nextends past work on planning as inference and advances an alternative\ndirection to optimal control.\n","authors":["Matteo Priorelli","Ivilin Peev Stoianov"],"pdf_url":"https://arxiv.org/pdf/2402.10088v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14018v1","updated":"2024-12-18T16:34:51Z","published":"2024-12-18T16:34:51Z","title":"SurgSora: Decoupled RGBD-Flow Diffusion Model for Controllable Surgical\n  Video Generation","summary":"  Medical video generation has transformative potential for enhancing surgical\nunderstanding and pathology insights through precise and controllable visual\nrepresentations. However, current models face limitations in controllability\nand authenticity. To bridge this gap, we propose SurgSora, a\nmotion-controllable surgical video generation framework that uses a single\ninput frame and user-controllable motion cues. SurgSora consists of three key\nmodules: the Dual Semantic Injector (DSI), which extracts object-relevant RGB\nand depth features from the input frame and integrates them with segmentation\ncues to capture detailed spatial features of complex anatomical structures; the\nDecoupled Flow Mapper (DFM), which fuses optical flow with semantic-RGB-D\nfeatures at multiple scales to enhance temporal understanding and object\nspatial dynamics; and the Trajectory Controller (TC), which allows users to\nspecify motion directions and estimates sparse optical flow, guiding the video\ngeneration process. The fused features are used as conditions for a frozen\nStable Diffusion model to produce realistic, temporally coherent surgical\nvideos. Extensive evaluations demonstrate that SurgSora outperforms\nstate-of-the-art methods in controllability and authenticity, showing its\npotential to advance surgical video generation for medical education, training,\nand research.\n","authors":["Tong Chen","Shuya Yang","Junyi Wang","Long Bai","Hongliang Ren","Luping Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.14018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13912v1","updated":"2024-12-18T14:53:10Z","published":"2024-12-18T14:53:10Z","title":"Energy-Efficient SLAM via Joint Design of Sensing, Communication, and\n  Exploration Speed","summary":"  To support future spatial machine intelligence applications, lifelong\nsimultaneous localization and mapping (SLAM) has drawn significant attentions.\nSLAM is usually realized based on various types of mobile robots performing\nsimultaneous and continuous sensing and communication. This paper focuses on\nanalyzing the energy efficiency of robot operation for lifelong SLAM by jointly\nconsidering sensing, communication and mechanical factors. The system model is\nbuilt based on a robot equipped with a 2D light detection and ranging (LiDAR)\nand an odometry. The cloud point raw data as well as the odometry data are\nwirelessly transmitted to data center where real-time map reconstruction is\nrealized based on an unsupervised deep learning based method. The sensing\nduration, transmit power, transmit duration and exploration speed are jointly\noptimized to minimize the energy consumption. Simulations and experiments\ndemonstrate the performance of our proposed method.\n","authors":["Zidong Han","Ruibo Jin","Xiaoyang Li","Bingpeng Zhou","Qinyu Zhang","Yi Gong"],"pdf_url":"https://arxiv.org/pdf/2412.13912v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13877v1","updated":"2024-12-18T14:17:16Z","published":"2024-12-18T14:17:16Z","title":"RoboMIND: Benchmark on Multi-embodiment Intelligence Normative Data for\n  Robot Manipulation","summary":"  Developing robust and general-purpose robotic manipulation policies is a key\ngoal in the field of robotics. To achieve effective generalization, it is\nessential to construct comprehensive datasets that encompass a large number of\ndemonstration trajectories and diverse tasks. Unlike vision or language data\nthat can be collected from the Internet, robotic datasets require detailed\nobservations and manipulation actions, necessitating significant investment in\nhardware-software infrastructure and human labor. While existing works have\nfocused on assembling various individual robot datasets, there remains a lack\nof a unified data collection standard and insufficient diversity in tasks,\nscenarios, and robot types. In this paper, we introduce RoboMIND\n(Multi-embodiment Intelligence Normative Data for Robot manipulation),\nfeaturing 55k real-world demonstration trajectories across 279 diverse tasks\ninvolving 61 different object classes. RoboMIND is collected through human\nteleoperation and encompasses comprehensive robotic-related information,\nincluding multi-view RGB-D images, proprioceptive robot state information, end\neffector details, and linguistic task descriptions. To ensure dataset\nconsistency and reliability during policy learning, RoboMIND is built on a\nunified data collection platform and standardized protocol, covering four\ndistinct robotic embodiments. We provide a thorough quantitative and\nqualitative analysis of RoboMIND across multiple dimensions, offering detailed\ninsights into the diversity of our datasets. In our experiments, we conduct\nextensive real-world testing with four state-of-the-art imitation learning\nmethods, demonstrating that training with RoboMIND data results in a high\nmanipulation success rate and strong generalization. Our project is at\nhttps://x-humanoid-robomind.github.io/.\n","authors":["Kun Wu","Chengkai Hou","Jiaming Liu","Zhengping Che","Xiaozhu Ju","Zhuqin Yang","Meng Li","Yinuo Zhao","Zhiyuan Xu","Guang Yang","Zhen Zhao","Guangyu Li","Zhao Jin","Lecheng Wang","Jilei Mao","Xinhua Wang","Shichao Fan","Ning Liu","Pei Ren","Qiang Zhang","Yaoxu Lyu","Mengzhen Liu","Jingyang He","Yulin Luo","Zeyu Gao","Chenxuan Li","Chenyang Gu","Yankai Fu","Di Wu","Xingyu Wang","Sixiang Chen","Zhenyu Wang","Pengju An","Siyuan Qian","Shanghang Zhang","Jian Tang"],"pdf_url":"https://arxiv.org/pdf/2412.13877v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13873v1","updated":"2024-12-18T14:12:11Z","published":"2024-12-18T14:12:11Z","title":"UA-MPC: Uncertainty-Aware Model Predictive Control for Motorized LiDAR\n  Odometry","summary":"  Accurate and comprehensive 3D sensing using LiDAR systems is crucial for\nvarious applications in photogrammetry and robotics, including facility\ninspection, Building Information Modeling (BIM), and robot navigation.\nMotorized LiDAR systems can expand the Field of View (FoV) without adding\nmultiple scanners, but existing motorized LiDAR systems often rely on\nconstant-speed motor control, leading to suboptimal performance in complex\nenvironments. To address this, we propose UA-MPC, an uncertainty-aware motor\ncontrol strategy that balances scanning accuracy and efficiency. By predicting\ndiscrete observabilities of LiDAR Odometry (LO) through ray tracing and\nmodeling their distribution with a surrogate function, UA-MPC efficiently\noptimizes motor speed control according to different scenes. Additionally, we\ndevelop a ROS-based realistic simulation environment for motorized LiDAR\nsystems, enabling the evaluation of control strategies across diverse\nscenarios. Extensive experiments, conducted on both simulated and real-world\nscenarios, demonstrate that our method significantly improves odometry accuracy\nwhile preserving the scanning efficiency of motorized LiDAR systems.\nSpecifically, it achieves over a 60\\% reduction in positioning error with less\nthan a 2\\% decrease in efficiency compared to constant-speed control, offering\na smarter and more effective solution for active 3D sensing tasks. The\nsimulation environment for control motorized LiDAR is open-sourced at:\n\\url{https://github.com/kafeiyin00/UA-MPC.git}.\n","authors":["Jianping Li","Xinhang Xu","Jinxin Liu","Kun Cao","Shenghai Yuan","Lihua Xie"],"pdf_url":"https://arxiv.org/pdf/2412.13873v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13178v2","updated":"2024-12-18T14:00:02Z","published":"2024-12-17T18:55:58Z","title":"SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM\n  Agents","summary":"  With the integration of large language models (LLMs), embodied agents have\nstrong capabilities to execute complicated instructions in natural language,\npaving a way for the potential deployment of embodied robots. However, a\nforeseeable issue is that those embodied agents can also flawlessly execute\nsome hazardous tasks, potentially causing damages in real world. To study this\nissue, we present SafeAgentBench -- a new benchmark for safety-aware task\nplanning of embodied LLM agents. SafeAgentBench includes: (1) a new dataset\nwith 750 tasks, covering 10 potential hazards and 3 task types; (2)\nSafeAgentEnv, a universal embodied environment with a low-level controller,\nsupporting multi-agent execution with 17 high-level actions for 8\nstate-of-the-art baselines; and (3) reliable evaluation methods from both\nexecution and semantic perspectives. Experimental results show that the\nbest-performing baseline gets 69% success rate for safe tasks, but only 5%\nrejection rate for hazardous tasks, indicating significant safety risks. More\ndetails and codes are available at\nhttps://github.com/shengyin1224/SafeAgentBench.\n","authors":["Sheng Yin","Xianghe Pang","Yuanzhuo Ding","Menglan Chen","Yutong Bi","Yichen Xiong","Wenhao Huang","Zhen Xiang","Jing Shao","Siheng Chen"],"pdf_url":"https://arxiv.org/pdf/2412.13178v2.pdf","comment":"21 pages, 14 tables, 7 figures, submitted to ICRA 2024"},{"id":"http://arxiv.org/abs/2412.13810v1","updated":"2024-12-18T12:57:56Z","published":"2024-12-18T12:57:56Z","title":"CAD-Assistant: Tool-Augmented VLLMs as Generic CAD Task Solvers?","summary":"  We propose CAD-Assistant, a general-purpose CAD agent for AI-assisted design.\nOur approach is based on a powerful Vision and Large Language Model (VLLM) as a\nplanner and a tool-augmentation paradigm using CAD-specific modules.\nCAD-Assistant addresses multimodal user queries by generating actions that are\niteratively executed on a Python interpreter equipped with the FreeCAD\nsoftware, accessed via its Python API. Our framework is able to assess the\nimpact of generated CAD commands on geometry and adapts subsequent actions\nbased on the evolving state of the CAD design. We consider a wide range of\nCAD-specific tools including Python libraries, modules of the FreeCAD Python\nAPI, helpful routines, rendering functions and other specialized modules. We\nevaluate our method on multiple CAD benchmarks and qualitatively demonstrate\nthe potential of tool-augmented VLLMs as generic CAD task solvers across\ndiverse CAD workflows.\n","authors":["Dimitrios Mallis","Ahmet Serdar Karadeniz","Sebastian Cavada","Danila Rukhovich","Niki Foteinopoulou","Kseniya Cherenkova","Anis Kacem","Djamila Aouada"],"pdf_url":"https://arxiv.org/pdf/2412.13810v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13802v1","updated":"2024-12-18T12:49:57Z","published":"2024-12-18T12:49:57Z","title":"SimADFuzz: Simulation-Feedback Fuzz Testing for Autonomous Driving\n  Systems","summary":"  Autonomous driving systems (ADS) have achieved remarkable progress in recent\nyears. However, ensuring their safety and reliability remains a critical\nchallenge due to the complexity and uncertainty of driving scenarios. In this\npaper, we focus on simulation testing for ADS, where generating diverse and\neffective testing scenarios is a central task. Existing fuzz testing methods\nface limitations, such as overlooking the temporal and spatial dynamics of\nscenarios and failing to leverage simulation feedback (e.g., speed,\nacceleration and heading) to guide scenario selection and mutation. To address\nthese issues, we propose SimADFuzz, a novel framework designed to generate\nhigh-quality scenarios that reveal violations in ADS behavior. Specifically,\nSimADFuzz employs violation prediction models, which evaluate the likelihood of\nADS violations, to optimize scenario selection. Moreover, SimADFuzz proposes\ndistance-guided mutation strategies to enhance interactions among vehicles in\noffspring scenarios, thereby triggering more edge-case behaviors of vehicles.\nComprehensive experiments demonstrate that SimADFuzz outperforms\nstate-of-the-art fuzzers by identifying 32 more unique violations, including 4\nreproducible cases of vehicle-vehicle and vehicle-pedestrian collisions. These\nresults demonstrate SimADFuzz's effectiveness in enhancing the robustness and\nsafety of autonomous driving systems.\n","authors":["Huiwen Yang","Yu Zhou","Taolue Chen"],"pdf_url":"https://arxiv.org/pdf/2412.13802v1.pdf","comment":"27 pages, 13 figures. Under peer review"},{"id":"http://arxiv.org/abs/2409.13964v3","updated":"2024-12-18T12:20:51Z","published":"2024-09-21T00:54:15Z","title":"Adaptive bias for dissensus in nonlinear opinion dynamics with\n  application to evolutionary division of labor games","summary":"  This paper addresses the problem of adaptively controlling the bias parameter\nin nonlinear opinion dynamics (NOD) to allocate agents into groups of arbitrary\nsizes for the purpose of maximizing collective rewards. In previous work, an\nalgorithm based on the coupling of NOD with an multi-objective behavior\noptimization was successfully deployed as part of a multi-robot system in an\nautonomous task allocation field experiment. Motivated by the field results, in\nthis paper we propose and analyze a new task allocation model that synthesizes\nNOD with an evolutionary game framework. We prove sufficient conditions under\nwhich it is possible to control the opinion state in the group to a desired\nallocation of agents between two tasks through an adaptive bias using\ndecentralized feedback. We then verify the theoretical results with a\nsimulation study of a collaborative evolutionary division of labor game.\n","authors":["Tyler M. Paine","Anastasia Bizyaeva","Michael R. Benjamin"],"pdf_url":"https://arxiv.org/pdf/2409.13964v3.pdf","comment":"v1) To appear at the 2024 IEEE Conference on Decision and Control\n  (CDC) in Milan, Italy. 8 Pages, 5 Figures. v2) Fixed typo. v3) Fixed typo and\n  corrected axis on Figure 4b"},{"id":"http://arxiv.org/abs/2412.13774v1","updated":"2024-12-18T12:11:39Z","published":"2024-12-18T12:11:39Z","title":"Designing an LLM-Based Copilot for Manufacturing Equipment Selection","summary":"  Effective decision-making in automation equipment selection is critical for\nreducing ramp-up time and maintaining production quality, especially in the\nface of increasing product variation and market demands. However, limited\nexpertise and resource constraints often result in inefficiencies during the\nramp-up phase when new products are integrated into production lines. Existing\nmethods often lack structured and tailored solutions to support automation\nengineers in reducing ramp-up time, leading to compromises in quality. This\nresearch investigates whether large-language models (LLMs), combined with\nRetrieval-Augmented Generation (RAG), can assist in streamlining equipment\nselection in ramp-up planning. We propose a factual-driven copilot integrating\nLLMs with structured and semi-structured knowledge retrieval for three\ncomponent types (robots, feeders and vision systems), providing a guided and\ntraceable state-machine process for decision-making in automation equipment\nselection. The system was demonstrated to an industrial partner, who tested it\non three internal use-cases. Their feedback affirmed its capability to provide\nlogical and actionable recommendations for automation equipment. More\nspecifically, among 22 equipment prompts analyzed, 19 involved selecting the\ncorrect equipment while considering most requirements, and in 6 cases, all\nrequirements were fully met.\n","authors":["Jonas Werheid","Oleksandr Melnychuk","Hans Zhou","Meike Huber","Christoph Rippe","Dominik Joosten","Zozan Keskin","Max Wittstamm","Sathya Subramani","Benny Drescher","Amon Göppert","Anas Abdelrazeq","Robert H. Schmitt"],"pdf_url":"https://arxiv.org/pdf/2412.13774v1.pdf","comment":"Preprint submitted to Manufacturing Letters (MFGLET)"},{"id":"http://arxiv.org/abs/2412.13752v1","updated":"2024-12-18T11:43:17Z","published":"2024-12-18T11:43:17Z","title":"Immersive Human-in-the-Loop Control: Real-Time 3D Surface Meshing and\n  Physics Simulation","summary":"  This paper introduces the TactiMesh Teleoperator Interface (TTI), a novel\npredictive visual and haptic system designed explicitly for human-in-the-loop\nrobot control using a head-mounted display (HMD). By employing simultaneous\nlocalization and mapping (SLAM)in tandem with a space carving method (CARV),\nTTI creates a real time 3D surface mesh of remote environments from an RGB\ncamera mounted on a Barrett WAM arm. The generated mesh is integrated into a\nphysics simulator, featuring a digital twin of the WAM robot arm to create a\nvirtual environment. In this virtual environment, TTI provides haptic feedback\ndirectly in response to the operator's movements, eliminating the problem with\ndelayed response from the haptic follower robot. Furthermore, texturing the 3D\nmesh with keyframes from SLAM allows the operator to control the viewpoint of\ntheir Head Mounted Display (HMD) independently of the arm-mounted robot camera,\ngiving a better visual immersion and improving manipulation speed.\nIncorporating predictive visual and haptic feedback significantly improves\nteleoperation in applications such as search and rescue, inspection, and remote\nmaintenance.\n","authors":["Sait Akturk","Justin Valentine","Junaid Ahmad","Martin Jagersand"],"pdf_url":"https://arxiv.org/pdf/2412.13752v1.pdf","comment":"IROS 2024"},{"id":"http://arxiv.org/abs/2411.01850v2","updated":"2024-12-18T11:25:55Z","published":"2024-11-04T07:05:02Z","title":"ManiBox: Enhancing Spatial Grasping Generalization via Scalable\n  Simulation Data Generation","summary":"  Learning a precise robotic grasping policy is crucial for embodied agents\noperating in complex real-world manipulation tasks. Despite significant\nadvancements, most models still struggle with accurate spatial positioning of\nobjects to be grasped. We first show that this spatial generalization challenge\nstems primarily from the extensive data requirements for adequate spatial\nunderstanding. However, collecting such data with real robots is prohibitively\nexpensive, and relying on simulation data often leads to visual generalization\ngaps upon deployment. To overcome these challenges, we then focus on\nstate-based policy generalization and present \\textbf{ManiBox}, a novel\nbounding-box-guided manipulation method built on a simulation-based\nteacher-student framework. The teacher policy efficiently generates scalable\nsimulation data using bounding boxes, which are proven to uniquely determine\nthe objects' spatial positions. The student policy then utilizes these\nlow-dimensional spatial states to enable zero-shot transfer to real robots.\nThrough comprehensive evaluations in simulated and real-world environments,\nManiBox demonstrates a marked improvement in spatial grasping generalization\nand adaptability to diverse objects and backgrounds. Further, our empirical\nstudy into scaling laws for policy performance indicates that spatial volume\ngeneralization scales with data volume in a power law. For a certain level of\nspatial volume, the success rate of grasping empirically follows\nMichaelis-Menten kinetics relative to data volume, showing a saturation effect\nas data increases. Our videos and code are available in\nhttps://thkkk.github.io/manibox.\n","authors":["Hengkai Tan","Xuezhou Xu","Chengyang Ying","Xinyi Mao","Songming Liu","Xingxing Zhang","Hang Su","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2411.01850v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13729v1","updated":"2024-12-18T11:08:25Z","published":"2024-12-18T11:08:25Z","title":"THÖR-MAGNI Act: Actions for Human Motion Modeling in Robot-Shared\n  Industrial Spaces","summary":"  Accurate human activity and trajectory prediction are crucial for ensuring\nsafe and reliable human-robot interactions in dynamic environments, such as\nindustrial settings, with mobile robots. Datasets with fine-grained action\nlabels for moving people in industrial environments with mobile robots are\nscarce, as most existing datasets focus on social navigation in public spaces.\nThis paper introduces the TH\\\"OR-MAGNI Act dataset, a substantial extension of\nthe TH\\\"OR-MAGNI dataset, which captures participant movements alongside robots\nin diverse semantic and spatial contexts. TH\\\"OR-MAGNI Act provides 8.3 hours\nof manually labeled participant actions derived from egocentric videos recorded\nvia eye-tracking glasses. These actions, aligned with the provided TH\\\"OR-MAGNI\nmotion cues, follow a long-tailed distribution with diversified acceleration,\nvelocity, and navigation distance profiles. We demonstrate the utility of\nTH\\\"OR-MAGNI Act for two tasks: action-conditioned trajectory prediction and\njoint action and trajectory prediction. We propose two efficient\ntransformer-based models that outperform the baselines to address these tasks.\nThese results underscore the potential of TH\\\"OR-MAGNI Act to develop\npredictive models for enhanced human-robot interaction in complex environments.\n","authors":["Tiago Rodrigues de Almeida","Tim Schreiter","Andrey Rudenko","Luigi Palmieiri","Johannes A. Stork","Achim J. Lilienthal"],"pdf_url":"https://arxiv.org/pdf/2412.13729v1.pdf","comment":"This paper has been accepted to the the 20th edition of the IEEE/ACM\n  International Conference on Human-Robot Interaction (HRI'25), which will be\n  held in Melbourne, Australia on March 4-6, 2025. Code:\n  https://github.com/tmralmeida/thor-magni-actions"},{"id":"http://arxiv.org/abs/2412.13726v1","updated":"2024-12-18T11:05:56Z","published":"2024-12-18T11:05:56Z","title":"Unified Understanding of Environment, Task, and Human for Human-Robot\n  Interaction in Real-World Environments","summary":"  To facilitate human--robot interaction (HRI) tasks in real-world scenarios,\nservice robots must adapt to dynamic environments and understand the required\ntasks while effectively communicating with humans. To accomplish HRI in\npractice, we propose a novel indoor dynamic map, task understanding system, and\nresponse generation system. The indoor dynamic map optimizes robot behavior by\nmanaging an occupancy grid map and dynamic information, such as furniture and\nhumans, in separate layers. The task understanding system targets tasks that\nrequire multiple actions, such as serving ordered items. Task representations\nthat predefine the flow of necessary actions are applied to achieve highly\naccurate understanding. The response generation system is executed in parallel\nwith task understanding to facilitate smooth HRI by informing humans of the\nsubsequent actions of the robot. In this study, we focused on waiter duties in\na restaurant setting as a representative application of HRI in a dynamic\nenvironment. We developed an HRI system that could perform tasks such as\nserving food and cleaning up while communicating with customers. In experiments\nconducted in a simulated restaurant environment, the proposed HRI system\nsuccessfully communicated with customers and served ordered food with 90\\%\naccuracy. In a questionnaire administered after the experiment, the HRI system\nof the robot received 4.2 points out of 5. These outcomes indicated the\neffectiveness of the proposed method and HRI system in executing waiter tasks\nin real-world environments.\n","authors":["Yuga Yano","Akinobu Mizutani","Yukiya Fukuda","Daiju Kanaoka","Tomohiro Ono","Hakaru Tamukoh"],"pdf_url":"https://arxiv.org/pdf/2412.13726v1.pdf","comment":"2024 33rd IEEE International Conference on Robot and Human\n  Interactive Communication (RO-MAN)"},{"id":"http://arxiv.org/abs/2412.13719v1","updated":"2024-12-18T11:00:46Z","published":"2024-12-18T11:00:46Z","title":"Heuristic Planner for Communication-Constrained Multi-Agent Multi-Goal\n  Path Planning","summary":"  In robotics, coordinating a group of robots is an essential task. This work\npresents the communication-constrained multi-agent multi-goal path planning\nproblem and proposes a graph-search based algorithm to address this task. Given\na fleet of robots, an environment represented by a weighted graph, and a\nsequence of goals, the aim is to visit all the goals without breaking the\ncommunication constraints between the agents, minimizing the completion time.\nThe resulting paths produced by our approach show how the agents can coordinate\ntheir individual paths, not only with respect to the next goal but also with\nrespect to all future goals, all the time keeping the communication within the\nfleet intact.\n","authors":["Jáchym Herynek","Stefan Edelkamp"],"pdf_url":"https://arxiv.org/pdf/2412.13719v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.13681v1","updated":"2024-12-18T10:08:40Z","published":"2024-12-18T10:08:40Z","title":"Dynamics of Parallel Manipulators with Hybrid Complex Limbs -- Modular\n  Modeling and Parallel Computing","summary":"  Parallel manipulators, also called parallel kinematics machines (PKM), enable\nrobotic solutions for highly dynamic handling and machining applications. The\nsafe and accurate design and control necessitates high-fidelity dynamics\nmodels. Such modeling approaches have already been presented for PKM with\nsimple limbs (i.e. each limb is a serial kinematic chain). A systematic\nmodeling approach for PKM with complex limbs (i.e. limbs that possess kinematic\nloops) was not yet proposed despite the fact that many successful PKM comprise\ncomplex limbs. This paper presents a systematic modular approach to the\nkinematics and dynamics modeling of PKM with complex limbs that are built as\nserial arrangement of closed loops. The latter are referred to as hybrid limbs,\nand can be found in almost all PKM with complex limbs, such as the Delta robot.\nThe proposed method generalizes the formulation for PKM with simple limbs by\nmeans of local resolution of loop constraints, which is known as constraint\nembedding in multibody dynamics. The constituent elements of the method are the\nkinematic and dynamic equations of motions (EOM), and the inverse kinematics\nsolution of the limbs, i.e. the relation of platform motion and the motion of\nthe limbs. While the approach is conceptually independent of the used\nkinematics and dynamics formulation, a Lie group formulation is employed for\nderiving the EOM. The frame invariance of the Lie group formulation is used for\ndevising a modular modeling method where the EOM of a representative limb are\nused to derived the EOM of the limbs of a particular PKM. The PKM topology is\nexploited in a parallel computation scheme that shall allow for computationally\nefficient distributed evaluation of the overall EOM of the PKM. Finally, the\nmethod is applied to the IRSBot-2 and a 3\\underline{R}R[2RR]R Delta robot,\nwhich is presented in detail.\n","authors":["Andreas Mueller"],"pdf_url":"https://arxiv.org/pdf/2412.13681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15919v2","updated":"2024-12-18T10:05:46Z","published":"2024-08-28T16:33:21Z","title":"DeMoBot: Deformable Mobile Manipulation with Vision-based Sub-goal\n  Retrieval","summary":"  Imitation learning (IL) algorithms typically distil experience into\nparametric behavior policies to mimic expert demonstrations. With limited\nexperience previous methods often struggle and cannot accurately align the\ncurrent state with expert demonstrations, particularly in tasks that are\ncharacterised by partial observations or dynamic object deformations. We\nconsider imitation learning in deformable mobile manipulation with an\nego-centric limited field of view and introduce a novel IL approach called\nDeMoBot that directly retrieves observations from demonstrations. DeMoBot\nutilizes vision foundation models to identify relevant expert data based on\nvisual similarity and matches the current trajectory with demonstrated\ntrajectories using trajectory similarity and forward reachability constraints\nto select suitable sub-goals. A goal-conditioned motion generation policy shall\nguide the robot to the sub-goal until the task is completed. We evaluate\nDeMoBot using a Spot robot in several simulated and real-world settings,\ndemonstrating its effectiveness and generalizability. DeMoBot outperforms\nbaselines with only 20 demonstrations, attaining high success rates in gap\ncovering (85% simulation, 80% real-world) and table uncovering (87.5%\nsimulation, 70% real-world), while showing promise in complex tasks like\ncurtain opening (47.5% simulation, 35% real-world). Additional details are\navailable at: https://sites.google.com/view/demobot-fewshot/home\n","authors":["Yuying Zhang","Wenyan Yang","Guhan Sivasubramanian","Joni Pajarinen"],"pdf_url":"https://arxiv.org/pdf/2408.15919v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.15674v3","updated":"2024-12-18T10:00:27Z","published":"2023-11-27T10:03:01Z","title":"MOT-DETR: 3D Single Shot Detection and Tracking with Transformers to\n  build 3D representations for Agro-Food Robots","summary":"  In the current demand for automation in the agro-food industry, accurately\ndetecting and localizing relevant objects in 3D is essential for successful\nrobotic operations. However, this is a challenge due the presence of\nocclusions. Multi-view perception approaches allow robots to overcome\nocclusions, but a tracking component is needed to associate the objects\ndetected by the robot over multiple viewpoints. Most multi-object tracking\n(MOT) algorithms are designed for high frame rate sequences and struggle with\nthe occlusions generated by robots' motions and 3D environments. In this paper,\nwe introduce MOT-DETR, a novel approach to detect and track objects in 3D over\ntime using a combination of convolutional networks and transformers. Our method\nprocesses 2D and 3D data, and employs a transformer architecture to perform\ndata fusion. We show that MOT-DETR outperforms state-of-the-art multi-object\ntracking methods. Furthermore, we prove that MOT-DETR can leverage 3D data to\ndeal with long-term occlusions and large frame-to-frame distances better than\nstate-of-the-art methods. Finally, we show how our method is resilient to\ncamera pose noise that can affect the accuracy of point clouds. The\nimplementation of MOT-DETR can be found here:\nhttps://github.com/drapado/mot-detr\n","authors":["David Rapado-Rincon","Henk Nap","Katarina Smolenova","Eldert J. van Henten","Gert Kootstra"],"pdf_url":"https://arxiv.org/pdf/2311.15674v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13664v1","updated":"2024-12-18T09:45:46Z","published":"2024-12-18T09:45:46Z","title":"A Skeleton-Based Topological Planner for Exploration in Complex Unknown\n  Environments","summary":"  The capability of autonomous exploration in complex, unknown environments is\nimportant in many robotic applications. While recent research on autonomous\nexploration have achieved much progress, there are still limitations, e.g.,\nexisting methods relying on greedy heuristics or optimal path planning are\noften hindered by repetitive paths and high computational demands. To address\nsuch limitations, we propose a novel exploration framework that utilizes the\nglobal topology information of observed environment to improve exploration\nefficiency while reducing computational overhead. Specifically, global\ninformation is utilized based on a skeletal topological graph representation of\nthe environment geometry. We first propose an incremental skeleton extraction\nmethod based on wavefront propagation, based on which we then design an\napproach to generate a lightweight topological graph that can effectively\ncapture the environment's structural characteristics. Building upon this, we\nintroduce a finite state machine that leverages the topological structure to\nefficiently plan coverage paths, which can substantially mitigate the\nback-and-forth maneuvers (BFMs) problem. Experimental results demonstrate the\nsuperiority of our method in comparison with state-of-the-art methods. The\nsource code will be made publicly available at:\n\\url{https://github.com/Haochen-Niu/STGPlanner}.\n","authors":["Haochen Niu","Xingwu Ji","Lantao Zhang","Fei Wen","Rendong Ying","Peilin Liu"],"pdf_url":"https://arxiv.org/pdf/2412.13664v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13662v1","updated":"2024-12-18T09:39:12Z","published":"2024-12-18T09:39:12Z","title":"When Should We Prefer State-to-Visual DAgger Over Visual Reinforcement\n  Learning?","summary":"  Learning policies from high-dimensional visual inputs, such as pixels and\npoint clouds, is crucial in various applications. Visual reinforcement learning\nis a promising approach that directly trains policies from visual observations,\nalthough it faces challenges in sample efficiency and computational costs. This\nstudy conducts an empirical comparison of State-to-Visual DAgger, a two-stage\nframework that initially trains a state policy before adopting online imitation\nto learn a visual policy, and Visual RL across a diverse set of tasks. We\nevaluate both methods across 16 tasks from three benchmarks, focusing on their\nasymptotic performance, sample efficiency, and computational costs.\nSurprisingly, our findings reveal that State-to-Visual DAgger does not\nuniversally outperform Visual RL but shows significant advantages in\nchallenging tasks, offering more consistent performance. In contrast, its\nbenefits in sample efficiency are less pronounced, although it often reduces\nthe overall wall-clock time required for training. Based on our findings, we\nprovide recommendations for practitioners and hope that our results contribute\nvaluable perspectives for future research in visual policy learning.\n","authors":["Tongzhou Mu","Zhaoyang Li","Stanisław Wiktor Strzelecki","Xiu Yuan","Yunchao Yao","Litian Liang","Hao Su"],"pdf_url":"https://arxiv.org/pdf/2412.13662v1.pdf","comment":"Accepted by The 39th Annual AAAI Conference on Artificial\n  Intelligence (AAAI 2025)"},{"id":"http://arxiv.org/abs/2306.09801v3","updated":"2024-12-18T09:34:18Z","published":"2023-06-16T12:22:19Z","title":"Semantics-Aware Next-best-view Planning for Efficient Search and\n  Detection of Task-relevant Plant Parts","summary":"  Searching and detecting the task-relevant parts of plants is important to\nautomate harvesting and de-leafing of tomato plants using robots. This is\nchallenging due to high levels of occlusion in tomato plants. Active vision is\na promising approach in which the robot strategically plans its camera\nviewpoints to overcome occlusion and improve perception accuracy. However,\ncurrent active-vision algorithms cannot differentiate between relevant and\nirrelevant plant parts and spend time on perceiving irrelevant plant parts.\nThis work proposed a semantics-aware active-vision strategy that uses semantic\ninformation to identify the relevant plant parts and prioritise them during\nview planning. The proposed strategy was evaluated on the task of searching and\ndetecting the relevant plant parts using simulation and real-world experiments.\nIn simulation experiments, the semantics-aware strategy proposed could search\nand detect 81.8% of the relevant plant parts using nine viewpoints. It was\nsignificantly faster and detected more plant parts than predefined, random, and\nvolumetric active-vision strategies that do not use semantic information. The\nstrategy proposed was also robust to uncertainty in plant and plant-part\npositions, plant complexity, and different viewpoint-sampling strategies. In\nreal-world experiments, the semantics-aware strategy could search and detect\n82.7% of the relevant plant parts using seven viewpoints, under complex\ngreenhouse conditions with natural variation and occlusion, natural\nillumination, sensor noise, and uncertainty in camera poses. The results of\nthis work clearly indicate the advantage of using semantics-aware active vision\nfor targeted perception of plant parts and its applicability in the real world.\nIt can significantly improve the efficiency of automated harvesting and\nde-leafing in tomato crop production.\n","authors":["Akshay K. Burusa","Joost Scholten","David Rapado Rincon","Xin Wang","Eldert J. van Henten","Gert Kootstra"],"pdf_url":"https://arxiv.org/pdf/2306.09801v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13641v1","updated":"2024-12-18T09:20:20Z","published":"2024-12-18T09:20:20Z","title":"Learning to Control an Android Robot Head for Facial Animation","summary":"  The ability to display rich facial expressions is crucial for human-like\nrobotic heads. While manually defining such expressions is intricate, there\nalready exist approaches to automatically learn them. In this work one such\napproach is applied to evaluate and control a robot head different from the one\nin the original study. To improve the mapping of facial expressions from human\nactors onto a robot head, it is proposed to use 3D landmarks and their pairwise\ndistances as input to the learning algorithm instead of the previously used\nfacial action units. Participants of an online survey preferred mappings from\nour proposed approach in most cases, though there are still further\nimprovements required.\n","authors":["Marcel Heisler","Christian Becker-Asano"],"pdf_url":"https://arxiv.org/pdf/2412.13641v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13639v1","updated":"2024-12-18T09:11:24Z","published":"2024-12-18T09:11:24Z","title":"4D Radar-Inertial Odometry based on Gaussian Modeling and\n  Multi-Hypothesis Scan Matching","summary":"  4D millimeter-wave (mmWave) radars are sensors that provide robustness\nagainst adverse weather conditions (rain, snow, fog, etc.), and as such they\nare increasingly being used for odometry and SLAM applications. However, the\nnoisy and sparse nature of the returned scan data proves to be a challenging\nobstacle for existing point cloud matching based solutions, especially those\noriginally intended for more accurate sensors such as LiDAR. Inspired by visual\nodometry research around 3D Gaussian Splatting, in this paper we propose using\nfreely positioned 3D Gaussians to create a summarized representation of a radar\npoint cloud tolerant to sensor noise, and subsequently leverage its inherent\nprobability distribution function for registration (similar to NDT). Moreover,\nwe propose simultaneously optimizing multiple scan matching hypotheses in order\nto further increase the robustness of the system against local optima of the\nfunction. Finally, we fuse our Gaussian modeling and scan matching algorithms\ninto an EKF radar-inertial odometry system designed after current best\npractices. Experiments show that our Gaussian-based odometry is able to\noutperform current baselines on a well-known 4D radar dataset used for\nevaluation.\n","authors":["Fernando Amodeo","Luis Merino","Fernando Caballero"],"pdf_url":"https://arxiv.org/pdf/2412.13639v1.pdf","comment":"Our code and results can be publicly accessed at:\n  https://github.com/robotics-upo/gaussian-rio"},{"id":"http://arxiv.org/abs/2412.13638v1","updated":"2024-12-18T09:09:53Z","published":"2024-12-18T09:09:53Z","title":"A Constraint Embedding Approach for Dynamics Modeling of Parallel\n  Kinematic Manipulators with Hybrid Limbs","summary":"  Parallel kinematic manipulators (PKM) are characterized by closed kinematic\nloops, due to the parallel arrangement of limbs but also due to the existence\nof kinematic loops within the limbs. Moreover, many PKM are built with limbs\nconstructed by serially combining kinematic loops. Such limbs are called\nhybrid, which form a particular class of complex limbs. Design and model-based\ncontrol requires accurate dynamic PKM models desirably without model\nsimplifications. Dynamics modeling then necessitates kinematic relations of all\nmembers of the PKM, in contrast to the standard kinematics modeling of PKM,\nwhere only the forward and inverse kinematics solution for the manipulator\n(relating input and output motions) are computed. This becomes more involved\nfor PKM with hybrid limbs. In this paper a modular modeling approach is\nemployed, where limbs are treated separately, and the individual dynamic\nequations of motions (EOM) are subsequently assembled to the overall model. Key\nto the kinematic modeling is the constraint resolution for the individual loops\nwithin the limbs. This local constraint resolution is a special case of the\ngeneral \\emph{constraint embedding} technique. The proposed method finally\nallows for a systematic modeling of general PKM. The method is demonstrated for\nthe IRSBot-2, where each limb comprises two independent loops.\n","authors":["Andreas Mueller"],"pdf_url":"https://arxiv.org/pdf/2412.13638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15161v2","updated":"2024-12-18T09:07:47Z","published":"2024-07-21T13:33:08Z","title":"FFHFlow: A Flow-based Variational Approach for Learning Diverse\n  Dexterous Grasps with Shape-Aware Introspection","summary":"  Synthesizing diverse dexterous grasps from uncertain partial observation is\nan important yet challenging task for physically intelligent embodiments.\nPrevious works on generative grasp synthesis fell short of precisely capturing\nthe complex grasp distribution and reasoning about shape uncertainty in the\nunstructured and often partially perceived reality. In this work, we introduce\na novel model that can generate diverse grasps for a multi-fingered hand while\nintrospectively handling perceptual uncertainty and recognizing unknown object\ngeometry to avoid performance degradation. Specifically, we devise a Deep\nLatent Variable Model (DLVM) based on Normalizing Flows (NFs), facilitating\nhierarchical and expressive latent representation for modeling versatile\ngrasps. Our model design counteracts typical pitfalls of its popular\nalternative in generative grasping, i.e., conditional Variational Autoencoders\n(cVAEs) whose performance is limited by mode collapse and miss-specified prior\nissues. Moreover, the resultant feature hierarchy and the exact flow likelihood\ncomputation endow our model with shape-aware introspective capabilities,\nenabling it to quantify the shape uncertainty of partial point clouds and\ndetect objects of novel geometry. We further achieve performance gain by fusing\nthis information with a discriminative grasp evaluator, facilitating a novel\nhybrid way for grasp evaluation. Comprehensive simulated and real-world\nexperiments show that the proposed idea gains superior performance and higher\nrun-time efficiency against strong baselines, including diffusion models. We\nalso demonstrate substantial benefits of greater diversity for grasping objects\nin clutter and a confined workspace in the real world.\n","authors":["Qian Feng","Jianxiang Feng","Zhaopeng Chen","Rudolph Triebel","Alois Knoll"],"pdf_url":"https://arxiv.org/pdf/2407.15161v2.pdf","comment":"First two authors contributed equally, whose ordering decided via\n  coin-tossing. Under Reivew"},{"id":"http://arxiv.org/abs/2412.13630v1","updated":"2024-12-18T09:06:16Z","published":"2024-12-18T09:06:16Z","title":"Policy Decorator: Model-Agnostic Online Refinement for Large Policy\n  Model","summary":"  Recent advancements in robot learning have used imitation learning with large\nmodels and extensive demonstrations to develop effective policies. However,\nthese models are often limited by the quantity, quality, and diversity of\ndemonstrations. This paper explores improving offline-trained imitation\nlearning models through online interactions with the environment. We introduce\nPolicy Decorator, which uses a model-agnostic residual policy to refine large\nimitation learning models during online interactions. By implementing\ncontrolled exploration strategies, Policy Decorator enables stable,\nsample-efficient online learning. Our evaluation spans eight tasks across two\nbenchmarks-ManiSkill and Adroit-and involves two state-of-the-art imitation\nlearning models (Behavior Transformer and Diffusion Policy). The results show\nPolicy Decorator effectively improves the offline-trained policies and\npreserves the smooth motion of imitation learning models, avoiding the erratic\nbehaviors of pure RL policies. See our project page\n(https://policydecorator.github.io) for videos.\n","authors":["Xiu Yuan","Tongzhou Mu","Stone Tao","Yunhao Fang","Mengke Zhang","Hao Su"],"pdf_url":"https://arxiv.org/pdf/2412.13630v1.pdf","comment":"Explore videos, data, code, and more at\n  https://policydecorator.github.io"},{"id":"http://arxiv.org/abs/2412.13621v1","updated":"2024-12-18T08:58:06Z","published":"2024-12-18T08:58:06Z","title":"Learning Quadrupedal Robot Locomotion for Narrow Pipe Inspection","summary":"  Various pipes are extensively used in both industrial settings and daily\nlife, but the pipe inspection especially those with narrow sizes are still very\nchallenging with tremendous time and manufacturing consumed. Quadrupedal\nrobots, inspired from patrol dogs, can be a substitution of traditional\nsolutions but always suffer from navigation and locomotion difficulties. In\nthis paper, we introduce a Reinforcement Learning (RL) based method to train a\npolicy enabling the quadrupedal robots to cross narrow pipes adaptively. A new\nprivileged visual information and a new reward function are defined to tackle\nthe problems. Experiments on both simulation and real world scenarios were\ncompleted, demonstrated that the proposed method can achieve the pipe-crossing\ntask even with unexpected obstacles inside.\n","authors":["Jing Guo","Ziwei Wang","Weibang Bai"],"pdf_url":"https://arxiv.org/pdf/2412.13621v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13618v1","updated":"2024-12-18T08:57:05Z","published":"2024-12-18T08:57:05Z","title":"NPC: Neural Predictive Control for Fuel-Efficient Autonomous Trucks","summary":"  Fuel efficiency is a crucial aspect of long-distance cargo transportation by\noil-powered trucks that economize on costs and decrease carbon emissions.\nCurrent predictive control methods depend on an accurate model of vehicle\ndynamics and engine, including weight, drag coefficient, and the Brake-specific\nFuel Consumption (BSFC) map of the engine. We propose a pure data-driven\nmethod, Neural Predictive Control (NPC), which does not use any physical model\nfor the vehicle. After training with over 20,000 km of historical data, the\nnovel proposed NVFormer implicitly models the relationship between vehicle\ndynamics, road slope, fuel consumption, and control commands using the\nattention mechanism. Based on the online sampled primitives from the past of\nthe current freight trip and anchor-based future data synthesis, the NVFormer\ncan infer optimal control command for reasonable fuel consumption. The physical\nmodel-free NPC outperforms the base PCC method with 2.41% and 3.45% more\nsignificant fuel saving in simulation and open-road highway testing,\nrespectively.\n","authors":["Jiaping Ren","Jiahao Xiang","Hongfei Gao","Jinchuan Zhang","Yiming Ren","Yuexin Ma","Yi Wu","Ruigang Yang","Wei Li"],"pdf_url":"https://arxiv.org/pdf/2412.13618v1.pdf","comment":"7 pages, 6 figures, for associated mpeg file, see\n  https://www.youtube.com/watch?v=hqgpj7LhiL4"},{"id":"http://arxiv.org/abs/2412.10050v2","updated":"2024-12-18T07:08:26Z","published":"2024-12-13T11:22:01Z","title":"ManipGPT: Is Affordance Segmentation by Large Vision Models Enough for\n  Articulated Object Manipulation?","summary":"  Visual actionable affordance has emerged as a transformative approach in\nrobotics, focusing on perceiving interaction areas prior to manipulation.\nTraditional methods rely on pixel sampling to identify successful interaction\nsamples or processing pointclouds for affordance mapping. However, these\napproaches are computationally intensive and struggle to adapt to diverse and\ndynamic environments. This paper introduces ManipGPT, a framework designed to\npredict optimal interaction areas for articulated objects using a large\npre-trained vision transformer (ViT). We created a dataset of 9.9k simulated\nand real images to bridge the sim-to-real gap and enhance real-world\napplicability. By fine-tuning the vision transformer on this small dataset, we\nsignificantly improved part-level affordance segmentation, adapting the model's\nin-context segmentation capabilities to robot manipulation scenarios. This\nenables effective manipulation across simulated and real-world environments by\ngenerating part-level affordance masks, paired with an impedance adaptation\npolicy, sufficiently eliminating the need for complex datasets or perception\nsystems.\n","authors":["Taewhan Kim","Hojin Bae","Zeming Li","Xiaoqi Li","Iaroslav Ponomarenko","Ruihai Wu","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2412.10050v2.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.13548v1","updated":"2024-12-18T06:49:46Z","published":"2024-12-18T06:49:46Z","title":"TelePhantom: A User-Friendly Teleoperation System with Virtual\n  Assistance for Enhanced Effectiveness","summary":"  Dexterous manipulation is a critical area of robotics. In this field,\nteleoperation faces three key challenges: user-friendliness for novices, safety\nassurance, and transferability across different platforms. While collecting\nreal robot dexterous manipulation data by teleoperation to train robots has\nshown impressive results on diverse tasks, due to the morphological differences\nbetween human and robot hands, it is not only hard for new users to understand\nthe action mapping but also raises potential safety concerns during operation.\nTo address these limitations, we introduce TelePhantom. This teleoperation\nsystem offers real-time visual feedback on robot actions based on human user\ninputs, with a total hardware cost of less than $1,000. TelePhantom allows the\nuser to see a virtual robot that represents the outcome of the user's next\nmovement. By enabling flexible switching between command visualization and\nactual execution, this system helps new users learn how to demonstrate quickly\nand safely. We demonstrate its superiority over other teleoperation systems\nacross five tasks, emphasize its ease of use, and highlight its ease of\ndeployment across diverse input sensors and robotic platforms. We will release\nour code and a deployment document on our website:\nhttps://telephantom.github.io/.\n","authors":["Jingxiang Guo","Jiayu Luo","Zhenyu Wei","Yiwen Hou","Zhixuan Xu","Xiaoyi Lin","Chongkai Gao","Lin Shao"],"pdf_url":"https://arxiv.org/pdf/2412.13548v1.pdf","comment":"Submitted to RA-L"},{"id":"http://arxiv.org/abs/2412.12716v2","updated":"2024-12-18T04:42:07Z","published":"2024-12-17T09:30:31Z","title":"Unsupervised UAV 3D Trajectories Estimation with Sparse Point Clouds","summary":"  Compact UAV systems, while advancing delivery and surveillance, pose\nsignificant security challenges due to their small size, which hinders\ndetection by traditional methods. This paper presents a cost-effective,\nunsupervised UAV detection method using spatial-temporal sequence processing to\nfuse multiple LiDAR scans for accurate UAV tracking in real-world scenarios.\nOur approach segments point clouds into foreground and background, analyzes\nspatial-temporal data, and employs a scoring mechanism to enhance detection\naccuracy. Tested on a public dataset, our solution placed 4th in the CVPR 2024\nUG2+ Challenge, demonstrating its practical effectiveness. We plan to\nopen-source all designs, code, and sample data for the research community\ngithub.com/lianghanfang/UnLiDAR-UAV-Est.\n","authors":["Hanfang Liang","Yizhuo Yang","Jinming Hu","Jianfei Yang","Fen Liu","Shenghai Yuan"],"pdf_url":"https://arxiv.org/pdf/2412.12716v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12698v2","updated":"2024-12-18T04:41:24Z","published":"2024-12-17T09:16:28Z","title":"Audio Array-Based 3D UAV Trajectory Estimation with LiDAR\n  Pseudo-Labeling","summary":"  As small unmanned aerial vehicles (UAVs) become increasingly prevalent, there\nis growing concern regarding their impact on public safety and privacy,\nhighlighting the need for advanced tracking and trajectory estimation\nsolutions. In response, this paper introduces a novel framework that utilizes\naudio array for 3D UAV trajectory estimation. Our approach incorporates a\nself-supervised learning model, starting with the conversion of audio data into\nmel-spectrograms, which are analyzed through an encoder to extract crucial\ntemporal and spectral information. Simultaneously, UAV trajectories are\nestimated using LiDAR point clouds via unsupervised methods. These LiDAR-based\nestimations act as pseudo labels, enabling the training of an Audio Perception\nNetwork without requiring labeled data. In this architecture, the LiDAR-based\nsystem operates as the Teacher Network, guiding the Audio Perception Network,\nwhich serves as the Student Network. Once trained, the model can independently\npredict 3D trajectories using only audio signals, with no need for LiDAR data\nor external ground truth during deployment. To further enhance precision, we\napply Gaussian Process modeling for improved spatiotemporal tracking. Our\nmethod delivers top-tier performance on the MMAUD dataset, establishing a new\nbenchmark in trajectory estimation using self-supervised learning techniques\nwithout reliance on ground truth annotations.\n","authors":["Allen Lei","Tianchen Deng","Han Wang","Jianfei Yang","Shenghai Yuan"],"pdf_url":"https://arxiv.org/pdf/2412.12698v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13492v1","updated":"2024-12-18T04:20:33Z","published":"2024-12-18T04:20:33Z","title":"Efficient Language-instructed Skill Acquisition via Reward-Policy\n  Co-Evolution","summary":"  The ability to autonomously explore and resolve tasks with minimal human\nguidance is crucial for the self-development of embodied intelligence. Although\nreinforcement learning methods can largely ease human effort, it's challenging\nto design reward functions for real-world tasks, especially for\nhigh-dimensional robotic control, due to complex relationships among joints and\ntasks. Recent advancements large language models (LLMs) enable automatic reward\nfunction design. However, approaches evaluate reward functions by re-training\npolicies from scratch placing an undue burden on the reward function, expecting\nit to be effective throughout the whole policy improvement process. We argue\nfor a more practical strategy in robotic autonomy, focusing on refining\nexisting policies with policy-dependent reward functions rather than a\nuniversal one. To this end, we propose a novel reward-policy co-evolution\nframework where the reward function and the learned policy benefit from each\nother's progressive on-the-fly improvements, resulting in more efficient and\nhigher-performing skill acquisition. Specifically, the reward evolution process\ntranslates the robot's previous best reward function, descriptions of tasks and\nenvironment into text inputs. These inputs are used to query LLMs to generate a\ndynamic amount of reward function candidates, ensuring continuous improvement\nat each round of evolution. For policy evolution, our method generates new\npolicy populations by hybridizing historically optimal and random policies.\nThrough an improved Bayesian optimization, our approach efficiently and\nrobustly identifies the most capable and plastic reward-policy combination,\nwhich then proceeds to the next round of co-evolution. Despite using less data,\nour approach demonstrates an average normalized improvement of 95.3% across\nvarious high-dimensional robotic skill learning tasks.\n","authors":["Changxin Huang","Yanbin Chang","Junfan Lin","Junyang Liang","Runhao Zeng","Jianqiang Li"],"pdf_url":"https://arxiv.org/pdf/2412.13492v1.pdf","comment":"14 pages, 5 figures, published to AAAI2025"},{"id":"http://arxiv.org/abs/2303.00146v4","updated":"2024-12-18T03:49:06Z","published":"2023-03-01T00:36:27Z","title":"I Know Your Feelings Before You Do: Predicting Future Affective\n  Reactions in Human-Computer Dialogue","summary":"  Current Spoken Dialogue Systems (SDSs) often serve as passive listeners that\nrespond only after receiving user speech. To achieve human-like dialogue, we\npropose a novel future prediction architecture that allows an SDS to anticipate\nfuture affective reactions based on its current behaviors before the user\nspeaks. In this work, we investigate two scenarios: speech and laughter. In\nspeech, we propose to predict the user's future emotion based on its temporal\nrelationship with the system's current emotion and its causal relationship with\nthe system's current Dialogue Act (DA). In laughter, we propose to predict the\noccurrence and type of the user's laughter using the system's laughter\nbehaviors in the current turn. Preliminary analysis of human-robot dialogue\ndemonstrated synchronicity in the emotions and laughter displayed by the human\nand robot, as well as DA-emotion causality in their dialogue. This verifies\nthat our architecture can contribute to the development of an anticipatory SDS.\n","authors":["Yuanchao Li","Koji Inoue","Leimin Tian","Changzeng Fu","Carlos Ishi","Hiroshi Ishiguro","Tatsuya Kawahara","Catherine Lai"],"pdf_url":"https://arxiv.org/pdf/2303.00146v4.pdf","comment":"Accepted to CHI2023 Late-Breaking Work"},{"id":"http://arxiv.org/abs/2412.13474v1","updated":"2024-12-18T03:39:20Z","published":"2024-12-18T03:39:20Z","title":"Planning Human-Robot Co-manipulation with Human Motor Control Objectives\n  and Multi-component Reaching Strategies","summary":"  For successful goal-directed human-robot interaction, the robot should adapt\nto the intentions and actions of the collaborating human. This can be supported\nby musculoskeletal or data-driven human models, where the former are limited to\nlower-level functioning such as ergonomics, and the latter have limited\ngeneralizability or data efficiency. What is missing, is the inclusion of human\nmotor control models that can provide generalizable human behavior estimates\nand integrate into robot planning methods. We use well-studied models from\nhuman motor control based on the speed-accuracy and cost-benefit trade-offs to\nplan collaborative robot motions. In these models, the human trajectory\nminimizes an objective function, a formulation we adapt to numerical trajectory\noptimization. This can then be extended with constraints and new variables to\nrealize collaborative motion planning and goal estimation. We deploy this\nmodel, as well as a multi-component movement strategy, in physical\ncollaboration with uncertain goal-reaching and synchronized motion tasks,\nshowing the ability of the approach to produce human-like trajectories over a\nrange of conditions.\n","authors":["Kevin Haninger","Luka Peternel"],"pdf_url":"https://arxiv.org/pdf/2412.13474v1.pdf","comment":"10 Pages"},{"id":"http://arxiv.org/abs/2412.13419v1","updated":"2024-12-18T01:31:08Z","published":"2024-12-18T01:31:08Z","title":"Exploring Transformer-Augmented LSTM for Temporal and Spatial Feature\n  Learning in Trajectory Prediction","summary":"  Accurate vehicle trajectory prediction is crucial for ensuring safe and\nefficient autonomous driving. This work explores the integration of Transformer\nbased model with Long Short-Term Memory (LSTM) based technique to enhance\nspatial and temporal feature learning in vehicle trajectory prediction. Here, a\nhybrid model that combines LSTMs for temporal encoding with a Transformer\nencoder for capturing complex interactions between vehicles is proposed.\nSpatial trajectory features of the neighboring vehicles are processed and goes\nthrough a masked scatter mechanism in a grid based environment, which is then\ncombined with temporal trajectory of the vehicles. This combined trajectory\ndata are learned by sequential LSTM encoding and Transformer based attention\nlayers. The proposed model is benchmarked against predecessor LSTM based\nmethods, including STA-LSTM, SA-LSTM, CS-LSTM, and NaiveLSTM. Our results,\nwhile not outperforming it's predecessor, demonstrate the potential of\nintegrating Transformers with LSTM based technique to build interpretable\ntrajectory prediction model. Future work will explore alternative architectures\nusing Transformer applications to further enhance performance. This study\nprovides a promising direction for improving trajectory prediction models by\nleveraging transformer based architectures, paving the way for more robust and\ninterpretable vehicle trajectory prediction system.\n","authors":["Chandra Raskoti","Weizi Li"],"pdf_url":"https://arxiv.org/pdf/2412.13419v1.pdf","comment":null}]}}